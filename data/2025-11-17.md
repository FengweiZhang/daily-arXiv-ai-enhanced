<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 7]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [ResBench: A Comprehensive Framework for Evaluating Database Resilience](https://arxiv.org/abs/2511.11088)
*Puyun Hu,Wei Pan,Xun Jian,Zeqi Ma,Tianjie Li,Yang Shen,Chengzhi Han,Yudong Zhao,Zhanhuai Li*

Main category: cs.DB

TL;DR: 该论文提出了ResBench，一个用于评估数据库在面对不利事件时韧性的新基准框架，通过自动化、标准化和可视化手段，在八个维度上全面衡量数据库韧性。


<details>
  <summary>Details</summary>
Motivation: 现有数据库基准主要关注理想环境下的性能，缺乏对真实世界中数据库应对各种不利事件能力的综合评估方法。

Method: 提出数据库韧性的定义，并构建ResBench基准框架，通过分层解耦实现测试过程的自动化与标准化；在正常事务处理过程中注入模拟的不利事件，并收集多维指标进行评估。

Result: ResBench可在吞吐量、延迟、稳定性、抗扰性、恢复能力、干扰持续时间、适应能力和指标偏差等八个维度评估数据库韧性，并通过图形界面直观展示结果；作者使用两类不利事件数据集展示了该框架的执行流程与结果解读。

Conclusion: ResBench为数据库韧性提供了一种系统化、可量化的评估方法，填补了现有基准在非理想运行环境评估方面的空白。

Abstract: Existing database benchmarks primarily focus on performance under ideal running environments. However, in real-world scenarios, databases probably face numerous adverse events. Quantifying the ability to cope with these events from a comprehensive perspective remains an open problem. We provide the definition of database resilience to describe its performance when facing adversity and propose ResBench, a benchmark for evaluating database resilience. This framework achieves automation, standardization, and visualization of the testing process through clear hierarchical decoupling. ResBench simulates adverse events and injects them during normal transaction processing, utilizing a module to gather multiple metrics for the evaluation model. We assess database resilience across eight dimensions: throughput, latency, stability, resistance, recovery, disturbance period, adaptation capability and metric deviation. All the results are presented to users via a user-friendly graphical interface. We demonstrate the execution process and result interpretation of ResBench using two types of adversity datasets.

</details>


### [2] [Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database](https://arxiv.org/abs/2511.11399)
*Rosario Napoli,Antonio Celesti,Massimo Villari,Maria Fazio*

Main category: cs.DB

TL;DR: 本文提出一种将知识补全（KC）阶段整合进图数据库-图机器学习（GDB-GML）应用的新架构，通过引入可扩展的传递关系揭示隐藏知识，显著改善数据集拓扑结构与动态特性，从而提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前GDB-GML应用在处理知识图谱时忽略了知识补全，导致使用看似不完整或碎片化的数据集进行建模，可能引发错误解读。

Method: 提出一种集成知识补全的新架构，利用基于衰减函数建模的可扩展传递关系，在图中传播信息并实现确定性的知识流动。

Result: 实验表明，该方法显著改变了数据集的拓扑结构和整体动态行为，验证了知识补全对GML模型效果的重要影响。

Conclusion: 将知识补全流程整合到GDB-GML架构中，有助于释放图数据的全部潜力，构建更优的机器学习模型。

Abstract: Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.
  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.
  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [3] [Advancing IoT System Dependability: A Deep Dive into Management and Operation Plane Separation](https://arxiv.org/abs/2511.11204)
*Luoyao Hao,Shuo Zhang,Henning Schulzrinne*

Main category: cs.NI

TL;DR: 该论文提出通过分离管理平面与操作平面来提升大规模物联网系统的可靠性，设计了一种不依赖身份的灵活策略框架，并在三个数据集上验证了其接近最优的表达能力和可靠的策略执行效果。


<details>
  <summary>Details</summary>
Motivation: 当前大规模物联网系统缺乏统一、灵活且可适应变化的策略管理机制，难以有效实施安全规范、操作标准和能耗限制等全局性策略，因此需要一种新的管理架构来提升系统可靠性。

Method: 将物联网系统的管理平面与操作平面解耦，在管理平面中引入由监管机构和制造商等多方参与的策略管理机制，并构建一个基于灵活描述符而非固定标识符的身份无关策略框架，以支持动态、前瞻性的策略部署。

Result: 在三个数据集上的评估表明，所提出的框架在策略表达能力和执行可靠性方面均接近最优水平。

Conclusion: 该研究通过创新的管理平面架构和身份无关策略框架，有效提升了大规模物联网系统中全局策略的可部署性与适应性，为构建更可靠、可控的物联网系统提供了可行路径。

Abstract: We propose to enhance the dependability of large-scale IoT systems by separating the management and operation plane. We innovate the management plane to enforce overarching policies, such as safety norms, operation standards, and energy restrictions, and integrate multi-faceted management entities, including regulatory agencies and manufacturers, while the current IoT operational workflow remains unchanged. Central to the management plane is a meticulously designed, identity-independent policy framework that employs flexible descriptors rather than fixed identifiers, allowing for proactive deployment of overarching policies with adaptability to system changes. Our evaluation across three datasets indicates that the proposed framework can achieve near-optimal expressiveness and dependable policy enforcement.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [Tiny Chiplets Enabled by Packaging Scaling: Opportunities in ESD Protection and Signal Integrity](https://arxiv.org/abs/2511.10760)
*Emad Haque,Pragnya Sudershan Nalla,Jeff Zhang,Sachin S. Sapatnekar,Chaitali Chakrabarti,Yu Cao*

Main category: cs.AR

TL;DR: 本文指出，在先进2.5D/3D封装技术下，传统I/O电路（如ESD保护和信号传输）带来的面积开销限制了chiplet进一步缩小；通过寄生参数提取与SPICE仿真，作者证明这些电路可大幅简化，从而支持更小chiplet的设计并提升其组合性与复用性。


<details>
  <summary>Details</summary>
Motivation: 传统I/O电路在chiplet中引入显著面积开销，阻碍其尺寸缩小至100 mm²以下，限制了异构集成的潜力。

Method: 从chiplet接口设计角度重新审视可靠性需求，结合寄生参数提取与SPICE仿真分析ESD保护与片间信号传输的简化可能性。

Result: 在先进2.5D/3D封装技术下，ESD保护与信号电路可大幅简化，为chiplet微型化、组合性与复用性提供新路径。

Conclusion: 未来先进封装技术允许简化chiplet接口电路，从而突破面积瓶颈，推动更小、更灵活的chiplet设计。

Abstract: The scaling of advanced packaging technologies provides abundant interconnection resources for 2.5D/3D heterogeneous integration (HI), thereby enabling the construction of larger-scale VLSI systems with higher energy efficiency in data movement. However, conventional I/O circuitry, including electrostatic discharge (ESD) protection and signaling, introduces significant area overhead. Prior studies have identified this overhead as a major constraint in reducing chiplet size below 100 mm2. In this study, we revisit reliability requirements from the perspective of chiplet interface design. Through parasitic extraction and SPICE simulations, we demonstrate that ESD protection and inter-chiplet signaling can be substantially simplified in future 2.5D/3D packaging technologies. Such simplification, in turn, paves the road for further chiplet miniaturization and improves the composability and reusability of tiny chiplets.

</details>


### [5] [T-MAN: Enabling End-to-End Low-Bit LLM Inference on NPUs via Unified Table Lookup](https://arxiv.org/abs/2511.11248)
*Jianyu Wei,Qingtao Li,Shijie Cao,Lingxiao Ma,Zixu Hao,Yanyong Zhang,Xiaoyan Hu,Ting Cao*

Main category: cs.AR

TL;DR: 该论文提出了一种基于查表法的新型方法，通过统一的表布局和分块策略，在NPU上高效执行大语言模型（LLM）推理中的非GEMM操作（如反量化），从而显著提升预填充和解码阶段的速度并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 当前设备上的NPU在执行LLM推理时性能不如CPU，主要因为NPU对GEMM以外的操作（如反量化）支持较差。现有方法要么将预填充和解码阶段分别放在NPU和CPU上执行，要么全部放在NPU上但牺牲精度。因此，亟需一种能在NPU上高效、准确地执行完整LLM推理的方法。

Method: 作者利用低比特表示可将目标计算编码到可接受大小的查找表中的洞察，提出用查表操作替代NPU不擅长的硬件操作。具体包括：(1) 融合两级查表反量化；(2) 基于并发-层次结构引导的分块策略；(3) 三阶段流水线实现预填充；(4) 将基于查表的解码映射到NPU向量单元。

Result: 实验结果表明，相比基线NPU方法，该方法在预填充阶段提速1.4倍，解码阶段提速3.1倍，并节省84%的能耗。

Conclusion: 通过查表法与硬件感知的优化策略，可以在NPU上高效执行完整的LLM推理，显著提升性能并降低能耗，为端侧LLM部署提供了有效解决方案。

Abstract: Large language models (LLMs) are increasingly deployed on customer devices. To support them, current devices are adopting SoCs (System on Chip) with NPUs (Neural Processing Unit) installed. Although high performance is expected, LLM inference on NPUs is slower than its CPU counterpart. The reason is that NPUs have poor performance on computations other than GEMM, like dequantization. Current works either disaggregate prefill on the NPUs and decoding on the CPUs, or put both on the NPUs but with an accuracy loss. To solve this issue, based on the insight that low-bit can enable target computation encoded within an acceptably sized table, we propose table lookup to subsume hardware operations otherwise unsupported. To realize this, we overcome the conflicting hardware behavior of prefill and decoding to design a unified table layout and tiling through (1) fused two-level table-based dequantization and (2) concurrency-hierarchy-guided tiling. Based on that, we implement the prefill phase by three-stage pipeline and map the table-lookup-based decoding to NPU's vector units. Results show 1.4x and 3.1x speedup for prefill and decoding respectively, and 84% energy savings compared to the baseline NPU methods. The code is available at https://github.com/microsoft/T-MAC/tree/main/t-man.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [FengHuang: Next-Generation Memory Orchestration for AI Inferencing](https://arxiv.org/abs/2511.10753)
*Jiamin Li,Lei Qu,Tao Zhang,Grigory Chirkov,Shuotao Xu,Peng Cheng,Lidong Zhou*

Main category: cs.DC

TL;DR: 本文提出了一种名为FengHuang的新型解耦式AI推理基础设施架构，通过多级共享内存设计、主动张量分页和近内存计算，在保持性能的同时显著降低GPU使用量与通信开销，实现高扩展性、灵活性与成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统以GPU为中心的AI基础设施在大模型推理中面临内存容量、带宽及互连扩展性瓶颈，难以满足日益增长的生成式AI需求，亟需一种可扩展、高效且成本可控的新架构。

Method: FengHuang平台采用解耦式多级共享内存架构，结合高速本地内存与集中式远程内存，并引入主动张量分页和近内存计算技术优化张量操作。

Result: 仿真表明，FengHuang最多可减少93%的本地内存占用、节省50%的GPU算力，并将GPU间通信速度提升16至70倍；在GPT-3、Grok-1和QWEN3-235B等模型上可减少最多50%的GPU数量而维持端到端性能。

Conclusion: FengHuang为AI推理提供了一种机架级的高效扩展方案，在性能、成本与供应链灵活性之间取得良好平衡，具备开放性和异构兼容性，有助于降低基础设施与能耗成本。

Abstract: This document presents a vision for a novel AI infrastructure design that has been initially validated through inference simulations on state-of-the-art large language models. Advancements in deep learning and specialized hardware have driven the rapid growth of large language models (LLMs) and generative AI systems. However, traditional GPU-centric architectures face scalability challenges for inference workloads due to limitations in memory capacity, bandwidth, and interconnect scaling. To address these issues, the FengHuang Platform, a disaggregated AI infrastructure platform, is proposed to overcome memory and communication scaling limits for AI inference. FengHuang features a multi-tier shared-memory architecture combining high-speed local memory with centralized disaggregated remote memory, enhanced by active tensor paging and near-memory compute for tensor operations. Simulations demonstrate that FengHuang achieves up to 93% local memory capacity reduction, 50% GPU compute savings, and 16x to 70x faster inter-GPU communication compared to conventional GPU scaling. Across workloads such as GPT-3, Grok-1, and QWEN3-235B, FengHuang enables up to 50% GPU reductions while maintaining end-user performance, offering a scalable, flexible, and cost-effective solution for AI inference infrastructure. FengHuang provides an optimal balance as a rack-level AI infrastructure scale-up solution. Its open, heterogeneous design eliminates vendor lock-in and enhances supply chain flexibility, enabling significant infrastructure and power cost reductions.

</details>


### [7] [HPCAgentTester: A Multi-Agent LLM Approach for Enhanced HPC Unit Test Generation](https://arxiv.org/abs/2511.10860)
*Rabimba Karanjai,Lei Xu,Weidong Shi*

Main category: cs.DC

TL;DR: 本文提出HPCAgentTester，一种基于多智能体大语言模型的自动化单元测试生成框架，专门用于提升高性能计算（HPC）中OpenMP和MPI程序的测试质量。


<details>
  <summary>Details</summary>
Motivation: 高性能计算中的单元测试面临并行性、复杂算法和硬件多样性等挑战，传统方法难以处理非确定性行为和同步问题。

Method: 设计了一个多智能体LLM框架HPCAgentTester，包含Recipe Agent和Test Agent，通过协作式批评循环迭代生成并优化针对并行结构和通信模式的上下文感知单元测试。

Result: HPCAgentTester能生成可编译且功能正确的测试用例，有效发现传统方法遗漏的细微错误，在编译成功率和正确性方面显著优于单一大语言模型。

Conclusion: HPCAgentTester为并行软件系统提供了一种更鲁棒、可扩展的单元测试解决方案，显著提升了HPC软件的可靠性保障能力。

Abstract: Unit testing in High-Performance Computing (HPC) is critical but challenged by parallelism, complex algorithms, and diverse hardware. Traditional methods often fail to address non-deterministic behavior and synchronization issues in HPC applications. This paper introduces HPCAgentTester, a novel multi-agent Large Language Model (LLM) framework designed to automate and enhance unit test generation for HPC software utilizing OpenMP and MPI. HPCAgentTester employs a unique collaborative workflow where specialized LLM agents (Recipe Agent and Test Agent) iteratively generate and refine test cases through a critique loop. This architecture enables the generation of context-aware unit tests that specifically target parallel execution constructs, complex communication patterns, and hierarchical parallelism. We demonstrate HPCAgentTester's ability to produce compilable and functionally correct tests for OpenMP and MPI primitives, effectively identifying subtle bugs that are often missed by conventional techniques. Our evaluation shows that HPCAgentTester significantly improves test compilation rates and correctness compared to standalone LLMs, offering a more robust and scalable solution for ensuring the reliability of parallel software systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [8] [Peer Code Review in Research Software Development: The Research Software Engineer Perspective](https://arxiv.org/abs/2511.10781)
*Md Ariful Islam Malik,Jeffrey C. Carver,Nasir U. Eisty*

Main category: cs.SE

TL;DR: 本研究通过调查61名科研软件工程师（RSEs），探讨其对同行代码审查的实践、挑战与改进建议，发现尽管RSEs面临独特困难，但通过结构化流程、改进工具和针对性培训可提升审查采纳率与效果。


<details>
  <summary>Details</summary>
Motivation: 科研软件对研究至关重要，但其质量与可维护性常受需求变化、复杂输入和遗留依赖影响；同行代码审查虽能提升质量，但在RSE群体中的采纳情况尚不明确。

Method: 通过问卷调查收集RSE对同行代码审查的看法，问卷设计与先前研究对齐以支持比较分析，并增加了针对RSE的特定问题。

Result: 获得61份有效问卷，结果既与以往研究一致，也揭示了RSE相较于其他开发者在审查实践与挑战方面的独特见解。

Conclusion: 同行代码审查对提升科研软件质量、可维护性和可靠性至关重要；通过结构化流程、更好工具和针对性培训可有效促进RSE采纳并提升审查效果。

Abstract: Background: Research software is crucial for enabling research discoveries and supporting data analysis, simulation, and interpretation across domains. However, evolving requirements, complex inputs, and legacy dependencies hinder the software quality and maintainability. While peer code review can improve software quality, its adoption by research software engineers (RSEs) remains unexplored. Aims: This study explores RSE perspectives on peer code review, focusing on their practices, challenges, and potential improvements. Building on prior work, it aims to uncover how RSEs insights differ from those of other research software developers and identify factors that can enhance code review adoption in this domain. Method: We surveyed RSEs to gather insights into their perspectives on peer code review. The survey design aligned with previous research to enable comparative analysis while including additional questions tailored to RSEs. Results: We received 61 valid responses from the survey. The findings align with prior research while uncovering unique insights about the challenges and practices of RSEs compared to broader developer groups. Conclusions: Peer code review is vital in improving research software's quality, maintainability, and reliability. Despite the unique challenges RSEs face, addressing these through structured processes, improved tools, and targeted training can enhance peer review adoption and effectiveness in research software development.

</details>


### [9] [Towards a Human-in-the-Loop Framework for Reliable Patch Evaluation Using an LLM-as-a-Judge](https://arxiv.org/abs/2511.10865)
*Sherry Shi,Renyao Wei,Michele Tufano,José Cambronero,Runxiang Cheng,Franjo Ivančić,Pat Rondon*

Main category: cs.SE

TL;DR: 本文提出一种结合人类反馈与大语言模型（LLM）的“人在回路”方法，用于自动程序修复（APR）中补丁有效性的判断。该方法首先由LLM为每个缺陷生成评分标准（rubric），经人工一次性审核和优化后，再由LLM依据该标准判断补丁有效性，在与人工共识对比中展现出高一致性、高召回率和高精确率。


<details>
  <summary>Details</summary>
Motivation: 当前自动程序修复（APR）领域依赖基于单元测试的评估方法（如pass@k），无法准确反映补丁的真实有效性；而人工标注成本高昂。因此，亟需一种低成本且可靠的补丁有效性评估方法。

Method: 提出一种人在回路的方法：首先利用大语言模型为每个bug生成一个评分标准（rubric），然后由人类专家对该标准进行一次性审查和可选优化，最后使用优化后的标准指导大语言模型对补丁进行有效性判断。

Result: 在人工标注者对补丁有效性达成一致的数据子集上，该方法与人工共识高度一致（Cohen's kappa 0.75），召回率达0.94，精确率达0.80；在包含人工意见分歧的完整数据集上，性能略有下降（kappa 0.57，召回率0.93，精确率0.65），并指出了未来改进方向。

Conclusion: 所提出的人在回路LLM评估框架能有效降低人工标注成本，同时在补丁有效性判断上取得与人类高度一致的结果，为自动程序修复的可靠评估提供了可行路径。

Abstract: Reliable evaluation is crucial for advancing Automated Program Repair (APR), but prevailing benchmarks rely on execution-based evaluation methods (unit test pass@k), which fail to capture true patch validity. Determining validity can require costly manual annotation. To reduce this cost, we introduce a human-in-the-loop approach to LLM-based patch validity judgment. Inspired by the observation that human judgment is better aligned when using a shared rubric, we first employ an LLM to generate a per-bug rubric, followed by a one-time human review and optional refinement to this rubric, and then employ an LLM to judge patches using the refined rubric. We apply this approach to assign binary validity labels to patches for issues found by Google sanitizer tools. Our results show that this approach yields substantial agreement with human consensus (Cohen's kappa 0.75), high recall (0.94) and high precision (0.80), when considering patches that have unanimous agreement from 3 human raters on the validity labels. On the full dataset including patches where human raters disagree, we find this approach can still be further improved (Cohen's kappa 0.57, recall 0.93, precision 0.65) and identify possible future directions.

</details>


### [10] [Architecting software monitors for control-flow anomaly detection through large language models and conformance checking](https://arxiv.org/abs/2511.10876)
*Francesco Vitale,Francesco Flammini,Mauro Caporuscio,Nicola Mazzocca*

Main category: cs.SE

TL;DR: 本文提出一种结合大语言模型（LLMs）与一致性检查的软件监控方法，用于在运行时检测控制流异常。该方法通过LLMs自动将设计模型映射到源代码并进行插桩，生成事件日志后利用一致性检查进行异常检测，在ERTMS/ETCS铁路系统案例中实现了高达84.775%的控制流覆盖率和96.610%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现代计算机系统的复杂性使得在运行时可能出现设计阶段未预料到的控制流异常（“未知的未知”），仅靠传统验证与确认（V&V）手段难以保障其可靠性，因此需要一种能在运行时有效检测控制流偏差的新方法。

Method: 提出一种基于大语言模型（LLMs）和一致性检查的软件监控方法：利用LLMs将设计时模型与实现代码对齐，自动完成源代码插桩以生成事件日志；再通过一致性检查技术分析日志，检测控制流异常。

Result: 在ERTMS/ETCS铁路系统案例研究中，LLM驱动的插桩实现了84.775%的控制流覆盖率，一致性检查异常检测达到96.610%的F1分数和93.515%的AUC。

Conclusion: 将领域知识融入LLM引导的源代码插桩过程，能显著提升日志质量与可靠性，从而支持高效、可解释的控制流异常检测。

Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.

</details>


### [11] [Utilizing LLMs for Industrial Process Automation: A Case Study on Modifying RAPID Programs](https://arxiv.org/abs/2511.11125)
*Salim Fares,Steffen Herbold*

Main category: cs.SE

TL;DR: 本文探讨了在工业过程自动化领域中，企业如何在不投入大量资源训练专用模型的情况下，利用少量示例提示（few-shot prompting）有效使用大语言模型（LLM）处理专有领域语言中的简单任务，并支持本地部署以保护敏感数据。


<details>
  <summary>Details</summary>
Motivation: 当前大多数关于大语言模型在软件工程中应用的研究集中于通用编程语言，而工业过程自动化领域使用的高度专业化、专有语言尚未得到充分探索。本文旨在填补这一空白，研究企业在不进行大规模领域模型训练的前提下，能否有效利用现有LLM处理此类语言。

Method: 采用少量示例提示（few-shot prompting）方法，在无需微调或训练专用模型的情况下，测试通用大语言模型对工业自动化领域中非主流专有语言的支持能力，并强调本地部署以保障数据安全。

Result: 研究表明，few-shot prompting 足以解决该专有语言中的简单问题，且可在本地环境中实现，从而保护企业敏感数据。

Conclusion: 对于工业过程自动化等使用专有语言的领域，企业无需大量投入即可通过 few-shot prompting 有效利用现有大语言模型完成基本任务，同时满足数据隐私和本地部署需求。

Abstract: How to best use Large Language Models (LLMs) for software engineering is covered in many publications in recent years. However, most of this work focuses on widely-used general purpose programming languages. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, is still underexplored. Within this paper, we study enterprises can achieve on their own without investing large amounts of effort into the training of models specific to the domain-specific languages that are used. We show that few-shot prompting approaches are sufficient to solve simple problems in a language that is otherwise not well-supported by an LLM and that is possible on-premise, thereby ensuring the protection of sensitive company data.

</details>


### [12] [SQuaD: The Software Quality Dataset](https://arxiv.org/abs/2511.11265)
*Mikel Robredo,Matteo Esposito,Davide Taibi,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: 本文提出了SQuaD（Software Quality Dataset），一个涵盖450个成熟开源项目的多维度、时间感知的软件质量数据集，整合了9种静态分析工具提取的700多个指标，并包含版本控制、缺陷追踪、漏洞信息等，支持大规模软件质量研究。


<details>
  <summary>Details</summary>
Motivation: 现有软件质量数据集通常只关注有限维度（如代码异味、技术债或重构活动），难以支持跨时间和多质量维度的综合分析。

Method: 通过整合9种先进的静态分析工具（如SonarQube、PMD、RefactoringMiner等），从450个开源项目中提取方法、类、文件和项目级别的700多个质量指标，并结合版本历史、问题跟踪、CVE/CWE漏洞数据及JIT缺陷预测相关的过程指标。

Result: 构建了覆盖63,586个项目发布版本的SQuaD数据集，支持对可维护性、技术债、软件演化和质量评估的大规模实证研究，并已公开发布于ZENODO。

Conclusion: SQuaD填补了现有软件质量数据在维度和时间覆盖上的不足，为软件工程领域的实证研究提供了丰富资源，并提出了自动化更新与跨项目建模等未来方向。

Abstract: Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).

</details>


### [13] [SCRUTINEER: Detecting Logic-Level Usage Violations of Reusable Components in Smart Contracts](https://arxiv.org/abs/2511.11411)
*Xingshuang Lin,Binbin Zhao,Jinwen Wang,Qinge Xie,Xibin Zhao,Shouling Ji*

Main category: cs.SE

TL;DR: 本文提出了SCRUTINEER，首个用于自动检测智能合约可重用组件（SCR）逻辑级使用违规的实用系统，通过多阶段特征提取、大语言模型驱动的知识构建和检索增强生成技术，实现了高精度的漏洞识别。


<details>
  <summary>Details</summary>
Motivation: 智能合约可重用组件（SCR）虽能提升开发效率，但其逻辑级使用违规（即符合使用规范却违背具体业务逻辑）会引发严重安全漏洞，而现有方法缺乏对业务逻辑语义的深入理解，难以有效检测此类问题。

Method: 作者设计了复合特征提取方法生成三种互补特征表示；利用大语言模型结合领域工具构建SCR知识库；开发基于检索增强生成的检查器进行快速检索与细粒度分析；并集成相似性检查器与快照推理冲突检查器实现精准检测。

Result: 在三个真实数据集上的评估表明，SCRUTINEER在检测SCR逻辑级使用违规方面达到80.77%的精确率、82.35%的召回率和81.55%的F1分数。

Conclusion: SCRUTINEER是首个能有效、自动地检测智能合约可重用组件逻辑级使用违规的系统，实验证明其具备高准确性和实用性，为提升智能合约安全性提供了新思路。

Abstract: Smart Contract Reusable Components(SCRs) play a vital role in accelerating the development of business-specific contracts by promoting modularity and code reuse. However, the risks associated with SCR usage violations have become a growing concern. One particular type of SCR usage violation, known as a logic-level usage violation, is becoming especially harmful. This violation occurs when the SCR adheres to its specified usage rules but fails to align with the specific business logic of the current context, leading to significant vulnerabilities. Detecting such violations necessitates a deep semantic understanding of the contract's business logic, including the ability to extract implicit usage patterns and analyze fine-grained logical behaviors. To address these challenges, we propose SCRUTINEER, the first automated and practical system for detecting logic-level usage violations of SCRs. First, we design a composite feature extraction approach that produces three complementary feature representations, supporting subsequent analysis. We then introduce a Large Language Model-powered knowledge construction framework, which leverages comprehension-oriented prompts and domain-specific tools to extract logic-level usage and build the SCR knowledge base. Next, we develop a Retrieval-Augmented Generation-driven inspector, which combines a rapid retrieval strategy with both comprehensive and targeted analysis to identify potentially insecure logic-level usages. Finally, we implement a logic-level usage violation analysis engine that integrates a similarity-based checker and a snapshot-based inference conflict checker to enable accurate and robust detection. We evaluate SCRUTINEER from multiple perspectives on 3 ground-truth datasets. The results show that SCRUTINEER achieves a precision of 80.77%, a recall of 82.35%, and an F1-score of 81.55% in detecting logic-level usage violations of SCRs.

</details>


### [14] [CertiA360: Enhance Compliance Agility in Aerospace Software Development](https://arxiv.org/abs/2511.11550)
*J. Antonio Dantas Macedo,Hugo Fernandes,J. Eduardo Ferreira Ribeiro*

Main category: cs.SE

TL;DR: 本文提出了一种名为CertiA360的工具，旨在将敏捷方法的灵活性与航空航天领域安全关键系统开发中DO-178C标准的严格合规要求相结合，通过自动化变更管理和需求追溯，提升效率并支持认证。


<details>
  <summary>Details</summary>
Motivation: 在航空航天等高度监管的行业中，敏捷方法因其灵活应对变化的优势而受到关注，但其与DO-178C等安全认证标准对文档、可追溯性和验证验证的严格要求存在冲突。因此，亟需一种方法在保持敏捷优势的同时满足合规性。

Method: 研究设计并开发了CertiA360工具，用于在整个软件开发生命周期中自动化管理变更请求、增强需求成熟度，并确保需求跨层级和学科的可追溯性；该工具在航空航天行业专家的密切合作下进行设计与验证。

Result: 专家反馈表明，CertiA360能显著减少人工工作量，在响应需求变更的同时保障DO-178C合规性；尽管该工具尚未通过DO-330认证，但初步结果证明适当定制的敏捷方法可在安全关键系统开发中有效应用。

Conclusion: 研究表明，经过适当调整的敏捷方法不仅能够与航空航天等高监管领域中的安全系统开发和认证要求共存，还能提升开发效率；CertiA360为实现这一目标提供了可行的技术支持。

Abstract: Agile methods are characterised by iterative and incremental processes with a strong focus on flexibility and accommodating changing requirements based on either technical, regulatory, or stakeholder feedback. However, integrating Agile methods into safety-critical system development in the aerospace industry presents substantial challenges due to its strict compliance requirements, such as those outlined in the DO-178C standard. To achieve this vision, the flexibility of Agile must align with the rigorous certification guidelines, which emphasize documentation, traceability of requirements across different levels and disciplines, and comprehensive verification and validation (V&V) activities. The research work described in this paper proposes a way of using the strengths of the flexible nature of Agile methods to automate and manage change requests throughout the whole software development lifecycle, ensuring robust traceability, regulatory compliance and ultimately facilitating successful certification. This study proposes CertiA360, a tool designed to help teams improve requirement maturity, automate the changes in traceability, and align with the regulatory objectives. The tool was designed and validated in close collaboration with aerospace industry experts, using their feedback to ensure practical application and real-life effectiveness. The feedback collected demonstrated that the automation given by CertiA360 may reduce manual effort and allow response to changing requirements while ensuring compliance with DO-178C. While the tool is not yet qualified under DO-330 (Tool Qualification), findings suggest that when tailored appropriately, Agile methods can not only coexist with the requirements of safety-system development and certification in highly regulated domains like aerospace, but also add efficiency.

</details>
