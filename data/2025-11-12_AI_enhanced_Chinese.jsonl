{"id": "2511.07423", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.07423", "abs": "https://arxiv.org/abs/2511.07423", "authors": ["Genglin Wang", "Liekang Zeng", "Bufang Yang", "Kaiwei Liu", "Guoliang Xing", "Chumin Sun", "Li Zhou", "Jie Sun", "Zhenyu Yan"], "title": "Synera: Synergistic LLM Serving across Device and Cloud at Scale", "comment": null, "summary": "Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSynera\uff0c\u4e00\u79cd\u8bbe\u5907-\u4e91\u534f\u540c\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u4e0eLLM\u534f\u540c\u673a\u5236\uff0c\u5728\u4fdd\u6301\u5ef6\u8fdf\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u5e76\u964d\u4f4e\u4e91\u670d\u52a1\u6210\u672c\u3002", "motivation": "\u73b0\u6709LLM\u5728\u79fb\u52a8\u7aef\u90e8\u7f72\u9762\u4e34\u751f\u6210\u8d28\u91cf\u4e0b\u964d\u548c\u5ef6\u8fdf\u589e\u52a0\u7684\u95ee\u9898\uff1b\u7eaf\u4e91\u7aef\u5378\u8f7d\u53d7\u901a\u4fe1\u74f6\u9888\u9650\u5236\uff0c\u7eaf\u7aef\u4fa7\u5c0f\u6a21\u578b\u5219\u56e0\u8d44\u6e90\u53d7\u9650\u727a\u7272\u751f\u6210\u8d28\u91cf\u3002", "method": "Synera\u5f15\u5165\u4e09\u79cd\u9488\u5bf9\u6027\u8bbe\u8ba1\uff1a\u901a\u4fe1\u9ad8\u6548\u7684\u6709\u9009\u62e9\u6027\u5378\u8f7d\u3001\u65e0\u505c\u6ede\u5e76\u884c\u63a8\u7406\u548c\u53ef\u6269\u5c55\u7684\u4e91\u7aef\u6279\u5904\u7406\uff0c\u4ee5\u4f18\u5316\u8bbe\u5907-\u4e91\u534f\u540c\u63a8\u7406\u4e2d\u7684\u5378\u8f7d\u51b3\u7b56\u3001\u6d41\u6c34\u7ebf\u505c\u987f\u548c\u6279\u5904\u7406\u74f6\u9888\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSynera\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u5728\u5ef6\u8fdf\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u8d28\u91cf\u63d0\u53471.20\u20135.47\u500d\uff0c\u5e76\u6bd4\u4f20\u7edf\u4e91\u670d\u52a1\u8282\u77018.2%\u201316.5%\u7684\u4e91\u670d\u52a1\u6210\u672c\u3002", "conclusion": "Synera\u6709\u6548\u7ed3\u5408\u8bbe\u5907\u4e0e\u4e91\u7aef\u4f18\u52bf\uff0c\u901a\u8fc7\u534f\u540c\u63a8\u7406\u673a\u5236\u5728\u79fb\u52a8\u7aef\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u4f4e\u5ef6\u8fdf\u4e14\u6210\u672c\u6548\u76ca\u66f4\u4f18\u7684LLM\u670d\u52a1\u3002"}}
{"id": "2511.07424", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.07424", "abs": "https://arxiv.org/abs/2511.07424", "authors": ["Bhala Ranganathan", "Mickey Zhang", "Kai Wu"], "title": "Enhancing reliability in AI inference services: An empirical study on real production incidents", "comment": null, "summary": "Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u4e00\u5e74\u7684\u8fd0\u8425\u7ecf\u9a8c\uff0c\u5bf9156\u8d77\u9ad8\u4e25\u91cd\u6027\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e8b\u6545\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5177\u6709\u9ad8\u6807\u6ce8\u4e00\u81f4\u6027\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u8bc6\u522b\u51fa\u4e3b\u8981\u6545\u969c\u6a21\u5f0f\uff08\u5982\u63a8\u7406\u5f15\u64ce\u6545\u969c\u548c\u8d85\u65f6\uff09\uff0c\u5e76\u603b\u7ed3\u4e86\u591a\u79cd\u7f13\u89e3\u7b56\u7565\uff08\u5982\u81ea\u52a8\u68c0\u6d4b\u3001\u6d41\u91cf\u8c03\u5ea6\u3001\u8282\u70b9\u518d\u5e73\u8861\u7b49\uff09\uff0c\u6700\u7ec8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f9b\u4ece\u4e1a\u8005\u91c7\u7528\u7684\u68c0\u67e5\u6e05\u5355\uff0c\u4ee5\u63d0\u5347\u5927\u89c4\u6a21LLM\u63a8\u7406\u670d\u52a1\u7684\u53ef\u9760\u6027\u4e0e\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u8d85\u5927\u89c4\u6a21\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5bf9\u4e91\u7cfb\u7edf\u63d0\u51fa\u6781\u9ad8\u8981\u6c42\uff0c\u77ed\u6682\u6545\u969c\u4e5f\u53ef\u80fd\u9020\u6210\u91cd\u5927\u7528\u6237\u548c\u4e1a\u52a1\u5f71\u54cd\u3002\u4e3a\u7406\u89e3\u5e76\u7f13\u89e3\u8fd9\u4e9b\u98ce\u9669\uff0c\u4e9f\u9700\u57fa\u4e8e\u5b9e\u9645\u8fd0\u7ef4\u7ecf\u9a8c\u7684\u7cfb\u7edf\u6027\u5206\u6790\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u4e00\u5e74\u7684\u8fd0\u7ef4\u5b9e\u8df5\u6784\u5efa\u4e86\u4e00\u5957\u4e8b\u6545\u5206\u7c7b\u6cd5\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u5728156\u8d77\u9ad8\u4e25\u91cd\u6027\u4e8b\u6545\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5e76\u5bf92025\u5e744\u6708\u81f36\u6708\u7684\u6570\u636e\u8fdb\u884c\u805a\u7126\u5b9a\u91cf\u7814\u7a76\uff1b\u4f7f\u7528Cohen's Kappa\u8bc4\u4f30\u6807\u6ce8\u4e00\u81f4\u6027\uff0c\u5e76\u7ed3\u5408\u6848\u4f8b\u63d0\u70bc\u7f13\u89e3\u63aa\u65bd\u548c\u4f18\u5316\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6807\u6ce8\u4e00\u81f4\u6027\uff08Cohen's K \u2248 0.89\uff09\uff1b\u53d1\u73b0\u7ea660%\u7684\u4e8b\u6545\u6e90\u4e8e\u63a8\u7406\u5f15\u64ce\u6545\u969c\uff0c\u5176\u4e2d\u7ea640%\u4e3a\u8d85\u65f6\uff1b\u7ea674%\u7684\u4e8b\u6545\u53ef\u88ab\u81ea\u52a8\u68c0\u6d4b\uff0c28%\u9700\u70ed\u4fee\u590d\uff1b\u5176\u4f59\u591a\u901a\u8fc7\u6d41\u91cf\u8def\u7531\u3001\u8282\u70b9\u518d\u5e73\u8861\u6216\u6269\u5bb9\u7b56\u7565\u7f13\u89e3\uff1b\u6240\u63d0\u5206\u7c7b\u6cd5\u8fd8\u6307\u5bfc\u4e86\u8fde\u63a5\u6d3b\u6027\u68c0\u6d4b\u3001GPU\u5bb9\u91cf\u611f\u77e5\u8def\u7531\u548c\u7aef\u70b9\u9694\u79bb\u7b49\u9488\u5bf9\u6027\u7b56\u7565\uff0c\u6709\u6548\u964d\u4f4e\u4e8b\u6545\u5f71\u54cd\u5e76\u52a0\u901f\u6062\u590d\u3002", "conclusion": "\u7cfb\u7edf\u6027\u3001\u57fa\u4e8e\u5b9e\u8bc1\u7684LLM\u63a8\u7406\u8fd0\u7ef4\u5206\u6790\u80fd\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u670d\u52a1\u7684\u53ef\u9760\u6027\u4e0e\u6210\u672c\u6548\u7387\uff0c\u4f5c\u8005\u63d0\u4f9b\u7684\u5b9e\u8df5\u5bfc\u5411\u68c0\u67e5\u6e05\u5355\u6709\u52a9\u4e8e\u4e1a\u754c\u590d\u7528\u8be5\u65b9\u6cd5\u8bba\u4e0e\u81ea\u52a8\u5316\u673a\u4f1a\u3002"}}
{"id": "2511.07770", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.07770", "abs": "https://arxiv.org/abs/2511.07770", "authors": ["Zewei Guo", "Zhen Jia", "JinXiao Zhu", "Wenhao Huang", "Yin Chen"], "title": "A Large-Scale Dataset and Reproducible Framework for RF Fingerprinting on IEEE 802.11g Same-Model Devices", "comment": null, "summary": "Radio frequency (RF) fingerprinting exploits hardware imperfections for device identification, but distinguishing between same-model devices remains challenging due to their minimal hardware variations. Existing datasets for RF fingerprinting are constrained by small device scales and heterogeneous models, which hinders robust training and fair evaluation for machine learning models. To address this gap, we introduce a large-scale dataset of same-model devices along with a fully reproducible, open-source experimental framework. The dataset is built using 123 identical commercial IEEE 802.11g devices and contains 35.42 million raw I/Q samples from the preambles and corresponding 1.85 million RF features. The open-source framework further ensures full reproducibility from data collection to final evaluation. Within this framework, a Random Forest-based algorithm is proposed to achieve 89.06% identification accuracy on this dataset. Extensive experimental evaluations further confirm the relationships between the extracted features.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u540c\u578b\u53f7\u8bbe\u5907\u7684\u5c04\u9891\uff08RF\uff09\u6307\u7eb9\u6570\u636e\u96c6\u53ca\u914d\u5957\u7684\u5f00\u6e90\u53ef\u590d\u73b0\u5b9e\u9a8c\u6846\u67b6\uff0c\u5305\u542b123\u4e2a\u76f8\u540c\u5546\u7528IEEE 802.11g\u8bbe\u5907\u76843542\u4e07\u539f\u59cbI/Q\u6837\u672c\u548c185\u4e07RF\u7279\u5f81\uff0c\u5e76\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u5b9e\u73b0\u4e8689.06%\u7684\u8bbe\u5907\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u5c04\u9891\u6307\u7eb9\u6570\u636e\u96c6\u53d7\u9650\u4e8e\u8bbe\u5907\u6570\u91cf\u5c11\u548c\u578b\u53f7\u5f02\u6784\uff0c\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\u548c\u516c\u5e73\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5c24\u5176\u5728\u533a\u5206\u540c\u578b\u53f7\u8bbe\u5907\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u6784\u5efa\u5305\u542b123\u4e2a\u76f8\u540c\u578b\u53f7IEEE 802.11g\u8bbe\u5907\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u91c7\u96c6\u524d\u5bfc\u7801\u4e2d\u7684\u539f\u59cbI/Q\u6837\u672c\u5e76\u63d0\u53d6RF\u7279\u5f81\uff1b\u5f00\u53d1\u5b8c\u5168\u5f00\u6e90\u3001\u53ef\u590d\u73b0\u7684\u5b9e\u9a8c\u6846\u67b6\uff1b\u91c7\u7528\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7684\u7b97\u6cd5\u8fdb\u884c\u8bbe\u5907\u8bc6\u522b\u3002", "result": "\u5728\u6240\u63d0\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8689.06%\u7684\u8bbe\u5907\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u53d6\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u8054\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u540c\u578b\u53f7\u8bbe\u5907\u6570\u636e\u96c6\u4e0e\u53ef\u590d\u73b0\u6846\u67b6\uff0c\u663e\u8457\u63a8\u52a8\u4e86\u5c04\u9891\u6307\u7eb9\u8bc6\u522b\u9886\u57df\u5728\u6a21\u578b\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u65b9\u9762\u7684\u6807\u51c6\u5316\u4e0e\u53ef\u9760\u6027\u3002"}}
{"id": "2511.07709", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07709", "abs": "https://arxiv.org/abs/2511.07709", "authors": ["Lars Olt", "Luis Diego Fonseca Flores", "Ian Mckinley"], "title": "Post Processing Graphical User Interface for Heat Flow Visualization", "comment": "Presented at the 53rd International Conference on Environmental Systems (ICES 2024), Louisville, Kentucky, USA, 2024. Official link: https://hdl.handle.net/2346/98969", "summary": "Thermal Desktop (TD) is an industry-standard thermal analysis tool used to create and analyze thermal models for landers, rovers, spacecraft, and instrument payloads. Currently, limited software exists to extract and visualize metrics relevant to heat flow within TD, impeding thermal engineers from analyzing their results quickly. This paper discusses a graphical user interface (GUI) built in MATLAB and C++ which uses TDs application programming interface (API), OpenTD, and a custom parser to address this void. Specifically, we present a method for efficiently loading temperature, conductance, and submodel metrics using a side effect of TDs Compressed Solution Results (CSR) files. This approach can reduce the runtime for correlating model nodes and conductors with submodel IDs by orders of magnitude. Lastly, we reflect on the shortcomings of this method for reading data, consider the future of the GUI, and provide recommendations for subsequent OpenTD releases.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eMATLAB\u548cC++\u5f00\u53d1\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\uff0c\u5229\u7528Thermal Desktop\u7684API\uff08OpenTD\uff09\u548c\u81ea\u5b9a\u4e49\u89e3\u6790\u5668\uff0c\u9ad8\u6548\u8bfb\u53d6\u5e76\u53ef\u89c6\u5316\u70ed\u6a21\u578b\u4e2d\u7684\u6e29\u5ea6\u3001\u5bfc\u70ed\u548c\u5b50\u6a21\u578b\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u70ed\u5de5\u7a0b\u5e08\u5206\u6790\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u5de5\u5177\u4eceThermal Desktop\u4e2d\u63d0\u53d6\u548c\u53ef\u89c6\u5316\u70ed\u6d41\u76f8\u5173\u6307\u6807\uff0c\u963b\u788d\u4e86\u70ed\u5de5\u7a0b\u5e08\u5feb\u901f\u5206\u6790\u7ed3\u679c\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408MATLAB\u4e0eC++\u7684GUI\uff0c\u901a\u8fc7\u8c03\u7528Thermal Desktop\u7684OpenTD API\uff0c\u5e76\u5229\u7528\u5176\u538b\u7f29\u89e3\u7b97\u7ed3\u679c\uff08CSR\uff09\u6587\u4ef6\u7684\u526f\u4f5c\u7528\uff0c\u9ad8\u6548\u52a0\u8f7d\u6e29\u5ea6\u3001\u5bfc\u70ed\u53ca\u5b50\u6a21\u578b\u6307\u6807\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u6a21\u578b\u8282\u70b9\u3001\u5bfc\u4f53\u4e0e\u5b50\u6a21\u578bID\u5173\u8054\u7684\u8fd0\u884c\u65f6\u95f4\u7f29\u77ed\u4e86\u591a\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5GUI\u663e\u8457\u63d0\u5347\u4e86Thermal Desktop\u70ed\u5206\u6790\u7ed3\u679c\u7684\u5904\u7406\u6548\u7387\uff0c\u4f46\u4f5c\u8005\u4e5f\u6307\u51fa\u4e86\u5f53\u524d\u6570\u636e\u8bfb\u53d6\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5bf9GUI\u672a\u6765\u53d1\u5c55\u53caOpenTD\u540e\u7eed\u7248\u672c\u63d0\u51fa\u4e86\u5efa\u8bae\u3002"}}
{"id": "2511.07426", "categories": ["cs.DC", "cs.AI", "cs.CL", "cs.NI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.07426", "abs": "https://arxiv.org/abs/2511.07426", "authors": ["Zihao Ding", "Mufeng Zhu", "Yao Liu"], "title": "Network and Systems Performance Characterization of MCP-Enabled LLM Agents", "comment": null, "summary": "Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.", "AI": {"tldr": "\u672c\u6587\u5bf9\u542f\u7528MCP\uff08Model Context Protocol\uff09\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ea4\u4e92\u8fdb\u884c\u4e86\u6d4b\u91cf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u80fd\u529b\u3001\u6027\u80fd\u4e0e\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u63d0\u5347\u6548\u7387\u548c\u964d\u4f4e\u6210\u672c\u7684\u4f18\u5316\u7b56\u7565\u3002", "motivation": "MCP\u867d\u80fd\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5916\u90e8\u5de5\u5177\u4ea4\u4e92\u7684\u80fd\u529b\uff0c\u4f46\u5176\u5f15\u5165\u7684\u5927\u91cf\u4e0a\u4e0b\u6587\u4fe1\u606f\u663e\u8457\u589e\u52a0\u4e86token\u4f7f\u7528\u91cf\uff0c\u8fdb\u800c\u63a8\u9ad8\u4e86\u7ecf\u6d4e\u6210\u672c\u548c\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u901a\u8fc7\u57fa\u4e8e\u6d4b\u91cf\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u548cMCP\u914d\u7f6e\u5bf9token\u6548\u7387\u3001\u91d1\u94b1\u6210\u672c\u3001\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u6210\u529f\u7387\u7b49\u5173\u952e\u6307\u6807\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5e76\u91cf\u5316\u4e86MCP\u5728\u63d0\u5347\u529f\u80fd\u7684\u540c\u65f6\u5e26\u6765\u7684\u6210\u672c\u4e0e\u6027\u80fd\u5f00\u9500\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5e76\u884c\u5de5\u5177\u8c03\u7528\u548c\u4efb\u52a1\u4e2d\u6b62\u673a\u5236\u7b49\u4f18\u5316\u624b\u6bb5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u3001\u7a33\u5065\u4e14\u7ecf\u6d4e\u7684MCP\u9a71\u52a8\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2511.08135", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.08135", "abs": "https://arxiv.org/abs/2511.08135", "authors": ["Zhuoheng Ran", "Chong Wu", "Renjie Xu", "Maolin Che", "Hong Yan"], "title": "UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing", "comment": "Accepted on 24 September 2025 at NeurIPS 2025 Efficient Reasoning Workshop", "summary": "The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86UniFormer\uff0c\u4e00\u79cd\u9762\u5411\u901a\u7528\u8ba1\u7b97\u548c\u5b9a\u5236\u8ba1\u7b97\u5e73\u53f0\u7684\u7edf\u4e00\u9ad8\u6548Transformer\u67b6\u6784\uff0c\u5728GPU\u4e0a\u5b9e\u73b0SOTA\u7cbe\u5ea6\u4e0e\u5ef6\u8fdf\uff0c\u5e76\u5728FPGA\u4e0a\u5c55\u73b0\u51fa\u826f\u597d\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u901a\u7528\u8ba1\u7b97\u5e73\u53f0\uff08\u5982GPU\uff09\u4e0e\u5b9a\u5236\u8ba1\u7b97\u5e73\u53f0\uff08\u5982FPGA\u3001ASIC\uff09\u4e4b\u95f4\u90e8\u7f72\u65f6\uff0c\u56e0\u5e76\u884c\u8ba1\u7b97\u8303\u5f0f\u5dee\u5f02\u5bfc\u81f4\u6a21\u578b\u8fc1\u79fb\u56f0\u96be\uff0c\u5e38\u9700\u5728\u590d\u6742\u5ea6\u3001\u6548\u7387\u6216\u7cbe\u5ea6\u4e0a\u505a\u51fa\u59a5\u534f\uff0c\u4e14\u8de8\u5e73\u53f0\u4f18\u5316\u539f\u5219\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u63d0\u51faUniFormer\u67b6\u6784\uff0c\u901a\u8fc7\u589e\u5f3a\u5e76\u884c\u6027\u548c\u8ba1\u7b97-\u5b58\u50a8\u878d\u5408\uff0c\u7edf\u4e00\u9002\u914d\u901a\u7528\u4e0e\u5b9a\u5236\u8ba1\u7b97\u5e73\u53f0\u3002", "result": "UniFormer\u5728GPU\u4e0a\u8fbe\u5230SOTA\u7684\u51c6\u786e\u7387\u4e0e\u5ef6\u8fdf\u8868\u73b0\uff0c\u540c\u65f6\u5728FPGA\u4e0a\u5c55\u73b0\u51fa\u5f3a\u9002\u5e94\u6027\u3002", "conclusion": "UniFormer\u662f\u9996\u4e2a\u8054\u5408\u8003\u8651\u901a\u7528\u4e0e\u5b9a\u5236\u8ba1\u7b97\u67b6\u6784\u7684\u9ad8\u6548Transformer\u5de5\u4f5c\uff0c\u4e3a\u8de8\u5e73\u53f0\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.08127", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.08127", "abs": "https://arxiv.org/abs/2511.08127", "authors": ["Weiye Li", "Wenyi Tang"], "title": "A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models", "comment": null, "summary": "Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u4f20\u7edf\u6e90\u4ee3\u7801\u6a21\u578b\uff08SCMs\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u4ee3\u7801\uff08LLM4Code\uff09\u4e4b\u95f4\u7684\u53ef\u8fc1\u79fb\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bbf\u95ee\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u901a\u7528\u5bf9\u6297\u6837\u672c\u751f\u6210\u65b9\u6cd5HABITAT\uff0c\u5e76\u63ed\u793a\u4e86\u5f71\u54cd\u8de8\u6a21\u578b\u653b\u51fb\u6210\u529f\u7387\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u6e90\u4ee3\u7801\u6a21\u578b\uff08\u5305\u62ec\u4f20\u7edfSCMs\u548cLLM4Code\uff09\u4e2d\u53ef\u8fc1\u79fb\u8106\u5f31\u6027\u7684\u6df1\u5165\u63a2\u7d22\uff0c\u4e14\u591a\u6570\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u8bbf\u95ee\uff0c\u96be\u4ee5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u4e0d\u4f9d\u8d56\u76ee\u6807\u6a21\u578b\u4fe1\u606f\u3001\u9002\u7528\u4e8e\u73b0\u4ee3\u5f00\u53d1\u5e73\u53f0\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684LLM4Code\u7684\u901a\u7528\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86HABITAT\u6846\u67b6\uff0c\u5305\u542b\u5b9a\u5236\u5316\u7684\u6270\u52a8\u63d2\u5165\u673a\u5236\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7ed3\u6784\uff0c\u80fd\u591f\u5728\u65e0\u9700\u8bbf\u95ee\u4e0b\u6e38\u5206\u7c7b\u5668\u7684\u60c5\u51b5\u4e0b\u81ea\u9002\u5e94\u9009\u62e9\u6700\u4f18\u6270\u52a8\uff0c\u751f\u6210\u9488\u5bf9SCMs\u548cLLM4Code\u7684\u6709\u6548\u5bf9\u6297\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u4f20\u7edfSCMs\u751f\u6210\u7684\u5bf9\u6297\u6837\u672c\u5bf9LLM4Code\u7684\u653b\u51fb\u6210\u529f\u7387\u6700\u9ad8\u53ef\u8fbe64%\uff0c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u9ad8\u51fa15%\u4ee5\u4e0a\uff1b\u540c\u65f6\u63ed\u793a\u4e86\u4f20\u7edfSCMs\u4e0eLLM4Code\u4e4b\u95f4\u5b58\u5728\u5185\u5728\u8106\u5f31\u6027\u5173\u8054\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6e90\u4ee3\u7801\u6a21\u578b\u4e2d\u53ef\u8fc1\u79fb\u8106\u5f31\u6027\u7684\u672c\u8d28\uff0c\u4e3a\u672a\u6765\u6784\u5efa\u66f4\u9c81\u68d2\u7684AI\u9a71\u52a8\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u9632\u5fa1\u65b9\u5411\u3002"}}
{"id": "2511.08147", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08147", "abs": "https://arxiv.org/abs/2511.08147", "authors": ["Andrija Stanisic", "Stefan Nastic"], "title": "ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum", "comment": null, "summary": "Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faProbSelect\uff0c\u4e00\u79cd\u65e0\u9700\u5386\u53f2\u6570\u636e\u6216\u6301\u7eed\u76d1\u63a7\u3001\u57fa\u4e8e\u5206\u6790\u5efa\u6a21\u4e0e\u6982\u7387\u9884\u6d4b\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728GPU\u52a0\u901f\u8bbe\u5907\u4e0a\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\u5408\u89c4\u6027\u5e76\u51cf\u5c11\u8ba1\u7b97\u6d6a\u8d39\u3002", "motivation": "\u5728\u7531\u8fb9\u7f18\u3001\u4e91\u548c\u7a7a\u95f4\u8bbe\u5907\u6784\u6210\u7684\u52a8\u60013D\u8fde\u7eed\u4f53\u4e2d\uff0c\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u5386\u53f2\u6570\u636e\u548c\u6301\u7eed\u76d1\u63a7\uff0c\u5728\u9891\u7e41\u53d8\u5316\u7684\u64cd\u4f5c\u6761\u4ef6\u4e0b\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff1b\u540c\u65f6\u73b0\u6709\u65b9\u6cd5\u591a\u57fa\u4e8eCPU\u8ba1\u7b97\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u5e7f\u6cdb\u5b58\u5728\u7684GPU\u52a0\u901f\u8bad\u7ec3\u573a\u666f\u3002", "method": "\u63d0\u51faProbSelect\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5efa\u6a21\u548c\u6982\u7387\u9884\u6d4b\uff0c\u5728\u7528\u6237\u5b9a\u4e49\u7684\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\u7ea6\u675f\u4e0b\u8fdb\u884c\u5ba2\u6237\u7aef\u9009\u62e9\uff0c\u9002\u7528\u4e8eGPU\u52a0\u901f\u8bbe\u5907\uff0c\u4e14\u65e0\u9700\u5386\u53f2\u6570\u636e\u6216\u6301\u7eed\u76d1\u63a7\u3002", "result": "\u5728\u591a\u79cdGPU\u67b6\u6784\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cProbSelect\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u534713.77%\u7684SLO\u5408\u89c4\u7387\uff0c\u5e76\u51cf\u5c1172.5%\u7684\u8ba1\u7b97\u6d6a\u8d39\u3002", "conclusion": "ProbSelect\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u60013D\u8fde\u7eed\u4f53\u4e2dGPU\u52a0\u901f\u8054\u90a6\u5b66\u4e60\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u96be\u9898\uff0c\u5728\u4e0d\u4f9d\u8d56\u5386\u53f2\u6570\u636e\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u4e0e\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2511.08232", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.08232", "abs": "https://arxiv.org/abs/2511.08232", "authors": ["Alkid Baci", "Luke Friedrichs", "Caglar Demir", "Axel-Cyrille Ngonga Ngomo"], "title": "OWLAPY: A Pythonic Framework for OWL Ontology Engineering", "comment": null, "summary": "In this paper, we introduce OWLAPY, a comprehensive Python framework for OWL ontology engineering. OWLAPY streamlines the creation, modification, and serialization of OWL 2 ontologies. It uniquely integrates native Python-based reasoners with support for external Java reasoners, offering flexibility for users. OWLAPY facilitates multiple implementations of core ontology components and provides robust conversion capabilities between OWL class expressions and formats such as Description Logics, Manchester Syntax, and SPARQL. It also allows users to define custom workflows to leverage large language models (LLMs) in ontology generation from natural language text. OWLAPY serves as a well-tested software framework for users seeking a flexible Python library for advanced ontology engineering, including those transitioning from Java-based environments. The project is publicly available on GitHub at https://github.com/dice-group/owlapy and on the Python Package Index (PyPI) at https://pypi.org/project/owlapy/ , with over 50,000 downloads at the time of writing.", "AI": {"tldr": "OWLAPY \u662f\u4e00\u4e2a\u529f\u80fd\u5168\u9762\u7684 Python \u6846\u67b6\uff0c\u7528\u4e8e OWL \u672c\u4f53\u5de5\u7a0b\uff0c\u652f\u6301\u672c\u4f53\u7684\u521b\u5efa\u3001\u4fee\u6539\u3001\u5e8f\u5217\u5316\uff0c\u5e76\u96c6\u6210\u672c\u5730\u4e0e\u5916\u90e8\u63a8\u7406\u673a\uff0c\u517c\u5bb9\u591a\u79cd\u8868\u8fbe\u683c\u5f0f\uff0c\u8fd8\u652f\u6301\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u81ea\u7136\u8bed\u8a00\u751f\u6210\u672c\u4f53\u3002", "motivation": "\u4e3a\u6ee1\u8db3\u7528\u6237\u5728 Python \u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u7ea7 OWL \u672c\u4f53\u5de5\u7a0b\u7684\u9700\u6c42\uff0c\u7279\u522b\u662f\u4ece Java \u73af\u5883\u8fc1\u79fb\u7684\u7528\u6237\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7075\u6d3b\u3001\u6613\u7528\u4e14\u529f\u80fd\u5b8c\u6574\u7684\u5de5\u5177\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u5e76\u5b9e\u73b0 OWLAPY \u6846\u67b6\uff0c\u96c6\u6210\u539f\u751f Python \u63a8\u7406\u5668\u548c\u5916\u90e8 Java \u63a8\u7406\u5668\uff0c\u652f\u6301\u591a\u79cd\u672c\u4f53\u7ec4\u4ef6\u5b9e\u73b0\u53ca OWL \u8868\u8fbe\u5f0f\u4e0e\u63cf\u8ff0\u903b\u8f91\u3001Manchester \u8bed\u6cd5\u3001SPARQL \u7b49\u683c\u5f0f\u4e4b\u95f4\u7684\u8f6c\u6362\uff0c\u5e76\u5141\u8bb8\u81ea\u5b9a\u4e49\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u672c\u4f53\u751f\u6210\u6d41\u7a0b\u3002", "result": "OWLAPY \u6210\u529f\u5b9e\u73b0\u4e86\u5bf9 OWL 2 \u672c\u4f53\u7684\u5168\u9762\u652f\u6301\uff0c\u5177\u5907\u826f\u597d\u7684\u7075\u6d3b\u6027\u548c\u6269\u5c55\u6027\uff0c\u5df2\u5728 GitHub \u548c PyPI \u4e0a\u53d1\u5e03\u5e76\u83b7\u5f97\u8d85\u8fc7 5 \u4e07\u6b21\u4e0b\u8f7d\u3002", "conclusion": "OWLAPY \u4e3a\u672c\u4f53\u5de5\u7a0b\u5e08\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u7075\u6d3b\u4e14\u7ecf\u8fc7\u5145\u5206\u6d4b\u8bd5\u7684 Python \u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u9ad8\u7ea7\u672c\u4f53\u5de5\u7a0b\u4efb\u52a1\uff0c\u5c24\u5176\u9002\u5408\u5e0c\u671b\u4ece Java \u8fc1\u79fb\u81f3 Python \u7684\u7528\u6237\u3002"}}
{"id": "2511.08475", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08475", "abs": "https://arxiv.org/abs/2511.08475", "authors": ["Yangxiao Cai", "Ruiyin Li", "Peng Liang", "Mojtaba Shahin", "Zengyang Li"], "title": "Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale", "comment": null, "summary": "As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u9762\u5411\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08LLM-based MASs\uff09\u7684\u8bbe\u8ba1\uff0c\u5206\u6790\u4e86\u5176\u5173\u6ce8\u7684\u8d28\u91cf\u5c5e\u6027\u3001\u5e38\u7528\u8bbe\u8ba1\u6a21\u5f0f\u53ca\u8bbe\u8ba1\u52a8\u673a\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u8bbe\u8ba1\u542f\u793a\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9LLM-based MASs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7cfb\u7edf\u6027\u8bbe\u8ba1\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5bf9\u5176\u5173\u6ce8\u7684\u8d28\u91cf\u5c5e\u6027\u3001\u91c7\u7528\u7684\u8bbe\u8ba1\u6a21\u5f0f\u53ca\u8bbe\u8ba1\u52a8\u673a\u7684\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u4f5c\u8005\u6536\u96c6\u4e8694\u7bc7\u5173\u4e8eLLM-based MASs\u5e94\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u7684\u8bba\u6587\uff0c\u901a\u8fc7\u5185\u5bb9\u5206\u6790\u8bc6\u522b\u5176\u4e2d\u5173\u6ce8\u7684\u8d28\u91cf\u5c5e\u6027\u3001\u4f7f\u7528\u7684\u8bbe\u8ba1\u6a21\u5f0f\u4ee5\u53ca\u8bbe\u8ba1\u80cc\u540e\u7684\u52a8\u673a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a(1) \u4ee3\u7801\u751f\u6210\u662f\u6700\u5e38\u89c1\u7684\u5e94\u7528\u4efb\u52a1\uff1b(2) \u529f\u80fd\u9002\u7528\u6027\u662f\u6700\u53d7\u5173\u6ce8\u7684\u8d28\u91cf\u5c5e\u6027\uff1b(3) \u57fa\u4e8e\u89d2\u8272\u7684\u534f\u4f5c\u662f\u6700\u5e38\u7528\u7684\u8bbe\u8ba1\u6a21\u5f0f\uff1b(4) \u63d0\u5347\u751f\u6210\u4ee3\u7801\u8d28\u91cf\u662f\u6700\u4e3b\u8981\u7684\u8bbe\u8ba1\u52a8\u673a\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM-based MASs\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u548c\u5b9e\u8df5\u542f\u793a\uff0c\u6709\u52a9\u4e8e\u6307\u5bfc\u672a\u6765\u7cfb\u7edf\u7684\u8bbe\u8ba1\u4e0e\u4f18\u5316\u3002"}}
{"id": "2511.08222", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08222", "abs": "https://arxiv.org/abs/2511.08222", "authors": ["Serafino Cicerone", "Alessia Di Fonso", "Gabriele Di Stefano", "Alfredo Navarra"], "title": "Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin", "comment": "25 pages, 9 fugures, 2 tables", "summary": "In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.\n  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u53d7\u9650\u6761\u4ef6\u4e0b\uff08\u5982\u65e0\u6cd5\u68c0\u6d4b\u591a\u91cd\u5360\u636e\u3001\u521d\u59cb\u914d\u7f6e\u5141\u8bb8\u591a\u91cd\u5360\u636e\uff09\u7684\u56fe\u4e0a\u7fa4\u4f53\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\uff0c\u9488\u5bf9\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\u4e24\u79cd\u9876\u70b9\u4e0e\u8fb9\u4f20\u9012\u56fe\u63d0\u51fa\u4e86\u65f6\u95f4\u6700\u4f18\u7684\u805a\u96c6\u7b97\u6cd5\uff0c\u5e76\u6307\u51fa\u53ef\u80fd\u4e0d\u5b58\u5728\u9002\u7528\u4e8e\u6240\u6709\u53ef\u89e3\u60c5\u5f62\u7684\u901a\u7528\u7b97\u6cd5\u3002", "motivation": "\u7ecf\u5178\u7fa4\u4f53\u673a\u5668\u4eba\u805a\u96c6\u95ee\u9898\u901a\u5e38\u5047\u8bbe\u673a\u5668\u4eba\u80fd\u611f\u77e5\u5176\u4ed6\u673a\u5668\u4eba\u7684\u6570\u91cf\u6216\u521d\u59cb\u65e0\u591a\u91cd\u5360\u636e\uff0c\u800c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u201c\u654c\u5bf9\u201d\u7684\u8bbe\u5b9a\u4e0b\uff08\u5982\u65e0\u6cd5\u68c0\u6d4b\u591a\u91cd\u5360\u636e\u3001\u521d\u59cb\u5b58\u5728\u591a\u91cd\u5360\u636e\uff09\u662f\u5426\u4ecd\u80fd\u5b9e\u73b0\u805a\u96c6\uff0c\u7279\u522b\u662f\u5728\u9876\u70b9\u4e0e\u8fb9\u4f20\u9012\u56fe\u4e0a\u3002", "method": "\u4f5c\u8005\u5728OBLOT\u6a21\u578b\u4e0b\uff0c\u91c7\u7528\u8f6e\u8f6c\u8c03\u5ea6\u673a\u5236\uff08round-robin\uff09\uff0c\u5206\u522b\u9488\u5bf9\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\u8fd9\u4e24\u79cd\u7279\u5b9a\u7684\u9876\u70b9-\u8fb9\u4f20\u9012\u56fe\uff0c\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u5229\u7528\u56fe\u7ed3\u6784\u7279\u6027\u7684\u5206\u5e03\u5f0f\u805a\u96c6\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u65f6\u95f4\u6700\u4f18\u7684\u805a\u96c6\u7b97\u6cd5\uff0c\u5206\u522b\u9002\u7528\u4e8e\u65e0\u9650\u7f51\u683c\u548c\u8d85\u7acb\u65b9\u4f53\uff1b\u540c\u65f6\u8bc1\u660e\u4e86\u4e00\u4e9b\u57fa\u672c\u7684\u4e0d\u53ef\u80fd\u6027\u7ed3\u679c\uff0c\u5e76\u63a8\u6d4b\u4e0d\u5b58\u5728\u9002\u7528\u4e8e\u6240\u6709\u53ef\u89e3\u60c5\u5f62\u7684\u901a\u7528\u7b97\u6cd5\u3002", "conclusion": "\u5728\u6240\u8bbe\u5b9a\u7684\u5f3a\u9650\u5236\u6761\u4ef6\u4e0b\uff0c\u867d\u7136\u5bf9\u7279\u5b9a\u62d3\u6251\u7ed3\u6784\u53ef\u4ee5\u8bbe\u8ba1\u51fa\u9ad8\u6548\u805a\u96c6\u7b97\u6cd5\uff0c\u4f46\u7531\u4e8e\u7b97\u6cd5\u9ad8\u5ea6\u4f9d\u8d56\u5e95\u5c42\u56fe\u7684\u7ed3\u6784\u6027\u8d28\uff0c\u56e0\u6b64\u5f88\u53ef\u80fd\u4e0d\u5b58\u5728\u9002\u7528\u4e8e\u6240\u6709\u53ef\u89e3\u60c5\u51b5\u7684\u901a\u7528\u89e3\u6cd5\u3002"}}
{"id": "2511.08530", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.08530", "abs": "https://arxiv.org/abs/2511.08530", "authors": ["Rong Feng", "Vanisha Gupta", "Vivek Patel", "Viroopaksh Reddy Ernampati", "Suman Saha"], "title": "Can Large Language Models Simulate Symbolic Execution Output Like KLEE?", "comment": null, "summary": "Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.\n  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u6a21\u62df\u7b26\u53f7\u6267\u884c\u5de5\u5177KLEE\u8f93\u51fa\u7684\u53ef\u884c\u6027\uff0c\u91cd\u70b9\u5728\u4e8e\u8bc6\u522b\u7a0b\u5e8f\u4e2d\u6700\u53d7\u7ea6\u675f\u7684\u8def\u5f84\u3002\u5728100\u4e2aC\u7a0b\u5e8f\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPT-4o\u751f\u6210\u7c7b\u4f3cKLEE\u8f93\u51fa\u5e76\u8bc6\u522b\u6700\u590d\u6742\u8def\u5f84\u7684\u51c6\u786e\u7387\u7ea6\u4e3a20%\uff0c\u867d\u4e0d\u9ad8\uff0c\u4f46\u63ed\u793a\u4e86\u5f53\u524dLLM\u5728\u6a21\u62df\u7b26\u53f7\u6267\u884c\u65b9\u9762\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u3002", "motivation": "\u7b26\u53f7\u6267\u884c\u5de5\u5177\uff08\u5982KLEE\uff09\u5728\u5904\u7406\u5177\u6709\u5927\u91cf\u5206\u652f\u8def\u5f84\u7684\u7a0b\u5e8f\u65f6\u8ba1\u7b97\u5f00\u9500\u5de8\u5927\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u578b\u590d\u6742\u4ee3\u7801\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u6765\u6a21\u62dfKLEE\u7684\u90e8\u5206\u529f\u80fd\uff0c\u4ece\u800c\u8282\u7701\u65f6\u95f4\u548c\u8d44\u6e90\uff0c\u7279\u522b\u662f\u5728\u8bc6\u522b\u5305\u542b\u6700\u591a\u7b26\u53f7\u6761\u4ef6\u7684\u201c\u6700\u53d7\u7ea6\u675f\u8def\u5f84\u201d\u65b9\u9762\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b100\u4e2aC\u7a0b\u5e8f\u7684\u6570\u636e\u96c6\uff0c\u5229\u7528GPT-4o\u5c1d\u8bd5\u9884\u6d4bKLEE\u7684\u8f93\u51fa\uff0c\u5e76\u8bc6\u522b\u6bcf\u4e2a\u7a0b\u5e8f\u4e2d\u7b26\u53f7\u6761\u4ef6\u6700\u591a\u7684\u201c\u6700\u53d7\u7ea6\u675f\u8def\u5f84\u201d\uff0c\u4ee5\u6b64\u8bc4\u4f30LLM\u6a21\u62df\u7b26\u53f7\u6267\u884c\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cGPT-4o\u5728\u751f\u6210\u7c7b\u4f3cKLEE\u7684\u8f93\u51fa\u4ee5\u53ca\u8bc6\u522b\u6700\u53d7\u7ea6\u675f\u8def\u5f84\u65b9\u9762\u7684\u51c6\u786e\u7387\u7ea6\u4e3a20%\u3002", "conclusion": "\u5c3d\u7ba1\u51c6\u786e\u7387\u4e0d\u9ad8\uff0c\u4f46\u8be5\u521d\u6b65\u7814\u7a76\u8868\u660e\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u7b26\u53f7\u6267\u884c\u65b9\u9762\u5177\u5907\u4e00\u5b9a\u6f5c\u529b\uff0c\u540c\u65f6\u4e5f\u660e\u786e\u4e86\u5176\u73b0\u6709\u80fd\u529b\u7684\u8fb9\u754c\u3002"}}
{"id": "2511.08373", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.08373", "abs": "https://arxiv.org/abs/2511.08373", "authors": ["Henrik Daniel Christensen", "Saverio Giallorenzo", "Jacopo Mauro"], "title": "Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing", "comment": null, "summary": "Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.\n  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\\% of scenarios. With a 10-second window, our approach improves placements in over 73\\% and still certifies that the default scheduler's placement is already optimal in over 19\\% of scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u89c4\u5212\u7684Kubernetes\u8c03\u5ea6\u63d2\u4ef6\uff0c\u4f5c\u4e3a\u9ed8\u8ba4\u8c03\u5ea6\u5668\u7684\u540e\u5907\u673a\u5236\uff0c\u5728\u9ed8\u8ba4\u8c03\u5ea6\u5931\u8d25\u65f6\u4f18\u5316Pod\u5206\u914d\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u57281\u79d2\u548c10\u79d2\u8c03\u5ea6\u7a97\u53e3\u5185\uff0c\u8be5\u65b9\u6cd5\u5206\u522b\u572844%\u548c73%\u4ee5\u4e0a\u7684\u573a\u666f\u4e2d\u4f18\u4e8e\u9ed8\u8ba4\u8c03\u5ea6\u5668\uff0c\u5e76\u80fd\u5728\u7ea619%\u7684\u573a\u666f\u4e2d\u9a8c\u8bc1\u9ed8\u8ba4\u8c03\u5ea6\u7ed3\u679c\u5df2\u662f\u6700\u4f18\u3002", "motivation": "Kubernetes\u9ed8\u8ba4\u8c03\u5ea6\u5668\u4f7f\u7528\u8f7b\u91cf\u7ea7\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7684Pod\u653e\u7f6e\u548c\u8d44\u6e90\u788e\u7247\u5316\uff0c\u4ece\u800c\u65e0\u6cd5\u90e8\u7f72\u672c\u53ef\u8fd0\u884c\u7684Pod\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u5f15\u5165\u66f4\u7cbe\u786e\u7684\u4f18\u5316\u65b9\u6cd5\u4ee5\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u548c\u9ad8\u4f18\u5148\u7ea7Pod\u7684\u8c03\u5ea6\u6210\u529f\u7387\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u7ea6\u675f\u89c4\u5212\u65b9\u6cd5\uff0c\u5229\u7528OR-Tools\u7ea6\u675f\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u4e00\u4e2aKubernetes\u8c03\u5ea6\u63d2\u4ef6\u3002\u8be5\u63d2\u4ef6\u5728\u9ed8\u8ba4\u8c03\u5ea6\u5668\u65e0\u6cd5\u5206\u914dPod\u65f6\u4ecb\u5165\uff0c\u5c1d\u8bd5\u5bfb\u627e\u6ee1\u8db3\u6240\u6709\u4f18\u5148\u7ea7\u548c\u8d44\u6e90\u9700\u6c42\u7684\u6700\u4f18Pod\u5206\u914d\u65b9\u6848\u3002", "result": "\u5728\u5c0f\u5230\u4e2d\u578b\u96c6\u7fa4\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u57281\u79d2\u8c03\u5ea6\u7a97\u53e3\u5185\uff0c\u8be5\u65b9\u6cd5\u5728\u8d85\u8fc744%\u7684\u53ef\u5b9e\u73b0\u573a\u666f\u4e2d\u6bd4\u9ed8\u8ba4\u8c03\u5ea6\u5668\u653e\u7f6e\u66f4\u591a\u9ad8\u4f18\u5148\u7ea7Pod\uff1b\u572810\u79d2\u7a97\u53e3\u5185\uff0c\u8fd9\u4e00\u6bd4\u4f8b\u63d0\u5347\u81f373%\u4ee5\u4e0a\u3002\u6b64\u5916\uff0c\u5728\u7ea619%\u7684\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u8bc1\u660e\u9ed8\u8ba4\u8c03\u5ea6\u5668\u7684\u7ed3\u679c\u5df2\u662f\u6700\u4f18\u3002", "conclusion": "\u5c06\u7ea6\u675f\u89c4\u5212\u4f5c\u4e3aKubernetes\u8c03\u5ea6\u7684\u540e\u5907\u673a\u5236\uff0c\u80fd\u591f\u5728\u5408\u7406\u65f6\u95f4\u5185\u663e\u8457\u63d0\u5347\u8c03\u5ea6\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u9ed8\u8ba4\u8c03\u5ea6\u5668\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u51cf\u5c11\u8d44\u6e90\u788e\u7247\u5e76\u63d0\u9ad8\u9ad8\u4f18\u5148\u7ea7\u5de5\u4f5c\u8d1f\u8f7d\u7684\u90e8\u7f72\u6210\u529f\u7387\u3002"}}
{"id": "2511.07886", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2511.07886", "abs": "https://arxiv.org/abs/2511.07886", "authors": ["Dechuang Chen", "Sibo Wang", "Qintian Guo"], "title": "ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework", "comment": "Accepted by SIGMOD'26", "summary": "Graphs are a ubiquitous data structure in diverse domains such as machine learning, social networks, and data mining. As real-world graphs continue to grow beyond the memory capacity of single machines, out-of-core graph processing systems have emerged as a viable solution. Yet, existing systems that rely on strictly synchronous, iteration-by-iteration execution incur significant overheads. In particular, their scheduling mechanisms lead to I/O inefficiencies, stemming from read and work amplification, and induce costly synchronization stalls hindering sustained disk utilization. To overcome these limitations, we present {\\em ACGraph}, a novel asynchronous graph processing system optimized for SSD-based environments with constrained memory resources. ACGraph employs a dynamic, block-centric priority scheduler that adjusts in real time based on workload, along with an online asynchronous worklist that minimizes redundant disk accesses by efficiently reusing active blocks in memory. Moreover, ACGraph unifies asynchronous I/O with computation in a pipelined execution model that maintains sustained I/O activation, and leverages a highly optimized hybrid storage format to expedite access to low-degree vertices. We implement popular graph algorithms, such as Breadth-First Search (BFS), Weakly Connected Components (WCC), personalized PageRank (PPR), PageRank (PR), and $k$-core on ACGraph and demonstrate that ACGraph substantially outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency.", "AI": {"tldr": "ACGraph \u662f\u4e00\u79cd\u9762\u5411 SSD \u4e14\u5185\u5b58\u53d7\u9650\u73af\u5883\u7684\u65b0\u578b\u5f02\u6b65\u56fe\u5904\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u5757\u4f18\u5148\u7ea7\u8c03\u5ea6\u3001\u5f02\u6b65\u5de5\u4f5c\u5217\u8868\u3001\u8ba1\u7b97\u4e0e I/O \u6d41\u6c34\u7ebf\u4ee5\u53ca\u6df7\u5408\u5b58\u50a8\u683c\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u884c\u901f\u5ea6\u548c I/O \u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e25\u683c\u540c\u6b65\u8fed\u4ee3\u6267\u884c\u7684\u5916\u5b58\u56fe\u5904\u7406\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u7684 I/O \u4f4e\u6548\u95ee\u9898\uff0c\u5305\u62ec\u8bfb\u53d6\u4e0e\u8ba1\u7b97\u653e\u5927\u4ee5\u53ca\u540c\u6b65\u7b49\u5f85\u9020\u6210\u7684\u78c1\u76d8\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u56fe\u6570\u636e\u5904\u7406\u9700\u6c42\u3002", "method": "ACGraph \u5f15\u5165\u52a8\u6001\u5757\u4e2d\u5fc3\u4f18\u5148\u7ea7\u8c03\u5ea6\u5668\u3001\u5728\u7ebf\u5f02\u6b65\u5de5\u4f5c\u5217\u8868\u4ee5\u51cf\u5c11\u5197\u4f59\u78c1\u76d8\u8bbf\u95ee\uff0c\u91c7\u7528\u8ba1\u7b97\u4e0e\u5f02\u6b65 I/O \u6d41\u6c34\u7ebf\u6267\u884c\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4f18\u5316\u7684\u6df7\u5408\u5b58\u50a8\u683c\u5f0f\u52a0\u901f\u4f4e\u5ea6\u9876\u70b9\u8bbf\u95ee\u3002", "result": "\u5728 BFS\u3001WCC\u3001PPR\u3001PR \u548c k-core \u7b49\u5178\u578b\u56fe\u7b97\u6cd5\u4e0a\uff0cACGraph \u5728\u8fd0\u884c\u65f6\u95f4\u548c I/O \u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5916\u5b58\u56fe\u5904\u7406\u7cfb\u7edf\u3002", "conclusion": "ACGraph \u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5916\u5b58\u56fe\u5904\u7406\u7cfb\u7edf\u4e2d\u7684 I/O \u4e0e\u540c\u6b65\u74f6\u9888\uff0c\u4e3a\u5185\u5b58\u53d7\u9650\u7684\u5927\u89c4\u6a21\u56fe\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u65b0\u65b9\u6848\u3002"}}
