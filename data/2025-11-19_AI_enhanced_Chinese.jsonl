{"id": "2511.13950", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13950", "abs": "https://arxiv.org/abs/2511.13950", "authors": ["Lei Zhao", "Luca Buonanno", "Archit Gajjar", "John Moon", "Aishwarya Natarajan", "Sergey Serebryakov", "Ron M. Roth", "Xia Sheng", "Youtao Zhang", "Paolo Faraboschi", "Jim Ignowski", "Giacomo Pedretti"], "title": "NL-DPE: An Analog In-memory Non-Linear Dot Product Engine for Efficient CNN and LLM Inference", "comment": null, "summary": "Resistive Random Access Memory (RRAM) based in-memory computing (IMC) accelerators offer significant performance and energy advantages for deep neural networks (DNNs), but face three major limitations: (1) they support only \\textit{static} dot-product operations and cannot accelerate arbitrary non-linear functions or data-dependent multiplications essential to modern LLMs; (2) they demand large, power-hungry analog-to-digital converter (ADC) circuits; and (3) mapping model weights to device conductance introduces errors from cell nonidealities. These challenges hinder scalable and accurate IMC acceleration as models grow.\n  We propose NL-DPE, a Non-Linear Dot Product Engine that overcomes these barriers. NL-DPE augments crosspoint arrays with RRAM-based Analog Content Addressable Memory (ACAM) to execute arbitrary non-linear functions and data-dependent matrix multiplications in the analog domain by transforming them into decision trees, fully eliminating ADCs. To address device noise, NL-DPE uses software-based Noise Aware Fine-tuning (NAF), requiring no in-device calibration. Experiments show that NL-DPE delivers 28X energy efficiency and 249X speedup over a GPU baseline, and 22X energy efficiency and 245X speedup over existing IMC accelerators, while maintaining high accuracy.", "AI": {"tldr": "NL-DPE\u662f\u4e00\u79cd\u65b0\u578b\u7684\u57fa\u4e8eRRAM\u7684\u5b58\u5185\u8ba1\u7b97\u5f15\u64ce\uff0c\u901a\u8fc7\u5f15\u5165\u6a21\u62df\u5185\u5bb9\u53ef\u5bfb\u5740\u5b58\u50a8\u5668\uff08ACAM\uff09\u548c\u566a\u58f0\u611f\u77e5\u5fae\u8c03\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u975e\u7ebf\u6027\u51fd\u6570\u4e0e\u6570\u636e\u4f9d\u8d56\u4e58\u6cd5\u7684\u9ad8\u6548\u6a21\u62df\u57df\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u4e0e\u901f\u5ea6\uff0c\u540c\u65f6\u907f\u514d\u4e86\u4f20\u7edfIMC\u67b6\u6784\u4e2d\u7684ADC\u5f00\u9500\u4e0e\u5668\u4ef6\u975e\u7406\u60f3\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eRRAM\u7684\u5b58\u5185\u8ba1\u7b97\u52a0\u901f\u5668\u5b58\u5728\u4e09\u5927\u5c40\u9650\uff1a\u4ec5\u652f\u6301\u9759\u6001\u70b9\u79ef\u8fd0\u7b97\u3001\u9700\u8981\u9ad8\u529f\u8017ADC\u7535\u8def\u3001\u4ee5\u53ca\u56e0\u5668\u4ef6\u975e\u7406\u60f3\u6027\u5bfc\u81f4\u6743\u91cd\u6620\u5c04\u8bef\u5dee\uff0c\u8fd9\u4e9b\u95ee\u9898\u9650\u5236\u4e86\u5176\u5728\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u7cbe\u5ea6\u3002", "method": "NL-DPE\u901a\u8fc7\u5728\u4ea4\u53c9\u9635\u5217\u4e2d\u96c6\u6210RRAM-based ACAM\uff0c\u5c06\u4efb\u610f\u975e\u7ebf\u6027\u51fd\u6570\u548c\u6570\u636e\u4f9d\u8d56\u77e9\u9635\u4e58\u6cd5\u8f6c\u5316\u4e3a\u51b3\u7b56\u6811\u5f62\u5f0f\uff0c\u5728\u6a21\u62df\u57df\u6267\u884c\uff1b\u540c\u65f6\u91c7\u7528\u65e0\u9700\u7247\u4e0a\u6821\u51c6\u7684\u8f6f\u4ef6\u7ea7\u566a\u58f0\u611f\u77e5\u5fae\u8c03\uff08NAF\uff09\u65b9\u6cd5\u6765\u7f13\u89e3\u5668\u4ef6\u566a\u58f0\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cNL-DPE\u76f8\u6bd4GPU\u57fa\u7ebf\u5b9e\u73b028\u500d\u80fd\u6548\u63d0\u5347\u548c249\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u73b0\u6709IMC\u52a0\u901f\u5668\u5b9e\u73b022\u500d\u80fd\u6548\u63d0\u5347\u548c245\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "NL-DPE\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfRRAM IMC\u5728\u529f\u80fd\u7075\u6d3b\u6027\u3001\u80fd\u6548\u548c\u7cbe\u5ea6\u65b9\u9762\u7684\u5173\u952e\u74f6\u9888\uff0c\u4e3a\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u5b58\u5185\u8ba1\u7b97\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.13738", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.13738", "abs": "https://arxiv.org/abs/2511.13738", "authors": ["Hyunseok Kwak", "Kyeongwon Lee", "Kyeongpil Min", "Chaebin Jung", "Woojoo Lee"], "title": "TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI", "comment": "8 pages, 6 figures, 4 Tables, DATE 2026 accepted paper", "summary": "The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86TT-Edge\uff0c\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u8bbe\u5907\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5f20\u91cf\u5217\u5206\u89e3\uff08TTD\uff09\u4e2d\u7684\u5173\u952e\u8ba1\u7b97\u4efb\u52a1\u5378\u8f7d\u5230\u4e13\u7528\u786c\u4ef6\u5f15\u64ce\uff0c\u5728\u4fdd\u6301\u9ad8\u6a21\u578b\u538b\u7f29\u7387\u548c\u4f4e\u7cbe\u5ea6\u635f\u5931\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86TTD\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u80fd\u6548\u4e0e\u901f\u5ea6\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u5206\u5e03\u5f0f\u5b66\u4e60\u65f6\uff0c\u9ad8\u6548\u7684\u8bbe\u5907\u7aef\u6a21\u578b\u538b\u7f29\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u5f20\u91cf\u5217\u5206\u89e3\uff08TTD\uff09\u80fd\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u4e14\u7cbe\u5ea6\u635f\u5931\u5c0f\uff0c\u4f46\u5176\u4f9d\u8d56\u7684\u91cd\u590d\u5947\u5f02\u503c\u5206\u89e3\uff08SVD\uff09\u548c\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c\u5728\u4f4e\u529f\u8017\u5904\u7406\u5668\u4e0a\u5e26\u6765\u663e\u8457\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u5f00\u9500\u3002", "method": "TT-Edge\u5c06SVD\u5206\u4e3a\u53cc\u5bf9\u89d2\u5316\u548c\u5bf9\u89d2\u5316\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u5c06\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\u5378\u8f7d\u81f3\u4e13\u95e8\u8bbe\u8ba1\u7684TTD\u5f15\u64ce\uff1b\u8be5\u5f15\u64ce\u4e0e\u73b0\u6709GEMM\u52a0\u901f\u5668\u7d27\u5bc6\u96c6\u6210\uff0c\u51cf\u5c11\u9891\u7e41\u7684\u77e9\u9635-\u5411\u91cf\u6570\u636e\u4f20\u8f93\u3002\u6574\u4f53\u65b9\u6848\u57fa\u4e8eRISC-V\u8fb9\u7f18AI\u5904\u7406\u5668\u5b9e\u73b0\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u590d\u7528GEMM\u8d44\u6e90\u5e76\u5171\u4eab\u6d6e\u70b9\u5355\u5143\u3002", "result": "\u5728ResNet-32\u6a21\u578b\u7684TTD\u538b\u7f29\u4efb\u52a1\u4e2d\uff0cTT-Edge\u76f8\u6bd4\u4ec5\u4f7f\u7528GEMM\u7684\u57fa\u7ebf\u65b9\u6848\u5b9e\u73b0\u4e861.7\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c40.2%\u7684\u80fd\u8017\u964d\u4f4e\uff0c\u4ec5\u589e\u52a04%\u7684\u603b\u529f\u8017\u548c\u6781\u5c0f\u7684\u786c\u4ef6\u5f00\u9500\u3002FPGA\u539f\u578b\u548c45nm\u5de5\u827a\u4e0b\u7684\u529f\u8017\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "TT-Edge\u6709\u6548\u7f13\u89e3\u4e86TTD\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5e94\u7528\u65f6\u7684\u5ef6\u8fdf\u4e0e\u80fd\u8017\u74f6\u9888\uff0c\u5c55\u793a\u4e86\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u5728\u9ad8\u6548\u6a21\u578b\u538b\u7f29\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.13723", "categories": ["cs.CE", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2511.13723", "abs": "https://arxiv.org/abs/2511.13723", "authors": ["Abhishek Arora", "Caglar Oskay"], "title": "Variational multiscale enrichment method for dynamic response of hyperelastic materials at finite deformation", "comment": null, "summary": "In this manuscript, we extend the variational multiscale enrichment (VME) method to model the dynamic response of hyperelastic materials undergoing large deformations. This approach enables the simulation of wave propagation under scale-inseparable conditions, including short-wavelength regimes, while accounting for material and geometric nonlinearities that lead to wave steepening or flattening. By employing an additive decomposition of the displacement field, we derive multiscale governing equations for the coarse- and fine-scale problems, which naturally incorporate micro-inertial effects. The framework allows the discretization of each unit cell with a patch of coarse-scale elements, which is essential to accurately capture wave propagation in short-wavelength regimes. An operator-split procedure is used to iteratively solve the semi-discrete equations at both scales until convergence is achieved. The coarse-scale problem is integrated explicitly, while the fine-scale problem is solved using either explicit or implicit time integration schemes, including both dissipative and non-dissipative methods. Numerical examples demonstrate that multiscale dissipative schemes effectively suppress spurious oscillations. The multiscale framework was applied to investigate how material and geometric nonlinearities, along with elastic stiffness contrast in heterogeneous microstructures, influence key wave characteristics such as dispersion, attenuation, and steepening. This multiscale computational framework provides a foundation for studying the dynamic response of architected materials.", "AI": {"tldr": "\u672c\u6587\u5c06\u53d8\u5206\u591a\u5c3a\u5ea6\u589e\u5f3a\uff08VME\uff09\u65b9\u6cd5\u6269\u5c55\u81f3\u8d85\u5f39\u6027\u6750\u6599\u5927\u53d8\u5f62\u4e0b\u7684\u52a8\u6001\u54cd\u5e94\u6a21\u62df\uff0c\u80fd\u591f\u5904\u7406\u6ce2\u4f20\u64ad\u4e2d\u7684\u5c3a\u5ea6\u4e0d\u53ef\u5206\u95ee\u9898\uff0c\u5e76\u8003\u8651\u6750\u6599\u4e0e\u51e0\u4f55\u975e\u7ebf\u6027\u5bf9\u6ce2\u5f62\u6f14\u5316\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u62df\u5177\u6709\u5f3a\u975e\u7ebf\u6027\u548c\u77ed\u6ce2\u957f\u7279\u5f81\u7684\u8d85\u5f39\u6027\u6750\u6599\u52a8\u6001\u54cd\u5e94\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u540c\u65f6\u6355\u6349\u5b8f\u89c2\u4e0e\u5fae\u89c2\u52a8\u529b\u5b66\u884c\u4e3a\u7684\u591a\u5c3a\u5ea6\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u91c7\u7528\u4f4d\u79fb\u573a\u7684\u52a0\u6027\u5206\u89e3\uff0c\u63a8\u5bfc\u51fa\u5305\u542b\u5fae\u60ef\u6027\u6548\u5e94\u7684\u7c97\u7ec6\u5c3a\u5ea6\u63a7\u5236\u65b9\u7a0b\uff1b\u901a\u8fc7\u7b97\u5b50\u5206\u88c2\u6cd5\u8fed\u4ee3\u6c42\u89e3\u534a\u79bb\u6563\u65b9\u7a0b\uff0c\u7c97\u5c3a\u5ea6\u663e\u5f0f\u79ef\u5206\uff0c\u7ec6\u5c3a\u5ea6\u53ef\u9009\u663e\u5f0f\u6216\u9690\u5f0f\u65f6\u95f4\u79ef\u5206\u65b9\u6848\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u591a\u5c3a\u5ea6\u8017\u6563\u683c\u5f0f\u80fd\u6709\u6548\u6291\u5236\u865a\u5047\u632f\u8361\uff0c\u5e76\u63ed\u793a\u4e86\u6750\u6599/\u51e0\u4f55\u975e\u7ebf\u6027\u53ca\u5f02\u8d28\u5fae\u7ed3\u6784\u521a\u5ea6\u5dee\u5f02\u5bf9\u6ce2\u7684\u8272\u6563\u3001\u8870\u51cf\u548c\u9661\u5316\u7b49\u7279\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u591a\u5c3a\u5ea6\u8ba1\u7b97\u6846\u67b6\u4e3a\u7814\u7a76\u5177\u6709\u590d\u6742\u5fae\u7ed3\u6784\u7684\u8d85\u5f39\u6027\u6750\u6599\u5728\u52a8\u6001\u8f7d\u8377\u4e0b\u7684\u54cd\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u77ed\u6ce2\u957f\u548c\u5f3a\u975e\u7ebf\u6027\u60c5\u5f62\u3002"}}
{"id": "2511.13751", "categories": ["cs.DC", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.13751", "abs": "https://arxiv.org/abs/2511.13751", "authors": ["Shinnung Jeong", "Chihyo Ahn", "Huanzhi Pu", "Jisheng Zhao", "Hyesoon Kim", "Blaise Tine"], "title": "Inside VOLT: Designing an Open-Source GPU Compiler", "comment": "11 pages, 10 figures, two tables, two algorithms", "summary": "Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.\n  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Vortex\u4f18\u5316\u8f7b\u91cf\u7ea7\u5de5\u5177\u94fe\uff08VOLT\uff09\uff0c\u7528\u4e8e\u652f\u6301\u5f00\u6e90GPU\u67b6\u6784\u4e0a\u7684SIMT\u4ee3\u7801\u751f\u6210\u4e0e\u4f18\u5316\uff0c\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u5b9e\u73b0\u591a\u524d\u7aef\u8bed\u8a00\u517c\u5bb9\u548c\u4e2d\u95f4\u7aef\u6838\u5fc3\u4f18\u5316\u590d\u7528\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728ISA\u6269\u5c55\u548c\u4e3b\u673a\u8fd0\u884c\u65f6API\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5f00\u6e90GPU\u7814\u7a76\u867d\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u3001\u53ef\u590d\u7528\u7684\u7f16\u8bd1\u5668\u6846\u67b6\u6765\u652f\u6301\u73b0\u6709GPU\u7a0b\u5e8f\u5728\u65b0\u578b\u5f00\u6e90SIMT ISA\u4e0a\u7684\u6267\u884c\u4e0e\u6027\u80fd\u4f18\u5316\uff0c\u800c\u6784\u5efa\u6b64\u7c7b\u6846\u67b6\u6280\u672f\u590d\u6742\u4e14\u5e38\u88ab\u4f4e\u4f30\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86VOLT\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff0c\u5c06\u5173\u952e\u7684SIMT\u76f8\u5173\u5206\u6790\u4e0e\u4f18\u5316\u96c6\u4e2d\u4e8e\u4e2d\u95f4\u7aef\uff0c\u4ee5\u652f\u6301\u591a\u79cd\u524d\u7aef\u8bed\u8a00\u548c\u4e0d\u65ad\u6f14\u5316\u7684\u5f00\u6e90GPU\u786c\u4ef6\u3002", "result": "VOLT\u6210\u529f\u652f\u6301\u4e86SIMT\u4ee3\u7801\u5728\u591a\u4e2a\u62bd\u8c61\u5c42\u7ea7\u4e0a\u7684\u751f\u6210\u4e0e\u4f18\u5316\uff0c\u5e76\u901a\u8fc7ISA\u6269\u5c55\u548c\u4e3b\u673a\u8fd0\u884c\u65f6API\u4e24\u4e2a\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "VOLT\u4e3a\u5f00\u6e90GPU\u751f\u6001\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u7f16\u8bd1\u5668\u57fa\u7840\u8bbe\u65bd\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u5f00\u6e90GPU\u8f6f\u786c\u4ef6\u534f\u540c\u5f00\u53d1\u7684\u95e8\u69db\u3002"}}
{"id": "2511.13905", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2511.13905", "abs": "https://arxiv.org/abs/2511.13905", "authors": ["Amin Heyrani Nobari", "Faez Ahmed"], "title": "PGD-TO: A Scalable Alternative to MMA Using Projected Gradient Descent for Multi-Constraint Topology Optimization", "comment": null, "summary": "Projected Gradient Descent (PGD) methods offer a simple and scalable approach to topology optimization (TO), yet they often struggle with nonlinear and multi-constraint problems due to the complexity of active-set detection. This paper introduces PGD-TO, a framework that reformulates the projection step into a regularized convex quadratic problem, eliminating the need for active-set search and ensuring well-posedness even when constraints are infeasible. The framework employs a semismooth Newton solver for general multi-constraint cases and a binary search projection for single or independent constraints, achieving fast and reliable convergence. It further integrates spectral step-size adaptation and nonlinear conjugate-gradient directions for improved stability and efficiency. We evaluate PGD-TO on four benchmark families representing the breadth of TO problems: (i) minimum compliance with a linear volume constraint, (ii) minimum volume under a nonlinear compliance constraint, (iii) multi-material minimum compliance with four independent volume constraints, and (iv) minimum compliance with coupled volume and center-of-mass constraints. Across these single- and multi-constraint, linear and nonlinear cases, PGD-TO achieves convergence and final compliance comparable to the Method of Moving Asymptotes (MMA) and Optimality Criteria (OC), while reducing per-iteration computation time by 10-43x on general problems and 115-312x when constraints are independent. Overall, PGD-TO establishes a fast, robust, and scalable alternative to MMA, advancing topology optimization toward practical large-scale, multi-constraint, and nonlinear design problems. Public code available at: https://github.com/ahnobari/pyFANTOM", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PGD-TO\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6295\u5f71\u6b65\u9aa4\u91cd\u6784\u4e3a\u6b63\u5219\u5316\u51f8\u4e8c\u6b21\u95ee\u9898\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u6cd5\u5728\u62d3\u6251\u4f18\u5316\u4e2d\u5904\u7406\u591a\u7ea6\u675f\u548c\u975e\u7ebf\u6027\u95ee\u9898\u65f6\u7684\u6d3b\u8dc3\u96c6\u68c0\u6d4b\u96be\u9898\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u534a\u5149\u6ed1\u725b\u987f\u6c42\u89e3\u5668\u3001\u4e8c\u5206\u641c\u7d22\u6295\u5f71\u3001\u8c31\u6b65\u957f\u81ea\u9002\u5e94\u4e0e\u975e\u7ebf\u6027\u5171\u8f6d\u68af\u5ea6\u65b9\u5411\uff0c\u5728\u591a\u79cd\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u3001\u5355\u7ea6\u675f\u4e0e\u591a\u7ea6\u675f\u57fa\u51c6\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u4e0eMMA\u548cOC\u76f8\u5f53\u7684\u6536\u655b\u6027\u548c\u67d4\u5ea6\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u6bcf\u8f6e\u8fed\u4ee3\u7684\u8ba1\u7b97\u65f6\u95f4\uff08\u4e00\u822c\u95ee\u9898\u5feb10\u201343\u500d\uff0c\u72ec\u7acb\u7ea6\u675f\u95ee\u9898\u5feb115\u2013312\u500d\uff09\uff0c\u4e3a\u5927\u89c4\u6a21\u62d3\u6251\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u65b0\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\uff08PGD\uff09\u65b9\u6cd5\u5728\u5904\u7406\u62d3\u6251\u4f18\u5316\u4e2d\u7684\u975e\u7ebf\u6027\u548c\u591a\u7ea6\u675f\u95ee\u9898\u65f6\uff0c\u56e0\u6d3b\u8dc3\u96c6\u68c0\u6d4b\u590d\u6742\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u7ea6\u675f\u4e0d\u53ef\u884c\u65f6\u7f3a\u4e4f\u826f\u597d\u5b9a\u4e49\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65e0\u9700\u6d3b\u8dc3\u96c6\u641c\u7d22\u3001\u80fd\u7a33\u5b9a\u5904\u7406\u5404\u7c7b\u7ea6\u675f\u5e76\u4fdd\u6301\u9ad8\u6548\u8ba1\u7b97\u7684\u4f18\u5316\u6846\u67b6\u3002", "method": "PGD-TO\u5c06\u6295\u5f71\u6b65\u9aa4\u8f6c\u5316\u4e3a\u6b63\u5219\u5316\u51f8\u4e8c\u6b21\u95ee\u9898\uff0c\u6d88\u9664\u5bf9\u6d3b\u8dc3\u96c6\u641c\u7d22\u7684\u4f9d\u8d56\uff1b\u5bf9\u4e00\u822c\u591a\u7ea6\u675f\u60c5\u5f62\u91c7\u7528\u534a\u5149\u6ed1\u725b\u987f\u6c42\u89e3\u5668\uff0c\u5bf9\u5355\u7ea6\u675f\u6216\u72ec\u7acb\u7ea6\u675f\u4f7f\u7528\u4e8c\u5206\u641c\u7d22\u6295\u5f71\uff1b\u5e76\u5f15\u5165\u8c31\u6b65\u957f\u81ea\u9002\u5e94\u548c\u975e\u7ebf\u6027\u5171\u8f6d\u68af\u5ea6\u65b9\u5411\u4ee5\u63d0\u5347\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "result": "\u5728\u56db\u7c7b\u4ee3\u8868\u6027\u62d3\u6251\u4f18\u5316\u57fa\u51c6\u95ee\u9898\uff08\u5305\u62ec\u7ebf\u6027/\u975e\u7ebf\u6027\u3001\u5355/\u591a\u7ea6\u675f\uff09\u4e0a\uff0cPGD-TO\u5b9e\u73b0\u4e86\u4e0eMMA\u548cOC\u65b9\u6cd5\u76f8\u5f53\u7684\u6536\u655b\u6027\u548c\u6700\u7ec8\u67d4\u5ea6\uff0c\u540c\u65f6\u6bcf\u8f6e\u8fed\u4ee3\u8ba1\u7b97\u65f6\u95f4\u5927\u5e45\u51cf\u5c11\uff1a\u4e00\u822c\u95ee\u9898\u5feb10\u201343\u500d\uff0c\u72ec\u7acb\u7ea6\u675f\u95ee\u9898\u5feb115\u2013312\u500d\u3002", "conclusion": "PGD-TO\u662f\u4e00\u79cd\u5feb\u901f\u3001\u7a33\u5065\u4e14\u53ef\u6269\u5c55\u7684\u62d3\u6251\u4f18\u5316\u65b9\u6cd5\uff0c\u6709\u6548\u514b\u670d\u4e86\u4f20\u7edfPGD\u5728\u591a\u7ea6\u675f\u548c\u975e\u7ebf\u6027\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u9645\u5927\u89c4\u6a21\u590d\u6742\u8bbe\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2511.14457", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2511.14457", "abs": "https://arxiv.org/abs/2511.14457", "authors": ["Michael Gundall", "Jan Herbst", "Robin M\u00fcller", "Hans D. Schotten"], "title": "Benchmarking OpenWiFiSync on ESP32: Towards Cost-Effective Wireless Time Synchronization", "comment": null, "summary": "Wireless time synchronization of mobile devices is a key enabler for numerous Industry 4.0 applications, such as coordinated and synchronized tasks or the generation of high-precision timestamps for machine learning or artificial intelligence algorithms. Traditional wireline clock synchronization protocols, however, cannot achieve the performance in wireless environments without significant modifications. To address this challenge, we make use of the Reference Broadcast Infrastructure Synchronization protocol, which leverages the broadcast nature of wireless communications and remains both non-invasive and standard-compliant. We implement and validate this protocol on a low-cost testbed using ESP32 modules and a commercial Wi-Fi access point. To support further research and development, we release our implementation as open-source software under the GNU General Public License Version 3 license via the OpenWifiSync project on GitHub.\n  Our results demonstrate that synchronization accuracies within +/-30 microseconds are achievable using energy-efficient and affordable hardware, making this approach suitable for a wide range of use cases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u7ebf\u5e7f\u64ad\u7684\u4f4e\u6210\u672c\u65f6\u95f4\u540c\u6b65\u534f\u8bae\uff0c\u53ef\u5728ESP32\u7b49\u4f4e\u529f\u8017\u786c\u4ef6\u4e0a\u5b9e\u73b0\u00b130\u5fae\u79d2\u7684\u540c\u6b65\u7cbe\u5ea6\u3002", "motivation": "\u5de5\u4e1a4.0\u5e94\u7528\u9700\u8981\u79fb\u52a8\u8bbe\u5907\u95f4\u9ad8\u7cbe\u5ea6\u65e0\u7ebf\u65f6\u95f4\u540c\u6b65\uff0c\u800c\u4f20\u7edf\u6709\u7ebf\u540c\u6b65\u534f\u8bae\u5728\u65e0\u7ebf\u73af\u5883\u4e2d\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u53c2\u8003\u5e7f\u64ad\u57fa\u7840\u8bbe\u65bd\u540c\u6b65\u534f\u8bae\uff08RBIS\uff09\uff0c\u5229\u7528\u65e0\u7ebf\u901a\u4fe1\u7684\u5e7f\u64ad\u7279\u6027\uff0c\u5728ESP32\u6a21\u5757\u548c\u5546\u7528Wi-Fi\u63a5\u5165\u70b9\u6784\u6210\u7684\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u5b9e\u73b0\uff0c\u5e76\u5f00\u6e90\u53d1\u5e03\u3002", "result": "\u5728\u4f4e\u6210\u672c\u3001\u4f4e\u529f\u8017\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u00b130\u5fae\u79d2\u7684\u65f6\u95f4\u540c\u6b65\u7cbe\u5ea6\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u5177\u6709\u975e\u4fb5\u5165\u6027\u3001\u6807\u51c6\u517c\u5bb9\u6027\u53ca\u9ad8\u6027\u4ef7\u6bd4\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5de5\u4e1a4.0\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2511.13727", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13727", "abs": "https://arxiv.org/abs/2511.13727", "authors": ["Sophie Wenning"], "title": "Boosting performance: Gradient Clock Synchronisation with two-way measured links", "comment": "Master's thesis", "summary": "This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (F\u00fcgger et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\\% to 0,1\\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.", "AI": {"tldr": "\u8be5\u7855\u58eb\u8bba\u6587\u5c06GCS\u7b97\u6cd5\u7684\u73b0\u6709\u5f62\u5f0f\u5316\u6a21\u578b\u4ece\u5355\u5411\u6d4b\u91cf\u8303\u5f0f\u6269\u5c55\u4e3a\u66f4\u8d34\u8fd1\u5b9e\u9645\u5b9e\u73b0\u7684\u53cc\u5411\u6d4b\u91cf\u8303\u5f0f\uff0c\u5728\u4fdd\u7559GCS\u6838\u5fc3\u884c\u4e3a\u7684\u540c\u65f6\uff0c\u663e\u8457\u653e\u5bbd\u4e86\u5148\u524d\u7684\u9650\u5236\u6761\u4ef6\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u5148\u524d\u5173\u4e8eGCS\u7b97\u6cd5\u7684\u7814\u7a76\u57fa\u4e8e\u5355\u5411\u6d4b\u91cf\u8303\u5f0f\uff0c\u5f15\u5165\u4e86\u8bf8\u591a\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\uff08\u5982\u5355\u4f4d\u94fe\u8def\u957f\u5ea6\uff09\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9002\u7528\u6027\uff1b\u672c\u6587\u65e8\u5728\u901a\u8fc7\u91c7\u7528\u53cc\u5411\u6d4b\u91cf\u8303\u5f0f\u6784\u5efa\u66f4\u8d34\u8fd1\u5b9e\u73b0\u7684\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u7b97\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\u3002", "method": "\u5c06GCS\u7b97\u6cd5\u7684\u5f62\u5f0f\u5316\u6a21\u578b\u4ece\u5355\u5411\u6d4b\u91cf\u8303\u5f0f\u8f6c\u6362\u4e3a\u53cc\u5411\u6d4b\u91cf\u8303\u5f0f\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\uff1a1\uff09\u53d6\u6d88\u5355\u4f4d\u94fe\u8def\u957f\u5ea6\u8981\u6c42\uff1b2\uff09\u5bf9\u5148\u524d\u5de5\u4f5c\u4e2d\u5047\u8bbe\u7684\u9891\u7387\u6e90\u8fdb\u884c\u5f62\u5f0f\u5316\u5efa\u6a21\uff1b3\uff09\u7ec6\u7c92\u5ea6\u533a\u5206\u7b97\u6cd5\u4f30\u8ba1\u8bef\u5dee\u7684\u4e0d\u540c\u7ec4\u6210\u90e8\u5206\uff1b4\uff09\u5206\u6790\u5e76\u964d\u4f4e\u4e0d\u786e\u5b9a\u6027\u5bf9\u4f30\u8ba1\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "result": "1\uff09\u5b9e\u73b0\u4e86\u9002\u7528\u4e8e\u975e\u5355\u4f4d\u94fe\u8def\u957f\u5ea6\u7684\u73b0\u5b9e\u90e8\u7f72\u6a21\u578b\uff1b2\uff09\u5efa\u7acb\u4e86\u9891\u7387\u6e90\u7684\u5f62\u5f0f\u5316\u6a21\u578b\uff1b3\uff09\u5c06\u7b97\u6cd5\u7684\u603b\u4f53\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e\u4e86\u591a\u4e2a\u6570\u91cf\u7ea7\uff1b4\uff09\u5c06\u6bcf\u6761\u94fe\u8def\u4e0a\u4e0d\u786e\u5b9a\u6027\u5bf9\u4f30\u8ba1\u8bef\u5dee\u7684\u8d21\u732e\u4ece\u4e0e\u94fe\u8def\u5ef6\u8fdf\u540c\u9636\u964d\u4f4e\u81f3\u5ef6\u8fdf\u76840.1%\u201310%\uff0c\u5e76\u7ed9\u51fa\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u504f\u659c\u7684\u5339\u914d\u4e0a\u754c\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u53cc\u5411\u6d4b\u91cf\u8303\u5f0f\uff0c\u672c\u6587\u663e\u8457\u63d0\u5347\u4e86GCS\u7b97\u6cd5\u6a21\u578b\u7684\u5b9e\u7528\u6027\u4e0e\u7cbe\u5ea6\uff0c\u5728\u4fdd\u6301\u5176\u6838\u5fc3\u673a\u5236\u7684\u540c\u65f6\uff0c\u5927\u5e45\u524a\u5f31\u4e86\u539f\u6709\u7406\u8bba\u6a21\u578b\u4e2d\u7684\u7406\u60f3\u5316\u5047\u8bbe\uff0c\u5e76\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.13983", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2511.13983", "abs": "https://arxiv.org/abs/2511.13983", "authors": ["Peng Shu", "Junhao Chen", "Zhengliang Liu", "Hanqi Jiang", "Yi Pan", "Khanh Nhu Nguyen", "Zihao Wu", "Huaqin Zhao", "Yiwei Li", "Enze Shi", "ShaoChen Xu"], "title": "MoMoE: A Mixture of Expert Agent Model for Financial Sentiment Analysis", "comment": null, "summary": "We present a novel approach called Mixture of Mixture of Expert (MoMoE) that combines the strengths of Mixture-of-Experts (MoE) architectures with collaborative multi-agent frameworks. By modifying the LLaMA 3.1 8B architecture to incorporate MoE layers in each agent of a layered collaborative structure, we create an ensemble of specialized expert agents that iteratively refine their outputs. Each agent leverages an MoE layer in its final attention block, enabling efficient task decomposition while maintaining computational feasibility. This hybrid approach creates specialized pathways through both the model architecture and the agent collaboration layers. Experimental results demonstrate significant improvements across multiple language understanding and generation benchmarks, highlighting the synergistic benefits of combining expert routing at both the neural and agent levels.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4e13\u5bb6\u6df7\u5408\u7684\u6df7\u5408\u201d\uff08MoMoE\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\u4e0e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u5728LLaMA 3.1 8B\u6a21\u578b\u4e2d\u4e3a\u6bcf\u5c42\u667a\u80fd\u4f53\u5f15\u5165MoE\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u548c\u4efb\u52a1\u4e13\u4e1a\u5316\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u878d\u5408\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u4e0e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u4efb\u52a1\u5206\u89e3\u4e0e\u6027\u80fd\u63d0\u5347\u3002", "method": "\u5728LLaMA 3.1 8B\u67b6\u6784\u57fa\u7840\u4e0a\uff0c\u4e3a\u5206\u5c42\u534f\u4f5c\u7ed3\u6784\u4e2d\u7684\u6bcf\u4e2a\u667a\u80fd\u4f53\u5728\u5176\u6700\u540e\u4e00\u5c42\u6ce8\u610f\u529b\u6a21\u5757\u4e2d\u5f15\u5165MoE\u5c42\uff0c\u6784\u5efa\u7531\u591a\u4e2a\u4e13\u4e1a\u5316\u4e13\u5bb6\u667a\u80fd\u4f53\u7ec4\u6210\u7684\u96c6\u6210\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8f93\u51fa\u3002", "result": "\u5728\u591a\u4e2a\u8bed\u8a00\u7406\u89e3\u4e0e\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u795e\u7ecf\u5c42\u9762\u4e0e\u667a\u80fd\u4f53\u5c42\u9762\u4e13\u5bb6\u8def\u7531\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "MoMoE\u901a\u8fc7\u5728\u6a21\u578b\u67b6\u6784\u548c\u667a\u80fd\u4f53\u534f\u4f5c\u4e24\u4e2a\u5c42\u9762\u5f15\u5165\u4e13\u5bb6\u6df7\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u8bed\u8a00\u5904\u7406\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u591a\u7c92\u5ea6\u4e13\u4e1a\u5316\u8def\u5f84\u7684\u534f\u540c\u4f18\u52bf\u3002"}}
{"id": "2511.13728", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13728", "abs": "https://arxiv.org/abs/2511.13728", "authors": ["Maximilian Reisecker", "Cynthia Marcelino", "Thomas Pusztai", "Stefan Nastic"], "title": "Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum", "comment": "In IEEE ACM 12th International Conference on Big Data Computing, Applications and Technologies (BDCAT 25), 2025, Nantes, France", "summary": "Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.", "AI": {"tldr": "Gaia \u662f\u4e00\u79cd\u9762\u5411\u65e0\u670d\u52a1\u5668 AI \u5de5\u4f5c\u8d1f\u8f7d\u7684 GPU \u5373\u670d\u52a1\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u90e8\u7f72\u65f6\u8bc6\u522b\u6267\u884c\u6a21\u5f0f\u5e76\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u6574 CPU/GPU \u540e\u7aef\uff0c\u5b9e\u73b0 SLO \u611f\u77e5\u4e14\u6210\u672c\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\uff0c\u5728\u5f02\u6784\u73af\u5883\u4e2d\u5c06\u7aef\u5230\u7aef\u5ef6\u8fdf\u6700\u591a\u964d\u4f4e 95%\u3002", "motivation": "\u5f53\u524d\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u5728\u5f02\u6784\u73af\u5883\uff08\u5982\u8fb9-\u4e91-\u7a7a\u4e09\u7ef4\u8fde\u7eed\u4f53\uff09\u4e2d\u96be\u4ee5\u6709\u6548\u7ba1\u7406\u786c\u4ef6\u52a0\u901f\uff1a\u9759\u6001\u5206\u914d\u65e0\u6cd5\u6ee1\u8db3\u53d8\u5316\u8d1f\u8f7d\u4e0b\u7684 SLO \u8981\u6c42\uff0c\u4e00\u6b21\u6027\u52a8\u6001\u9009\u62e9\u53c8\u5e38\u5bfc\u81f4\u6b21\u4f18\u6216\u9ad8\u6210\u672c\u914d\u7f6e\u3002", "method": "\u63d0\u51fa Gaia \u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(i) \u8f7b\u91cf\u7ea7\u6267\u884c\u6a21\u5f0f\u8bc6\u522b\u5668\uff0c\u5728\u90e8\u7f72\u65f6\u5206\u6790\u51fd\u6570\u4ee3\u7801\u5e76\u6307\u5b9a\u56db\u79cd\u6267\u884c\u6a21\u5f0f\u4e4b\u4e00\uff1b(ii) \u52a8\u6001\u51fd\u6570\u8fd0\u884c\u65f6\uff0c\u6301\u7eed\u8bc4\u4f30\u7528\u6237\u5b9a\u4e49\u7684 SLO\uff0c\u5e76\u5728 CPU \u4e0e GPU \u540e\u7aef\u4e4b\u95f4\u52a8\u6001\u5347\u964d\u7ea7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e Gaia \u80fd\u65e0\u7f1d\u9009\u62e9\u6700\u9002\u5408\u5de5\u4f5c\u8d1f\u8f7d\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u6700\u591a\u964d\u4f4e 95%\u3002", "conclusion": "Gaia \u5c06\u786c\u4ef6\u52a0\u901f\u53d8\u4e3a\u5e73\u53f0\u7ea7\u5173\u6ce8\u70b9\uff0c\u5b9e\u73b0\u4e86\u5728\u5f02\u6784\u73af\u5883\u4e2d\u5bf9\u65e0\u670d\u52a1\u5668 AI \u5de5\u4f5c\u8d1f\u8f7d\u7684 SLO \u611f\u77e5\u3001\u6210\u672c\u9ad8\u6548\u7684\u52a0\u901f\u652f\u6301\u3002"}}
{"id": "2511.13907", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.13907", "abs": "https://arxiv.org/abs/2511.13907", "authors": ["Sriom Chakrabarti", "Chuangtao Ma", "Arijit Khan", "Sebastian Link"], "title": "SQL-to-Text Generation with Weighted-AST Few-Shot Prompting", "comment": null, "summary": "SQL-to-Text generation aims at translating structured SQL queries into natural language descriptions, thereby facilitating comprehension of complex database operations for non-technical users. Although large language models (LLMs) have recently demonstrated promising results, current methods often fail to maintain the exact semantics of SQL queries, particularly when there are multiple possible correct phrasings. To address this problem, our work proposes Weighted-AST retrieval with prompting, an architecture that integrates structural query representations and LLM prompting. This method retrieves semantically relevant examples as few-shot prompts using a similarity metric based on an Abstract Syntax Tree (AST) with learned feature weights. Our structure-aware prompting technique ensures that generated descriptions are both fluent and faithful to the original query logic. Numerous experiments on three benchmark datasets - Spider, SParC, and CoSQL show that our method outperforms the current baselines by up to +17.24% in execution Accuracy (EX), performs superior in Exact Match (EM) and provides more consistent semantic fidelity when evaluated by humans, all while preserving competitive runtime performance. These results demonstrate that Weighted-AST prompting is a scalable and effective method for deriving natural language explanations from structured database queries.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u52a0\u6743\u62bd\u8c61\u8bed\u6cd5\u6811\uff08Weighted-AST\uff09\u68c0\u7d22\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06SQL\u67e5\u8be2\u51c6\u786e\u7ffb\u8bd1\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728SQL-to-Text\u4efb\u52a1\u4e2d\u96be\u4ee5\u4fdd\u6301SQL\u67e5\u8be2\u7684\u7cbe\u786e\u8bed\u4e49\uff0c\u5c24\u5176\u5728\u5b58\u5728\u591a\u79cd\u6b63\u786e\u8868\u8ff0\u65f6\u5bb9\u6613\u4ea7\u751f\u504f\u5dee\u3002", "method": "\u7ed3\u5408\u7ed3\u6784\u5316\u67e5\u8be2\u8868\u793a\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u673a\u5236\uff0c\u5229\u7528\u5e26\u5b66\u4e60\u6743\u91cd\u7684\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff0c\u68c0\u7d22\u8bed\u4e49\u76f8\u5173\u7684\u793a\u4f8b\u4f5c\u4e3a\u5c11\u6837\u672c\u63d0\u793a\uff0c\u5f15\u5bfc\u751f\u6210\u5fe0\u5b9e\u4e14\u6d41\u7545\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3002", "result": "\u5728Spider\u3001SParC\u548cCoSQL\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6267\u884c\u51c6\u786e\u7387\uff08EX\uff09\u4e0a\u6700\u9ad8\u63d0\u534717.24%\uff0c\u5728\u7cbe\u786e\u5339\u914d\uff08EM\uff09\u548c\u4eba\u5de5\u8bc4\u4f30\u7684\u8bed\u4e49\u4fdd\u771f\u5ea6\u65b9\u9762\u4e5f\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u8fd0\u884c\u6548\u7387\u3002", "conclusion": "Weighted-AST\u63d0\u793a\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u4ece\u7ed3\u6784\u5316\u6570\u636e\u5e93\u67e5\u8be2\u4e2d\u751f\u6210\u51c6\u786e\u3001\u8bed\u4e49\u4e00\u81f4\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002"}}
{"id": "2511.14550", "categories": ["cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.14550", "abs": "https://arxiv.org/abs/2511.14550", "authors": ["Dimitrios Dimopoulos", "Apostolis K. Salkintzis", "Dimitris Tsolkas", "Nikos Passas", "Lazaros Merakos"], "title": "Evaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over Heterogeneous Networks", "comment": null, "summary": "Modern mobile and stationary devices are equipped with multiple network interfaces aiming to provide wireless and wireline connectivity either in a local LAN or the Internet. Multipath TCP (MPTCP) protocol has been developed on top of legacy TCP to allow the simultaneous use of multiple network paths in the communication route between two end-systems. Although the combination of multiple paths is beneficial in case of links with similar network characteristics, MPTCP performance is challenged as heterogeneity among the used paths increases. This work provides an overview of the MPTCP protocol operation, analyzes the state-of-art packet scheduling and congestion control algorithms available in literature, and examines the impact of the various algorithm combinations on MPTCP performance, by conducting an extensive experimental evaluation under diverse path-heterogeneity conditions.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u8def\u5f84TCP\uff08MPTCP\uff09\u534f\u8bae\uff0c\u5206\u6790\u4e86\u73b0\u6709\u7684\u5206\u7ec4\u8c03\u5ea6\u4e0e\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4e0d\u540c\u7b97\u6cd5\u7ec4\u5408\u5728\u8def\u5f84\u5f02\u6784\u6027\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "MPTCP\u5728\u8def\u5f84\u7279\u6027\u76f8\u4f3c\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8def\u5f84\u5f02\u6784\u6027\u589e\u52a0\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u8c03\u5ea6\u4e0e\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u5bf9MPTCP\u534f\u8bae\u673a\u5236\u8fdb\u884c\u6982\u8ff0\uff0c\u68b3\u7406\u73b0\u6709\u5206\u7ec4\u8c03\u5ea6\u548c\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\uff0c\u5e76\u5728\u591a\u79cd\u8def\u5f84\u5f02\u6784\u6761\u4ef6\u4e0b\u5f00\u5c55\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e0d\u540c\u7b97\u6cd5\u7ec4\u5408\u5728\u5f02\u6784\u8def\u5f84\u73af\u5883\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3aMPTCP\u4f18\u5316\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "conclusion": "MPTCP\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6240\u91c7\u7528\u7684\u8c03\u5ea6\u4e0e\u62e5\u585e\u63a7\u5236\u7b97\u6cd5\u7ec4\u5408\uff0c\u5728\u8def\u5f84\u5f02\u6784\u573a\u666f\u4e0b\u9700\u8c28\u614e\u9009\u62e9\u6216\u8bbe\u8ba1\u9002\u914d\u7b97\u6cd5\u4ee5\u63d0\u5347\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2511.13972", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.13972", "abs": "https://arxiv.org/abs/2511.13972", "authors": ["Jeremiah Bohr"], "title": "Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation", "comment": "23 pages, 2 figures, 3 tables. Under review", "summary": "Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u4e0d\u540c\u63d0\u793a\u7b56\u7565\uff08\u6307\u4ee4\u578b\u3001\u793a\u4f8b\u578b\u53ca\u7ec4\u5408\u578b\uff09\u5bf9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4ee3\u7801\u98ce\u683c\u7684\u63a7\u5236\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u5bf9\u521d\u59cb\u4ee3\u7801\u8fdb\u884c\u529f\u80fd\u589e\u5f3a\u65f6\u80fd\u5426\u7ef4\u6301\u7b80\u6d01\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7ec4\u5408\u63d0\u793a\u5728\u521d\u59cb\u538b\u7f29\u548c\u6269\u5c55\u7ea6\u675f\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u867d\u7136\u529f\u80fd\u6b63\u786e\uff0c\u4f46\u5f80\u5f80\u8fc7\u4e8e\u5197\u957f\uff0c\u504f\u79bb\u4eba\u7c7b\u7f16\u5199\u98ce\u683c\u3002\u5982\u4f55\u5728\u6a21\u578b\u589e\u5f3a\u4ee3\u7801\u529f\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u9884\u8bbe\u7684\u4ee3\u7801\u98ce\u683c\uff08\u5982\u7b80\u6d01\u6027\uff09\uff0c\u662f\u5f53\u524d\u63d0\u793a\u5de5\u7a0b\u9762\u4e34\u7684\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u7814\u7a76\u91c7\u7528\u914d\u5bf9\u4e24\u8f6e\u534f\u8bae\uff1a\u9996\u5148\u8ba9\u6a21\u578b\u5728\u56db\u79cd\u7cfb\u7edf\u63d0\u793a\u6761\u4ef6\uff08\u6307\u4ee4\u578b\u3001\u793a\u4f8b\u578b\u3001\u7ec4\u5408\u578b\u3001\u5bf9\u7167\u7ec4\uff09\u4e0b\u751f\u6210Python\u4efb\u52a1\u7684\u521d\u59cb\u89e3\u51b3\u65b9\u6848\uff0c\u7136\u540e\u5728\u4efb\u52a1\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u6839\u636e\u901a\u7528\u6539\u8fdb\u6307\u4ee4\u5bf9\u4ee3\u7801\u8fdb\u884c\u4e00\u8f6e\u589e\u5f3a\uff08\u5171160\u5bf9\u7a0b\u5e8f\uff09\u3002\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u5728\u521d\u59cb\u751f\u6210\u548c\u589e\u5f3a\u9636\u6bb5\u7684\u4ee3\u7801\u957f\u5ea6\u53d8\u5316\uff0c\u8bc4\u4f30\u5176\u98ce\u683c\u63a7\u5236\u80fd\u529b\u3002", "result": "\u7ec4\u5408\u63d0\u793a\u5728\u521d\u59cb\u9636\u6bb5\u5b9e\u73b0\u6700\u5f3a\u7684\u4ee3\u7801\u538b\u7f29\uff0c\u5e76\u5728\u589e\u5f3a\u9636\u6bb5\u5c55\u73b0\u51fa\u6700\u4f73\u7684\u6269\u5c55\u7ea6\u675f\uff1b\u6307\u4ee4\u578b\u63d0\u793a\u521d\u59cb\u6548\u679c\u663e\u8457\u4f46\u6269\u5c55\u7ea6\u675f\u4e2d\u7b49\uff1b\u793a\u4f8b\u578b\u63d0\u793a\u521d\u59cb\u6548\u679c\u5fae\u5f31\u4e14\u65e0\u6269\u5c55\u7ea6\u675f\u3002", "conclusion": "\u521d\u59cb\u63d0\u793a\u6548\u679c\u4e0e\u6269\u5c55\u7ea6\u675f\u662f\u63d0\u793a\u8bbe\u8ba1\u4e2d\u4e24\u4e2a\u72ec\u7acb\u7684\u7ef4\u5ea6\uff0c\u7ec4\u5408\u63d0\u793a\u7b56\u7565\u5728\u4e24\u8f6e\u5de5\u4f5c\u6d41\u4e2d\u63d0\u4f9b\u4e86\u6700\u7a33\u5b9a\u7684\u4ee3\u7801\u98ce\u683c\u63a7\u5236\u3002"}}
{"id": "2511.13779", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.13779", "abs": "https://arxiv.org/abs/2511.13779", "authors": ["Mohammad Abdi", "Francesca Meneghello", "Francesco Restuccia"], "title": "Semantic Multiplexing", "comment": null, "summary": "Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\\times$, 25$\\times$, and 54$\\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bed\u4e49\u590d\u7528\uff08Semantic Multiplexing\uff09\u65b0\u6982\u5ff5\uff0c\u901a\u8fc7\u5728\u8bed\u4e49\u5c42\u5c06\u591a\u4e2a\u4efb\u52a1\u7684\u538b\u7f29\u8868\u793a\u878d\u5408\u4e3a\u5355\u4e00\u8bed\u4e49\u8868\u793a\uff0c\u5728\u4e0d\u589e\u52a0\u5929\u7ebf\u6216\u5e26\u5bbd\u7684\u524d\u63d0\u4e0b\u7a81\u7834\u4f20\u7edf\u901a\u4fe1\u7cfb\u7edf\u4ec5\u652f\u6301\u6bd4\u7279\u7ea7\u5e76\u884c\u4f20\u8f93\u7684\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u53ef\u5e76\u53d1\u5904\u7406\u7684\u4efb\u52a1\u6570\u91cf\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u56fe\u50cf\u5206\u7c7b\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u4e0e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4ec5\u652f\u6301\u6bd4\u7279\u7ea7\u5e76\u884c\u4f20\u8f93\uff0c\u9650\u5236\u4e86\u53ef\u5728\u65e0\u7ebf\u8fb9\u7f18\u5e76\u53d1\u5904\u7406\u7684\u4efb\u52a1\u6570\u91cf\uff0c\u96be\u4ee5\u6ee1\u8db3\u79fb\u52a8\u8bbe\u5907\u5bf9\u591a\u4efb\u52a1\u5e76\u884c\u5378\u8f7d\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u590d\u7528\u65b9\u6cd5\uff0c\u5c06\u591a\u4e2a\u4efb\u52a1\u76f8\u5173\u7684\u538b\u7f29\u8868\u793a\u5728\u8bed\u4e49\u5c42\u9762\u878d\u5408\u4e3a\u5355\u4e00\u8bed\u4e49\u8868\u793a\u8fdb\u884c\u4f20\u8f93\uff0c\u4ece\u800c\u5728\u4e0d\u8fdd\u53cd\u9999\u519c\u5bb9\u91cf\u9650\u5236\u7684\u524d\u63d0\u4e0b\u6269\u5c55\u8bed\u4e49\u5c42\u7684\u6709\u6548\u81ea\u7531\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bed\u4e49\u590d\u7528\u57284\u00d74\u4fe1\u9053\u4e0a\u5c06\u590d\u7528\u4efb\u52a1\u6570\u4ece2\u589e\u81f38\u65f6\uff0c\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u52304%\uff1b\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\uff0c\u5ef6\u8fdf\u3001\u80fd\u8017\u548c\u901a\u4fe1\u8d1f\u8f7d\u5206\u522b\u6700\u591a\u964d\u4f4e8\u500d\u300125\u500d\u548c54\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u7684\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8bed\u4e49\u590d\u7528\u662f\u4e00\u79cd\u6709\u6548\u63d0\u5347\u65e0\u7ebf\u8fb9\u7f18\u591a\u4efb\u52a1\u5e76\u53d1\u5904\u7406\u80fd\u529b\u7684\u65b0\u8303\u5f0f\uff0c\u5728\u4fdd\u8bc1\u4efb\u52a1\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\u663e\u8457\u4f18\u5316\u4e86\u7cfb\u7edf\u6548\u7387\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2511.14162", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.14162", "abs": "https://arxiv.org/abs/2511.14162", "authors": ["Supawit Chockchowwat", "Sumay Thakurdesai", "Zhaoheng Li", "Matthew Krafczyk", "Yongjoo Park"], "title": "Chipmink: Efficient Delta Identification for Massive Object Graph", "comment": "17 pages, 21 figures, to appear at VLDB 2026", "summary": "Ranging from batch scripts to computational notebooks, modern data science tools rely on massive and evolving object graphs that represent structured data, models, plots, and more. Persisting these objects is critical, not only to enhance system robustness against unexpected failures but also to support continuous, non-linear data exploration via versioning. Existing object persistence mechanisms (e.g., Pickle, Dill) rely on complete snapshotting, often redundantly storing unchanged objects during execution and exploration, resulting in significant inefficiency in both time and storage. Unlike DBMSs, data science systems lack centralized buffer managers that track dirty objects. Worse, object states span various locations such as memory heaps, shared memory, GPUs, and remote machines, making dirty object identification fundamentally more challenging. In this work, we propose a graph-based object store, named Chipmink, that acts like the centralized buffer manager. Unlike static pages in DBMSs, persistence units in Chipmink are dynamically induced by partitioning objects into appropriate subgroups (called pods), minimizing expected persistence costs based on object sizes and reference structure. These pods effectively isolate dirty objects, enabling efficient partial persistence. Our experiments show that Chipmink is general, supporting libraries that rely on shared memory, GPUs, and remote objects. Moreover, Chipmink achieves up to 36.5x smaller storage sizes and 12.4x faster persistence than the best baselines in real-world notebooks and scripts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aChipmink\u7684\u57fa\u4e8e\u56fe\u7684\u5bf9\u8c61\u5b58\u50a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u5212\u5206\u5bf9\u8c61\u4e3a\u201cpods\u201d\u6765\u5b9e\u73b0\u9ad8\u6548\u7684\u90e8\u5206\u6301\u4e45\u5316\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5b58\u50a8\u5f00\u9500\u548c\u6301\u4e45\u5316\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u79d1\u5b66\u5de5\u5177\u4e2d\u7684\u5bf9\u8c61\u6301\u4e45\u5316\u673a\u5236\uff08\u5982Pickle\u3001Dill\uff09\u4f9d\u8d56\u5b8c\u6574\u5feb\u7167\uff0c\u5bfc\u81f4\u5728\u6267\u884c\u548c\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u91cd\u590d\u5b58\u50a8\u672a\u66f4\u6539\u7684\u5bf9\u8c61\uff0c\u9020\u6210\u65f6\u95f4\u548c\u5b58\u50a8\u4e0a\u7684\u4f4e\u6548\uff1b\u540c\u65f6\u7f3a\u4e4f\u7c7b\u4f3c\u6570\u636e\u5e93\u7cfb\u7edf\u7684\u96c6\u4e2d\u5f0f\u7f13\u51b2\u7ba1\u7406\u5668\u6765\u8ffd\u8e2a\u810f\u5bf9\u8c61\uff0c\u4e14\u5bf9\u8c61\u72b6\u6001\u5206\u5e03\u5728\u5185\u5b58\u3001\u5171\u4eab\u5185\u5b58\u3001GPU\u548c\u8fdc\u7a0b\u673a\u5668\u7b49\u591a\u79cd\u4f4d\u7f6e\uff0c\u4f7f\u5f97\u810f\u5bf9\u8c61\u8bc6\u522b\u6781\u5177\u6311\u6218\u3002", "method": "\u63d0\u51faChipmink\u7cfb\u7edf\uff0c\u5176\u5c06\u5bf9\u8c61\u6309\u5f15\u7528\u7ed3\u6784\u548c\u5927\u5c0f\u52a8\u6001\u5212\u5206\u4e3a\u79f0\u4e3a\u201cpods\u201d\u7684\u5b50\u7ec4\uff0c\u4f5c\u4e3a\u6301\u4e45\u5316\u5355\u5143\uff0c\u4ece\u800c\u9694\u79bb\u810f\u5bf9\u8c61\u5e76\u652f\u6301\u9ad8\u6548\u7684\u5c40\u90e8\u6301\u4e45\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cChipmink\u5177\u6709\u826f\u597d\u7684\u901a\u7528\u6027\uff0c\u652f\u6301\u5171\u4eab\u5185\u5b58\u3001GPU\u548c\u8fdc\u7a0b\u5bf9\u8c61\u7684\u5e93\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684notebook\u548c\u811a\u672c\u4e2d\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6700\u9ad836.5\u500d\u7684\u5b58\u50a8\u8282\u7701\u548c12.4\u500d\u7684\u6301\u4e45\u5316\u52a0\u901f\u3002", "conclusion": "Chipmink\u901a\u8fc7\u5f15\u5165\u7c7b\u4f3c\u6570\u636e\u5e93\u7f13\u51b2\u7ba1\u7406\u5668\u7684\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u79d1\u5b66\u73af\u5883\u4e2d\u5bf9\u8c61\u6301\u4e45\u5316\u7684\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u975e\u7ebf\u6027\u3001\u6301\u7eed\u7684\u6570\u636e\u63a2\u7d22\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u652f\u6301\u3002"}}
{"id": "2511.13996", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.13996", "abs": "https://arxiv.org/abs/2511.13996", "authors": ["Daihan Xu", "Diana Martin"], "title": "Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education", "comment": "Full paper oral presentation at the European Society for Engineering Education (SEFI) 2025 Annual Conference (September 2025)", "summary": "ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional \"independent thinking-manual coding-iterative debugging\" to \"AI-assisted ideation-interactive programming-collaborative optimization.\" Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8d28\u6027\u8bbf\u8c08\u63a2\u8ba8\u82f1\u56fd\u67d0\u9ad8\u6821\u8ba1\u7b97\u673a\u4e13\u4e1a\u5b66\u751f\u5728\u8f6f\u4ef6\u5f00\u53d1\u9879\u76ee\u4e2d\u5982\u4f55\u7b56\u7565\u6027\u548c\u4f26\u7406\u5730\u4f7f\u7528ChatGPT\uff0c\u53d1\u73b0\u5b66\u751f\u5c06\u5176\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u4ee5\u52a0\u6df1\u7406\u89e3\uff0c\u4f46\u4fdd\u7559\u9ad8\u9636\u51b3\u7b56\uff0c\u5e76\u666e\u904d\u9650\u5236\u5176\u8d21\u732e\u7ea630%\uff1b\u540c\u65f6\uff0c\u5b66\u751f\u5173\u6ce8\u9690\u79c1\u3001\u6280\u80fd\u9000\u5316\u7b49\u98ce\u9669\uff0c\u547c\u5401\u6559\u5e08\u5236\u5b9a\u660e\u786e\u4f7f\u7528\u89c4\u8303\u3002", "motivation": "\u5c3d\u7ba1ChatGPT\u5728\u8ba1\u7b97\u673a\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u95ee\u5377\u8c03\u67e5\uff0c\u7f3a\u4e4f\u5bf9\u5b66\u751f\u4f7f\u7528\u7b56\u7565\u4e0e\u4f26\u7406\u610f\u8bc6\u7684\u6df1\u5165\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7a76\u5b66\u751f\u5982\u4f55\u5728\u5b66\u672f\u4e0e\u804c\u4e1a\u80cc\u666f\u4e0b\u7b56\u7565\u6027\u4e14\u5408\u4e4e\u4f26\u7406\u5730\u4f7f\u7528ChatGPT\u3002", "method": "\u91c7\u7528\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\u5bf9\u4e00\u6240\u82f1\u56fd\u9ad8\u6821\u7684\u8ba1\u7b97\u673a\u79d1\u5b66\u5b66\u751f\u8fdb\u884c\u8d28\u6027\u7814\u7a76\uff0c\u805a\u7126\u5176\u5728\u8f6f\u4ef6\u5f00\u53d1\u9879\u76ee\u4e2d\u4f7f\u7528ChatGPT\u7684\u7b56\u7565\u4e0e\u4f26\u7406\u8ba4\u77e5\u3002", "result": "\u5b66\u751f\u5c06\u5b66\u4e60\u6a21\u5f0f\u4ece\u201c\u72ec\u7acb\u601d\u8003\u2014\u624b\u52a8\u7f16\u7801\u2014\u8fed\u4ee3\u8c03\u8bd5\u201d\u8f6c\u53d8\u4e3a\u201cAI\u8f85\u52a9\u6784\u601d\u2014\u4ea4\u4e92\u5f0f\u7f16\u7a0b\u2014\u534f\u4f5c\u4f18\u5316\u201d\uff1b\u591a\u6570\u4eba\u5c06ChatGPT\u7528\u4e8e\u5bf9\u8bdd\u5f0f\u5b66\u4e60\uff0c\u4fdd\u7559\u521b\u9020\u6027\u4efb\u52a1\uff0c\u9650\u5236\u5176\u8d21\u732e\u7ea630%\uff0c\u5e76\u8bc4\u4f30\u8f93\u51fa\u4ee5\u9632\u8fc7\u5ea6\u4f9d\u8d56\uff1b\u4f46\u4ec5\u5c11\u6570\u6df1\u5165\u5206\u6790AI\u751f\u6210\u4ee3\u7801\uff1b\u5b66\u751f\u666e\u904d\u53cd\u5bf9\u672a\u7f72\u540d\u4f7f\u7528\uff0c\u5173\u6ce8\u9690\u79c1\u6cc4\u9732\u4e0e\u6280\u80fd\u9000\u5316\uff0c\u5e76\u5e0c\u671b\u6559\u5e08\u63d0\u4f9b\u6e05\u6670\u6307\u5f15\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5b66\u751f\u4e0eAI\u5de5\u5177\u4e4b\u95f4\u4e0d\u65ad\u6f14\u53d8\u7684\u4e92\u52a8\u5173\u7cfb\uff0c\u5f3a\u8c03\u9700\u901a\u8fc7\u660e\u786e\u7684\u6559\u5b66\u6307\u5bfc\u4fc3\u8fdbChatGPT\u5728\u6559\u80b2\u4e2d\u7684\u8d1f\u8d23\u4efb\u548c\u6709\u6548\u4f7f\u7528\u3002"}}
{"id": "2511.13778", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13778", "abs": "https://arxiv.org/abs/2511.13778", "authors": ["Angelika Schwarz", "Anton Anders", "Cole Brower", "Harun Bayraktar", "John Gunnels", "Kate Clark", "RuQing G. Xu", "Samuel Rodriguez", "Sebastien Cayrols", "Pawe\u0142 Tabaszewski", "Victor Podlozhnyuk"], "title": "Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme", "comment": null, "summary": "The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u52a8\u52a8\u6001\u7cbe\u5ea6\uff08ADP\uff09\u7684\u5168GPU\u9a7b\u7559\u6846\u67b6\uff0c\u5229\u7528\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\u5355\u5143\u9ad8\u6548\u53ef\u9760\u5730\u6a21\u62df\u53cc\u7cbe\u5ea6\u77e9\u9635\u4e58\u6cd5\uff0c\u5e76\u901a\u8fc7\u6307\u6570\u8de8\u5ea6\u5bb9\u91cf\uff08ESC\uff09\u7b49\u6280\u672f\u4fdd\u8bc1FP64\u7ea7\u522b\u7684\u7cbe\u5ea6\uff0c\u5728\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u968f\u7740AI\u53d1\u5c55\uff0c\u73b0\u4ee3GPU\u8d8a\u6765\u8d8a\u4fa7\u91cd\u4e8eFP16\u3001FP8\u4e43\u81f3FP4\u7b49\u4f4e\u7cbe\u5ea6\u683c\u5f0f\uff0c\u800c\u4f20\u7edfFP64\u8ba1\u7b97\u6027\u80fd\u76f8\u5bf9\u53d7\u9650\u3002\u56e0\u6b64\uff0c\u5982\u4f55\u5229\u7528\u4f4e\u7cbe\u5ea6\u786c\u4ef6\u9ad8\u6548\u5b9e\u73b0\u53cc\u7cbe\u5ea6\u7cbe\u5ea6\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u65b9\u5411\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u81ea\u52a8\u52a8\u6001\u7cbe\u5ea6\uff08ADP\uff09\u6846\u67b6\uff0c\u5176\u6838\u5fc3\u662f\u786c\u4ef6\u65e0\u5173\u7684\u6307\u6570\u8de8\u5ea6\u5bb9\u91cf\uff08ESC\uff09\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u786e\u5b9a\u5b9e\u73b0FP64\u7cbe\u5ea6\u6240\u9700\u7684\u5206\u89e3\u53c2\u6570\u3002ADP\u8fd8\u96c6\u6210\u4e86\u5f02\u5e38\u5904\u7406\u3001\u8fd0\u884c\u65f6\u542f\u53d1\u5f0f\u7b56\u7565\u548c\u65e0\u7f1d\u56de\u9000\u673a\u5236\uff0c\u5e76\u6539\u8fdb\u4e86Ozaki\u5206\u89e3\u65b9\u6cd5\uff0c\u91c7\u7528\u65e0\u7b26\u53f7\u6574\u6570\u5207\u7247\u65b9\u6848\u4ee5\u63d0\u9ad8\u8868\u793a\u6548\u7387\u3002", "result": "\u5728BLAS\u5206\u7ea7\u6d4b\u8bd5\u4e2d\uff0cADP\u5728\u6311\u6218\u6027\u8f93\u5165\u4e0b\u59cb\u7ec8\u7ef4\u6301FP64\u7cbe\u5ea6\uff0c\u8fd0\u884c\u65f6\u5f00\u9500\u4f4e\u4e8e10%\uff1b\u572855\u4f4d\u5c3e\u6570\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u8f83\u4e8e\u539f\u751fFP64 GEMM\uff0c\u5728NVIDIA Blackwell GB200\u548cRTX Pro 6000 Blackwell Server Edition\u4e0a\u5206\u522b\u5b9e\u73b0\u6700\u9ad82.3\u500d\u548c13.2\u500d\u52a0\u901f\u3002", "conclusion": "\u4f4e\u7cbe\u5ea6\u52a0\u901f\u5668\u53ef\u4f5c\u4e3a\u9ad8\u6027\u80fd\u3001\u9ad8\u4fdd\u771f\u79d1\u5b66\u8ba1\u7b97\u4efb\u52a1\u7684\u5b9e\u7528\u4e14\u751f\u4ea7\u5c31\u7eea\u7684\u57fa\u7840\u3002"}}
{"id": "2511.13998", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.13998", "abs": "https://arxiv.org/abs/2511.13998", "authors": ["Jielin Qiu", "Zuxin Liu", "Zhiwei Liu", "Rithesh Murthy", "Jianguo Zhang", "Haolin Chen", "Shiyu Wang", "Ming Zhu", "Liangwei Yang", "Juntao Tan", "Roshan Ram", "Akshara Prabhakar", "Tulika Awalgaonkar", "Zixiang Chen", "Zhepeng Cen", "Cheng Qian", "Shelby Heinecke", "Weiran Yao", "Silvio Savarese", "Caiming Xiong", "Huan Wang"], "title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering", "comment": "54-pages", "summary": "As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \\textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 LoCoBench-Agent\uff0c\u4e00\u4e2a\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\u5728\u957f\u4e0a\u4e0b\u6587\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u7efc\u5408\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u3001\u5de5\u5177\u4f7f\u7528\u548c\u6548\u7387-\u7406\u89e3\u6743\u8861\u7b49\u7ef4\u5ea6\uff0c\u7cfb\u7edf\u8bc4\u6d4b\u667a\u80fd\u4f53\u5728\u771f\u5b9e\u5f00\u53d1\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\uff08\u5982 LoCoBench\uff09\u4ec5\u652f\u6301\u5355\u8f6e\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6355\u6349\u771f\u5b9e\u7f16\u7801\u667a\u80fd\u4f53\u6240\u9700\u7684\u591a\u8f6e\u4ea4\u4e92\u3001\u5de5\u5177\u4f7f\u7528\u548c\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u8d34\u8fd1\u5b9e\u9645\u5f00\u53d1\u6d41\u7a0b\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5c06 LoCoBench \u7684 8,000 \u4e2a\u573a\u666f\u6269\u5c55\u4e3a\u652f\u6301\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u73af\u5883\uff0c\u63d0\u4f9b 8 \u79cd\u4e13\u7528\u5de5\u5177\uff08\u5982\u6587\u4ef6\u64cd\u4f5c\u3001\u641c\u7d22\u3001\u4ee3\u7801\u5206\u6790\uff09\uff0c\u5e76\u5728 10K \u81f3 1M token \u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u8303\u56f4\u5185\uff0c\u901a\u8fc7 9 \u9879\u6db5\u76d6\u7406\u89e3\u4e0e\u6548\u7387\u7684\u6307\u6807\u5bf9 LLM \u667a\u80fd\u4f53\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a(1) \u667a\u80fd\u4f53\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff1b(2) \u7406\u89e3\u6df1\u5ea6\u4e0e\u6267\u884c\u6548\u7387\u5b58\u5728\u8d1f\u76f8\u5173\u6743\u8861\uff1b(3) \u4e0d\u540c\u6a21\u578b\u5728\u5bf9\u8bdd\u6548\u7387\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u4e0a\u5dee\u5f02\u663e\u8457\u3002", "conclusion": "LoCoBench-Agent \u662f\u9996\u4e2a\u9762\u5411\u8f6f\u4ef6\u5de5\u7a0b\u7684\u957f\u4e0a\u4e0b\u6587 LLM \u667a\u80fd\u4f53\u57fa\u51c6\uff0c\u4e3a\u8861\u91cf\u667a\u80fd\u4f53\u80fd\u529b\u3001\u8bc6\u522b\u6027\u80fd\u5dee\u8ddd\u548c\u63a8\u52a8\u5927\u89c4\u6a21\u81ea\u4e3b\u8f6f\u4ef6\u5f00\u53d1\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2511.14002", "categories": ["cs.SE", "cs.AI", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2511.14002", "abs": "https://arxiv.org/abs/2511.14002", "authors": ["Chengpeng Li", "Farnaz Behrang", "August Shi", "Peng Liu"], "title": "FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale", "comment": "To appear in ASE 2025", "summary": "Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.", "AI": {"tldr": "FlakyGuard \u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7ed3\u6784\u9009\u62e9\u6027\u63a2\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u4fee\u590d\u975e\u786e\u5b9a\u6027\u5931\u8d25\u7684\u201c\u8106\u5f31\u6d4b\u8bd5\u201d\uff08flaky tests\uff09\uff0c\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u8106\u5f31\u6d4b\u8bd5\u4fee\u590d\u65b9\u6cd5\uff08\u5982 FlakyDoctor\uff09\u5728\u5de5\u4e1a\u573a\u666f\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u201c\u4e0a\u4e0b\u6587\u95ee\u9898\u201d\uff1a\u8981\u4e48\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u592a\u5c11\uff08\u7f3a\u5c11\u5173\u952e\u751f\u4ea7\u4ee3\u7801\uff09\uff0c\u8981\u4e48\u592a\u591a\uff08\u5305\u542b\u5927\u91cf\u65e0\u5173\u4fe1\u606f\uff09\uff0c\u5f71\u54cd\u4fee\u590d\u6548\u679c\u3002", "method": "FlakyGuard \u5c06\u4ee3\u7801\u5efa\u6a21\u4e3a\u56fe\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u9009\u62e9\u6027\u56fe\u63a2\u7d22\u4ec5\u63d0\u53d6\u4e0e\u4fee\u590d\u6700\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u4ece\u800c\u6709\u6548\u7f13\u89e3\u4e0a\u4e0b\u6587\u95ee\u9898\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4e1a\u4ed3\u5e93\u7684\u8106\u5f31\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff0cFlakyGuard \u6210\u529f\u4fee\u590d\u4e86 47.6% \u7684\u53ef\u590d\u73b0\u8106\u5f31\u6d4b\u8bd5\uff0c\u5176\u4e2d 51.8% \u7684\u4fee\u590d\u88ab\u5f00\u53d1\u8005\u63a5\u53d7\uff1b\u5176\u4fee\u590d\u6210\u529f\u7387\u6bd4\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u81f3\u5c11\u9ad8\u51fa 22%\uff1b\u5f00\u53d1\u8005\u8c03\u67e5\u8868\u660e 100% \u8ba4\u4e3a\u5176\u6839\u56e0\u89e3\u91ca\u6709\u7528\u3002", "conclusion": "FlakyGuard \u901a\u8fc7\u56fe\u7ed3\u6784\u548c\u9009\u62e9\u6027\u4e0a\u4e0b\u6587\u63d0\u53d6\uff0c\u6709\u6548\u63d0\u5347\u4e86 LLM \u5728\u5de5\u4e1a\u7ea7\u8106\u5f31\u6d4b\u8bd5\u4fee\u590d\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u548c\u5b9e\u7528\u6027\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5f00\u53d1\u8005\u63a5\u53d7\u5ea6\u548c\u89e3\u91ca\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2511.13804", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.13804", "abs": "https://arxiv.org/abs/2511.13804", "authors": ["Temitayo Adefemi"], "title": "Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication", "comment": "9 pages, 6 figures", "summary": "MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9MPI\u6d3e\u751f\u6570\u636e\u7c7b\u578b\uff08DDTs\uff09\u5728\u591a\u4e2aMPI\u5b9e\u73b0\u4e2d\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u8de8\u5e73\u53f0\u8bc4\u4f30\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5177\u4f53\u5e94\u7528\u3001\u901a\u4fe1\u8bed\u4e49\u548cMPI\u5e93\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u53ef\u79fb\u690d\u6027\uff0c\u5efa\u8bae\u5728\u76ee\u6807\u73af\u5883\u4e2d\u540c\u65f6\u6d4b\u8bd5DDT\u548c\u624b\u52a8\u6253\u5305\u65b9\u6848\u3002", "motivation": "MPI\u7684\u6d3e\u751f\u6570\u636e\u7c7b\u578b\u867d\u627f\u8bfa\u7b80\u5316\u975e\u8fde\u7eed\u6570\u636e\u7684\u65e0\u62f7\u8d1d\u901a\u4fe1\uff0c\u4f46\u5176\u5b9e\u9645\u6027\u80fd\u5b58\u5728\u4e89\u8bae\u4e14\u7f3a\u4e4f\u8de8MPI\u5b9e\u73b0\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u4f5c\u8005\u4f7f\u7528\u4e09\u4e2a2D\u5e94\u7528\uff08Jacobi CFD\u6c42\u89e3\u5668\u3001\u5eb7\u5a01\u751f\u547d\u6e38\u620f\u3001\u57fa\u4e8e\u683c\u70b9\u7684\u56fe\u50cf\u91cd\u5efa\uff09\uff0c\u5206\u522b\u4ee5\u624b\u52a8\u6253\u5305\uff08BASIC\uff09\u548cDDT\u65b9\u5f0f\u5b9e\u73b0\uff0c\u5e76\u5728\u56db\u79cd\u4e3b\u6d41MPI\u5b9e\u73b0\uff08MPICH\u3001Open MPI\u3001Intel MPI\u3001MVAPICH2\uff09\u4e0a\uff0c\u9488\u5bf9\u591a\u79cd\u901a\u4fe1\u8bed\u4e49\uff08\u975e\u963b\u585e\u70b9\u5bf9\u70b9\u3001\u90bb\u57df\u96c6\u5408\u901a\u4fe1\u3001MPI-4\u6301\u4e45\u64cd\u4f5c\uff09\u8fdb\u884c\u5f3a/\u5f31\u6269\u5c55\u6027\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u7ed3\u679c\u4e00\u81f4\u6027\u5e76\u6bd4\u8f83\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u8868\u660eDDT\u6027\u80fd\u8868\u73b0\u4e0d\u4e00\uff1a\u5728\u67d0\u4e9b\u7ec4\u5408\u4e0b\u6700\u5feb\uff08\u5982\u56fe\u50cf\u91cd\u5efa\u5728Intel MPI\u548cMPICH\u4e0a\uff09\uff0c\u4f46\u5728\u5176\u4ed6\u7ec4\u5408\u4e0b\u6700\u6162\uff08\u5982\u540c\u4e00\u4ee3\u7801\u5728Open MPI\u548cMVAPICH2\u4e0a\uff09\uff1bCFD\u6c42\u89e3\u5668\u4e2dBASIC\u666e\u904d\u4f18\u4e8eDDT\uff0c\u800c\u751f\u547d\u6e38\u620f\u4e2d\u6027\u80fd\u6392\u5e8f\u968fMPI\u5e93\u53d8\u5316\uff1b\u8fd8\u89c2\u5bdf\u5230\u7279\u5b9aMPI\u6808\u7684\u5f02\u5e38\u884c\u4e3a\uff08\u5982MPICH\u5728DDT\u90bb\u57df\u548c\u6301\u4e45\u6a21\u5f0f\u4e0b\u7684\u51cf\u901f\uff09\u3002", "conclusion": "\u6ca1\u6709\u4e00\u79cd\u7b56\u7565\u80fd\u5728\u6240\u6709\u7a0b\u5e8f\u3001\u901a\u4fe1\u8bed\u4e49\u548cMPI\u5b9e\u73b0\u4e2d\u5360\u4f18\uff0cDDT\u7684\u6027\u80fd\u53ef\u79fb\u690d\u6027\u65e0\u6cd5\u4fdd\u8bc1\uff0c\u5efa\u8bae\u5728\u76ee\u6807MPI\u5b9e\u73b0\u548c\u901a\u4fe1\u6a21\u5f0f\u4e0b\u5bf9DDT\u4e0e\u624b\u52a8\u6253\u5305\u65b9\u6848\u90fd\u8fdb\u884c\u6027\u80fd\u5256\u6790\u3002"}}
{"id": "2511.14629", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.14629", "abs": "https://arxiv.org/abs/2511.14629", "authors": ["Anadi Shakya", "Primal Pappachan", "David Maier", "Roberto Yus", "Sharad Mehrotra", "Johann-Christoph Freytag"], "title": "Scalable Enforcement of Fine Grained Access Control Policies in Relational Database Management Systems", "comment": null, "summary": "The proliferation of smart technologies and evolving privacy regulations such as the GDPR and CPRA has increased the need to manage fine-grained access control (FGAC) policies in database management systems (DBMSs). Existing approaches to enforcing FGAC policies do not scale to thousands of policies, leading to degraded query performance and reduced system effectiveness. We present Sieve, a middleware for relational DBMSs that combines query rewriting and caching to optimize FGAC policy enforcement. Sieve rewrites a query with guarded expressions that group and filter policies and can efficiently use indexes in the DBMS. It also integrates a caching mechanism with an effective replacement strategy and a refresh mechanism to adapt to dynamic workloads. Experiments on two DBMSs with real and synthetic datasets show that Sieve scales to large datasets and policy corpora, maintaining low query latency and system load and improving policy evaluation performance by between 2x and 10x on workloads with 200 to 1,200 policies. The caching extension further improves query performance by between 6 and 22 percent under dynamic workloads, especially with larger cache sizes. These results highlight Sieve's applicability for real-time access control in smart environments and its support for efficient, scalable management of user preferences and privacy policies.", "AI": {"tldr": "Sieve \u662f\u4e00\u79cd\u7528\u4e8e\u5173\u7cfb\u578b\u6570\u636e\u5e93\u7684\u4e2d\u95f4\u4ef6\uff0c\u901a\u8fc7\u67e5\u8be2\u91cd\u5199\u548c\u7f13\u5b58\u673a\u5236\u9ad8\u6548\u6267\u884c\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u63a7\u5236\uff08FGAC\uff09\u7b56\u7565\uff0c\u5728\u5927\u89c4\u6a21\u7b56\u7565\u96c6\u4e0b\u663e\u8457\u63d0\u5347\u67e5\u8be2\u6027\u80fd\u548c\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u6280\u672f\u666e\u53ca\u548c GDPR\u3001CPRA \u7b49\u9690\u79c1\u6cd5\u89c4\u51fa\u53f0\uff0c\u6570\u636e\u5e93\u7cfb\u7edf\u9700\u9ad8\u6548\u7ba1\u7406\u5927\u91cf\u7ec6\u7c92\u5ea6\u8bbf\u95ee\u63a7\u5236\u7b56\u7565\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u7b56\u7565\u6570\u91cf\u6fc0\u589e\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002", "method": "Sieve \u91c7\u7528\u67e5\u8be2\u91cd\u5199\u6280\u672f\uff0c\u5c06\u7b56\u7565\u8f6c\u5316\u4e3a\u5e26\u4fdd\u62a4\u8868\u8fbe\u5f0f\u7684\u67e5\u8be2\u4ee5\u5229\u7528\u6570\u636e\u5e93\u7d22\u5f15\uff0c\u5e76\u7ed3\u5408\u5177\u5907\u9ad8\u6548\u66ff\u6362\u4e0e\u5237\u65b0\u673a\u5236\u7684\u7f13\u5b58\u7b56\u7565\u6765\u9002\u5e94\u52a8\u6001\u8d1f\u8f7d\u3002", "result": "\u5728\u4e24\u4e2a DBMS \u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSieve \u5728 200 \u81f3 1,200 \u6761\u7b56\u7565\u4e0b\u53ef\u5c06\u7b56\u7565\u8bc4\u4f30\u6027\u80fd\u63d0\u5347 2 \u81f3 10 \u500d\uff1b\u7f13\u5b58\u673a\u5236\u5728\u52a8\u6001\u8d1f\u8f7d\u4e0b\u8fdb\u4e00\u6b65\u63d0\u5347\u67e5\u8be2\u6027\u80fd 6%\u201322%\uff0c\u5c24\u5176\u5728\u5927\u7f13\u5b58\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "Sieve \u80fd\u591f\u652f\u6301\u667a\u80fd\u73af\u5883\u4e2d\u5b9e\u65f6\u8bbf\u95ee\u63a7\u5236\u9700\u6c42\uff0c\u4e3a\u7528\u6237\u504f\u597d\u4e0e\u9690\u79c1\u7b56\u7565\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u7ba1\u7406\u65b9\u6848\u3002"}}
{"id": "2511.14022", "categories": ["cs.SE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14022", "abs": "https://arxiv.org/abs/2511.14022", "authors": ["Pradeep Kumar Sharma", "Ishaan Puri", "Mantinder Jit Singh", "Swapnil Shivaprasad", "Hritvik Shrivastava"], "title": "Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning", "comment": null, "summary": "Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u5728\u4ee3\u7801\u5e93\u6301\u7eed\u6f14\u8fdb\u7684\u80cc\u666f\u4e0b\uff0c\u4fdd\u6301\u81ea\u7136\u8bed\u8a00\u5230\u4ee3\u7801\u6587\u4ef6\u8def\u5f84\u6620\u5c04\u6a21\u578b\u7684\u65f6\u6548\u6027\uff0c\u540c\u65f6\u907f\u514d\u9057\u5fd8\u65e7\u77e5\u8bc6\u3002\u4f5c\u8005\u6bd4\u8f83\u4e86\u4e09\u79cd\u7b56\u7565\uff1a\u5b8c\u5168\u91cd\u8bad\u7ec3\uff08Full Refresh\uff09\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u548c\u589e\u91cf\u5fae\u8c03\uff08Inc-FT\uff09\uff0c\u5e76\u5728\u591a\u4e2a\u4e3b\u6d41\u4ee3\u7801\u5e93\u4e0a\u8bc4\u4f30\u5176\u6548\u679c\u3002", "motivation": "\u73b0\u4ee3\u4ee3\u7801\u5e93\u4e0d\u65ad\u6f14\u53d8\uff08\u5982\u6587\u4ef6\u91cd\u547d\u540d\u3001\u5220\u9664\u3001API\u6f02\u79fb\u7b49\uff09\uff0c\u5bfc\u81f4\u6628\u5929\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5728\u4eca\u5929\u6027\u80fd\u4e0b\u964d\uff0c\u5373\u4f7f\u7528\u6237\u67e5\u8be2\u672a\u53d8\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4e0d\u4e22\u5931\u5bf9\u65e7\u4ee3\u7801\u8bb0\u5fc6\u7684\u524d\u63d0\u4e0b\uff0c\u4f7f\u6a21\u578b\u4fdd\u6301\u5bf9\u65b0\u4ee3\u7801\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u5c06\u6a21\u578b\u65f6\u6548\u6027\u95ee\u9898\u89c6\u4e3a\u57fa\u7840\u5feb\u7167\u4e0e\u5f53\u524dHEAD\u4e4b\u95f4\u7684\u9886\u57df\u6f02\u79fb\uff0c\u5bf9\u6bd4\u4e09\u7c7b\u66f4\u65b0\u7b56\u7565\uff1a(A) \u5b8c\u5168\u91cd\u8bad\u7ec3\uff1b(B) \u63a8\u7406\u65f6\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6ce8\u5165\u8fd1\u671f\u53d8\u66f4\uff08\u539f\u59cbgit diff\u6216\u82f1\u6587\u6458\u8981\uff09\uff1b(C) \u57fa\u4e8e\u53d8\u66f4\u6570\u636e\u96c6\u8fdb\u884c\u589e\u91cf\u5fae\u8c03\uff0c\u5e76\u63a7\u5236\u65b0\u65e7\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u4ee5\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002\u63d0\u51fa\u522b\u540d\u611f\u77e5\u8bc4\u4f30\u534f\u8bae\u548c\u9057\u5fd8\u63a2\u9488\u7528\u4e8e\u8bc4\u6d4b\u3002", "result": "\u5728Flask\u3001SQLAlchemy\u3001Pandas\u548cPoetry\u7b49\u4ee3\u7801\u5e93\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aInc-FT\u7ed3\u5408\u65b0\u65e7\u6570\u636e\u6df7\u5408\u5728\u6574\u4f53\u8868\u73b0\u4e0a\u6700\u4f18\uff1b\u5f53\u65e0\u6cd5\u8bad\u7ec3\u65f6\uff0c\u4f7f\u7528\u82f1\u6587\u6458\u8981\u7684ICL\u80fd\u6700\u5feb\u63d0\u5347\u5bf9\u65b0\u4ee3\u7801\u7684\u54cd\u5e94\u80fd\u529b\uff1b\u82e5\u8ffd\u6c42\u6700\u9ad8\u65b0\u4ee3\u7801\u51c6\u786e\u7387\uff0cFull Refresh\u4ecd\u662f\u4e0a\u9650\u3002\u6b64\u5916\uff0c\u57fa\u4e8egit diff\u7684Inc-FT\u5728\u91cd\u547d\u540d/\u5220\u9664\u9891\u7e41\u7684\u7a97\u53e3\u4e2d\u8868\u73b0\u66f4\u597d\uff0c\u800c\u57fa\u4e8e\u5b8c\u6574\u6587\u4ef6\u7684Inc-FT\u5728\u884c\u4e3a\u53d8\u66f4\u4e3a\u4e3b\u7684\u7a97\u53e3\u4e2d\u66f4\u4f18\u3002", "conclusion": "\u4fdd\u6301\u4ee3\u7801\u7406\u89e3\u6a21\u578b\u65f6\u6548\u6027\u9700\u6743\u8861\u65b0\u65e7\u77e5\u8bc6\u4fdd\u7559\u3002\u589e\u91cf\u5fae\u8c03\u914d\u5408\u65b0\u65e7\u6570\u636e\u6df7\u5408\u662f\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6848\uff0c\u800c\u4e0a\u4e0b\u6587\u5b66\u4e60\u53ef\u4f5c\u4e3a\u65e0\u8bad\u7ec3\u573a\u666f\u4e0b\u7684\u5feb\u901f\u66ff\u4ee3\u3002\u8bc4\u4f30\u5e94\u8003\u8651\u6587\u4ef6\u91cd\u547d\u540d\u5e76\u907f\u514d\u5956\u52b1\u5df2\u5220\u9664\u8def\u5f84\u3002"}}
{"id": "2511.13940", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.13940", "abs": "https://arxiv.org/abs/2511.13940", "authors": ["Stuart H. Sul", "Simran Arora", "Benjamin F. Spector", "Christopher R\u00e9"], "title": "ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels", "comment": null, "summary": "Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \\times$ speedup for data- and tensor-parallel workloads, $4.08 \\times$ for sequence-parallel workloads, and $1.22 \\times$ for expert-parallel workloads.", "AI": {"tldr": "ParallelKittens (PK) \u662f\u4e00\u4e2a\u57fa\u4e8e ThunderKittens \u7684\u6781\u7b80 CUDA \u6846\u67b6\uff0c\u901a\u8fc7\u516b\u4e2a\u6838\u5fc3\u539f\u8bed\u548c\u7edf\u4e00\u7f16\u7a0b\u6a21\u677f\uff0c\u7cfb\u7edf\u5316\u6307\u5bfc\u591a GPU \u91cd\u53e0\u901a\u4fe1\u4e0e\u8ba1\u7b97\u5185\u6838\u7684\u8bbe\u8ba1\uff0c\u5728 Hopper \u548c Blackwell \u67b6\u6784\u4e0a\u663e\u8457\u63d0\u5347\u5404\u7c7b\u5e76\u884c\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u3002", "motivation": "\u968f\u7740 AI \u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0cGPU \u95f4\u901a\u4fe1\u5df2\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u73b0\u6709\u7cfb\u7edf\u867d\u91c7\u7528\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\u7b56\u7565\uff0c\u4f46\u5728\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u548c\u65b0\u578b\u52a0\u901f\u5668\u4e0a\u96be\u4ee5\u8fbe\u5230\u7406\u8bba\u5cf0\u503c\u6027\u80fd\u3002\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u662f\u5426\u53ef\u901a\u8fc7\u5c11\u91cf\u901a\u7528\u3001\u53ef\u590d\u7528\u7684\u539f\u5219\u7cfb\u7edf\u6027\u5730\u6307\u5bfc\u9ad8\u6548\u591a GPU \u5185\u6838\u8bbe\u8ba1\u3002", "method": "\u63d0\u51fa ParallelKittens\uff08PK\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u5bf9\u5f71\u54cd\u591a GPU \u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff08\u6570\u636e\u4f20\u8f93\u673a\u5236\u3001\u8d44\u6e90\u8c03\u5ea6\u548c\u8bbe\u8ba1\u5f00\u9500\uff09\u7684\u5168\u9762\u5206\u6790\uff0c\u63d0\u70bc\u51fa\u516b\u4e2a\u591a GPU \u5185\u6838\u8bbe\u8ba1\u539f\u8bed\uff0c\u5e76\u63d0\u4f9b\u7edf\u4e00\u7f16\u7a0b\u6a21\u677f\uff0c\u7b80\u5316\u91cd\u53e0\u5185\u6838\u5f00\u53d1\u3002", "result": "\u5728 Hopper \u548c Blackwell \u67b6\u6784\u4e0a\u9a8c\u8bc1\uff1a\u4ec5\u9700\u4e0d\u5230 50 \u884c\u8bbe\u5907\u4ee3\u7801\uff0cPK \u5728\u6570\u636e/\u5f20\u91cf\u5e76\u884c\u4efb\u52a1\u4e2d\u63d0\u901f\u6700\u9ad8\u8fbe 2.33 \u500d\uff0c\u5e8f\u5217\u5e76\u884c\u4efb\u52a1\u8fbe 4.08 \u500d\uff0c\u4e13\u5bb6\u5e76\u884c\u4efb\u52a1\u8fbe 1.22 \u500d\u3002", "conclusion": "ParallelKittens \u8bc1\u660e\u4e86\u901a\u8fc7\u5c11\u91cf\u901a\u7528\u8bbe\u8ba1\u539f\u5219\u53ef\u9ad8\u6548\u6784\u5efa\u9ad8\u6027\u80fd\u591a GPU \u91cd\u53e0\u5185\u6838\uff0c\u663e\u8457\u7b80\u5316\u5f00\u53d1\u5e76\u63d0\u5347\u591a\u79cd\u5e76\u884c\u6a21\u5f0f\u4e0b\u7684\u5b9e\u9645\u6027\u80fd\u3002"}}
{"id": "2511.14062", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14062", "abs": "https://arxiv.org/abs/2511.14062", "authors": ["Shenglin Zhang", "Ziang Chen", "Zijing Que", "Yilun Liu", "Yongqian Sun", "Sicheng Wei", "Dan Pei", "Hailin Li"], "title": "LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering", "comment": null, "summary": "Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLogPurge\u7684\u4f4e\u6210\u672c\u3001\u89c4\u5219\u589e\u5f3a\u7684\u65e5\u5fd7\u51c0\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u6ee4\u7b97\u6cd5\u81ea\u52a8\u4ece\u542b\u5f02\u5e38\u7684\u65e5\u5fd7\u5e8f\u5217\u4e2d\u7b5b\u9009\u51fa\u6b63\u5e38\u6837\u672c\u7528\u4e8e\u8bad\u7ec3\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5e72\u51c0\u65e0\u5f02\u5e38\u7684\u65e5\u5fd7\u6570\u636e\u8fdb\u884c\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u83b7\u53d6\u6b64\u7c7b\u6570\u636e\u9700\u6602\u8d35\u4e14\u7e41\u7410\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u800c\u73b0\u6709\u7684\u81ea\u52a8\u6e05\u6d17\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u7ed3\u5408\u65e5\u5fd7\u7684\u7279\u6027\u548c\u8bed\u4e49\u4fe1\u606f\u3002", "method": "LogPurge\u91c7\u7528\u4e24\u9636\u6bb5\u8fc7\u6ee4\u7b97\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53bb\u9664\u805a\u7c7b\u7684\u5f02\u5e38\u6a21\u5f0f\uff0c\u5e76\u589e\u5f3a\u7cfb\u7edf\u89c4\u5219\u4ee5\u63d0\u5347LLM\u5bf9\u7cfb\u7edf\u65e5\u5fd7\u7684\u7406\u89e3\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u5206\u6cbb\u7b56\u7565\uff0c\u5c06\u5269\u4f59\u6c61\u67d3\u533a\u57df\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u9012\u5f52\u5e94\u7528\u7b2c\u4e00\u9636\u6bb5\u6d41\u7a0b\u8fdb\u884c\u51c0\u5316\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5e73\u5747\u53ef\u53bb\u966498.74%\u7684\u5f02\u5e38\uff0c\u540c\u65f6\u4fdd\u755982.39%\u7684\u6b63\u5e38\u6837\u672c\uff1b\u76f8\u6bd4\u6700\u65b0\u65e0\u76d1\u7763\u65e5\u5fd7\u6837\u672c\u9009\u62e9\u7b97\u6cd5\uff0cF1\u5206\u6570\u5206\u522b\u63d0\u5347\u4e8635.7%\u300184.11%\u548c149.72%\u3002", "conclusion": "LogPurge\u80fd\u9ad8\u6548\u81ea\u52a8\u5730\u4ece\u6c61\u67d3\u65e5\u5fd7\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u6b63\u5e38\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.14116", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.14116", "abs": "https://arxiv.org/abs/2511.14116", "authors": ["Ziyi Xu", "Zhiqiang Xie", "Swapnil Gandhi", "Christos Kozyrakis"], "title": "FailSafe: High-performance Resilient Serving", "comment": null, "summary": "Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.", "AI": {"tldr": "FailSafe \u662f\u4e00\u79cd\u9762\u5411\u5f20\u91cf\u5e76\u884c\uff08TP\uff09\u7684\u5bb9\u9519\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5faa\u73af KVCache \u653e\u7f6e\u3001\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u611f\u77e5\u8def\u7531\uff0c\u5728 GPU \u6545\u969c\u60c5\u51b5\u4e0b\u4ecd\u80fd\u7ef4\u6301\u9ad8\u6027\u80fd\u4e0e\u8d44\u6e90\u5747\u8861\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u6062\u590d\u5ef6\u8fdf\u3002", "motivation": "\u5f20\u91cf\u5e76\u884c\u867d\u80fd\u63d0\u5347 LLM \u63a8\u7406\u6548\u7387\uff0c\u4f46\u5176\u5f3a\u8026\u5408\u7279\u6027\u5bfc\u81f4\u7cfb\u7edf\u5bf9 GPU \u6545\u969c\u6781\u4e3a\u654f\u611f\uff1a\u5355\u70b9\u6545\u969c\u4f1a\u4e2d\u65ad\u6267\u884c\u3001\u5f15\u53d1\u6602\u8d35\u7684 KVCache \u91cd\u8ba1\u7b97\uff0c\u5e76\u9020\u6210\u957f\u671f\u8ba1\u7b97\u4e0e\u5185\u5b58\u5931\u8861\u3002", "method": "FailSafe \u63d0\u51fa\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a(1) \u5faa\u73af KVCache \u653e\u7f6e\u4ee5\u5b9e\u73b0\u5185\u5b58\u5747\u5300\u5229\u7528\uff1b(2) \u6df7\u5408\u6ce8\u610f\u529b\uff08\u7ed3\u5408\u5f20\u91cf\u5e76\u884c\u4e0e\u6570\u636e\u5e76\u884c\uff09\u6d88\u9664\u62d6\u6162\u8282\u70b9\uff1b(3) \u7ec6\u7c92\u5ea6\u8d1f\u8f7d\u611f\u77e5\u8def\u7531\u52a8\u6001\u5e73\u8861\u8bf7\u6c42\u3002\u6b64\u5916\uff0c\u91c7\u7528\u4e3b\u52a8 KVCache \u5907\u4efd\u4e0e\u6309\u9700\u6743\u91cd\u6062\u590d\u673a\u5236\u907f\u514d\u91cd\u8ba1\u7b97\u548c\u5197\u4f59\u4f20\u8f93\uff0c\u5e76\u96c6\u6210\u5230\u8f7b\u91cf\u7ea7\u670d\u52a1\u5f15\u64ce\u4e2d\u3002", "result": "\u5728 8\u00d7H100 DGX \u7cfb\u7edf\u4e0a\u4f7f\u7528\u771f\u5b9e\u6545\u969c\u8f68\u8ff9\u548c\u5178\u578b\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\uff0cFailSafe \u76f8\u6bd4\u6807\u51c6\u5bb9\u9519\u65b9\u6cd5\u6700\u9ad8\u5b9e\u73b0 2 \u500d\u541e\u5410\u91cf\u63d0\u5347\u548c\u4e24\u4e2a\u6570\u91cf\u7ea7\u66f4\u4f4e\u7684\u6062\u590d\u5ef6\u8fdf\uff1b\u5373\u4f7f\u5728\u6700\u591a 3 \u4e2a GPU \u6545\u969c\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u541e\u5410\u4e0e\u8d44\u6e90\u5747\u8861\u3002", "conclusion": "FailSafe \u80fd\u5728\u52a8\u6001\u4e14\u4e0d\u53ef\u9760\u7684\u786c\u4ef6\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u3001\u9ad8\u6548\u7684 LLM \u63a8\u7406\u670d\u52a1\uff0c\u663e\u8457\u63d0\u5347\u5f20\u91cf\u5e76\u884c\u7cfb\u7edf\u7684\u5bb9\u9519\u80fd\u529b\u4e0e\u6027\u80fd\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.14215", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14215", "abs": "https://arxiv.org/abs/2511.14215", "authors": ["Malik Muhammad Umer"], "title": "A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints", "comment": null, "summary": "The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u4e13\u4e3aDO-178C\u5408\u89c4\u7684\u822a\u7a7a\u5b89\u5168\u5173\u952e\u8f6f\u4ef6\u5b9a\u5236\u7684Scrum\u654f\u6377\u6846\u67b6\uff0c\u901a\u8fc7\u89d2\u8272\u3001\u5de5\u4ef6\u548c\u4e8b\u4ef6\u7684\u8c03\u6574\uff0c\u5728\u6ee1\u8db3\u4e25\u683c\u8ba4\u8bc1\u8981\u6c42\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5f00\u53d1\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "motivation": "\u822a\u7a7a\u822a\u5929\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\uff0c\u4e9f\u9700\u5728\u4fdd\u6301\u654f\u6377\u6027\u7684\u540c\u65f6\u6ee1\u8db3\u4e25\u82db\u7684\u5b89\u5168\u6027\u548c\u9002\u822a\u8ba4\u8bc1\u8981\u6c42\uff1b\u4f20\u7edf\u654f\u6377\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u9002\u7528\u4e8eDO-178C\u7b49\u5b89\u5168\u5173\u952e\u6807\u51c6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5408\u89c4\u53c8\u80fd\u53d1\u6325\u654f\u6377\u4f18\u52bf\u7684\u5f00\u53d1\u6846\u67b6\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u5b9e\u8bc1\u68c0\u9a8c\u7684Scrum\u53d8\u4f53\u6846\u67b6\uff0c\u5f15\u5165\u591a\u5b66\u79d1\u4ea7\u54c1\u8d1f\u8d23\u4eba\u6a21\u578b\u3001\u53cc\u91cd\u9a8c\u6536\u6807\u51c6\uff08\u5408\u89c4\u6027+\u529f\u80fd\u6027\uff09\u3001\u72ec\u7acb\u6d4b\u8bd5\u4e0e\u6587\u6863\u56e2\u961f\u4ee5\u53ca\u4e13\u804c\u8ba4\u8bc1\u8054\u7edc\u4eba\uff0c\u5e76\u5728\u4e24\u4e2a\u53ef\u6bd4\u9879\u76ee\u4e2d\uff08\u4e00\u4e2a\u91c7\u7528\u8be5\u5b9a\u5236\u654f\u6377\u6d41\u7a0b\uff0c\u53e6\u4e00\u4e2a\u91c7\u7528\u4f20\u7edf\u7011\u5e03\u6a21\u578b\uff09\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u91c7\u7528\u5b9a\u5236\u654f\u6377\u6846\u67b6\u7684\u9879\u76ee\u5b9e\u73b0\u4e86\u6bcf\u9700\u6c42\u603b\u5de5\u4f5c\u91cf\u51cf\u5c1176%\u3001\u7f3a\u9677\u53d1\u73b0\u901f\u5ea6\u63d0\u534775%\u3001\u7f3a\u9677\u4fee\u590d\u901f\u5ea6\u63d0\u534778%\u3001\u7f3a\u9677\u5bc6\u5ea6\u964d\u4f4e\u8d8550%\uff0c\u540c\u65f6\u5b8c\u5168\u6ee1\u8db3DO-178C DAL A\u7ea7\u5408\u89c4\u8981\u6c42\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u6709\u7eaa\u5f8b\u7684\u6d41\u7a0b\u88c1\u526a\u548c\u4e0e\u8ba4\u8bc1\u673a\u6784\u7684\u4e3b\u52a8\u534f\u4f5c\uff0c\u654f\u6377\u5b9e\u8df5\u4e0e\u822a\u7a7a\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u7684\u5408\u89c4\u8981\u6c42\u53ef\u4ee5\u6709\u6548\u5171\u5b58\uff1b\u672a\u6765\u53ef\u901a\u8fc7\u81ea\u52a8\u5316\u3001CI/CD\u53ca\u6587\u6863/\u9a8c\u8bc1/\u914d\u7f6e\u7ba1\u7406\u7684\u81ea\u52a8\u5316\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u76ca\u3002"}}
{"id": "2511.14124", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14124", "abs": "https://arxiv.org/abs/2511.14124", "authors": ["Sabiha Afroz", "Redwan Ibne Seraj Khan", "Hadeel Albahar", "Jingoo Han", "Ali R. Butt"], "title": "10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training", "comment": "This paper accepted for presentation to the 16th ACM Symposium on Cloud Computing (SOCC'25)", "summary": "Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.\n  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.", "AI": {"tldr": "10Cache \u662f\u4e00\u79cd\u9762\u5411\u4e91\u73af\u5883\u7684\u8d44\u6e90\u611f\u77e5\u5f20\u91cf\u7f13\u5b58\u4e0e\u8fc1\u79fb\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316 GPU\u3001CPU \u548c NVMe \u4e4b\u95f4\u7684\u5185\u5b58\u534f\u540c\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4e91\u7aef\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34 GPU \u5185\u5b58\u5bb9\u91cf\u6709\u9650\u548c\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u73b0\u6709\u5185\u5b58\u5378\u8f7d\u65b9\u6cd5\u5b58\u5728\u5f20\u91cf\u8fc1\u79fb\u5ef6\u8fdf\u9ad8\u548c\u8bbe\u5907\u5185\u5b58\u5229\u7528\u7387\u4f4e\u7684\u7f3a\u9677\uff0c\u5bfc\u81f4\u8bad\u7ec3\u65f6\u95f4\u5ef6\u957f\u548c\u4e91\u6210\u672c\u589e\u52a0\u3002", "method": "10Cache \u901a\u8fc7\u5206\u6790\u5f20\u91cf\u6267\u884c\u987a\u5e8f\u6784\u5efa\u9884\u53d6\u7b56\u7565\uff0c\u5728\u56fa\u5b9a\u5185\u5b58\u4e2d\u6309\u5f20\u91cf\u5927\u5c0f\u5206\u5e03\u5206\u914d\u7f13\u51b2\u533a\uff0c\u5e76\u91cd\u7528\u7f13\u51b2\u533a\u4ee5\u51cf\u5c11\u5206\u914d\u5f00\u9500\uff0c\u5b9e\u73b0\u8de8 GPU\u3001CPU \u548c NVMe \u7684\u667a\u80fd\u5185\u5b58\u534f\u8c03\u3002", "result": "\u5728\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0c10Cache \u76f8\u6bd4\u73b0\u6709\u5148\u8fdb\u5378\u8f7d\u65b9\u6cd5\uff0c\u8bad\u7ec3\u901f\u5ea6\u6700\u9ad8\u63d0\u5347 2 \u500d\uff0cGPU \u7f13\u5b58\u547d\u4e2d\u7387\u6700\u9ad8\u63d0\u5347 86.6 \u500d\uff0cCPU \u548c GPU \u5185\u5b58\u5229\u7528\u7387\u5206\u522b\u6700\u9ad8\u63d0\u5347 2.15 \u500d\u548c 1.33 \u500d\u3002", "conclusion": "10Cache \u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u6709\u6548\u4f18\u5316\u4e91\u73af\u5883\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u7684\u541e\u5410\u91cf\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u3002"}}
{"id": "2511.14224", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14224", "abs": "https://arxiv.org/abs/2511.14224", "authors": ["Anji Li", "Mingwei Liu", "Zhenxi Chen", "Zheng Pei", "Zike Li", "Dekun Dai", "Yanlin Wang", "Zibin Zheng"], "title": "KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation", "comment": "13 pages, 11 figures", "summary": "Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.", "AI": {"tldr": "KTester is a knowledge-enhanced framework that improves LLM-based unit test generation by integrating project-specific and testing-domain knowledge, achieving better correctness, coverage, readability, and maintainability than existing methods.", "motivation": "Existing LLM-based unit test generators often produce tests that lack correctness and maintainability in real-world software projects, due to insufficient contextual and domain-specific guidance.", "method": "KTester extracts project structure and usage patterns via static analysis, separates test case design from test method generation guided by testing-domain knowledge, and uses multi-perspective prompting with structured templates to steer the LLM.", "result": "KTester outperforms state-of-the-art baselines by 5.69% in execution pass rate and 8.83% in line coverage, while generating fewer tests in less time; human evaluators also rate its outputs higher in correctness, readability, and maintainability.", "conclusion": "Integrating project-specific and testing-domain knowledge into LLM-based test generation significantly enhances the quality and practicality of automatically generated unit tests."}}
{"id": "2511.14456", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.14456", "abs": "https://arxiv.org/abs/2511.14456", "authors": ["Fabian Stricker", "David Bermbach", "Christian Zirpins"], "title": "Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning", "comment": "Accepted for publication in 3rd IEEE International Conference on Federated Learning Applications and Technologies (FLTA2025)", "summary": "Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.\n  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8de8\u7ec4\u7ec7\u5c0f\u89c4\u6a21\u53c2\u4e0e\u7684\u8de8\u5b64\u5c9b\u8054\u90a6\u5b66\u4e60\u4e2d\u53c2\u4e0e\u8005\u5931\u8d25\u5bf9\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6570\u636e\u504f\u659c\u4f1a\u5bfc\u81f4\u8bc4\u4f30\u8fc7\u4e8e\u4e50\u89c2\uff0c\u4e14\u5931\u8d25\u53d1\u751f\u7684\u65f6\u673a\u663e\u8457\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5728\u8de8\u5b64\u5c9b\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u53c2\u4e0e\u7ec4\u7ec7\u53ef\u80fd\u56e0\u901a\u4fe1\u95ee\u9898\u6216\u914d\u7f6e\u9519\u8bef\u800c\u5931\u8d25\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u6b64\u7c7b\u5931\u8d25\u5bf9\u6a21\u578b\u8d28\u91cf\u5f71\u54cd\u7684\u6df1\u5165\u7814\u7a76\uff0c\u5c24\u5176\u76f8\u8f83\u4e8e\u8de8\u8bbe\u5907\u8054\u90a6\u5b66\u4e60\u800c\u8a00\u3002", "method": "\u5f00\u5c55\u4e86\u4e00\u9879\u5e7f\u6cdb\u5b9e\u9a8c\u7814\u7a76\uff0c\u5206\u6790\u53c2\u4e0e\u8005\u5931\u8d25\u5bf9\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u5931\u8d25\u53d1\u751f\u7684\u65f6\u95f4\u3001\u6570\u636e\u5206\u5e03\u7279\u6027\u4ee5\u53ca\u5bf9\u6a21\u578b\u8bc4\u4f30\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u9ad8\u5ea6\u6570\u636e\u504f\u659c\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u8bc4\u4f30\u7ed3\u679c\u8fc7\u4e8e\u4e50\u89c2\uff0c\u63a9\u76d6\u4e86\u5931\u8d25\u7684\u771f\u5b9e\u5f71\u54cd\uff1b\u540c\u65f6\uff0c\u5931\u8d25\u53d1\u751f\u7684\u65f6\u673a\u5bf9\u6700\u7ec8\u6a21\u578b\u8d28\u91cf\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u9c81\u68d2\u7684\u8de8\u5b64\u5c9b\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u7814\u7a76\u4eba\u5458\u548c\u8f6f\u4ef6\u67b6\u6784\u5e08\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u5bf9\u53c2\u4e0e\u8005\u5931\u8d25\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2511.14435", "categories": ["cs.SE", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.14435", "abs": "https://arxiv.org/abs/2511.14435", "authors": ["Angelo Ferrando"], "title": "Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems", "comment": "In Proceedings FMAS 2025, arXiv:2511.13245", "summary": "Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u8fd0\u884c\u65f6\u9a8c\u8bc1\uff08RV\uff09\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u534f\u540c\u6574\u5408\uff0c\u4ee5\u63d0\u5347\u81ea\u4e3b\u7cfb\u7edf\u7684\u53ef\u4fe1\u6027\uff1aRV\u4e3aLLM\u63d0\u4f9b\u5b89\u5168\u4fdd\u969c\uff0cLLM\u5219\u589e\u5f3aRV\u5728\u89c4\u8303\u83b7\u53d6\u3001\u9884\u6d4b\u63a8\u7406\u548c\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u65b9\u9762\u7684\u80fd\u529b\u3002", "motivation": "\u5728\u5305\u542b\u5b66\u4e60\u578b\u7ec4\u4ef6\u548c\u5f00\u653e\u73af\u5883\u7684\u81ea\u4e3b\u7cfb\u7edf\u4e2d\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u6027\u4e0e\u53ef\u4fe1\u6027\u6781\u5177\u6311\u6218\u3002\u5f62\u5f0f\u5316\u65b9\u6cd5\u867d\u80fd\u63d0\u4f9b\u5f3a\u4fdd\u8bc1\uff0c\u4f46\u4f9d\u8d56\u5b8c\u6574\u6a21\u578b\u548c\u9759\u6001\u5047\u8bbe\uff1b\u800c\u5927\u8bed\u8a00\u6a21\u578b\u867d\u64c5\u957f\u81ea\u7136\u8bed\u8a00\u5230\u5f62\u5f0f\u5316\u8868\u8fbe\u7684\u8f6c\u6362\u53ca\u6a21\u5f0f\u8bc6\u522b\uff0c\u5374\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4fdd\u969c\u4e14\u6613\u51fa\u9519\u3002", "method": "\u63d0\u51fa\u4e00\u79cdRV\u4e0eLLMs\u7684\u5171\u751f\u96c6\u6210\u6846\u67b6\uff0c\u5176\u4e2dRV\u4f5c\u4e3aLLM\u9a71\u52a8\u81ea\u4e3b\u6027\u7684\u201c\u62a4\u680f\u201d\uff0c\u800cLLM\u5219\u8f85\u52a9RV\u8fdb\u884c\u89c4\u8303\u6355\u83b7\u3001\u652f\u6301\u524d\u77bb\u6027\u63a8\u7406\u5e76\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u8bba\u6587\u52fe\u52d2\u4e86\u8be5\u534f\u540c\u673a\u5236\u5982\u4f55\u533a\u522b\u4e8e\u73b0\u6709\u7efc\u8ff0\u4e0e\u8def\u7ebf\u56fe\uff0c\u63a2\u8ba8\u4e86\u76f8\u5173\u6311\u6218\u3001\u8ba4\u8bc1\u5f71\u54cd\uff0c\u5e76\u6307\u660e\u4e86\u9762\u5411\u53ef\u9760\u81ea\u4e3b\u7cfb\u7edf\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "RV\u4e0eLLMs\u7684\u76f8\u4e92\u589e\u5f3a\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\uff0c\u4f46\u4ecd\u9700\u514b\u670d\u6280\u672f\u4e0e\u8ba4\u8bc1\u5c42\u9762\u7684\u591a\u91cd\u6311\u6218\u3002"}}
{"id": "2511.14608", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.14608", "abs": "https://arxiv.org/abs/2511.14608", "authors": ["Dave Dice", "Alex Kogan"], "title": "Hapax Locks : Value-Based Mutual Exclusion", "comment": null, "summary": "We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.", "AI": {"tldr": "Hapax Locks \u662f\u4e00\u79cd\u65b0\u578b\u9501\u7b97\u6cd5\uff0c\u5177\u6709\u5e38\u6570\u65f6\u95f4\u7684\u52a0\u9501/\u89e3\u9501\u8def\u5f84\u3001FIFO \u6392\u961f\u987a\u5e8f\u3001\u7a7a\u95f4\u9ad8\u6548\u3001\u4f4e\u7f13\u5b58\u4e00\u81f4\u6027\u5f00\u9500\uff0c\u5e76\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\u3002", "motivation": "\u73b0\u6709\u9ad8\u6027\u80fd\u9501\u901a\u5e38\u5bf9\u8fd0\u884c\u65f6\u73af\u5883\u6709\u8f83\u591a\u4f9d\u8d56\u6216\u7ea6\u675f\uff0c\u96be\u4ee5\u5728\u5df2\u6709\u7cfb\u7edf\u4e2d\u96c6\u6210\uff1b\u4f5c\u8005\u5e0c\u671b\u8bbe\u8ba1\u4e00\u79cd\u7b80\u5355\u3001\u9ad8\u6548\u4e14\u517c\u5bb9\u6027\u5f3a\u7684\u9501\u673a\u5236\u3002", "method": "\u63d0\u51fa Hapax Locks \u7b97\u6cd5\uff0c\u5176\u7279\u70b9\u5305\u62ec\u5e38\u6570\u65f6\u95f4\u7684\u5230\u8fbe\u4e0e\u89e3\u9501\u8def\u5f84\u3001FIFO \u5165\u961f\u987a\u5e8f\u3001\u65e0\u6307\u9488\u5728\u7ebf\u7a0b\u95f4\u8f6c\u79fb\u6216\u9003\u9038\u6240\u6709\u6743\u3002", "result": "Hapax Locks \u5728\u5ef6\u8fdf\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u6027\u80fd\u5ab2\u7f8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u9501\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5bf9\u8fd0\u884c\u65f6\u73af\u5883\u7684\u4f9d\u8d56\u3002", "conclusion": "Hapax Locks \u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5177\u5907\u826f\u597d\u7684\u517c\u5bb9\u6027\u548c\u5b9e\u73b0\u7b80\u6d01\u6027\uff0c\u9002\u5408\u5728\u73b0\u6709\u7cfb\u7edf\u4e2d\u90e8\u7f72\u6216\u6539\u9020\u4f7f\u7528\u3002"}}
{"id": "2511.14528", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14528", "abs": "https://arxiv.org/abs/2511.14528", "authors": ["Tatiane Ornelas", "Allysson Allex Ara\u00fajo", "J\u00falia Ara\u00fajo", "Marina Ara\u00fajo", "Bianca Trinkenreich", "Marcos Kalinowski"], "title": "LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations", "comment": null, "summary": "[Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5de5\u4f5c\u574a\u63a2\u8ba8\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u8d44\u6df1\u7814\u7a76\u8005\u5982\u4f55\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e3b\u9898\u5206\u6790\u4e2d\u7684\u673a\u9047\u4e0e\u98ce\u9669\uff0c\u53d1\u73b0LLMs\u53ef\u63d0\u5347\u6548\u7387\u4f46\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\u89e3\u91ca\uff0c\u5f3a\u8c03\u9700\u4fdd\u6301\u4eba\u5de5\u76d1\u7763\u4e0e\u63d0\u793a\u7d20\u517b\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u5b9a\u6027\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u5176\u5bf9\u89e3\u91ca\u6027\u5206\u6790\u8fc7\u7a0b\uff08\u5982\u4e3b\u9898\u5206\u6790\uff09\u5728\u4e25\u8c28\u6027\u3001\u900f\u660e\u5ea6\u548c\u7814\u7a76\u8005\u4e3b\u4f53\u6027\u65b9\u9762\u7684\u65b9\u6cd5\u8bba\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u6df1\u5165\u63a2\u8ba8\u3002", "method": "\u7ec4\u7ec725\u4f4dISERN\u7814\u7a76\u4eba\u5458\u53c2\u4e0e\u53cd\u601d\u6027\u5de5\u4f5c\u574a\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8ba8\u8bba\u805a\u7126LLM\u8f85\u52a9\u7684\u5f00\u653e\u5f0f\u7f16\u7801\u3001\u4e3b\u9898\u751f\u6210\u4e0e\u4e3b\u9898\u5ba1\u67e5\uff0c\u5e76\u4f7f\u7528\u5f69\u8272\u753b\u5e03\u8bb0\u5f55\u53c2\u4e0e\u8005\u5bf9\u673a\u4f1a\u3001\u5c40\u9650\u4e0e\u5efa\u8bae\u7684\u770b\u6cd5\u3002", "result": "\u53c2\u4e0e\u8005\u8ba4\u53efLLM\u5e26\u6765\u7684\u6548\u7387\u4e0e\u53ef\u6269\u5c55\u6027\u4f18\u52bf\uff0c\u4f46\u4e5f\u6307\u51fa\u5176\u5b58\u5728\u504f\u89c1\u3001\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e22\u5931\u3001\u53ef\u590d\u73b0\u6027\u5dee\u53ca\u6a21\u578b\u5feb\u901f\u6f14\u8fdb\u7b49\u98ce\u9669\uff0c\u5e76\u5f3a\u8c03\u63d0\u793a\u7d20\u517b\u548c\u6301\u7eed\u4eba\u5de5\u76d1\u7763\u7684\u91cd\u8981\u6027\u3002", "conclusion": "LLM\u5e94\u88ab\u89c6\u4e3a\u652f\u6301\u800c\u975e\u66ff\u4ee3\u89e3\u91ca\u6027\u5206\u6790\u7684\u5de5\u5177\uff1b\u7814\u7a76\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u793e\u533a\u8d1f\u8d23\u4efb\u5730\u5c06LLM\u878d\u5165\u5b9a\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u53cd\u601d\u4e0e\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2511.14617", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.14617", "abs": "https://arxiv.org/abs/2511.14617", "authors": ["Ruoyu Qin", "Weiran He", "Weixiao Huang", "Yangkun Zhang", "Yikai Zhao", "Bo Pang", "Xinran Xu", "Yingdi Shan", "Yongwei Wu", "Mingxing Zhang"], "title": "Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning", "comment": "16 pages, 12 figures, 6 tables", "summary": "Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.", "AI": {"tldr": "Seer \u662f\u4e00\u79cd\u65b0\u578b\u5728\u7ebf\u4e0a\u4e0b\u6587\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5229\u7528\u5171\u4eab\u76f8\u540c\u63d0\u793a\u7684\u8bf7\u6c42\u5728\u8f93\u51fa\u957f\u5ea6\u548c\u751f\u6210\u6a21\u5f0f\u4e0a\u7684\u76f8\u4f3c\u6027\uff0c\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d rollout \u9636\u6bb5\u7684\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u957f\u5c3e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u540c\u6b65 RL \u7cfb\u7edf\u5728 rollout \u9636\u6bb5\u5b58\u5728\u4e25\u91cd\u6027\u80fd\u74f6\u9888\uff0c\u5305\u62ec\u957f\u5c3e\u5ef6\u8fdf\u9ad8\u548c\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u4e0d\u5e73\u8861\u3002", "method": "Seer \u5f15\u5165\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\u5206\u7247 rollout \u5b9e\u73b0\u52a8\u6001\u8d1f\u8f7d\u5747\u8861\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u5ea6\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u5206\u7ec4\u63a8\u6d4b\u89e3\u7801\u3002", "result": "\u5728\u751f\u4ea7\u7ea7 RL \u5de5\u4f5c\u8d1f\u8f7d\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSeer \u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u540c\u6b65 RL \u7cfb\u7edf\uff0c\u7aef\u5230\u7aef rollout \u541e\u5410\u91cf\u63d0\u5347 74%\u201397%\uff0c\u957f\u5c3e\u5ef6\u8fdf\u964d\u4f4e 75%\u201393%\u3002", "conclusion": "Seer \u663e\u8457\u52a0\u901f\u4e86 RL \u8bad\u7ec3\u8fed\u4ee3\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86 rollout \u9636\u6bb5\u7684\u6027\u80fd\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.14618", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.14618", "abs": "https://arxiv.org/abs/2511.14618", "authors": ["Severin Kohler", "Jordi Piera Jim\u00e9nez", "Michael Anywar", "Lars Fuhrmann", "Heather Leslie", "Maximilian Meixner", "Julian Sa\u00df", "Florian K\u00e4rcher", "Diego Bosc\u00e1", "Birger Haarbrandt", "Michael Marschollek", "Roland Eils"], "title": "FHIRconnect: Towards a seamless integration of openEHR and FHIR", "comment": "27 pages, 4 figures", "summary": "Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FHIRconnect\uff0c\u4e00\u79cd\u7528\u4e8eopenEHR\u4e0eHL7 FHIR\u4e4b\u95f4\u6807\u51c6\u5316\u53cc\u5411\u6570\u636e\u8f6c\u6362\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u53ca\u5f00\u6e90\u5f15\u64ce\uff0c\u901a\u8fc7\u4e09\u5c42\u67b6\u6784\u5b9e\u73b065%\u7684\u6620\u5c04\u590d\u7528\uff0c\u5e76\u8986\u76d6\u591a\u4e2a\u4e34\u5e8a\u9886\u57df\u7684\u56fd\u9645\u539f\u578b\u4e0eFHIR\u914d\u7f6e\u6587\u4ef6\u3002", "motivation": "\u7531\u4e8eopenEHR\u4e0eHL7 FHIR\u5728\u6570\u636e\u5efa\u6a21\u65b9\u6cd5\u4e0a\u7684\u6839\u672c\u5dee\u5f02\u4ee5\u53ca\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8f6c\u6362\u673a\u5236\uff0c\u533b\u7597\u7cfb\u7edf\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\u9762\u4e34\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u4e09\u5c42\u67b6\u6784\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u548c\u5f00\u6e90\u8f6c\u6362\u5f15\u64ce\uff08openFHIR\uff09\uff0c\u7ed3\u5408\u56fd\u9645\u539f\u578b\u57fa\u7840\u4e0e\u672c\u5730\u5b9a\u5236\u652f\u6301\uff0c\u5b9e\u73b0openEHR\u4e0eFHIR\u4e4b\u95f4\u7684\u53cc\u5411\u6620\u5c04\u3002", "result": "\u6210\u529f\u5c0624\u4e2a\u56fd\u9645openEHR\u539f\u578b\u6620\u5c04\u52307\u4e2a\u4e34\u5e8a\u9886\u57df\u4e2d\u768415\u4e2aFHIR\u914d\u7f6e\u6587\u4ef6\uff0c\u6620\u5c04\u590d\u7528\u7387\u8fbe65%\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5f00\u653e\u7684\u9ad8\u5f71\u54cd\u529b\u4e34\u5e8a\u539f\u578b\u6620\u5c04\u5e93\u3002", "conclusion": "FHIRconnect\u4e3a\u793e\u533a\u9a71\u52a8\u7684openEHR-FHIR\u6620\u5c04\u6807\u51c6\u5316\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\uff0c\u51cf\u5c11\u4e86\u5bf9\u5b9a\u5236ETL\u65b9\u6848\u7684\u4f9d\u8d56\uff0c\u63a8\u52a8\u4e86\u57fa\u4e8e\u5f00\u653e\u6807\u51c6\u7684\u533b\u7597IT\u7cfb\u7edf\u5728\u8bed\u6cd5\u4e0e\u8bed\u4e49\u5c42\u9762\u7684\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2509.12443", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.12443", "abs": "https://arxiv.org/abs/2509.12443", "authors": ["Sparsh Gupta", "Kamalavasan Kamalakkannan", "Maxim Moraru", "Galen Shipman", "Patrick Diehl"], "title": "From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow", "comment": "12 pages, 6 figures, 7 tables", "summary": "Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM \"agents\" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5c06\u9057\u7559 Fortran \u4ee3\u7801\u8f6c\u6362\u4e3a\u6027\u80fd\u53ef\u79fb\u690d\u7684 Kokkos C++ \u4ee3\u7801\uff0c\u5b9e\u73b0\u5728\u5f02\u6784 GPU \u67b6\u6784\u4e0a\u7684\u9ad8\u6548\u8fd0\u884c\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4ed8\u8d39\u6a21\u578b\uff08\u5982 GPT-5\uff09\u80fd\u4ee5\u8f83\u4f4e\u6210\u672c\u751f\u6210\u4f18\u4e8e\u539f\u59cb Fortran \u7684\u4f18\u5316\u4ee3\u7801\uff0c\u800c\u5f00\u6e90\u6a21\u578b\u5219\u5e38\u65e0\u6cd5\u4ea7\u51fa\u53ef\u7528\u7ed3\u679c\u3002", "motivation": "\u968f\u7740\u9ad8\u6027\u80fd\u8ba1\u7b97\u5411 GPU \u5f02\u6784\u67b6\u6784\u6f14\u8fdb\uff0c\u5927\u91cf\u4f9d\u8d56\u4f20\u7edf Fortran \u7f16\u5199\u7684\u79d1\u5b66\u5e94\u7528\u9762\u4e34\u7f3a\u4e4f\u539f\u751f GPU \u652f\u6301\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u73b0\u4ee3\u5316\u6539\u9020\u4ee5\u5b9e\u73b0\u8de8\u5e73\u53f0\u6027\u80fd\u53ef\u79fb\u690d\u6027\u3002\u7136\u800c\uff0c\u624b\u52a8\u5c06 Fortran \u8f6c\u6362\u4e3a Kokkos C++ \u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u56e0\u6b64\u63a2\u7d22\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5168\u81ea\u52a8\u8f6c\u6362\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7531\u591a\u4e2a LLM \u667a\u80fd\u4f53\u534f\u4f5c\u7684 AI \u5de5\u4f5c\u6d41\uff0c\u6db5\u76d6\u7ffb\u8bd1\u3001\u9a8c\u8bc1\u3001\u7f16\u8bd1\u3001\u8fd0\u884c\u3001\u6d4b\u8bd5\u3001\u8c03\u8bd5\u548c\u4f18\u5316\u7b49\u73af\u8282\uff0c\u5c06 Fortran \u5e76\u884c\u6838\u51fd\u6570\u81ea\u52a8\u8f6c\u6362\u4e3a Kokkos C++ \u7a0b\u5e8f\u3002", "result": "\u8be5\u6d41\u7a0b\u6210\u529f\u73b0\u4ee3\u5316\u4e86\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6838\u51fd\u6570\uff0c\u5728\u4e0d\u540c\u786c\u4ef6\u4e0a\u751f\u6210\u4e86\u6027\u80fd\u53ef\u79fb\u690d\u7684 Kokkos \u4ee3\u7801\uff1b\u4f7f\u7528 GPT-5 \u7b49\u4ed8\u8d39\u6a21\u578b\u4ec5\u82b1\u8d39\u51e0\u7f8e\u5143\u5373\u53ef\u751f\u6210\u4f18\u4e8e Fortran \u539f\u59cb\u7248\u672c\u7684\u4f18\u5316\u4ee3\u7801\uff0c\u800c Llama4-Maverick \u7b49\u5f00\u6e90\u6a21\u578b\u901a\u5e38\u65e0\u6cd5\u751f\u6210\u53ef\u8fd0\u884c\u4ee3\u7801\u3002", "conclusion": "\u672c\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728 Fortran \u5230 Kokkos \u81ea\u52a8\u8f6c\u6362\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u9057\u7559\u79d1\u5b66\u5e94\u7528\u7684\u81ea\u4e3b\u73b0\u4ee3\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\uff0c\u5e76\u5c55\u793a\u4e86 LLM \u5728\u79d1\u5b66\u4e0e\u7cfb\u7edf\u9886\u57df\u6267\u884c\u7ed3\u6784\u5316\u3001\u9886\u57df\u7279\u5b9a\u63a8\u7406\u4efb\u52a1\u7684\u6f5c\u529b\u3002"}}
