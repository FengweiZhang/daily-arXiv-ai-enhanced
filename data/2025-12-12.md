<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.SE](#cs.SE) [Total: 10]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [PANDAExpress: a Simpler and Faster PANDA Algorithm](https://arxiv.org/abs/2512.10217)
*Mahmoud Abo Khamis,Hung Q. Ngo,Dan Suciu*

Main category: cs.DB

TL;DR: 本文提出了一种名为PANDAExpress的新算法，通过引入新的概率不等式和基于数据偏斜动态构建的超平面划分策略，解决了原PANDA算法中隐藏的较大polylog(N)因子问题，在保持通用性的同时显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: PANDA算法虽在理论上具有通用性和最优时间复杂度（在特定条件下），但其运行时间中隐藏的较大polylog(N)因子使其在实践中不可行，远逊于专用算法。因此，亟需一种既保留PANDA通用性又具备实用效率的新方法。

Method: 作者首先证明了一个新的概率不等式，用于在任意度约束下上界估计析取数据日志规则（DDRs）的输出规模；随后，基于该不等式的证明思路，设计了新算法PANDAExpress。其核心创新在于采用动态构造的任意方向超平面（而非PANDA中的坐标轴平行超平面）进行划分，并在整个执行过程中跟踪数据偏斜统计信息以指导超平面选择。

Result: PANDAExpress成功去除了PANDA中的polylog(N)因子，在保持对任意度约束、自由变量、合取查询（CQs）与析取数据日志规则（DDRs）的通用支持的同时，达到了与复杂专用算法相匹配的运行效率。

Conclusion: PANDAExpress在不牺牲PANDA框架通用性的前提下，显著提升了实际运行效率，解决了原算法因polylog因子过大而缺乏实用性的关键缺陷，为处理带度约束的复杂查询提供了一个高效且通用的新工具。

Abstract: PANDA is a powerful generic algorithm for answering conjunctive queries (CQs) and disjunctive datalog rules (DDRs) given input degree constraints. In the special case where degree constraints are cardinality constraints and the query is Boolean, PANDA runs in $\tilde O (N^{subw})$-time, where $N$ is the input size, and $subw$ is the submodular width of the query, a notion introduced by Daniel Marx (JACM 2013). When specialized to certain classes of sub-graph pattern finding problems, the $\tilde O(N^{subw})$ runtime matches the optimal runtime possible, modulo some conjectures in fine-grained complexity (Bringmann and Gorbachev (STOC 25)). The PANDA framework is much more general, as it handles arbitrary input degree constraints, which capture common statistics and integrity constraints used in relational database management systems, it works for queries with free variables, and for both CQs and DDRs.
  The key weakness of PANDA is the large $polylog(N)$-factor hidden in the $\tilde O(\cdot)$ notation. This makes PANDA completely impractical, and fall short of what is achievable with specialized algorithms. This paper resolves this weakness with two novel ideas. First, we prove a new probabilistic inequality that upper-bounds the output size of DDRs under arbitrary degree constraints. Second, the proof of this inequality directly leads to a new algorithm named PANDAExpress that is both simpler and faster than PANDA. The novel feature of PANDAExpress is a new partitioning scheme that uses arbitrary hyperplane cuts instead of axis-parallel hyperplanes used in PANDA. These hyperplanes are dynamically constructed based on data-skewness statistics carefully tracked throughout the algorithm's execution. As a result, PANDAExpress removes the $polylog(N)$-factor from the runtime of PANDA, matching the runtimes of intricate specialized algorithms, while retaining all its generality and power.

</details>


### [2] [Efficient Hypergraph Pattern Matching via Match-and-Filter and Intersection Constraint](https://arxiv.org/abs/2512.10621)
*Siwoo Song,Wonseok Shin,Kunsoo Park,Giuseppe F. Italiano,Zhengyi Yang,Wenjie Zhang*

Main category: cs.DB

TL;DR: 本文提出了一种新颖的超图模式匹配算法，通过引入交集约束、候选超边空间和Match-and-Filter框架，显著提升了查询处理效率。


<details>
  <summary>Details</summary>
Motivation: 超图模式匹配是超图分析中的基本问题，但现有方法在处理复杂多对多关系时效率较低，亟需更高效的算法。

Method: 提出三项关键技术：（1）交集约束，作为有效嵌入的充要条件以加速验证；（2）候选超边空间，用于存储查询与数据超图之间可能的超边映射；（3）Match-and-Filter框架，在回溯过程中交替执行匹配与过滤，仅保留兼容的候选映射。

Result: 在真实数据集上的实验表明，该算法在查询处理时间上比现有最先进算法快几个数量级。

Conclusion: 所提出的算法通过结合交集约束、候选超边空间和Match-and-Filter框架，显著提高了超图模式匹配的效率，具有良好的实际应用前景。

Abstract: A hypergraph is a generalization of a graph, in which a hyperedge can connect multiple vertices, modeling complex relationships involving multiple vertices simultaneously. Hypergraph pattern matching, which is to find all isomorphic embeddings of a query hypergraph in a data hypergraph, is one of the fundamental problems. In this paper, we present a novel algorithm for hypergraph pattern matching by introducing (1) the intersection constraint, a necessary and sufficient condition for valid embeddings, which significantly speeds up the verification process, (2) the candidate hyperedge space, a data structure that stores potential mappings between hyperedges in the query hypergraph and the data hypergraph, and (3) the Match-and-Filter framework, which interleaves matching and filtering operations to maintain only compatible candidates in the candidate hyperedge space during backtracking. Experimental results on real-world datasets demonstrate that our algorithm significantly outperforms the state-of-the-art algorithms, by up to orders of magnitude in terms of query processing time.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [A study of the spectrum resource leasing method based on ERC4907 extension](https://arxiv.org/abs/2512.09942)
*Zhiming Liang,Bin Chen,Litao Ye,Chen Sun,Shuo Wang,Zhe Peng*

Main category: cs.DC

TL;DR: 本文提出了一种名为M-ERC4907的ERC4907标准扩展方法，支持批量配置多个时间段和多用户同时授权，从而显著降低链上交易次数与Gas消耗，提升可扩展性与资源分配效率。


<details>
  <summary>Details</summary>
Motivation: ERC4907标准仅支持单用户、单时间段的授权机制，在去中心化多时间段调度场景中存在应用局限性和效率瓶颈。

Method: 提出M-ERC4907扩展方法，引入支持多时间段批量配置和多用户并发授权的新功能，打破ERC4907原有的顺序授权限制。

Result: 在Remix平台上的实验表明，M-ERC4907显著减少了链上交易数量和总Gas消耗。

Conclusion: M-ERC4907有效提升了NFT租赁在多用户多时段场景下的可扩展性与资源利用效率，是对ERC4907的重要改进。

Abstract: The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.

</details>


### [4] [ELANA: A Simple Energy and Latency Analyzer for LLMs](https://arxiv.org/abs/2512.09946)
*Hung-Yueh Chiang,Bokun Wang,Diana Marculescu*

Main category: cs.DC

TL;DR: 本文开源了一个名为ELANA的轻量级大语言模型（LLM）性能分析工具，用于评估模型在多GPU和边缘GPU平台上的延迟、缓存大小、能耗等关键指标，兼容Hugging Face所有公开模型及API，适用于高效LLM研究和小规模验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在从移动边缘设备到云端GPU集群等多种硬件平台上部署时，面临延迟高和功耗大的问题，亟需有效的基准测试工具来优化模型部署效率并指导下一代模型开发。

Method: 开发并开源了名为ELANA的轻量级命令行性能分析工具，支持测量模型大小、KV缓存大小、首token延迟（TTFT）、每token生成延迟（TPOT）、端到端延迟（TTLT）以及可选的能耗日志，兼容Hugging Face所有公开模型和API，并可适配压缩或低比特模型。

Result: ELANA能够有效在多GPU和边缘GPU平台上对各类LLM进行性能剖析，提供全面的延迟与资源使用数据，且易于定制，适合学术研究和小规模实验。

Conclusion: ELANA作为一个轻量、易用且高度兼容的开源工具，填补了LLM高效性评估工具的空白，有助于推动高效大模型的研究与部署。

Abstract: The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.

</details>


### [5] [GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference](https://arxiv.org/abs/2512.09963)
*Phuong Tran,Tzu-Hao Liu,Long Tan Le,Tung-Anh Nguyen,Van Quan La,Eason Yu,Han Shu,Choong Seon Hong,Nguyen H. Tran*

Main category: cs.DC

TL;DR: GOODSPEED 是一种用于分布式大语言模型推理的新框架，通过自适应推测解码和基于梯度调度的资源分配，在多用户环境下同时优化吞吐量（goodput）与公平性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理计算开销大，在多用户服务器环境和资源受限场景中难以实现实时响应；现有推测解码方法在多草稿服务器协同工作时难以兼顾高 goodput 与公平性。

Method: 提出 GOODSPEED 框架，由一个中心验证服务器协调多个异构草稿服务器，采用梯度调度算法动态分配验证任务，并最大化对数效用函数以实现比例公平；所有草稿服务器并行生成推测 token，提升整体效率。

Result: 通过流体样本路径分析证明 GOODSPEED 在稳态下收敛到最优 goodput 分配，在动态负载下也保持近优性能且误差有理论界；实验表明该方法具有良好的可扩展性、公平性和效率。

Conclusion: GOODSPEED 为分布式大语言模型推理提供了一种兼顾吞吐量、公平性和可扩展性的高效解决方案。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi- in distributed LLM inference systems.

</details>


### [6] [Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters](https://arxiv.org/abs/2512.10271)
*Shruti Dongare,Redwan Ibne Seraj Khan,Hadeel Albahar,Nannan Zhao,Diego Melendez Maita,Ali R. Butt*

Main category: cs.DC

TL;DR: RLTune 是一个无需应用先验知识的强化学习调度框架，通过结合强化学习与混合整数线性规划，在异构 GPU 集群上显著提升资源利用率并降低作业排队延迟和完成时间。


<details>
  <summary>Details</summary>
Motivation: 现有 GPU 调度器难以应对日益异构的集群环境和对应用特征缺乏可见性的问题，通常依赖离线分析或特定假设，限制了其在大规模云平台上的通用性和效率。

Method: 提出 RLTune 框架，采用强化学习进行作业动态优先级排序，并结合基于 MILP 的作业到节点映射策略，以优化整体系统目标（如作业完成时间、排队延迟和资源利用率）。

Result: 在微软 Philly、Helios 和阿里云的真实生产轨迹上验证，RLTune 最多提升 GPU 利用率 20%，减少排队延迟 81%，缩短作业完成时间达 70%。

Conclusion: RLTune 无需逐作业分析即可泛化于多种工作负载，为云服务商提供了一种高效、公平且可扩展的深度学习任务调度方案。

Abstract: Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.

</details>


### [7] [High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments](https://arxiv.org/abs/2512.10312)
*Julian Rodriguez,Piotr Lopez,Emiliano Lerma,Rafael Medrano,Jacobo Hernandez*

Main category: cs.DC

TL;DR: 本文记录了大数据课程中实施的一系列实践与方法，包括Epsilon数据集处理、RestMex文本分析与分类、IMDb电影特征分析，以及基于Linux和Scala的Apache Spark分布式集群搭建。


<details>
  <summary>Details</summary>
Motivation: 为系统性地掌握大数据处理技术，通过课程项目整合多种工具与方法，提升在真实数据集上的分析与工程能力。

Method: 采用分组与个人策略处理Epsilon数据集；利用RestMex进行文本分析与分类；使用IMDb数据进行电影特征分析；在Linux环境下使用Scala构建基于Apache Spark的分布式计算集群。

Result: 成功完成了从数据预处理、文本与电影特征分析到分布式集群部署的完整流程，验证了所选方法在实际大数据任务中的可行性。

Conclusion: 该课程实践有效整合了多种大数据技术，为后续复杂数据分析与分布式系统开发奠定了坚实基础。

Abstract: This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.

</details>


### [8] [Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability](https://arxiv.org/abs/2512.10425)
*Fan Yu,Guodong Li,Si Wu,Weijun Fang,Sihuang Hu*

Main category: cs.DC

TL;DR: 本文提出了一种新型宽条带局部可修复编码（CP-LRCs），通过在局部与全局校验块之间引入级联依赖结构，显著降低单节点和多节点故障的修复开销，并在阿里云上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LRC在宽条带场景下面临局部组膨胀导致单节点修复成本高、多节点故障频繁触发昂贵的全局修复以及可靠性急剧下降等问题，根本原因在于局部与全局校验块独立设计，无法协同修复。

Method: 提出级联校验LRC（CP-LRCs），将全局校验块分解并嵌入所有局部校验块中，构建级联校验组；提供通用系数生成框架，设计利用级联特性的修复算法，并实例化为CP-Azure和CP-Uniform两种方案。

Result: 在阿里云上的评估表明，CP-LRCs相比现有方法最多可减少41%的单节点故障修复时间和26%的双节点故障修复时间。

Conclusion: CP-LRCs通过在局部与全局校验块间建立结构化依赖，在保持MDS级别容错能力的同时，显著提升了宽条带存储系统中单节点与多节点故障的修复效率。

Abstract: Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.

</details>


### [9] [Clustered Federated Learning with Hierarchical Knowledge Distillation](https://arxiv.org/abs/2512.10443)
*Sabtain Ahmad,Meerzhan Kanatbekova,Ivona Brandic,Atakan Aral*

Main category: cs.DC

TL;DR: 本文提出CFLHKD，一种基于多教师知识蒸馏的分层聚类联邦学习方法，通过双层聚合机制在保留簇特异性个性化的同时实现跨簇知识共享，显著提升模型准确率。


<details>
  <summary>Details</summary>
Motivation: 传统聚类联邦学习（CFL）为每个簇独立训练全局模型，导致学习碎片化，无法利用各簇间的集体知识；同时缺乏高效的层级结构来兼顾本地个性化与全局模型训练。

Method: 提出CFLHKD方案，采用分层CFL架构结合双层聚合机制，并基于多教师知识蒸馏实现簇间知识共享，在边缘端训练簇特定模型，在云端训练统一全局模型。

Result: 在标准基准数据集上的实验表明，CFLHKD在簇特定模型和全局模型的准确率上均优于代表性基线方法，性能提升达3.32–7.57%。

Conclusion: CFLHKD有效解决了传统CFL中的学习碎片化问题，在保障个性化的同时提升了整体训练效率与模型性能，为大规模异构IoT环境下的联邦学习提供了新思路。

Abstract: Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\%.

</details>


### [10] [ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp](https://arxiv.org/abs/2512.10576)
*Xinhang Chen,Chao Zhang,Jiahuan He,Wei Liu,Jianming Zhang,Wenlong Zhou,Xiao Li,Pai Zeng,Shiyong Li,Yuanpan Qian,Dong Li,Zhaogeng Li*

Main category: cs.DC

TL;DR: DeepSeek-V3.2-Exp 在长上下文推理中仍受限于 Decode 阶段的 Latent-Cache 内存瓶颈；为此提出的 ESS 系统通过将 Latent-Cache 选择性卸载至 CPU 内存，显著提升 Decode 阶段吞吐量，在 128K 上下文中实现最高 123% 的吞吐增益。


<details>
  <summary>Details</summary>
Motivation: 在 DeepSeek-V3.2-Exp 中，尽管整体吞吐有所提升，但 PD 解耦架构下的 Decode 阶段因 Latent-Cache 随序列长度线性增长而受限于 GPU 显存容量，导致批处理规模无法扩大，成为性能瓶颈。

Method: 提出 ESS（Extended Sparse Server）系统设计，以卸载为核心策略，将 Latent-Cache 选择性地迁移至 CPU 内存，同时保留对延迟敏感的关键组件在 GPU 上，从而解除批处理规模与 GPU 显存之间的耦合。

Result: 高保真仿真表明，ESS 在 32K 上下文长度下提升吞吐 69.4%，在 128K 下最高提升 123%，显著优化了长上下文大模型推理的 Decode 阶段性能。

Conclusion: ESS 是一种实用且可扩展的解决方案，有效缓解了长上下文 LLM 推理中由 GPU 显存限制带来的 Decode 阶段瓶颈，有助于降低实际部署成本。

Abstract: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.
  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.
  Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [11] [An Efficient Graph-Transformer Operator for Learning Physical Dynamics with Manifolds Embedding](https://arxiv.org/abs/2512.10227)
*Pengwei Liu,Xingyu Ren,Pengkai Wang,Hangjie Yuan,Zhongkai Hao,Guanyu Chen,Chao Xu,Dong Ni,Shengze Cai*

Main category: cs.CE

TL;DR: 本文提出PhysGTO，一种基于图Transformer的高效算子，通过在物理空间和潜在空间中显式嵌入流形结构，实现对复杂物理动态的高精度、低计算成本建模，尤其适用于非结构化网格场景。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器在处理复杂几何、多变边界/初始条件及多样物理参数的动态模拟时计算成本高昂；而现有深度学习方法在非结构化网格上泛化能力与灵活性不足，限制了实际应用。

Method: PhysGTO包含两个核心模块：物理空间中的统一图嵌入模块，用于对齐节点条件并构建稀疏但结构保持的图连接；潜在空间中结合轻量级通量导向消息传递与投影启发注意力机制，以捕捉局部与全局依赖关系。整体设计具有相对于网格点数的线性复杂度。

Result: 在涵盖非结构化网格、瞬态流动和大规模3D几何的11个数据集基准测试中，PhysGTO在保持SOTA精度的同时显著降低计算开销（参数量与FLOPs），展现出优异的灵活性、可扩展性和泛化能力。

Conclusion: PhysGTO为复杂物理系统的高效、准确模拟提供了一种通用且实用的深度学习框架，特别适用于需要实时推理的工程与科学应用场景。

Abstract: Accurate and efficient physical simulations are essential in science and engineering, yet traditional numerical solvers face significant challenges in computational cost when handling simulations across dynamic scenarios involving complex geometries, varying boundary/initial conditions, and diverse physical parameters. While deep learning offers promising alternatives, existing methods often struggle with flexibility and generalization, particularly on unstructured meshes, which significantly limits their practical applicability. To address these challenges, we propose PhysGTO, an efficient Graph-Transformer Operator for learning physical dynamics through explicit manifold embeddings in both physical and latent spaces. In the physical space, the proposed Unified Graph Embedding module aligns node-level conditions and constructs sparse yet structure-preserving graph connectivity to process heterogeneous inputs. In the latent space, PhysGTO integrates a lightweight flux-oriented message-passing scheme with projection-inspired attention to capture local and global dependencies, facilitating multilevel interactions among complex physical correlations. This design ensures linear complexity relative to the number of mesh points, reducing both the number of trainable parameters and computational costs in terms of floating-point operations (FLOPs), and thereby allowing efficient inference in real-time applications. We introduce a comprehensive benchmark spanning eleven datasets, covering problems with unstructured meshes, transient flow dynamics, and large-scale 3D geometries. PhysGTO consistently achieves state-of-the-art accuracy while significantly reducing computational costs, demonstrating superior flexibility, scalability, and generalization in a wide range of simulation tasks.

</details>


### [12] [Integrated Planning and Machine-Level Scheduling for High-Mix Discrete Manufacturing: A Profit-Driven Heuristic Framework](https://arxiv.org/abs/2512.10358)
*Runhao Liu,Ziming Chen,You Li,Zequn Xie,Peng Zhang*

Main category: cs.CE

TL;DR: 本文提出了一种利润驱动的集成框架，通过结合中期生产计划与机器级调度，在多品种、小批量和紧急订单条件下实现高效可靠的生产调度。


<details>
  <summary>Details</summary>
Motivation: 现代制造企业在多品种、小批量和紧急订单条件下难以制定高效可靠的生产计划，需在异构资源和严格交付承诺下联合优化中期计划与机器级调度。

Method: 采用分层方法：上层为混合整数规划模型，用于分配生产、配件共生产及外包；下层为结构感知调度启发式算法，确保执行可行性和机器行为稳定性。

Result: 在真实工业场景中，稳定性优先的执行策略实现了100%准时交付、零外包，并在仅损失1.9%–4.6%产能的情况下保持机器负载均衡。

Conclusion: 将计划决策与稳定性导向的执行规则对齐，可在复杂制造环境中实现可解释且实用的利润最大化调度方案。

Abstract: Modern manufacturing enterprises struggle to create efficient and reliable production schedules under multi-variety, small-batch, and rush-order conditions. High-mix discrete manufacturing systems require jointly optimizing mid-term production planning and machine-level scheduling under heterogeneous resources and stringent delivery commitments. We address this problem with a profit-driven integrated framework that couples a mixed-integer planning model with a machine-level scheduling heuristic. The planning layer allocates production, accessory co-production, and outsourcing under aggregate economic and capacity constraints, while the scheduling layer refines these allocations using a structure-aware procedure that enforces execution feasibility and stabilizes daily machine behavior. This hierarchical design preserves the tractability of aggregated optimization while capturing detailed operational restrictions. Evaluations are conducted on a real industrial scenario. A flexible machine-level execution scheme yields 73.3% on-time completion and significant outsourcing demand, revealing bottleneck congestion. In contrast, a stability-enforcing execution policy achieves 100% on-time completion, eliminates all outsourcing, and maintains balanced machine utilization with only 1.9 to 4.6% capacity loss from changeovers. These results show that aligning planning decisions with stability-oriented execution rules enables practical and interpretable profit-maximizing decisions in complex manufacturing environments.

</details>


### [13] [Robust Crop Planning under Uncertainty: Aligning Economic Optimality with Agronomic Sustainability](https://arxiv.org/abs/2512.10396)
*Runhao Liu,Ziming Chen,You Li,Peng Zhang*

Main category: cs.CE

TL;DR: 本文提出了一种多层鲁棒作物规划框架（MLRCPF），通过融合空间推理、时间动态和分布鲁棒优化，在考虑作物间相互作用与环境不确定性的前提下，生成可持续的轮作模式，提升土壤肥力并平衡经济收益与系统稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有农业规划方法通常忽略作物间的复杂互作关系（如豆类与谷物的互补性），且多采用静态确定性模型，难以应对市场与气候波动带来的风险，缺乏长期韧性。

Method: 构建多层鲁棒作物规划框架（MLRCPF）：1）通过结构化交互矩阵在状态转移逻辑中形式化作物间关系；2）引入基于数据驱动模糊集的分布鲁棒优化层以应对最坏情况风险。

Result: 在华北高混合作物数据集上的实验表明，该框架能自动生成恢复土壤肥力的棋盘式轮作模式，显著提高豆类种植比例，并有效权衡经济最优性与系统稳定性。

Conclusion: 将领域特定的结构先验（如作物互作）显式嵌入优化模型，对提升复杂农业系统中长期决策的鲁棒性至关重要。

Abstract: Long-horizon agricultural planning requires optimizing crop allocation under complex spatial heterogeneity, temporal agronomic dependencies, and multi-source environmental uncertainty. Existing approaches often treat crop interactions, such as legume-cereal complementarity, which implicitly or rely on static deterministic formulations that fail to guarantee resilience against market and climate volatility. To address these challenges, we propose a Multi-Layer Robust Crop Planning Framework (MLRCPF) that integrates spatial reasoning, temporal dynamics, and robust optimization. Specifically, we formalize crop-to-crop relationships through a structured interaction matrix embedded within the state-transition logic, and employ a distributionally robust optimization layer to mitigate worst-case risks defined by a data-driven ambiguity set. Evaluations on a real-world high-mix farming dataset from North China demonstrate the effectiveness of the proposed approach. The framework autonomously generates sustainable checkerboard rotation patterns that restore soil fertility, significantly increasing the legume planting ratio compared to deterministic baselines. Economically, it successfully resolves the trade-off between optimality and stability. These results highlight the importance of explicitly encoding domain-specific structural priors into optimization models for resilient decision-making in complex agricultural systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [Algorithm-Driven On-Chip Integration for High Density and Low Cost](https://arxiv.org/abs/2512.10089)
*Jeongeun Kim,Sabrina Yarzada,Paul Chen,Christopher Torng*

Main category: cs.AR

TL;DR: 本文提出一种新型可扩展芯片集成方法，通过结构化设计空间、窄边通信架构和片上电源域技术，显著降低多项目晶圆（MPW）中每个项目的面积开销，相比现有物理聚合方法最多节省13倍面积。


<details>
  <summary>Details</summary>
Motivation: 随着半导体人才培训和研究对支持大量独立硬件设计平台的需求增长，传统仅依赖物理共布局的多项目晶圆（MPW）服务在项目数量增加时难以扩展。现有可扩展方案虽尝试整合小型设计并分摊IO和存储等昂贵资源，但缺乏对密集集成设计站点排布、互连与验证的系统性研究。

Method: 作者提出三项关键技术：1）建立结构化设计空间，实现算法驱动的自动化项目布局；2）利用站点间狭窄区域构建片外通信及共享功能架构；3）设计实用的片上电源域方案，使每个项目可在标准实验室环境下进行功耗表征，无需低功耗ASIC设计专业知识。

Result: 实验表明，该方法相比当前最先进的纯物理聚合方法，最多可减少13倍的面积开销。

Conclusion: 所提方法为大规模芯片流片环境提供了一条可扩展且成本效益高的新路径，有效解决了高项目密度下的集成与验证难题。

Abstract: Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.

</details>


### [15] [A Vertically Integrated Framework for Templatized Chip Design](https://arxiv.org/abs/2512.10155)
*Jeongeun Kim,Christopher Torng*

Main category: cs.AR

TL;DR: 本文提出一种面向初学者的软硬件协同设计方法，通过将软件对象直接映射为芯片上的物理区域，保持从高级面向对象程序到芯片布局的结构一致性，并利用基于序列的类型系统确保模块间通信符合软件模型，从而降低软件开发者参与芯片设计的门槛。


<details>
  <summary>Details</summary>
Motivation: 软件开发者难以将定制硬件集成到其应用中，尽管专用芯片在机器学习和AI等领域具有显著优势。现有方法缺乏从高级软件抽象到硬件实现的直观映射，增加了初学者参与芯片设计的难度。

Method: 提出一种对象对齐的芯片生成方法：每个软件对象对应芯片上的一个物理区域，采用垂直堆叠的IP模块实现软件行为协议，并引入基于序列的类型系统验证模块间通信是否符合软件模型；同时开发适用于该设计风格的互连策略与布局技术。

Result: 实现了从面向对象软件规范到芯片布局的一对一结构映射，在满足基本性能要求的前提下，保持了软件与硬件之间的认知连续性，并支持实用的自动布局生成。

Conclusion: 该方法有效降低了软件开发者参与芯片设计所需的专业知识门槛，为软硬件协同设计提供了一种面向初学者的可行路径。

Abstract: Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.

</details>


### [16] [SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation](https://arxiv.org/abs/2512.10231)
*Zhenguo Liu,Chengao Shi,Chen Ding,Jiang Xu*

Main category: cs.AR

TL;DR: 本文提出SemanticBBV，一种两阶段框架，通过语义编码和顺序不变聚合生成具有性能感知能力的基本块签名，在保持单程序精度的同时实现高效的跨程序微架构仿真加速。


<details>
  <summary>Details</summary>
Motivation: 传统基于基本块向量（BBV）的采样方法存在两个关键缺陷：依赖顺序的ID阻碍跨程序知识复用，且缺乏对硬件性能有预测能力的语义信息，限制了仿真优化潜力。

Method: SemanticBBV包含两个阶段：首先使用轻量级RWKV编码器将汇编基本块转化为富含语义的基本块嵌入（BBE）；然后通过一个按执行频率加权、顺序不变的Set Transformer聚合BBE，并联合训练三元组损失和CPI回归任务，使最终签名具备性能敏感性。

Result: 实验表明，SemanticBBV在单程序仿真中精度与传统BBV相当，同时仅需模拟14个通用程序点即可在十个SPEC CPU基准上达到86.3%的平均性能预测准确率，实现7143倍的仿真加速，并对新微架构具有良好适应性。

Conclusion: SemanticBBV成功克服了传统BBV的语义缺失与顺序依赖问题，为跨程序、高性能感知的微架构仿真提供了高效可行的新范式。

Abstract: For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.
  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives](https://arxiv.org/abs/2512.10079)
*Federico Formica,Mark Lawford,Claudio Menghi*

Main category: cs.SE

TL;DR: 本文回顾了近期将工程师领域知识融入搜索式软件测试（SBST）的实验结果，揭示了一些出人意料的发现，并为未来研究提供了新视角。


<details>
  <summary>Details</summary>
Motivation: SBST虽能自动生成大量测试用例，但缺乏工程师所具备的领域知识，因此需要探索如何有效融合领域知识以提升测试效果。

Method: 回顾和分析近期将领域知识整合进SBST框架的实验研究，重点关注其中显著且意外的结果。

Result: 揭示了现有基于领域知识的SBST方法中一些出人意料的实验现象，挑战了既有假设。

Conclusion: 需从新视角重新审视融合领域知识的SBST技术，并据此提出未来研究的新方向。

Abstract: Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.

</details>


### [18] [ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis](https://arxiv.org/abs/2512.10173)
*Mantas Baksys,Stefan Zetzsche,Olivier Bouissou,Remi Delmas,Soonho Kong*

Main category: cs.SE

TL;DR: 本文提出ATLAS，一个自动生成大规模已验证Dafny程序的流水线，通过分解合成过程生成超过19K训练样本，显著提升大语言模型在形式化验证任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在程序验证方面具有潜力，但缺乏带验证的代码数据限制了其发展。

Method: 构建ATLAS自动化流水线，合成包含规范、实现和证明的完整Dafny程序，并将合成过程拆解为多个专门任务以提取大量训练样本；在此数据集上微调Qwen 2.5 7B Coder模型。

Result: 在DafnyBench上提升23个百分点，在DafnySynthesis上提升50个百分点。

Conclusion: 合成的已验证代码能有效增强大语言模型在形式化验证方面的能力。

Abstract: Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.

</details>


### [19] [Does SWE-Bench-Verified Test Agent Ability or Model Memory?](https://arxiv.org/abs/2512.10218)
*Thanosan Prathifkumar,Noble Saji Mathews,Meiyappan Nagappan*

Main category: cs.SE

TL;DR: 该论文指出SWE-Bench-Verified基准可能与大语言模型的训练数据重叠，导致评估结果反映的是记忆能力而非真实的问题解决能力，并通过实验验证了这一风险。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的SWE-Bench-Verified基准可能因与模型训练数据存在重叠，使得评估结果无法真实反映模型解决实际软件问题的能力，从而误导研究进展和模型选择。

Method: 作者测试了两个在SWE-Bench-Verified上表现优异的Claude模型，在仅提供问题描述或问题描述加文件路径的情况下，评估其定位相关文件的能力，并将相同设置应用于BeetleBox和SWE-rebench两个新基准进行对比。

Result: 模型在SWE-Bench-Verified上的表现比在其他两个基准上高出3倍，且在无额外上下文情况下定位被编辑文件的能力高出6倍，表明其可能在训练中接触过SWE-Bench-Verified中的任务。

Conclusion: 依赖可能存在数据污染的旧有热门基准（如SWE-Bench-Verified）会高估模型能力，应转向设计时就考虑数据污染问题的新基准以更准确评估模型的真实性能。

Abstract: SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.

</details>


### [20] [Studying and Automating Issue Resolution for Software Quality](https://arxiv.org/abs/2512.10238)
*Antu Saha*

Main category: cs.SE

TL;DR: 该研究通过提升问题报告质量、刻画开发者工作流和自动化高难度修复任务，推动AI驱动的软件问题解决。


<details>
  <summary>Details</summary>
Motivation: 开发者在软件维护中常面临低质量的问题报告、对真实工作流理解不足以及缺乏自动化支持等挑战。

Method: 结合大语言模型（LLM）推理与应用特定信息改进问题报告；实证分析传统与AI增强环境下的开发者工作流；利用机器学习、深度学习和LLM实现UI缺陷定位与解决方案识别等认知密集型任务的自动化。

Result: 提供了关于AI辅助开发的实证洞察，开发了实用工具，并实现了若干关键修复任务的自动化。

Conclusion: 本研究通过多方向协同，提升了AI驱动的问题解决能力，有助于构建更高质量、更易维护的软件系统。

Abstract: Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.

</details>


### [21] [Cross-modal Retrieval Models for Stripped Binary Analysis](https://arxiv.org/abs/2512.10393)
*Guoqiang Chen,Lingyun Ying,Ziyang Song,Daguang Liu,Qiang Wang,Zhiqi Wang,Li Hu,Shaoyin Cheng,Weiming Zhang,Nenghai Yu*

Main category: cs.SE

TL;DR: 本文提出了BinSeek，一种用于剥离符号信息的二进制代码与自然语言描述之间跨模态检索的两阶段框架，包含BinSeekEmbedding和BinSeek-Reranker两个模型，并通过LLM驱动的数据合成流程构建训练数据和领域基准，在多项指标上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在基于大语言模型（LLM）智能体的二进制代码分析中，如何从大量剥离符号信息的二进制函数中根据自然语言查询准确检索相关函数仍是一个未被充分研究且具有挑战性的问题，因为缺乏符号信息使其区别于源代码检索。

Method: 提出BinSeek两阶段跨模态检索框架：第一阶段使用BinSeekEmbedding模型学习二进制代码与自然语言描述之间的语义相关性；第二阶段通过BinSeek-Reranker模型结合上下文增强对候选代码与查询的相关性进行精细重排序。同时构建了基于LLM的自动化训练数据合成流水线，并建立了一个领域基准数据集。

Result: 实验结果表明，BinSeek在Rec@3上比同规模模型提升31.42%，在MRR@3上提升27.17%，并优于参数量达其16倍的先进通用模型。

Conclusion: BinSeek是首个面向剥离符号信息二进制代码分析的跨模态检索框架，在性能上显著超越现有方法，为未来二进制代码理解与检索研究提供了有效工具和基准。

Abstract: LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.

</details>


### [22] [UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval](https://arxiv.org/abs/2512.10452)
*Yang Yang,Li Kuang,Jiakun Liu,Zhongxin Liu,Yingjie Xia,David Lo*

Main category: cs.SE

TL;DR: 本文提出UniCoR，一种自监督框架，用于学习统一且鲁棒的代码表示，以解决混合查询（自然语言+代码）在跨语言代码检索中的语义理解不足、模态融合低效和跨语言泛化能力弱三大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在混合模式（自然语言与代码片段结合）下的代码检索，尤其是在跨语言场景中，难以有效利用混合查询，存在语义理解不足、融合效率低和泛化能力弱的问题。

Method: UniCoR包含两个核心模块：一是多视角监督对比学习模块，从代码-代码、自然语言-代码、自然语言-自然语言三个视角对齐表示，增强语义理解和模态融合；二是表示分布一致性学习模块，显式对齐不同编程语言的特征分布，提升跨语言泛化能力。

Result: 在多个基准数据集上的实验表明，UniCoR在MRR和MAP指标上分别平均优于最强基线8.64%和11.54%，并在混合检索和跨语言泛化方面表现出良好稳定性与能力。

Conclusion: UniCoR通过统一的代码表示学习有效提升了混合查询下代码检索的性能，尤其在跨语言场景中展现出显著优势，为未来代码检索研究提供了新思路。

Abstract: Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.

</details>


### [23] [Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild](https://arxiv.org/abs/2512.10493)
*Binquan Zhang,Li Zhang,Haoyuan Zhang,Fang Liu,Song Wang,Bo Shen,An Fu,Lin Shi*

Main category: cs.SE

TL;DR: 本文基于LMSYS-Chat-1M和WildChat数据集，对人类与大语言模型（LLM）在编码协作中的交互模式、指令遵循能力及用户满意度进行了实证分析，揭示了任务类型如何影响交互结构、LLM在不同任务中的指令遵循表现以及用户满意度的差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对人类与大语言模型在编码场景中协作机制的系统性探索，尤其不清楚用户在交互过程中经历的困难路径、LLM的指令遵循能力以及用户满意度情况。

Method: 作者利用LMSYS-Chat-1M和WildChat两个真实世界的人机对话数据集，开展实证分析，从交互模式、指令遵循能力和用户满意度三个维度研究人类与LLM在编码任务中的协作机制。

Result: 研究发现：1）任务类型塑造了三种交互模式（线性、星型、树状），不同编码任务偏好不同模式；2）在代码修复与重构任务中，LLM的指令不遵从率显著高于信息查询任务；3）代码质量优化和需求驱动开发任务的用户满意度较低，而结构化知识查询和算法设计任务满意度较高。

Conclusion: 该研究为改进LLM编码协作界面和提升用户满意度提供了实践建议，并指出了未来在自适应对话系统方向的研究路径，有助于深化对人机协同编程的理解。

Abstract: Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.

</details>


### [24] [Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories](https://arxiv.org/abs/2512.10618)
*Georgia M. Kapitsaki,Maria Papoutsoglou,Christoph Treude,Ioanna Theophilou*

Main category: cs.SE

TL;DR: 该研究通过分析GitHub上32,820个议题，识别出开源软件开发者在遵守GDPR和CCPA等隐私法规时讨论的24类问题，并归纳为六大主题集群，旨在帮助实践者、教育界和研究界更好地理解和应对隐私合规挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管隐私立法（如GDPR和CCPA）对软件开发产生了重大影响，但目前缺乏关于开源软件开发者如何在实际开发中讨论和处理合规问题的实证研究。

Method: 研究者从GitHub仓库中挖掘并分析了32,820个议题，部分通过自动方法识别涉及的法律权利与原则，并对其中1,186个议题进行人工分类，依据其所涉及的关注类型进行编码。

Result: 研究构建了一个包含24个讨论类别的分类体系，归入六大集群：功能/缺陷、同意相关、文档、数据存储/共享、适应性以及一般合规。开发者主要关注“删除权”、“退出权”和“访问权”，讨论焦点集中在用户同意、用户权利功能实现、缺陷修复及Cookie管理等方面。

Conclusion: 所提出的分类法可帮助从业者优先处理合规相关议题，教育界可据此优化课程内容，研究界则可识别现有不足并推动隐私合规支持工具的发展。

Abstract: Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.

</details>


### [25] [PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code](https://arxiv.org/abs/2512.10713)
*Itay Dreyfuss,Antonio Abu Nassar,Samuel Ackerman,Axel Ben David,Rami Katan,Orna Raz,Marcel Zalmanovici*

Main category: cs.SE

TL;DR: 本文提出了PACIFIC框架，用于自动生成可调节难度的基准测试，以严格评估大语言模型（LLM）在代码任务中遵循指令和干运行（dry-running）的能力，并通过简单输出比对实现可靠评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法常依赖工具调用或智能体行为，难以准确衡量LLM本身在无执行环境下逐步推理代码行为及遵循用户指令的能力；同时存在训练数据污染风险。因此，需要一种能隔离并精准评估LLM内在指令遵循与干运行能力、且可避免数据污染的评估框架。

Method: 提出PACIFIC框架，该框架可自动生成具有明确定义预期输出的基准测试变体，支持控制难度级别，通过直接比较模型输出与预期结果进行评估，无需执行代码或依赖外部工具。

Result: 在多个前沿LLM上验证表明，PACIFIC生成的基准能有效区分不同模型在指令遵循和干运行能力上的差异，且随着难度提升，评估效果更加显著。

Conclusion: PACIFIC提供了一种可扩展、抗数据污染的评估方法，能有效衡量LLM在代码任务中的核心能力，尤其在无执行环境下的指令遵循与逐步推理方面。

Abstract: Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.

</details>


### [26] [Zorya: Automated Concolic Execution of Single-Threaded Go Binaries](https://arxiv.org/abs/2512.10799)
*Karolina Gorna,Nicolas Iooss,Yannick Seurin,Rida Khatoun*

Main category: cs.SE

TL;DR: 本文改进了Zorya框架，通过引入panic可达性门控和多层过滤机制，在Go二进制程序中实现了高效且精准的漏洞检测，显著优于现有工具。


<details>
  <summary>Details</summary>
Motivation: Go语言在关键基础设施中的广泛应用使其漏洞检测变得尤为重要，但现有符号执行工具因Go运行时复杂性和可扩展性问题难以有效分析Go二进制文件。

Method: 在Zorya（一种将Go二进制转换为Ghidra P-Code中间表示的混合执行框架）基础上，增加了对具体未执行路径中漏洞的检测能力，并设计了多层过滤机制，聚焦于与panic相关的路径；同时采用函数级分析模式提升效率。

Result: 在五个Go漏洞上的评估表明，panic可达性门控可在过滤33%-70%分支的同时实现1.8-3.9倍加速；Zorya成功检测出全部panic，而现有工具最多仅检测出两个；函数级分析比从main函数开始快约两个数量级。

Conclusion: 针对具有运行时安全检查的语言生态（如Go），专门优化的混合执行方法能够实现高效、实用的漏洞检测。

Abstract: Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.

</details>
