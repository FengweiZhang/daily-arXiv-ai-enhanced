<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 2]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.SE](#cs.SE) [Total: 17]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration](https://arxiv.org/abs/2512.04527)
*Xingyu Liu,Jiawei Liang,Linfeng Du,Yipu Zhang,Chaofang Ma,Hanwei Fan,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: 本文提出了FLEX，一种用于混合单元高度合法化任务的FPGA-CPU加速器，通过优化任务分配、采用多粒度流水线技术以及针对计算密集型单元移动过程进行优化，在速度和合法化质量上均优于现有CPU-GPU和多线程CPU方案。


<details>
  <summary>Details</summary>
Motivation: 混合单元高度合法化是物理设计中的关键步骤，但其计算密集且耗时。现有基于CPU或GPU的方法在性能和可扩展性方面存在局限，因此需要一种更高效的硬件加速方案来提升合法化速度与质量。

Method: 作者提出FLEX加速器，结合FPGA与CPU的优势：首先优化任务分配策略，高效划分FPGA与CPU的任务；其次采用多粒度流水线技术加速最耗时的“寻找最优放置位置（FOP）”步骤；最后专门优化FOP中计算密集的单元移动过程，使其与多粒度流水线框架无缝集成。

Result: 实验结果表明，FLEX相比当前最先进的CPU-GPU和多线程CPU合法化器，分别实现了最高18.3倍和5.4倍的加速，并具有更好的可扩展性，同时将合法化质量提升了4%和1%。

Conclusion: FLEX通过软硬件协同设计有效解决了混合单元高度合法化的性能瓶颈，在速度和质量上均显著优于现有方法，展示了FPGA-CPU异构架构在电子设计自动化领域的潜力。

Abstract: In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.

</details>


### [2] [Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming](https://arxiv.org/abs/2512.04910)
*Fang Li*

Main category: cs.AR

TL;DR: 本文提出了一种基于答案集编程（ASP）的自动条形板电路布局设计新方法，将布局问题建模为合成与多目标优化任务，在保证可行性的前提下生成紧凑、可制造的布局。


<details>
  <summary>Details</summary>
Motivation: 传统条形板布局设计依赖人工，效率低且易出错；现有自动化方法在处理复杂几何与电气约束时存在局限，缺乏兼顾布局紧凑性与可制造性的有效方案。

Method: 利用答案集编程（ASP）的声明式特性，将条形板布局问题形式化为同时满足可行性约束并优化板面积与元件跨条数的多目标问题，采用两阶段求解策略：先确保布局可行，再优化布局质量。

Result: 实验表明该方法能为不同复杂度的电路自动生成紧凑且可制造的布局，验证了其有效性与实用性。

Conclusion: 该研究显著推进了条形板自动布局技术，不仅为电子原型制作和教育提供了实用工具，也展示了声明式编程在复杂设计自动化问题中的强大能力。

Abstract: This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [3] [NAWOA-XGBoost: A Novel Model for Early Prediction of Academic Potential in Computer Science Students](https://arxiv.org/abs/2512.04751)
*Junhao Wei,Yanzhao Gu,Ran Zhang,Mingjing Huang,Jinhong Song,Yanxiao Li,Wenxuan Zhu,Yapeng Wang,Zikun Li,Zhiwen Wang,Xu Yang,Ngai Cheong*

Main category: cs.CE

TL;DR: 本文提出了一种改进的鲸鱼优化算法NAWOA，并将其用于优化XGBoost模型的超参数，以预测学生的学术潜力，在多项指标上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 原始鲸鱼优化算法（WOA）存在全局搜索能力有限、收敛速度慢及易陷入局部最优等问题，限制了其在机器学习超参数优化中的效果。

Method: 提出非线性自适应鲸鱼优化算法（NAWOA），融合优质节点集初始化、领导者-跟随者觅食、动态包围猎物、三角狩猎策略及非线性收敛因子；并基于NAWOA构建NAWOA-XGBoost模型用于学术潜力预测。

Result: 在23个基准函数上验证了NAWOA的优越性能；在495名计算机科学本科生数据集上，NAWOA-XGBoost在Accuracy（0.8148）、Macro F1（0.8101）、AUC（0.8932）和G-Mean（0.8172）等指标上均优于传统XGBoost和WOA-XGBoost。

Conclusion: NAWOA显著提升了优化性能与稳定性，所构建的NAWOA-XGBoost模型在多类不平衡数据集上展现出优异的预测能力和适应性。

Abstract: Whale Optimization Algorithm (WOA) suffers from limited global search ability, slow convergence, and tendency to fall into local optima, restricting its effectiveness in hyperparameter optimization for machine learning models. To address these issues, this study proposes a Nonlinear Adaptive Whale Optimization Algorithm (NAWOA), which integrates strategies such as Good Nodes Set initialization, Leader-Followers Foraging, Dynamic Encircling Prey, Triangular Hunting, and a nonlinear convergence factor to enhance exploration, exploitation, and convergence stability. Experiments on 23 benchmark functions demonstrate NAWOA's superior optimization capability and robustness. Based on this optimizer, an NAWOA-XGBoost model was developed to predict academic potential using data from 495 Computer Science undergraduates at Macao Polytechnic University (2009-2019). Results show that NAWOA-XGBoost outperforms traditional XGBoost and WOA-XGBoost across key metrics, including Accuracy (0.8148), Macro F1 (0.8101), AUC (0.8932), and G-Mean (0.8172), demonstrating strong adaptability on multi-class imbalanced datasets.

</details>


### [4] [Customer Identification for Electricity Retailers Based on Monthly Demand Profiles by Activity Sectors and Locations](https://arxiv.org/abs/2512.04776)
*Joaquin Luque,Alejandro Carrasco,Enrique Personal,Francisco Perez,Carlos Leon*

Main category: cs.CE

TL;DR: 本文利用西班牙数百万用户的月度用电数据，结合其所属经济部门和地理位置信息，间接识别出高价值客户，并提出一种基于用电曲线与目标曲线距离的营销策略，在吸引新客户时比随机策略减少40%的距离偏差。


<details>
  <summary>Details</summary>
Motivation: 电力零售企业面临激烈竞争，需精准识别最有利可图的客户；然而客户用电数据通常匿名，难以直接关联到具体用户。

Method: 利用包含数百万月度用电曲线的大数据集，结合客户的经济部门和地理位置信息进行间接识别；定义用电曲线与目标曲线距离作为关键绩效指标（KPI），驱动营销策略。

Result: 成功唯一识别出10万客户，另有约30万客户被归入不超过10人的小集合中；相比随机吸引客户，该策略在获取1万名新客户时使与目标曲线的平均距离减少40%。

Conclusion: 结合经济活动类型与地理位置能有效实现客户间接识别，所提出的基于KPI的营销策略显著优于随机策略，有助于电力零售商更高效地吸引高价值客户。

Abstract: The increasing competition in the electric sector is challenging retail companies as they must assign its commercial efforts to attract the most profitable customers. Those are whose energy demand best fit certain target profiles, which usually depend on generation or cost policies. But, even when the demand profile is available, it is in an anonymous way, preventing its association to a particular client. In this paper, we explore a large dataset containing several millions of monthly demand profiles in Spain and use the available information about the associated economic sector and location for an indirect identification of the customers. The distance of the demand profile from the target is used to define a key performance indicator (KPI) which is used as the main driver of the proposed marketing strategy. The combined use of activity and location has been revealed as a powerful tool for indirect identification of customers, as 100,000 customers are uniquely identified, while about 300,000 clients are identifiable in small sets containing 10 or less consumers. To assess the proposed marketing strategy, it has been compared to the random attraction of new clients, showing a reduction of distance from the target of 40% for 10,000 new customers.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [Toward Sustainability-Aware LLM Inference on Edge Clusters](https://arxiv.org/abs/2512.04088)
*Kolichala Rajashekar,Nafiseh Sharghivand,Radu Prodan,Reza Farahani*

Main category: cs.DC

TL;DR: 该论文提出了一种面向边缘集群的可持续LLM推理方法，通过碳感知和延迟感知的路由策略，在NVIDIA Jetson Orin NX与Ada 2000设备上平衡推理延迟与碳足迹，并基于实证评估确定批量大小为4时可兼顾吞吐量与能效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）推理在云端部署存在高延迟与带宽限制，而边缘计算虽能缓解这些问题，却需权衡性能、能效与设备资源约束；同时，大规模推理带来的碳排放问题亟需解决。

Method: 在由Jetson Orin NX（8GB）和Ada 2000（16GB）组成的边缘集群上，基于对不同提示和批处理配置下的能耗与执行时间的实证基准测试，设计并比较了碳感知、延迟感知及贪婪基线的提示路由策略。

Result: 实验表明，批量大小为4可在吞吐量与能效之间取得良好平衡，而更大的批量易导致GPU内存饱和；碳感知与延迟感知策略有效优化了推理过程中的碳足迹与响应延迟。

Conclusion: 通过结合硬件特性和工作负载特征，采用可持续导向的路由策略可在边缘环境中实现高效且低碳的LLM推理。

Abstract: Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.

</details>


### [6] [VLCs: Managing Parallelism with Virtualized Libraries](https://arxiv.org/abs/2512.04320)
*Yineng Yan,William Ruys,Hochan Lee,Ian Henriksen,Arthur Peters,Sean Stephens,Bozhi You,Henrique Fingler,Martin Burtscher,Milos Gligoric,Keshav Pingali,Mattan Erez,George Biros,Christopher J. Rossbach*

Main category: cs.DC

TL;DR: 提出虚拟库上下文（VLCs）机制，无需修改库代码即可在单进程中对并行库进行资源隔离与多实例加载，从而提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代并行程序常组合使用多个并行库，但这些库通常假定独占系统资源，导致并发使用时产生资源争用和性能下降；而现有解决方案需修改库或操作系统，可行性低。

Method: 引入虚拟库上下文（VLCs），作为进程内的子单元，封装库及其资源分配，控制其资源使用，支持资源划分和同一库的多副本加载。

Result: 在C++和Python中实现VLC原型，在包含OpenMP、OpenBLAS和LibTorch的应用基准测试中，最高获得2.85倍加速。

Conclusion: VLCs提供了一种无需修改现有并行库即可有效管理资源争用和提升性能的方法，具有良好的实用性和可扩展性。

Abstract: As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.
  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.
  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.

</details>


### [7] [Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud](https://arxiv.org/abs/2512.04089)
*Mario Colosi,Reza Farahani,Lauri Loven,Radu Prodan,Massimo Villari*

Main category: cs.DC

TL;DR: 本文评估了 WebAssembly（Wasm）在浏览器、边缘和云环境中执行无服务器工作流的性能，发现 AOT 编译和实例预热显著降低启动延迟，且在处理大数据负载时，边缘和云节点的 AOT 执行明显优于浏览器。


<details>
  <summary>Details</summary>
Motivation: WebAssembly 虽具备跨平台、沙箱化和接近原生执行的优势，适用于无服务器工作流，但其性能受启动开销、运行时编译模型及部署环境资源差异影响，亟需系统性评估。

Method: 使用 wasm32-wasi 模块在浏览器（通过 Web Worker）、边缘和云节点（通过 HTTP shim 流式传输帧至 Wasm 运行时）统一执行无服务器工作流，并测量冷/热启动延迟、每步延迟、工作流总完成时间、吞吐量及 CPU/内存使用情况。

Result: AOT 编译和实例预热大幅减少启动延迟；小负载下浏览器因全内存数据交换表现良好；大负载下边缘和云节点的 AOT 执行在计算与内存密集型任务中显著优于浏览器。

Conclusion: WebAssembly 在不同部署环境下表现出差异化性能特征，合理选择编译策略和部署位置可优化无服务器工作流的整体效率。

Abstract: WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.

</details>


### [8] [Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions](https://arxiv.org/abs/2512.04093)
*Ali Akbar Vali,Sadoon Azizi,Mohammad Shojafar,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文对2020–2024年间136项以上关于微服务化雾计算与边缘计算中资源管理的研究进行了系统综述，聚焦于能效优化策略，并将其划分为服务放置、资源供应、任务调度与卸载、资源分配和实例选择五大子领域，同时指出当前研究缺乏组件间协同，并提出结合AI、量子计算和无服务器架构的未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备激增，雾计算与边缘计算虽可降低延迟与能耗，但其资源受限、异构性强、负载动态且QoS需求多样，亟需高效、节能的资源管理机制；现有研究分散，缺乏统一视角与组件协同。

Method: 采用系统性文献综述方法，对2020–2024年间136篇以上相关论文进行分类与分析，依据优化技术、目标函数及方法优缺点，将研究划分为五个关键子领域，并对比现有综述，识别研究空白。

Result: 成功构建了一个涵盖服务放置、资源供应、任务调度与卸载、资源分配和实例选择的分类框架，揭示了当前方法在能效优化方面的进展与局限，并明确了AI驱动、量子计算和无服务器等新兴技术的融合潜力。

Conclusion: 本综述为微服务化雾/边缘计算中的资源管理提供了统一且注重能效的参考框架，强调需加强各管理组件间的协同，并呼吁未来研究探索AI、量子与无服务器等前沿技术以实现更集成、高效和可持续的解决方案。

Abstract: The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.

</details>


### [9] [Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale](https://arxiv.org/abs/2512.04096)
*Sushant Kumar Gupta,Anil Raghunath Iyer,Chang Yu,Neel Bagora,Olivier Pomerleau,Vivek Kumar,Prunthaban Kanthakumar*

Main category: cs.DC

TL;DR: Fast ACS 是一种基于文件的有序消息传递系统，结合 RPC 与远程内存访问技术，在大规模跨地域集群中实现低延迟、高吞吐、有序且至少一次的消息投递。


<details>
  <summary>Details</summary>
Motivation: 实时系统需要在大规模、跨地域分布的消费者之间高效、可靠地传递消息，同时保证顺序性、至少一次投递，并避免消费者过载。

Method: Fast ACS 利用两方通信（跨集群的远程过程调用）和单方通信（集群内的远程内存访问）相结合的方式，构建基于文件的有序消息传递机制。

Result: 系统已在数十个生产集群中部署，支持每个集群数千消费者，峰值时实现 Tbps 级别的集群内流量，全球范围内 p99 延迟为秒级甚至亚秒级，资源开销低。

Conclusion: Fast ACS 能够有效满足大规模实时系统对低延迟、高可扩展性和可靠消息传递的需求，具备良好的生产适用性。

Abstract: Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.
  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.

</details>


### [10] [tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection](https://arxiv.org/abs/2512.04226)
*Ryan Swann,Muhammad Osama,Xiaohu Guo,Bryant Nelson,Lixun Zhang,Alex Brown,Yen Ong,Ali Yazdani,Sean Siddens,Ganesh Dasika,Alex Underwood*

Main category: cs.DC

TL;DR: tritonBLAS 是一个基于架构参数的快速、确定性分析模型，用于生成高性能 GPU GEMM 内核，无需运行时自动调优即可达到接近最优性能。


<details>
  <summary>Details</summary>
Motivation: 传统 GEMM 内核依赖运行时自动调优以获得高性能，但该过程耗时且不可预测；作者希望构建一个无需自动调优、能基于硬件架构参数直接预测近优配置的模型。

Method: tritonBLAS 利用缓存层次结构、代码与数据相对布局等架构参数，显式建模架构拓扑、矩阵形状与算法分块行为之间的关系，并在 Triton 中实现了一个轻量级 GEMM 框架。

Result: 在多种 GEMM 问题规模和现代 GPU 上评估表明，tritonBLAS 达到自动调优方案 95% 以上的性能，同时将自动调优时间降至零。

Conclusion: tritonBLAS 可作为高性能计算和机器学习生产环境中经验调优方法的实用替代方案。

Abstract: We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.

</details>


### [11] [Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity](https://arxiv.org/abs/2512.04355)
*Gregory Bolet,Giorgis Georgakoudis,Konstantinos Parasyris,Harshitha Menon,Niranjan Hasabnis,Kirk W. Cameron,Gal Oren*

Main category: cs.DC

TL;DR: 本文提出了 gpuFLOPBench 基准，用于评估大语言模型（LLMs）在未运行 CUDA 内核的情况下预测其单双精度浮点运算次数（FLOPs）的能力，揭示了当前模型在处理隐式 FLOPs 时的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现代 GPU 软件开发需要在启动内核前预判性能瓶颈，而现有 LLMs 缺乏对此类前瞻性性能推理能力的评估。

Method: 构建包含 577 个来自 HeCBench 的 CUDA 内核的数据集，每个内核标注了真实 FLOP 数量和八项执行属性，并以此评估主流闭源 LLM 在“不运行即计数”任务中的表现。

Result: 最新 LLM 在简单内核上可实现完美分类，但在涉及除法、内置数学函数或公共子表达式等隐式 FLOPs 场景中仍会出现数量级级别的预测错误。

Conclusion: 当前代码辅助工具难以内化硬件相关的微码效应，gpuFLOPBench 可作为推动 LLM 发展、使其具备专业 GPU 开发者级性能推理能力的重要测试平台。

Abstract: Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to "count without running" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench

</details>


### [12] [A Structure-Aware Irregular Blocking Method for Sparse LU Factorization](https://arxiv.org/abs/2512.04389)
*Zhen Hu,Dongliang Xiong,Kai Huang,Changjun Wu,Xiaowen Jiang*

Main category: cs.DC

TL;DR: 本文提出了一种结构感知的不规则分块方法，通过引入基于对角块的新特征来刻画稀疏矩阵中非零元的局部分布，并据此动态调整分块大小，在单GPU和多GPU环境下显著优于现有稀疏LU分解工具。


<details>
  <summary>Details</summary>
Motivation: 在稀疏LU分解中，符号分解后的非零元素通常集中在矩阵的对角线和右下区域，传统二维规则分块在这种非均匀分布下容易造成负载不均衡；同时，现有矩阵特征难以有效指导分块策略。

Method: 提出一种结构感知的不规则分块方法：首先引入一种新的基于对角块的特征以刻画稀疏矩阵局部非零分布；然后根据该特征动态调整分块粒度——在稠密区域使用细粒度块，在稀疏区域使用粗粒度块，从而在依赖树的同层及跨层间实现非零元数量的均衡。

Result: 在单块NVIDIA A100 GPU上，所提方法相比PanguLU和SuperLU_DIST分别获得1.50倍和3.32倍的平均加速；在4块A100 GPU上，加速比分别为1.40倍和3.84倍。

Conclusion: 所提出的不规则分块方法能有效应对稀疏LU分解中非零元分布不均的问题，显著提升并行性能，优于当前主流稀疏求解器。

Abstract: In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.

</details>


### [13] [Offloading to CXL-based Computational Memory](https://arxiv.org/abs/2512.04449)
*Suyeon Lee,Kangkyu Park,Kwangsik Shin,Ada Gavrilovska*

Main category: cs.DC

TL;DR: 本文提出了一种名为KAI的新系统，通过引入“异步回流”协议，在CXL计算内存（CCM）中实现异步数据传输与轻量级流水线，显著提升端到端性能并减少主机与CCM的空闲时间。


<details>
  <summary>Details</summary>
Motivation: 现有CXL计算内存中的任务卸载机制无法有效利用不同CXL协议之间的权衡，难以适应具有多样化数据和处理需求的工作负载，限制了系统整体性能和效率。

Method: 作者分析了不同CXL协议下的性能权衡，并在此基础上设计了“异步回流”协议，将数据与控制传输操作分层叠加于底层CXL协议之上，构建了支持异步数据移动和轻量级流水线的KAI系统。

Result: KAI系统最多可将端到端运行时间减少50.4%，平均降低CCM和主机空闲时间分别达22.11倍和3.85倍。

Conclusion: 通过合理利用CXL协议特性并引入异步机制，KAI有效提升了计算内存系统的性能与资源利用率，为未来异构内存架构中的高效任务卸载提供了新思路。

Abstract: CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.

</details>


### [14] [Federated Learning for Terahertz Wireless Communication](https://arxiv.org/abs/2512.04984)
*O. Tansel Baydas,Ozgur B. Akan*

Main category: cs.DC

TL;DR: 本文研究了太赫兹（THz）通信中宽带损伤对联邦学习（FL）优化动态的影响，提出多载波随机框架，揭示了标准聚合下因频谱空洞导致的“多样性陷阱”和带宽扩展的收敛性恶化问题，并提出SNR加权聚合策略以恢复高波束偏斜下的收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对太赫兹通信中实际宽带损伤（如波束偏斜、分子吸收和抖动）如何影响联邦学习优化动态的理论刻画。

Method: 构建了一个多载波随机框架，将本地梯度更新与频率选择性THz效应显式耦合，并分析了不同聚合策略下的收敛行为。

Result: 发现标准无偏聚合下收敛误差由子载波信噪比的调和平均决定，单个频谱空洞可使整个带宽失效；识别出带宽存在临界上限，超过后因热噪声和边缘增益崩溃反而恶化收敛；提出的SNR加权聚合策略能有效抑制频谱空洞处的方差奇异性，在高波束偏斜场景下恢复收敛。

Conclusion: 在太赫兹联邦学习系统中，必须考虑物理层宽带损伤对学习性能的影响，采用SNR感知的聚合机制是实现可靠收敛的关键。

Abstract: The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [15] [Vision and Causal Learning Based Channel Estimation for THz Communications](https://arxiv.org/abs/2512.04380)
*Kitae Kim,Yan Kyaw Tun,Md. Shirajum Munir,Chirsto Kurisummoottil Thomas,Walid Saad,Choong Seon Hong*

Main category: cs.NI

TL;DR: 本文提出了一种基于视觉的太赫兹（THz）信道估计方法，通过融合计算机视觉与变分因果动力学（VCD），利用实时城市环境图像提升信道预测精度，在非视距（NLoS）场景下显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信在6G中具有高数据速率和低延迟潜力，但其信道估计面临高传播损耗、环境遮挡和大气吸收等挑战，尤其在城市复杂非视距环境中传统方法效果不佳。

Method: 结合计算机视觉算法与变分因果动力学（VCD），通过分析城市环境的实时图像，建模物理对象与THz信号间的动态因果关系，实现更准确的信道估计。

Result: 所提方法在多种动态城市场景中显著优于传统AI方法，信道预测精度最高可达传统方法的两倍，并在非视距条件下表现出更强的鲁棒性和泛化能力。

Conclusion: 该基于视觉与因果推理的信道估计方法有效提升了太赫兹通信在复杂城市环境中的可靠性与准确性，为6G THz-MIMO系统提供了可行的技术路径。

Abstract: The use of terahertz (THz) communications with massive multiple input multiple output (MIMO) systems in 6G can potentially provide high data rates and low latency communications. However, accurate channel estimation in THz frequencies presents significant challenges due to factors such as high propagation losses, sensitivity to environmental obstructions, and strong atmospheric absorption. These challenges are par- ticularly pronounced in urban environments, where traditional channel estimation methods often fail to deliver reliable results, particularly in complex non-line-of-sight (NLoS) scenarios. This paper introduces a novel vision-based channel estimation tech- nique that integrates causal reasoning into urban THz communi- cation systems. The proposed method combines computer vision algorithms with variational causal dynamics (VCD) to analyze real-time images of the urban environment, allowing for a deeper understanding of the physical factors that influence THz signal propagation. By capturing the complex, dynamic interactions between physical objects (such as buildings, trees, and vehicles) and the transmitted signals, the model can predict the channel with up to twice the accuracy of conventional methods. This model improves estimation accuracy and demonstrates supe- rior generalization performance. Hence, it can provide reliable predictions even in previously unseen urban environments. The effectiveness of the proposed method is particularly evident in NLoS conditions, where it significantly outperforms traditional methods such as by accounting for indirect signal paths, such as reflections and diffractions. Simulation results confirm that the proposed vision-based approach surpasses conventional artificial intelligence (AI)-based estimation techniques in accuracy and robustness, showing a substantial improvement across various dynamic urban scenarios.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [16] [Energy Profiling of Data-Sharing Pipelines: Modeling, Estimation, and Reuse Strategies](https://arxiv.org/abs/2512.04086)
*Sepideh Masoudi,Sebastian Werner,Pierluigi Plebani,Stefan Tai*

Main category: cs.DB

TL;DR: 本文提出了一种新方法，用于建模和估算数据共享管道中不同执行配置的能耗，并识别跨管道共享阶段的复用潜力，从而降低大型数据共享联盟中的能耗。


<details>
  <summary>Details</summary>
Motivation: 现有数据共享管道的研究多聚焦于治理与策略执行，而对能源效率的关注不足；作者旨在填补这一空白，提升数据交换过程中的能效。

Method: 提出一种建模与估算方法，分析不同执行配置的能耗，并识别多个管道中可复用的共享阶段以优化能源使用。

Result: 通过仿真实验验证了该方法的有效性，展示了跨组织管道优化在节能方面的显著潜力。

Conclusion: 该方法为构建节能型数据共享管道提供了基础，有助于推动面向能效的数据交换策略发展。

Abstract: Data-sharing pipelines involve a series of stages that apply policy-based data transformations to enable secure and effective data exchange among organizations. Although numerous tools and platforms exist to manage governance and enforcement in these pipelines, energy efficiency in data exchange has received limited attention. This paper introduces a novel method to model and estimate the energy consumption of different execution configurations in data-sharing pipelines. Additionally, this method identifies reuse potential in shared stages across pipelines that hold the key to reducing energy in large data-sharing federations. We validate this method through simulation experiments, revealing promising potential for cross-organizational pipeline optimization and laying a foundation for energy-conscious execution strategies.

</details>


### [17] [A Fast Ethereum-Compatible Forkless Database](https://arxiv.org/abs/2512.04735)
*Herbert Jordan,Kamil Jezek,Pavle Subotic,Bernhard Scholz*

Main category: cs.DB

TL;DR: 本文提出了一种专为无分叉区块链设计的新型状态数据库，在保持以太坊兼容性的同时，显著提升了性能并大幅减少了存储开销。


<details>
  <summary>Details</summary>
Motivation: 以太坊的状态数据库（StateDB）为支持分叉链而设计，需维护多个状态版本，这在采用快速共识协议、无需分叉的现代区块链中造成效率低下；此外，现有StateDB基于键值存储（如LevelDB），进一步限制了性能。

Method: 设计并实现一种原生的状态数据库，专门针对无分叉区块链进行优化，同时保持与以太坊的兼容性。

Result: 该数据库为验证节点带来10倍的速度提升和99%的空间节省，并使归档节点的存储需求减少三分之二。

Conclusion: 所提出的新型状态数据库在兼容以太坊标准的前提下，有效解决了传统StateDB在无分叉高性能区块链中的效率瓶颈，显著优化了速度与存储开销。

Abstract: The State Database of a blockchain stores account data and enables authentication. Modern blockchains use fast consensus protocols to avoid forking, improving throughput and finality. However, Ethereum's StateDB was designed for a forking chain that maintains multiple state versions. While newer blockchains adopt Ethereum's standard for DApp compatibility, they do not require multiple state versions, making legacy Ethereum databases inefficient for fast, non-forking blockchains. Moreover, existing StateDB implementations have been built on key-value stores (e.g., LevelDB), which make them less efficient.
  This paper introduces a novel state database that is a native database implementation and maintains Ethereum compatibility while being specialized for non-forking blockchains. Our database delivers ten times speedups and 99% space reductions for validators, and a threefold decrease in storage requirements for archive nodes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection](https://arxiv.org/abs/2512.04106)
*Fouad Trad,Ali Chehab*

Main category: cs.SE

TL;DR: 本文研究了在代码漏洞检测任务中，利用检索增强的提示（retrieval-augmented prompting）提升大语言模型少样本学习性能的方法。实验表明，该方法显著优于随机示例的少样本提示、零样本提示以及微调后的Gemini模型，在20个示例下达到74.05%的F1分数和83.90%的部分匹配准确率，且无需微调成本。


<details>
  <summary>Details</summary>
Motivation: 少样本提示的效果高度依赖上下文示例的质量与选择，尤其在复杂任务如代码漏洞检测中。因此，作者希望探索通过检索增强策略来提升提示效果，以避免微调带来的高成本。

Method: 作者在Gemini-1.5-Flash模型上系统评估三种策略：(1) 随机选择示例的标准少样本提示；(2) 使用语义相似示例的检索增强提示；(3) 基于检索示例直接打标签的无模型推理方法。同时与零样本提示及多个微调模型（包括Gemini、DistilBERT、DistilGPT2、CodeBERT）进行对比。

Result: 检索增强提示在所有提示策略中表现最佳：20-shot下F1达74.05%，部分匹配准确率83.90%；优于零样本（F1: 36.35%）和微调Gemini（F1: 59.31%）。虽然微调CodeBERT性能更高（F1: 91.22%），但需额外训练资源。

Conclusion: 检索增强提示是一种高效且低成本的替代方案，能在不进行模型微调的前提下显著提升大语言模型在代码漏洞检测任务中的少样本性能。

Abstract: Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.

</details>


### [19] [HAI-Eval: Measuring Human-AI Synergy in Collaborative Coding](https://arxiv.org/abs/2512.04111)
*Hanjun Luo,Chiming Ni,Jiaheng Wen,Zhimu Huang,Yiran Wang,Bingduo Liao,Sylvia Chung,Yingbin Jin,Xinfeng Li,Wenyuan Xu,XiaoFeng Wang,Hanan Salam*

Main category: cs.SE

TL;DR: 本文提出了 HAI-Eval，一个用于评估人类与大语言模型（LLM）在编程任务中协作效能的统一基准。该基准包含45个“必须协作”问题模板，这些问题对单独的人类或LLM都难以解决，但通过有效协作可成功完成。实验表明，人机协作显著优于各自独立表现，并揭示了一种新兴的共同推理伙伴关系。


<details>
  <summary>Details</summary>
Motivation: 现有评估体系无法衡量人类与LLM在编程中的协作能力，因其聚焦于定义明确的算法问题，忽略了需人机协同解决的复杂任务。因此，亟需一种能反映真实开发场景中人机协同效能的新评估框架。

Method: 作者设计了 HAI-Eval 基准，包含45个“协作必要型”问题模板，可动态生成任务；提供标准化IDE供人类使用，并为LLM提供含450个任务实例的可复现工具包。通过一项包含45名参与者的研究，在四种人类干预程度下对比5个先进LLM的表现。

Result: 单独LLM和未辅助人类的通过率极低（分别为0.67%和18.89%），而人机协作将通过率提升至31.11%。研究还发现，战略突破可来自人类或AI，体现出共推理的合作模式。

Conclusion: HAI-Eval 不仅为下一代编码智能体提供了具有挑战性的评估基准，也为AI时代开发者核心能力的评估建立了可扩展、贴近实际的框架。

Abstract: LLM-powered coding agents are reshaping the development paradigm. However, existing evaluation systems, neither traditional tests for humans nor benchmarks for LLMs, fail to capture this shift. They remain focused on well-defined algorithmic problems, which excludes problems where success depends on human-AI collaboration. Such collaborative problems not only require human reasoning to interpret complex contexts and guide solution strategies, but also demand AI efficiency for implementation. To bridge this gap, we introduce HAI-Eval, a unified benchmark designed to measure the synergy of human-AI partnership in coding. HAI-Eval's core innovation is its "Collaboration-Necessary" problem templates, which are intractable for both standalone LLMs and unaided humans, but solvable through effective collaboration. Specifically, HAI-Eval uses 45 templates to dynamically create tasks. It also provides a standardized IDE for human participants and a reproducible toolkit with 450 task instances for LLMs, ensuring an ecologically valid evaluation. We conduct a within-subject study with 45 participants and benchmark their performance against 5 state-of-the-art LLMs under 4 different levels of human intervention. Results show that standalone LLMs and unaided participants achieve poor pass rates (0.67% and 18.89%), human-AI collaboration significantly improves performance to 31.11%. Our analysis reveals an emerging co-reasoning partnership. This finding challenges the traditional human-tool hierarchy by showing that strategic breakthroughs can originate from either humans or AI. HAI-Eval establishes not only a challenging benchmark for next-generation coding agents but also a grounded, scalable framework for assessing core developer competencies in the AI era. Our benchmark and interactive demo will be openly accessible.

</details>


### [20] [Reusing Model Validation Methods for the Continuous Validation of Digital Twins of Cyber-Physical Systems](https://arxiv.org/abs/2512.04117)
*Joost Mertens,Joachim Denil*

Main category: cs.SE

TL;DR: 本文提出了一种基于模型验证指标的通用方法，用于检测数字孪生系统中的异常，并通过参数估计修正数字孪生模型，以保持其与物理系统的同步；该方法在港口门式起重机案例中得到验证。


<details>
  <summary>Details</summary>
Motivation: 物理系统在部署后会因维护、磨损或人为错误等因素不断演化，而数字孪生若为仿真模型，则需随之更新以维持其有效性；因此需要一种机制来检测物理系统的变化并相应调整数字孪生。

Method: 复用基于模型设计中的验证技术，利用验证指标检测数字孪生系统中的异常，并基于历史数据进行参数估计，以修正数字孪生模型中的误差。

Result: 提出了一种通用的异常检测方法，并在一个具有工业相关性的门式起重机学术案例中成功演示了该技术的有效性。

Conclusion: 通过结合模型验证与参数估计，能够有效检测并修正数字孪生与物理系统之间的偏差，从而维持数字孪生的代表性与可信度。

Abstract: One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reflects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) we provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) we demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.

</details>


### [21] [DrP: Meta's Efficient Investigations Platform at Scale](https://arxiv.org/abs/2512.04250)
*Shubham Somani,Vanish Talwar,Madhura Parikh,Eduardo Hernandez,Jimmy Wang,Shreya Shah,Chinmay Gandhi,Sanjay Sundarajan,Neeru Sharma,Srikanth Kamath,Nitin Gupta,Benjamin Renard,Ohad Yahalom,Chris Davis*

Main category: cs.SE

TL;DR: 本文提出了DrP——一个端到端的自动化调查框架，通过可编程的分析器（analyzers）、可扩展后端、与告警和事件管理工具集成的插件以及事后处理系统，显著降低平均故障修复时间（MTTR）并减轻运维负担。


<details>
  <summary>Details</summary>
Motivation: 当前大规模系统中的故障调查流程多依赖人工或临时脚本，效率低下，导致故障修复时间长、运维负担重、生产力低下。

Method: 设计并实现DrP系统，包括用于编写调查剧本的SDK（称为analyzers）、可扩展执行后端、与主流工作流（如告警系统）集成的插件，以及支持自动缓解措施的事后处理系统。

Result: DrP已在Meta大规模部署5年，覆盖300多个团队和2000多个analyzers，日均执行5万次自动化分析，平均MTTR降低20%（部分团队超过80%），显著提升运维效率。

Conclusion: DrP有效实现了大规模系统中故障调查的自动化，显著缩短故障恢复时间并减轻运维负担，具有良好的可扩展性和实用性。

Abstract: Investigations are a significant step in the operational workflows for large scale systems across multiple domains such as services, data, AI/ML, mobile. Investigation processes followed by on-call engineers are often manual or rely on ad-hoc scripts. This leads to inefficient investigations resulting in increased time to mitigate and isolate failures/SLO violations. It also contributes to on-call toil and poor productivity leading to multiple hours/days spent in triaging/debugging incidents. In this paper, we present DrP, an end-to-end framework and system to automate investigations that reduces the mean time to resolve incidents (MTTR) and reduces on-call toil. DrP consists of an expressive and flexible SDK to author investigation playbooks in code (called analyzers), a scalable backend system to execute these automated playbooks, plug-ins to integrate playbooks into mainstream workflows such as alerts and incident management tools, and a post-processing system to take actions on investigations including mitigation steps.
  We have implemented and deployed DrP at large scale at Meta covering 300+ teams, 2000+ analyzers, across a large set of use cases across domains such as services, core infrastructure, AI/ML, hardware, mobile. DrP has been running in production for the past 5 years and executes 50K automated analyses per day. Overall, our results and experience show that DrP has been able to reduce average MTTR by 20 percent at large scale (with over 80 percent for some teams) and has significantly improved on-call productivity.

</details>


### [22] [On the Role and Impact of GenAI Tools in Software Engineering Education](https://arxiv.org/abs/2512.04256)
*Qiaolin Qin,Ronnie de Souza Santos,Rodrigo Spinola*

Main category: cs.SE

TL;DR: 本研究通过调查130名本科生，探讨了生成式AI（GenAI）在软件工程教育中的使用情况，发现学生主要将其用于渐进式学习和高级实现，虽获益于头脑风暴和信心提升，但也面临输出理解困难、伦理担忧等问题，并呼吁更清晰的教学指导。


<details>
  <summary>Details</summary>
Motivation: 随着ChatGPT和GitHub Copilot等生成式AI工具的兴起，软件工程教育既迎来新机遇，也面临学生过度依赖、伦理问题及学习效果受损等挑战，亟需了解学生实际使用体验以优化教学策略。

Method: 对来自两所大学的130名软件工程本科生进行问卷调查，结合李克特量表与开放式问题，从使用情境、感知收益、挑战、伦理认知和教学期望五个维度收集数据。

Result: 学生主要将GenAI用于渐进学习和高级编码，认为其有助于构思和增强信心；但常遇到输出逻辑不清、难以适配等问题；同时关注公平性和学术不端等伦理风险，并希望获得更明确的教学指引。

Conclusion: 生成式AI正以复杂方式重塑软件工程教育，需通过教学支架设计、伦理规范制定和适应性教学策略，确保其促进公平且有效的学习。

Abstract: Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.

</details>


### [23] [Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage](https://arxiv.org/abs/2512.04262)
*Nolan Platt,Ethan Luchs,Sehrish Nizamani*

Main category: cs.SE

TL;DR: 本文研究了大语言模型（LLM）在早期开发阶段进行启发式可用性评估的可靠性与一致性，发现GPT-4o在识别可用性问题方面表现中等一致，但在严重性判断上差异较大，需人工监督。


<details>
  <summary>Details</summary>
Motivation: 传统由人类专家进行的启发式可用性评估耗时且主观，尤其在开发早期阶段；因此探索是否可利用大语言模型实现自动化、可靠的启发式评估。

Method: 基于Jakob Nielsen的十大可用性启发式原则，对30个开源网站使用GPT-4o进行三次独立评估，共生成850多条启发式评估结果，并通过Cohen's Kappa和Krippendorff's Alpha等指标分析一致性。

Result: 模型在问题检测上表现出中等一致性（平均Cohen's Kappa为0.50，完全一致率为84%），但在严重性判断上一致性较低（加权Cohen's Kappa为0.63，完全一致率仅56%，Krippendorff's Alpha接近零）。

Conclusion: GPT-4o可用于早期自动化可用性测试，在识别问题方面具有一致性，但严重性评估仍需人工介入；本研究为提升LLM在UX评估中的一致性提供了基础和方法。

Abstract: Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.

</details>


### [24] [Polynomiogram: An Integrated Framework for Root Visualization and Generative Art](https://arxiv.org/abs/2512.04263)
*Hoang Duc Nguyen,Anh Van Pham,Hien D. Nguyen*

Main category: cs.SE

TL;DR: Polynomiogram 是一个结合科学计算与生成艺术的框架，通过灵活的参数采样和双引擎数值求解，支持对多项式根系统的可视化分析与个性化艺术创作。


<details>
  <summary>Details</summary>
Motivation: 将多项式根系统的数学研究与生成艺术相结合，提供一个既能用于科研教育又能用于创意表达的统一平台。

Method: 采用用户自定义域中抽取两个独立参数并通过生成函数映射到多项式系数的灵活采样方案；集成 NumPy 伴侣矩阵求解器（快速大规模计算）和 MPSolve（高精度验证）两种数值引擎。

Result: 成功验证了 Kac 和 Lucas 多项式等经典系综的数值准确性；应用于三次多项式系统揭示其分岔结构；并生成了如木槿花形态及致敬 AI 的个性化艺术作品。

Conclusion: Polynomiogram 框架在科学研究、教育可视化和生成艺术方面均展现出强大潜力，实现了数学严谨性与艺术创造力的有效融合。

Abstract: This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.

</details>


### [25] [Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures](https://arxiv.org/abs/2512.04273)
*Tyler Slater*

Main category: cs.SE

TL;DR: 该研究首次提出衡量AI生成微服务中“架构侵蚀”和“技术债务”的实证框架，发现开源权重模型（如Llama 3）在遵循六边形架构方面表现显著差于闭源模型（如GPT-5.1），存在高架构违规率和逻辑代码缺失问题。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型（LLM）的研究多聚焦于功能正确性（如pass@k指标），但缺乏对其生成代码长期可维护性影响的量化分析，尤其是架构层面的技术债务问题。

Method: 通过提示三种前沿模型（GPT-5.1、Claude 4.5 Sonnet 和 Llama 3 8B）在严格六边形架构约束下实现标准化的图书借阅微服务，并利用抽象语法树（AST）解析评估其架构合规性与逻辑代码行数（LLOC）。

Result: 闭源模型（如GPT-5.1）架构违规率为0%，而Llama 3的架构违规率达80%，常在领域层与基础设施层间引入非法循环依赖；此外，开源模型生成的逻辑代码行数比闭源模型少60%，表现出“实现惰性”。

Conclusion: 若无自动化架构检查机制，使用较小的开源权重模型进行系统脚手架构建会加速结构性技术债务的积累，影响软件长期可维护性。

Abstract: As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure "Architectural Erosion" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of "Implementation Laziness," where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.

</details>


### [26] [MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training](https://arxiv.org/abs/2512.04319)
*Zixiao Zhao,Fatemeh H. Fard,Jie JW Wu*

Main category: cs.SE

TL;DR: 本文提出MANTRA框架，通过在代码预训练语言模型和大语言模型的微调过程中嵌入噪声诊断与缓解机制，有效提升模型在含噪软件工程数据上的性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在软件工程任务中的可靠应用依赖于高质量训练数据，但大规模代码仓库常包含噪声或错误标签，影响模型准确性与鲁棒性；目前针对软件工程领域和大语言模型的噪声标签学习研究较少。

Method: 提出MANTRA多阶段自适应噪声处理框架，在微调过程中结合样本损失动态和高斯混合模型聚类，采用自适应dropout策略剔除持续噪声样本，保留干净数据。

Result: 在代码摘要和提交意图分类任务上实验表明，不同大语言模型对噪声敏感度不同，但使用MANTRA后所有模型性能均得到提升。

Conclusion: MANTRA能有效减轻训练数据中噪声带来的负面影响，减少数据清洗时间，并最大化微调效果，为软件工程任务中的噪声标签问题提供实用解决方案。

Abstract: The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.

</details>


### [27] [Targeted Testing of Compiler Optimizations via Grammar-Level Composition Styles](https://arxiv.org/abs/2512.04344)
*Zitong Zhou,Ben Limpanukorn,Hong Jin Kang,Jiyuan Wang,Yaoxuan Wu,Akos Kiss,Renata Hodovan,Miryung Kim*

Main category: cs.SE

TL;DR: 本文提出了一种名为TargetFuzz的新型模糊测试方法，通过针对单个编译器优化进行定向模糊测试，利用程序结构中的组合模式（如相邻、嵌套等）来有效触发优化逻辑。相比传统基于优化流水线的模糊测试方法，TargetFuzz在LLVM和MLIR上显著提升了覆盖率和优化触发次数，并能覆盖更多原本被遗漏的优化。


<details>
  <summary>Details</summary>
Motivation: 现有模糊测试工具难以有效测试编译器优化：一方面依赖固定顺序的优化流水线，受相位排序问题影响，无法充分暴露优化之间的交互；另一方面难以生成满足特定结构关系的输入以触发优化。因此需要一种更精准、灵活且可扩展的模糊测试方法。

Method: 作者提出TargetFuzz，一种基于文法的变异型模糊测试器。它首先从与优化相关的语料库中挖掘“组合风格”（即优化所依赖的结构关系），然后在更大的通用语料库中通过合成变异将这些组合风格重建到不同上下文中，从而测试各种优化逻辑。该方法只需轻量级的文法注解即可适配新语言，并自动合成变异和交叉操作，无需手工编写生成器或语言特定的变异策略。

Result: 在LLVM和MLIR上的实验表明，TargetFuzz相比基线模糊器，在定向模糊模式下分别提升8%和11%的覆盖率，触发优化次数提高2.8倍和2.6倍。此外，TargetFuzz成功测试了全部37个采样的LLVM优化，而流水线模糊测试遗漏了其中12个。

Conclusion: TargetFuzz通过定向模糊测试有效弥补了传统流水线模糊测试的不足，尤其适用于如MLIR这类模块化、快速演化的编译器框架，为编译器优化的正确性验证提供了高效且可扩展的新途径。

Abstract: Ensuring the correctness of compiler optimizations is critical, but existing fuzzers struggle to test optimizations effectively. First, most fuzzers use optimization pipelines (heuristics-based, fixed sequences of passes) as their harness. The phase-ordering problem can enable or preempt transformations, so pipelines inevitably miss optimization interactions; moreover, many optimizations are not scheduled, even at aggressive levels. Second, optimizations typically fire only when inputs satisfy specific structural relationships, which existing generators and mutations struggle to produce. We propose targeted fuzzing of individual optimizations to complement pipeline-based testing. Our key idea is to exploit composition styles - structural relations over program constructs (adjacency, nesting, repetition, ordering) - that optimizations look for. We build a general-purpose, grammar-based mutational fuzzer, TargetFuzz, that (i) mines composition styles from an optimization-relevant corpus, then (ii) rebuilds them inside different contexts offered by a larger, generic corpus via synthesized mutations to test variations of optimization logic. TargetFuzz is adaptable to a new programming language by lightweight, grammar-based, construct annotations - and it automatically synthesizes mutators and crossovers to rebuild composition styles. No need for hand-coded generators or language-specific mutators, which is particularly useful for modular frameworks such as MLIR, whose dialect-based, rapidly evolving ecosystem makes optimizations difficult to fuzz. Our evaluation on LLVM and MLIR shows that TargetFuzz improves coverage by 8% and 11% and triggers optimizations 2.8$\times$ and 2.6$\times$, compared to baseline fuzzers under the targeted fuzzing mode. We show that targeted fuzzing is complementary: it effectively tests all 37 sampled LLVM optimizations, while pipeline-fuzzing missed 12.

</details>


### [28] [Automating Complex Document Workflows via Stepwise and Rollback-Enabled Operation Orchestration](https://arxiv.org/abs/2512.04445)
*Yanbin Zhang,Hanhui Ye,Yue Bai,Qiming Zhang,Liao Xiang,Wu Mianzhi,Renjun Hu*

Main category: cs.SE

TL;DR: 本文提出AutoDW，一种支持逐步执行与回滚机制的文档工作流自动化框架，在包含多步骤、相互依赖指令的真实场景中显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有智能体系统虽能执行单条指令，但在处理多步骤、会话级文档工作流时缺乏对操作流程的有效控制，难以保证长期任务中与用户意图和文档状态的一致性。

Method: AutoDW通过逐步规划API调用，结合用户指令、意图过滤后的API候选集及文档状态演化进行决策，并在参数和API层面引入回滚机制，实现动态修正与容错。

Result: 在包含250个会话、1,708条人工标注指令的基准测试中，AutoDW在指令级和会话级任务上分别达到90%和62%的完成率，较强基线提升40%和76%，且对不同大语言模型和任务难度具有鲁棒性。

Conclusion: AutoDW有效解决了多步骤文档工作流自动化中的控制与容错问题，显著提升了复杂任务的执行成功率和一致性，具备良好的实用性和泛化能力。

Abstract: Workflow automation promises substantial productivity gains in everyday document-related tasks. While prior agentic systems can execute isolated instructions, they struggle with automating multi-step, session-level workflows due to limited control over the operational process. To this end, we introduce AutoDW, a novel execution framework that enables stepwise, rollback-enabled operation orchestration. AutoDW incrementally plans API actions conditioned on user instructions, intent-filtered API candidates, and the evolving states of the document. It further employs robust rollback mechanisms at both the argument and API levels, enabling dynamic correction and fault tolerance. These designs together ensure that the execution trajectory of AutoDW remains aligned with user intent and document context across long-horizon workflows. To assess its effectiveness, we construct a comprehensive benchmark of 250 sessions and 1,708 human-annotated instructions, reflecting realistic document processing scenarios with interdependent instructions. AutoDW achieves 90% and 62% completion rates on instruction- and session-level tasks, respectively, outperforming strong baselines by 40% and 76%. Moreover, AutoDW also remains robust for the decision of backbone LLMs and on tasks with varying difficulty. Code and data will be open-sourced. Code: https://github.com/YJett/AutoDW

</details>


### [29] [LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models](https://arxiv.org/abs/2512.04474)
*Jiaqi Sun,Wei Li,Heng Zhang,Chutong Ding,Shiyou Qian,Jian Cao,Guangtao Xue*

Main category: cs.SE

TL;DR: 本文提出LLM-SrcLog，一种结合源代码分析与数据驱动方法的日志模板解析框架，在准确性和效率之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有日志解析方法主要基于日志本身，忽略源代码信息，难以应对动态日志结构和系统演化；同时，逐条日志使用大语言模型（LLM）推理成本过高，不适用于实际部署。

Method: LLM-SrcLog框架包含三个核心组件：跨函数静态代码分析器以重建日志上下文、基于LLM的白盒模板提取器（含后处理以区分常量与变量）、以及用于处理无源码日志的黑盒数据驱动聚类解析器。

Result: 在Hadoop、Zookeeper和Sunfire-Compute三个数据集上，LLM-SrcLog相比两种基于LLM的基线方法，F1分数提升2–35%，在线解析延迟与数据驱动方法相当，比逐条LLM解析快约1000倍。

Conclusion: LLM-SrcLog通过融合源码先验与数据驱动策略，实现了高精度与高效率的统一，并在真实生产环境中验证了其有效性。

Abstract: Log parsing transforms raw logs into structured templates containing constants and variables. It underpins anomaly detection, failure diagnosis, and other AIOps tasks. Current parsers are mostly reactive and log-centric. They only infer templates from logs, mostly overlooking the source code. This restricts their capacity to grasp dynamic log structures or adjust to evolving systems. Moreover, per-log LLM inference is too costly for practical deployment. In this paper, we propose LLM-SrcLog, a proactive and unified framework for log template parsing. It extracts templates directly from source code prior to deployment and supplements them with data-driven parsing for logs without available code. LLM-SrcLog integrates a cross-function static code analyzer to reconstruct meaningful logging contexts, an LLM-based white-box template extractor with post-processing to distinguish constants from variables, and a black-box template extractor that incorporates data-driven clustering for remaining unmatched logs. Experiments on two public benchmarks (Hadoop and Zookeeper) and a large-scale industrial system (Sunfire-Compute) show that, compared to two LLM-based baselines, LLM-SrcLog improves average F1-score by 2-17% and 8-35%. Meanwhile, its online parsing latency is comparable to data-driven methods and about 1,000 times faster than per-log LLM parsing. LLM-SrcLog achieves a near-ideal balance between speed and accuracy. Finally, we further validate the effectiveness of LLM-SrcLog through practical case studies in a real-world production environment.

</details>


### [30] [Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding](https://arxiv.org/abs/2512.04538)
*Xinkui Zhao,Rongkai Liu,Yifan Zhang,Chen Zhi,Lufei Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

TL;DR: CoCo is a novel code completion framework that leverages multi-granularity structured context from large-scale repositories via static analysis and graph-based selection, significantly outperforming existing methods by up to 20.2% EM on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing RAG-based code completion methods treat code as plain text, ignoring structural semantics and code-specific dependencies, which limits their ability to understand control flow and developer intent.

Method: CoCo uses static code analysis to extract structured context at function, file, and project levels, applies a graph-based multi-granularity context selection mechanism to filter noise, converts the context into natural language prompts, and employs a structure-aware re-ranker to align generated code semantically and structurally.

Result: CoCo achieves consistent improvements over state-of-the-art baselines, with up to 20.2% gains in exact match (EM) on CrossCodeEval and RepoEval benchmarks, and is model-agnostic for easy integration.

Conclusion: By incorporating structured, multi-granularity context and preserving code semantics, CoCo significantly enhances repository-level code completion quality and offers a flexible, plug-and-play solution for existing systems.

Abstract: As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.

</details>


### [31] [Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models](https://arxiv.org/abs/2512.04673)
*Gunjan Das,Paheli Bhattacharya,Rishabh Gupta*

Main category: cs.SE

TL;DR: 本文系统评估了通用和代码专用大语言模型在语言、推理和代码理解等多领域任务中的表现，发现代码优化模型（如CodeLLaMA）在非编码任务中也展现出优于通用模型（如Mistral-7B、Llama-3-8B）的推理与语法精度。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于单个模型能力，缺乏对语言、推理与代码理解能力的统一跨领域系统性比较。

Method: 在六个涵盖语言能力、数学推理和可信度的基准上，对五种通用和三种代码专用的最先进大语言模型进行全面评估，并在CoNaLa数据集上分析其代码解释行为。

Result: 代码优化模型（如CodeLLaMA变体）展现出更强的推理能力和语法精确性，即使在非编码任务中也优于通用模型。

Conclusion: 专为代码设计的大语言模型在跨领域能力上具有显著优势，表明代码训练对提升综合语言与推理能力具有积极影响。

Abstract: Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.

</details>


### [32] [Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap](https://arxiv.org/abs/2512.04680)
*Jialong Li,Mingyue Zhang,Nianyu Li,Danny Weyns,Zhi Jin,Kenji Tei*

Main category: cs.SE

TL;DR: 本文综述了生成式人工智能（GenAI）在自适应系统（SASs）中的潜在优势与挑战，围绕MAPE-K反馈环功能增强和人机协同两个方面进行分析，并提出整合GenAI到SAS中的研究路线图及应对策略。


<details>
  <summary>Details</summary>
Motivation: 尽管GenAI在数据理解与逻辑推理方面表现出色，且其能力与SAS的核心功能高度契合，但其在SAS中应用的具体优势与挑战尚不明确。由于相关文献有限、SAS技术与应用场景多样以及GenAI技术快速演进，亟需系统性梳理以指导研究与实践。

Method: 作者从四个不同研究领域收集、筛选并分析相关文献，将GenAI对SAS的潜在益处归纳为两类：一是提升SAS在MAPE-K反馈环各环节中的自主性，二是改善人在环路中与SAS的交互。基于此，构建了一个整合GenAI到SAS的研究路线图。

Result: 研究识别出GenAI在增强SAS自主性和人机协作方面的具体潜力，并系统总结了当前面临的关键研究挑战，如可靠性、可解释性、实时性等。同时指出了GenAI自身存在的不足，并提出了可能的缓解策略。

Conclusion: GenAI为SAS的发展提供了新机遇，但其有效整合仍面临多重挑战。本文通过提供全面的现状快照和研究路线图，为未来研究者和实践者在该交叉领域的探索奠定了基础。

Abstract: Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.

</details>


### [33] [POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?](https://arxiv.org/abs/2512.04702)
*Divyansh Pandey,Vyakhya Gupta,Prakhar Singhal,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 本文提出POLARIS，一种三层多智能体自适应框架，通过整合低延迟适配、可解释推理与元学习能力，实现对未知环境的预测性、主动式自适应，标志着迈向“自适应3.0”的范式转变。


<details>
  <summary>Details</summary>
Motivation: 传统自适应方法难以应对现代软件系统中由规模、复杂性和自主性带来的未知不确定性，现有方案缺乏泛化能力和跨子系统协同机制，无法有效处理突发的“未知的未知”问题。

Method: 提出POLARIS框架，包含三层：（1）低延迟适配器层用于监控与安全执行；（2）透明推理层利用工具感知、可解释的智能体生成并验证适应计划；（3）元学习层记录经验并持续优化适应策略。

Result: 在SWIM和SWITCH两个自适应基准上的初步评估表明，POLARIS consistently优于当前最先进的基线方法。

Conclusion: POLARIS代表了向Self-Adaptation 3.0的演进，即系统不仅能从环境中学习，还能推理并自我演化其适应机制，持续提升应对新挑战的能力。

Abstract: The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.

</details>


### [34] [Configuration Defects in Kubernetes](https://arxiv.org/abs/2512.05062)
*Yue Zhang,Uchswas Paul,Marcelo d'Amorim,Akond Rahman*

Main category: cs.SE

TL;DR: 本文对Kubernetes配置缺陷进行了实证研究，识别出15类缺陷，并评估了现有静态分析工具的检测能力；作者开发了一个新的linter来检测两种严重但未被现有工具覆盖的缺陷类型，发现了26个未知缺陷，其中19个已被修复。


<details>
  <summary>Details</summary>
Motivation: Kubernetes配置容易出错，配置缺陷可能导致严重后果，因此需要系统性地理解这些缺陷并提升检测与预防能力。

Method: 从开源仓库中提取2,260个Kubernetes配置脚本中的719个缺陷，通过定性分析将其归类为15种类型；评估8个公开静态分析工具的检测能力，并开发一个新的linter以检测现有工具无法发现的两类严重缺陷。

Result: 现有工具仅能检测15类缺陷中的8类，且对数据字段相关缺陷检测效果最好；新开发的linter成功发现26个此前未知的缺陷，其中19个已被开发者确认并修复。

Conclusion: 应结合缺陷检测与修复技术来提升Kubernetes配置脚本的可靠性，并提供了相关实践建议；研究数据与代码已公开。

Abstract: Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.

</details>
