<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 8]
- [cs.DC](#cs.DC) [Total: 8]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Tyche: A Hybrid Computation Framework of Illumination Pattern for Satellite Beam Hopping](https://arxiv.org/abs/2512.09312)
*Ziheng Yang,Kun Qiu,Zhe Chen,Wenjun Zhu,Yue Gao*

Main category: cs.NI

TL;DR: 本文提出Tyche框架，结合MCTS-BH与G-BH算法，显著提升高通量卫星波束跳变中照明模式的计算效率与系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有波束跳变照明模式计算方法（如遗传算法、多智能体强化学习）在面对300以上地面小区时存在计算耗时过长或难以收敛的问题，无法满足实际应用需求。

Method: 提出Tyche混合计算框架：采用蒙特卡洛树搜索波束跳变（MCTS-BH）算法，并结合滑动窗口与剪枝技术加速计算；同时引入贪心波束跳变（G-BH）算法提供实时临时解。

Result: MCTS-BH在37个小区场景下仅需12秒即可生成照明模式，相较传统方法大幅提速；系统吞吐量最高提升98.76%。

Conclusion: Tyche框架有效解决了高通量卫星波束跳变中大规模小区照明模式的高效计算问题，兼具实时性与高性能，具有良好的实用前景。

Abstract: High-Throughput Satellites (HTS) use beam hopping to handle non-uniform and time-varying ground traffic demand. A significant technical challenge in beam hopping is the computation of effective illumination patterns. Traditional algorithms, like the genetic algorithm, require over 300 seconds to compute a single illumination pattern for just 37 cells, whereas modern HTS typically covers over 300 cells, rendering current methods impractical for real-world applications. Advanced approaches, such as multi-agent deep reinforcement learning, face convergence issues when the number of cells exceeds 40. In this paper, we introduce Tyche, a hybrid computation framework designed to address this challenge. Tyche incorporates a Monte Carlo Tree Search Beam Hopping (MCTS-BH) algorithm for computing illumination patterns and employs sliding window and pruning techniques to significantly reduce computation time. Specifically, MCTS-BH can compute one illumination pattern for 37 cells in just 12 seconds. To ensure real-time computation, we use a Greedy Beam Hopping (G-BH) algorithm, which provides a provisional solution while MCTS-BH completes its computation in the background. Our evaluation results show that MCTS-BH can increase throughput by up to 98.76%, demonstrating substantial improvements over existing solutions.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [2] [CNFinBench: A Benchmark for Safety and Compliance of Large Language Models in Finance](https://arxiv.org/abs/2512.09506)
*Jinru Ding,Chao Ding,Wenrao Pang,Boyi Xiao,Zhiqiang Liu,Pengcheng Chen,Jiayuan Chen,Tiantian Yuan,Junming Guan,Yidong Jiang,Dawei Cheng,Jie Xu*

Main category: cs.CE

TL;DR: 本文提出了CNFinBench，一个面向金融领域的安全评估基准，通过红队对话、能力-合规-安全三元结构和多轮对抗测试，揭示了当前大语言模型在金融合规与风险控制方面的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有金融领域的大模型评测主要关注教科书式问答和数值计算，缺乏对真实场景下安全性行为（如监管合规、投资者保护、多轮对抗攻击防御、长文档推理和工具使用风险）的系统评估，且评估协议不透明、不可审计。

Method: 构建CNFinBench基准，包含金融定制的红队对话、基于长报告的证据推理任务和考虑司法管辖区的规则/税务合规任务；提出有害指令遵从分数（HICS）量化模型在多轮对抗中的安全性；采用动态选项扰动和混合LLM集成+人工校准的评判机制确保评估可审计。

Result: 在21个模型、15个子任务上的实验显示，模型在能力任务上平均得分为61.0，但在合规与风控任务上骤降至34.18；多轮对抗测试中多数系统仅达到部分抵抗（HICS 60–79），表明单纯拒绝有害请求不足以保证安全，需辅以可验证的推理依据。

Conclusion: 当前金融大模型存在明显的能力-合规差距，需通过更贴近真实场景、可审计且强调证据支撑的安全评估框架（如CNFinBench）来推动模型在金融应用中的可靠性和安全性。

Abstract: Large language models are increasingly deployed across the financial sector for tasks such as research, compliance, risk analysis, and customer service, which makes rigorous safety evaluation essential. However, existing financial benchmarks primarily focus on textbook-style question answering and numerical problem solving, but fail to evaluate models' real-world safety behaviors. They weakly assess regulatory compliance and investor-protection norms, rarely stress-test multi-turn adversarial tactics such as jailbreaks or prompt injection, inconsistently ground answers in long filings, ignore tool- or RAG-induced over-reach risks, and rely on opaque or non-auditable evaluation protocols. To close these gaps, we introduce CNFinBench, a benchmark that employs finance-tailored red-team dialogues and is structured around a Capability-Compliance-Safety triad, including evidence-grounded reasoning over long reports and jurisdiction-aware rule/tax compliance tasks. For systematic safety quantification, we introduce the Harmful Instruction Compliance Score (HICS) to measure how consistently models resist harmful prompts across multi-turn adversarial dialogues. To ensure auditability, CNFinBench enforces strict output formats with dynamic option perturbation for objective tasks and employs a hybrid LLM-ensemble plus human-calibrated judge for open-ended evaluations. Experiments on 21 models across 15 subtasks confirm a persistent capability-compliance gap: models achieve an average score of 61.0 on capability tasks but fall to 34.18 on compliance and risk-control evaluations. Under multi-turn adversarial dialogue tests, most systems reach only partial resistance (HICS 60-79), demonstrating that refusal alone is not a reliable proxy for safety without cited and verifiable reasoning.

</details>


### [3] [A roadmap of geospatial soil quality analysis systems](https://arxiv.org/abs/2512.09817)
*Habiba BEN ABDERRAHMANE,Slimane Oulad-Naoui,Benameur ZIANI*

Main category: cs.CE

TL;DR: 本文提出了一种融合多源土壤数据、GIS、遥感和机器学习的模块化统一框架，用于透明且可扩展的土壤质量评估。


<details>
  <summary>Details</summary>
Motivation: 传统土壤质量评估方法成本高、耗时长，空间和时间覆盖有限，亟需高效、可扩展的新方法。

Method: 构建一个集成多源土壤数据、地理信息系统（GIS）、遥感技术和机器学习算法的统一模块化流程。

Result: 该框架整合了土壤质量评估全流程中的最新技术，支持透明、可扩展的评估，并包含实际应用案例。

Conclusion: 所提出的综合路线图克服了现有研究局限于单一参数或特定模型的不足，为下一代更透明、自适应且契合可持续土地管理的土壤质量系统奠定基础。

Abstract: Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [4] [ZeroOS: A Universal Modular Library OS for zkVMs](https://arxiv.org/abs/2512.09300)
*Guangxian Zou,Isaac Zhang,Ryan Zarick,Kelvin Wong,Thomas Kim,Daniel L. -K. Wong,Saeid Yazdinejad,Dan Boneh*

Main category: cs.OS

TL;DR: ZeroOS 是一个为可验证应用（vApp）设计的模块化库操作系统（libOS），通过仅链接 vApp 所需的 Linux ABI 子集，解决了 zkVM 中因静态链接运行时导致的版本混乱和可信计算基过大的问题，从而降低维护负担并统一 zkVM 生态。


<details>
  <summary>Details</summary>
Motivation: 现代程序依赖操作系统和 libc 才能编译和运行，而 zkVM 通常通过分叉语言运行时并静态链接成 unikernel 来解决此问题，但这种方式导致版本管理困难并扩大了可信计算基。

Method: 提出 ZeroOS，一种模块化的 libOS，允许开发者使用现成工具链仅链接其 vApp 所需的 Linux ABI 子集；zkVM 团队只需为其平台编写 ZeroOS 引导加载器即可接入该生态。

Result: 显著减少 vApp 的可信计算基和维护成本，并促进 zkVM 生态在开发与审计资源上的整合。

Conclusion: ZeroOS 为 zkVM 提供了一个统一、轻量且易于维护的运行时解决方案，推动了可验证计算生态的发展。

Abstract: zkVMs promise general-purpose verifiable computation through ISA-level compatibility with modern programs and toolchains. However, compatibility extends further than just the ISA; modern programs often cannot run or even compile without an operating system and libc. zkVMs attempt to address this by maintaining forks of language-specific runtimes and statically linking them into applications to create self-contained unikernels, but this ad-hoc approach leads to version hell and burdens verifiable applications (vApps) with an unnecessarily large trusted computing base. We solve this problem with ZeroOS, a modular library operating system (libOS) for vApp unikernels; vApp developers can use off-the-shelf toolchains to compile and link only the exact subset of the Linux ABI their vApp needs. Any zkVM team can easily leverage the ZeroOS ecosystem by writing a ZeroOS bootloader for their platform, resulting in a reduced maintainence burden and unifying the entire zkVM ecosystem with consolidated development and audit resources. ZeroOS is free and open-sourced at https://github.com/LayerZero-Labs/ZeroOS.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [5] [CUBE: A Cardinality Estimator Based on Neural CDF](https://arxiv.org/abs/2512.09622)
*Xiao Yan,Tiezheng Nie,Boyang Fang,Derong Shen,Kou Yue,Yu Ge*

Main category: cs.DB

TL;DR: 本文提出一种基于累积分布函数（CDF）的基数估计新方法，无需采样或积分即可实现快速、准确且可预测的范围查询基数估计，在高维数据下仍保持近似恒定的推理速度，比当前最先进的数据驱动方法快10倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有基于概率模型的数据驱动基数估计方法在高维数据下存在推理延迟高、可扩展性差、估计结果不稳定等问题，影响查询优化器选择最优执行计划并增加数据库调优难度。

Method: 利用累积分布函数（CDF）直接计算范围查询的基数，避免采样或积分过程，并通过合并计算操作加速推理。

Result: 所提方法在保持高估计精度的同时，实现了近似恒定且极快的推理速度，尤其在高维场景下比现有最先进方法快10倍以上，展现出优异的维度可扩展性。

Conclusion: 基于CDF的基数估计方法兼顾准确性、稳定性和高效性，特别适用于真实世界中的高维数据库应用场景。

Abstract: Modern database optimizer relies on cardinality estimator, whose accuracy directly affects the optimizer's ability to choose an optimal execution plan. Recent work on data-driven methods has leveraged probabilistic models to achieve higher estimation accuracy, but these approaches cannot guarantee low inference latency at the same time and neglect scalability. As data dimensionality grows, optimization time can even exceed actual query execution time. Furthermore, inference with probabilistic models by sampling or integration procedures unpredictable estimation result and violate stability, which brings unstable performance with query execution and make database tuning hard for database users. In this paper, we propose a novel approach to cardinality estimation based on cumulative distribution function(CDF), which calculates range query cardinality without sampling or integration, ensuring accurate and predictable estimation results. With inference acceleration by merging calculations, we can achieve fast and nearly constant inference speed while maintaining high accuracy, even as dimensionality increases, which is over 10x faster than current state-of-the-art data-driven cardinality estimator. This demonstrates its excellent dimensional scalability, making it well-suited for real-world database applications.

</details>


### [6] [Exqutor: Extended Query Optimizer for Vector-augmented Analytical Queries](https://arxiv.org/abs/2512.09695)
*Hyunjoon Kim,Chaerim Lim,Hyeonjun An,Rathijit Sen,Kwanghyun Park*

Main category: cs.DB

TL;DR: 本文提出Exqutor，一种用于向量增强分析查询的可插拔基数估计框架，通过利用精确基数优化技术和自适应采样策略，显著提升查询性能。


<details>
  <summary>Details</summary>
Motivation: 在结合表格与向量搜索的RAG场景中，由于向量搜索部分的基数估计不准确，导致查询计划次优，影响整体效率。

Method: Exqutor在存在向量索引（如HNSW、IVF）时采用精确基数估计技术；在无索引时使用自适应采样方法动态调整样本大小，以平衡精度与开销，并集成到pgvector、VBASE和DuckDB中。

Result: 在多个系统中集成后，Exqutor在向量增强分析查询上实现了最高达四个数量级的性能提升。

Conclusion: Exqutor有效解决了向量-表格混合查询中的基数估计难题，显著提升了查询执行效率，具有良好的实用性和可扩展性。

Abstract: Vector similarity search is becoming increasingly important for data science pipelines, particularly in Retrieval-Augmented Generation (RAG), where it enhances large language model inference by enabling efficient retrieval of relevant external knowledge. As RAG expands with table-augmented generation to incorporate structured data, workloads integrating table and vector search are becoming more prevalent. However, efficiently executing such queries remains challenging due to inaccurate cardinality estimation for vector search components, leading to suboptimal query plans. In this paper, we propose Exqutor, an extended query optimizer for vector-augmented analytical queries. Exqutor is a pluggable cardinality estimation framework designed to address this issue, leveraging exact cardinality query optimization techniques to enhance estimation accuracy when vector indexes (e.g., HNSW, IVF) are available. In scenarios lacking these indexes, we employ a sampling-based approach with adaptive sampling size adjustment, dynamically tuning the sample size to balance estimation accuracy and sampling overhead. This allows Exqutor to efficiently approximate vector search cardinalities while minimizing computational costs. We integrate our framework into pgvector, VBASE, and DuckDB, demonstrating performance improvements of up to four orders of magnitude on vector-augmented analytical queries.

</details>


### [7] [Baseline: Operation-Based Evolution and Versioning of Data](https://arxiv.org/abs/2512.09762)
*Jonathan Edwards,Tomas Petricek*

Main category: cs.DB

TL;DR: Baseline 是一个支持多维变更（时间上的修改、空间上的协作和设计演进）的结构化数据平台，基于一种名为“操作差异（Operational Differencing）”的新技术，实现对编程语言和关系数据库中数据结构的操作级版本控制，能处理包括模式变更在内的结构转换，并简化了用户对版本控制的理解与使用。


<details>
  <summary>Details</summary>
Motivation: 传统数据版本控制系统难以有效处理涉及结构变化（如模式演化）的细粒度差异与合并。作者旨在构建一个更灵活、直观且能自然支持结构演化的数据版本控制模型。

Method: 提出“操作差异”技术，将数据变更表示为高层操作（包括重构和模式变更），在此基础上构建无仓库（repo-less）、以分支即复制为核心理念的操作型版本控制系统；同时探索将查询（如 select 和 join）操作化为一系列模式与数据操作，并在假设性未来分支上执行。

Result: 该方法实现了对包含模式变更在内的结构化数据的细粒度 diff 与 merge，简化了用户模型，并能自动重写操作化查询以适应模式变化；解决了近期一篇论文提出的八个模式演化挑战中的四个。

Conclusion: 操作差异为数据版本控制提供了一种新范式，能够统一处理数据变更与结构演化，在提升灵活性的同时降低用户认知负担，为解决模式演化难题提供了有效路径。

Abstract: Baseline is a platform for richly structured data supporting change in multiple dimensions: mutation over time, collaboration across space, and evolution through design changes. It is built upon Operational Differencing, a new technique for managing data in terms of high-level operations that include refactorings and schema changes. We use operational differencing to construct an operation-based form of version control on data structures used in programming languages and relational databases.
  This approach to data version control does fine-grained diffing and merging despite intervening structural transformations like schema changes. It offers users a simplified conceptual model of version control for ad hoc usage: There is no repo; Branching is just copying. The informaton maintained in a repo can be synthesized more precisely from the append-only histories of branches. Branches can be flexibly shared as is commonly done with document files, except with the added benefit of diffing and merging.
  We conjecture that queries can be operationalized into a sequence of schema and data operations. We develop that idea on a query language fragment containing selects and joins.
  Operationalized queries are represented as a future timeline that is speculatively executed as a branch off of the present state, returning a value from its hypothetical future. Operationalized queries get rewritten to accommodate schema change "for free" by the machinery of operational differencing.
  Altogether we develop solutions to four of the eight challenge problems of schema evolution identified in a recent paper.

</details>


### [8] [Fast Factorized Learning: Powered by In-Memory Database Systems](https://arxiv.org/abs/2512.09836)
*Bernhard Stöckl,Maximilian E. Schüle*

Main category: cs.DB

TL;DR: 该论文实现了基于因子化连接的数据库内学习方法，并在内存数据库系统（如HyPer）和磁盘数据库系统（如PostgreSQL）上进行了基准测试，结果表明因子化学习在内存系统中比非因子化方法快70%，比磁盘系统快100倍。


<details>
  <summary>Details</summary>
Motivation: 先前关于因子化连接学习的研究缺乏开源代码，无法在内存数据库系统中复现实验；本文旨在填补这一空白，探索因子化学习在现代内存数据库中的性能优势。

Method: 作者实现了支持因子化连接的线性回归学习方法，并在PostgreSQL（磁盘型）和HyPer（内存型）两种数据库系统上进行开源实现与性能基准测试。

Result: 实验结果显示，在内存数据库系统中，因子化学习相比非因子化方法性能提升70%，相比磁盘数据库系统提升达100倍。

Conclusion: 现代数据库引擎可通过在数据提取前预计算聚合信息，有效加速机器学习训练流程，尤其在内存数据库中效果显著。

Abstract: Learning models over factorized joins avoids redundant computations by identifying and pre-computing shared cofactors. Previous work has investigated the performance gain when computing cofactors on traditional disk-based database systems. Due to the absence of published code, the experiments could not be reproduced on in-memory database systems. This work describes the implementation when using cofactors for in-database factorized learning. We benchmark our open-source implementation for learning linear regression on factorized joins with PostgreSQL -- as a disk-based database system -- and HyPer -- as an in-memory engine. The evaluation shows a performance gain of factorized learning on in-memory database systems by 70\% to non-factorized learning and by a factor of 100 compared to disk-based database systems. Thus, modern database engines can contribute to the machine learning pipeline by pre-computing aggregates prior to data extraction to accelerate training.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference](https://arxiv.org/abs/2512.09304)
*Siyuan Ma,Jiajun Hu,Jeeho Ryoo,Aman Arora,Lizy Kurian John*

Main category: cs.AR

TL;DR: 本文提出RACAM，一种新型的DRAM内位串行计算架构，通过引入局部性缓冲、位串行处理单元、popcount归约单元和广播单元，有效提升数据复用并减少冗余数据传输，同时结合工作负载映射机制，在LLM推理任务中显著优于GPU和现有DRAM-PIM系统Proteus。


<details>
  <summary>Details</summary>
Motivation: 现有DRAM-PIM架构在支持可变精度计算时存在数据复用不足、冗余数据传输严重以及缺乏高效工作负载映射机制等问题，限制了其在大语言模型推理等新兴内存密集型任务中的性能发挥。

Method: 提出RACAM架构，集成专用局部性缓冲区、位串行处理单元、popcount归约单元和广播单元，并设计配套的工作负载映射机制，以充分利用DRAM的高并行性并优化任务调度。

Result: 在端到端的大语言模型推理任务中，RACAM相比GPU提速9至102倍，相比当前最先进的DRAM-PIM系统Proteus，在GPT-3任务中实现233倍的每平方毫米性能提升。

Conclusion: RACAM有效解决了现有DRAM-PIM架构在数据复用、冗余传输和映射策略方面的关键瓶颈，为高效支持大语言模型推理等新兴应用提供了高性能、高能效的硬件解决方案。

Abstract: In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.

</details>


### [10] [ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators](https://arxiv.org/abs/2512.09427)
*Guoqiang Zou,Wanyu Wang,Hao Zheng,Longxiang Yin,Yinhe Han*

Main category: cs.AR

TL;DR: ODMA 是一种面向随机访问受限内存（RACM）加速器的按需内存分配框架，通过轻量级长度预测、动态桶划分和大桶保护机制，显著提升 LLM 推理时的内存利用率与吞吐性能。


<details>
  <summary>Details</summary>
Motivation: 当前在随机访问带宽较差的加速器（如基于 LPDDR5 的 Cambricon MLU370）上部署大语言模型（LLMs）时，现有内存管理策略存在不足：静态预分配浪费内存，细粒度分页因高随机访问开销而不适用，而以 HBM 为中心的方案未针对 RACM 特性优化。

Method: ODMA 框架结合轻量级请求长度预测器、动态桶分区及大桶保护机制，并定期从运行时轨迹更新桶边界，以应对请求长度分布漂移和重尾分布问题，从而提升内存利用效率。

Result: 在 Alpaca 和 Google-NQ 数据集上，ODMA 将请求长度预测准确率从 82.68% 提升至 93.36%；在 Cambricon MLU370-X4 上运行 DeepSeek-R1-Distill-Qwen-7B 时，内存利用率从 55.05% 提高到 72.45%，RPS 和 TPS 分别提升 29% 和 27%。

Conclusion: 硬件感知的内存分配策略（如 ODMA）能有效释放 RACM 平台在 LLM 服务中的潜力，显著提升资源利用率和推理性能。

Abstract: Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [11] [Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning](https://arxiv.org/abs/2512.09006)
*Dyna Soumhane Ouchebara,Stéphane Dupont*

Main category: cs.SE

TL;DR: 本文研究了大语言模型（LLM）在源代码漏洞检测（CVD）任务中的应用，提出了一种名为“双重微调”（Double Fine-tuning）的新方法，并评估了测试时微调（Test-Time fine-tuning）和检索增强生成（RAG）等技术。实验基于Llama-3.1 8B模型及BigVul和PrimeVul数据集，结果表明微调对提升性能至关重要，而单纯提示效果不佳。


<details>
  <summary>Details</summary>
Motivation: 随着软件开发周期加速，软件漏洞数量持续上升，自动化漏洞检测变得至关重要。传统程序分析方法和新兴AI方法已被提出，但大语言模型在此任务中的潜力尚需深入探索。

Method: 作者采用开源的Llama-3.1 8B模型，在BigVul和PrimeVul数据集上进行实验，探索多种提示工程和微调策略，包括提出的新方法“双重微调”以及测试时微调，并评估RAG作为示例选择技术的效果。

Result: 微调被证明对CVD任务至关重要，其中“双重微调”表现优异；Llama模型展现出良好潜力；单纯提示无效，但RAG作为示例选择方法表现相对较好。

Conclusion: 大语言模型在源代码漏洞检测中具有应用前景，尤其通过有效的微调策略可显著提升性能。研究回答了部分问题，但仍有许多方向值得未来探索。

Abstract: The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.

</details>


### [12] [TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization](https://arxiv.org/abs/2512.09196)
*Haonan Li,Keyu Man,Partha Kanuparthy,Hanning Chen,Wei Sun,Sreen Tallam,Chenguang Zhu,Kevin Zhu,Zhiyun Qian*

Main category: cs.SE

TL;DR: TritonForge 是一个基于性能分析的自动化 Triton 内核优化框架，通过集成内核分析、运行时剖析和迭代代码转换，在多种 GPU 架构上平均实现 1.76 倍、最高达 5 倍的性能提升。


<details>
  <summary>Details</summary>
Motivation: 高性能 GPU 内核优化对现代机器学习至关重要，但即便使用 Triton 这类领域特定语言，仍需专家级硬件知识，过程繁琐且依赖经验。

Method: TritonForge 结合内核分析、运行时性能剖析与迭代式代码变换，利用数据驱动的反馈识别瓶颈并自动提出和评估代码修改；其原型使用大语言模型辅助代码推理，但整体架构模块化且模型无关。

Result: 在多种内核类型和 GPU 架构上，TritonForge 相比基线实现最高获得 5 倍加速，平均在 1.76 倍的情况下优化成功。

Conclusion: TritonForge 有效降低了 Triton 内核优化门槛，为自动化 GPU 性能优化研究提供了可扩展、模块化的基础框架。

Abstract: High-performance GPU kernel optimization remains a critical yet labor-intensive task in modern machine learning workloads. Although Triton, a domain-specific language for GPU programming, enables developers to write efficient kernels with concise code, achieving expert-level performance still requires deep understanding of GPU architectures and low-level performance trade-offs. We present TritonForge, a profiling-guided framework for automated Triton kernel optimization. TritonForge integrates kernel analysis, runtime profiling, and iterative code transformation to streamline the optimization process. By incorporating data-driven feedback from profiling results, the system identifies performance bottlenecks, proposes targeted code modifications, and evaluates their impact automatically. While our prototype leverages large language models (LLMs) to assist in code reasoning and transformation, the framework remains modular and model-agnostic. Across diverse kernel types and GPU architectures, TritonForge achieves up to 5x performance improvement over baseline implementations and on average 1.76x of the cases are successful, providing a foundation for future research in automated GPU performance optimization.

</details>


### [13] [Bug Priority Change Prediction: An Exploratory Study on Apache Software](https://arxiv.org/abs/2512.09216)
*Guangzong Cai,Zengyang Li,Peng Liang,Ran Mo,Hui Liu,Yutao Ma*

Main category: cs.SE

TL;DR: 本文提出了一种基于缺陷修复演化特征和类别不平衡处理策略的两阶段缺陷报告优先级变更预测方法，在Apache项目数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 手动评估缺陷优先级变更依赖开发人员和项目经理的主观判断，易出错且效率低，而现有研究缺乏对缺陷优先级变更预测的有效方法。

Method: 将缺陷生命周期划分为缺陷报告阶段和缺陷修复阶段，分别构建优先级变更预测模型，并引入缺陷修复演化特征与类别不平衡处理策略。

Result: 在32个Apache项目构建的数据集上，报告阶段模型F1-score达0.798；修复阶段模型F1-weighted和F1-macro分别为0.712和0.613。跨项目适用性和不同优先级下的性能也进行了分析。

Conclusion: 所提方法能有效提升缺陷优先级变更预测性能，尽管跨项目表现存在差异，但整体效果良好，且在不同优先级下保持较高预测一致性。

Abstract: Bug fixing is a critical activity in the software development process. In issue tracking systems such as JIRA, each bug report is assigned a priority level to indicate the urgency and importance level of the bug. The priority may change during the bug fixing process, indicating that the urgency and importance level of the bug will change with the bug fixing. However, manually evaluating priority changes for bugs is a tedious process that heavily relies on the subjective judgment of developers and project managers, leading to incorrect priority changes and thus hindering timely bug fixes. Given the lack of research on bug priority change prediction, we propose a novel two-phase bug report priority change prediction method based on bug fixing evolution features and class imbalance handling strategy. Specifically, we divided the bug lifecycle into two phases: bug reporting and bug fixing, and constructed bug priority change prediction models for each phase. To evaluate the performance of our method, we conducted experiments on a bug dataset constructed from 32 non-trivial Apache projects. The experimental results show that our proposed bug fixing evolution features and the adopted class imbalance handling strategy can effectively improve the performance of prediction models. The F1-score of the prediction model constructed for the bug reporting phase reached 0.798, while the F1-weighted and F1-macro of the prediction model constructed for the bug fixing phase were 0.712 and 0.613, respectively. Furthermore, we explored the cross-project applicability of our prediction models and their performance at different priority levels. The findings indicate large variations in model performance across different projects, although the overall scores remain decent. Meanwhile, the predictive performance across various priority levels remained relatively consistently high.

</details>


### [14] [SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs](https://arxiv.org/abs/2512.09543)
*Arihant Tripathy,Ch Pavan Harshit,Karthik Vaidhyanathan*

Main category: cs.SE

TL;DR: 该论文评估了四个主流智能体框架在使用小型语言模型（SLMs）进行自动化问题修复时的性能、能效与资源消耗，发现当前为大语言模型设计的框架在搭配SLMs时效率低下，能量主要被浪费在无效推理循环中。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）驱动的软件工程智能体依赖专有大模型，难以本地部署，促使研究者关注小型语言模型（SLMs），但SLMs在复杂智能体框架中的实际效能和效率尚不明确。

Method: 在固定硬件上，使用两个SLMs（Gemma-3 4B、Qwen-3 1.7B）对四个智能体框架（SWE-Agent、OpenHands、Mini SWE Agent、AutoCodeRover）在SWE-bench Verified Mini基准上进行受控评估，每种配置运行150次，测量能耗、耗时、token使用量和内存占用。

Result: 框架架构是能耗的主要决定因素：最耗能的AutoCodeRover（Gemma）平均比最节能的OpenHands（Gemma）多消耗9.4倍能量；但任务解决率接近零，表明SLMs的有限推理能力是成功瓶颈，而框架设计是效率瓶颈。

Conclusion: 当前为强大LLM设计的智能体框架在搭配SLMs时效率低下且能耗浪费严重；实现低能耗可行方案需从被动编排转向主动管理SLM弱点的架构。

Abstract: Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.
  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.
  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.
  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.
  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.

</details>


### [15] [Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values](https://arxiv.org/abs/2512.09562)
*Radoslaw Klimek,Jakub Blazowski*

Main category: cs.SE

TL;DR: 该论文提出将工作流挖掘得到的流程模型转化为逻辑规范，利用自动定理证明器分析其属性，并结合Shapley值量化各元素对整体行为的贡献，从而实现可解释的工作流分析。


<details>
  <summary>Details</summary>
Motivation: 现有工作流挖掘方法缺乏对所发现模型为何满足或违反特定逻辑属性的解释能力，也难以评估单个元素对整体行为的具体贡献。

Method: 将挖掘出的工作流模型翻译为逻辑规范，使用自动定理证明器分析其可满足性、活性和安全性等属性，并引入合作博弈论中的Shapley值来量化各工作流元素对分析结果的贡献。

Result: 在基准数据集上的实验表明，该方法能有效识别关键节点、揭示冗余结构并暴露有害组件。

Conclusion: 该研究为可解释的工作流分析开辟了新方向，在合规检查、流程优化、冗余消除及新一代流程挖掘工具设计等方面具有实际应用价值。

Abstract: Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.

</details>


### [16] [LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection](https://arxiv.org/abs/2512.09627)
*Jingwei Ye,Zhi Wang,Chenbin Su,Jieshuai Yang,Jiayi Ding,Chunbo Liu,Ge Chu*

Main category: cs.SE

TL;DR: LogICL 是一种用于跨域日志异常检测的轻量级框架，通过将大语言模型（LLM）的推理能力蒸馏到编码器中，在训练阶段利用多目标损失优化表示，并在推理阶段结合语义相似性和增量得分检索示范样本，从而在少量或零样本设置下实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于 Transformer 的日志异常检测方法依赖大量标注数据和计算资源，在目标域日志稀缺时面临冷启动问题；而当前跨域方法多依赖表面词汇相似性，难以应对结构差异下的潜在语义等价关系，导致泛化能力不足。

Method: 提出 LogICL 框架：训练阶段构建 delta 矩阵衡量示范样本相对于零样本推理的效用，采用多目标损失（包括 ICL 引导项、最大均值差异域对齐项和监督对比损失）优化轻量编码器；推理阶段利用语义相似性和 delta 分数检索示范样本，驱动冻结 LLM 进行带思维链的上下文学习。

Result: 在少样本和零样本跨域基准测试中，LogICL 在异构系统上达到最先进性能；可视化与案例分析表明其能有效弥合语义鸿沟，超越表面词汇匹配，捕捉潜在语义等价关系。

Conclusion: LogICL 通过将 LLM 推理能力蒸馏为轻量编码器，并结合多目标优化与推理感知的示范检索机制，显著提升了跨域日志异常检测的准确性、可解释性与部署效率。

Abstract: Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.

</details>


### [17] [Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis](https://arxiv.org/abs/2512.09679)
*Naizhu Jin,Zhong Li,Guang Yang,Tian Zhang,Qingkai Zeng*

Main category: cs.SE

TL;DR: 本文通过信息论方法系统研究了思维链（CoT）在神经代码生成中的有效性，发现结构化CoT在多种编程语言和模型规模下显著优于直接生成，并揭示了CoT效果依赖于语言类型系统、模型容量及推理质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在代码生成任务中表现优异，但思维链（CoT）提示为何有效及其作用机制尚不明确，亟需系统性实证与理论分析。

Method: 作者在六种Python基准、涵盖12种语言的多语言基准以及7B至480B参数的六种模型上，评估五种CoT范式（零样本、零样本CoT、自规划、结构化CoT、推理型CoT），并以条件互信息 $I(Y;C|X)$ 作为分析框架。

Result: 外部引导的CoT始终优于直接生成；结构化CoT平均提升Pass@1达5–12%，且比反思式推理使用更少token；CoT效果受语言类型系统和模型容量影响；高质量结构化CoT显著优于轻量级替代方案，而朴素零样本CoT甚至可能损害性能。

Conclusion: 应根据模型容量、语言特性和任务复杂度选择合适的CoT策略，结构化且高质量的推理路径对提升代码生成性能至关重要。

Abstract: Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.

</details>


### [18] [Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition](https://arxiv.org/abs/2512.09775)
*Vladimir Balditsyn,Philippe Lalanda,German Vega,Stéphanie Chollet*

Main category: cs.SE

TL;DR: 本文提出在基于机器学习的系统中量化不确定性，通过在运行时评估模型预测的相关性，并在人类活动识别（HAR）领域验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型依赖高维数据训练而非手动编码，其运行边界不确定且无法保证完全无错，因此传统软件开发中的严格测试方法难以适用，亟需新的手段来评估模型预测的可靠性。

Method: 结合并调整一组选定的技术，在运行时评估机器学习模型预测的相关性，以量化其不确定性。

Result: 在人类活动识别（HAR）这一高度异构且动态变化的领域中，所提方法有效展示了其在量化模型不确定性方面的相关性和实用性。

Conclusion: 该方法能够有效辅助领域专家理解和应对机器学习系统中的不确定性，提升系统在实际应用中的可靠性和可解释性。

Abstract: The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [19] [Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens](https://arxiv.org/abs/2512.09277)
*Yanpeng Yu,Haiyue Ma,Krish Agarwal,Nicolai Oswald,Qijing Huang,Hugo Linsenmaier,Chunhui Mei,Ritchie Zhao,Ritika Borkar,Bita Darvish Rouhani,David Nellans,Ronny Krashinsky,Anurag Khandelwal*

Main category: cs.DC

TL;DR: 本文提出了一种名为METRO的新路由算法，通过平衡每个GPU上激活的专家数量（而非处理的token数量），在内存受限的MoE推理场景中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有专家并行（EP）方法通过平衡各GPU处理的token数量来缓解负载不均，但在内存受限的MoE服务（尤其是解码阶段）中，这种策略反而因增加激活专家数而加剧内存压力，降低性能。

Method: 提出Minimum Expert Token ROuting（METRO）算法，在路由时平衡每GPU激活的专家数量；设计低开销的allGather机制以获取全局top-k信息，并联合优化算法效率与GPU并行能力。

Result: 在真实系统（8×A100）和模拟器（8–16×B200）上评估表明，相比EPLB，METRO在Qwen3和DeepSeek-V3服务中降低解码延迟11–22%，提升总吞吐3–21%；在固定SLO下，解码吞吐最高提升4.11倍。

Conclusion: 在内存受限的MoE服务场景中，平衡激活专家数比平衡token数更有效；METRO通过新路由策略和通信优化，显著提升了推理性能与吞吐。

Abstract: Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.
  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.

</details>


### [20] [A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge](https://arxiv.org/abs/2512.09309)
*Zihao Ding,Mufeng Zhu,Zhongze Tang,Sheng Wei,Yao Liu*

Main category: cs.DC

TL;DR: 提出了一种面向视觉Transformer的分布式分层卸载框架，通过在可信边缘设备上分割图像并分发至多个云服务器，在保障隐私的同时维持接近原始性能的视觉任务效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉智能工具计算需求高，难以在资源受限的移动和可穿戴设备上运行；将数据上传至云端虽可缓解计算压力，但会带来传输与服务端的隐私泄露风险。

Method: 利用本地可信边缘设备（如手机或Jetson）作为协调器，将用户视觉数据分割后分发到多个独立云服务器，确保任一服务器无法获得完整图像；最终聚合计算仅在边缘设备完成，并以Segment Anything Model（SAM）为案例验证该方法。

Result: 实验表明，该框架在保持接近基线分割性能的同时，显著降低了内容重建与用户数据暴露的风险。

Conclusion: 所提框架为边缘-云协同环境下的视觉任务提供了一种可扩展且注重隐私保护的解决方案。

Abstract: Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.

</details>


### [21] [Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN](https://arxiv.org/abs/2512.09331)
*Nam Anh Dang,Ben Landrum,Ken Birman*

Main category: cs.DC

TL;DR: BatANN 是一种新型分布式磁盘向量检索系统，通过在服务器间传递完整查询状态以维持局部性，在十亿级数据集上实现了接近线性的吞吐扩展和低于6毫秒的平均延迟。


<details>
  <summary>Details</summary>
Motivation: 随着向量数据集规模扩展至数十亿级别，单机磁盘向量检索已难以满足需求，亟需一种可横向扩展、高效且低延迟的分布式解决方案。

Method: 提出 BatANN 系统，其核心创新在于跨机器访问邻域时将完整查询状态发送至目标机器继续执行，从而保持查询局部性，并在标准 TCP 上构建单一全局图结构实现分布式磁盘 ANN 检索。

Result: 在10台服务器、0.95召回率条件下，BatANN 在1亿和10亿规模数据集上分别达到基线方法6.21–6.49倍和2.5–5.10倍的吞吐量，同时平均延迟低于6毫秒。

Conclusion: BatANN 首次实现了基于单一全局图的开源分布式磁盘向量检索系统，在保证对数级搜索效率的同时显著提升吞吐能力，为超大规模向量检索提供了实用方案。

Abstract: Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

</details>


### [22] [WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving](https://arxiv.org/abs/2512.09472)
*Chiheng Lou,Sheng Qi,Rui Kang,Yong Zhang,Chen Sun,Pengcheng Wang,Bingyang Liu,Xuanzhe Liu,Xin Jin*

Main category: cs.DC

TL;DR: WarmServe 是一种新型多大语言模型（multi-LLM）服务系统，通过引入通用 GPU 工作节点和基于未来负载预测的主动预热机制，在提升 GPU 资源利用率的同时显著降低首 token 延迟（TTFT），相比现有系统最高可将 TTFT 降低 50.8 倍，并支持处理 2.5 倍更多的请求。


<details>
  <summary>Details</summary>
Motivation: 现有 multi-LLM 服务系统在优化 GPU 利用率时牺牲了推理性能，尤其是首 token 延迟（TTFT），其根本原因在于未考虑未来工作负载特征。而现实世界中的 LLM 服务负载具有高度周期性和长期可预测性，这为改进提供了机会。

Method: 提出“通用 GPU 工作节点”概念，构建 WarmServe 系统，包含三项关键技术：(1) 采用感知驱逐的模型放置策略以减少集群级预热干扰；(2) 基于未来负载预测进行主动预热；(3) 设计零开销的 GPU 内存切换机制。

Result: 在真实数据集上的评估表明，WarmServe 相比最先进的自动扩缩容系统，TTFT 最高提升 50.8 倍；相比 GPU 共享系统，可处理多达 2.5 倍的请求量。

Conclusion: 通过利用 LLM 服务负载的可预测性并引入通用 GPU 工作节点与高效内存管理机制，WarmServe 在保证高吞吐的同时显著改善了推理延迟，为多模型共享 GPU 集群提供了一种高效可行的解决方案。

Abstract: Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.

</details>


### [23] [PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing](https://arxiv.org/abs/2512.09568)
*Zhi Zhao,Hang Xiao,Wei Rang*

Main category: cs.DC

TL;DR: 本文提出了一种基于帕累托的混合鲸鱼-海鸥优化算法（PHWSOA），用于云环境中多目标任务调度，显著提升了完工时间、虚拟机负载均衡和经济成本三项指标。


<details>
  <summary>Details</summary>
Motivation: 现有云任务调度方法多聚焦单一或有限目标（如执行时间或资源利用率），缺乏对多目标协同优化的综合考虑。

Method: 结合鲸鱼优化算法（WOA）与海鸥优化算法（SOA）的优势，引入Halton序列初始化、帕累托引导变异机制、并行处理及动态虚拟机负载重分配策略，构建多目标优化框架。

Result: 在CloudSim上基于NASA-iPSC和HPC2N真实负载的实验表明，PHWSOA相较WOA、GA等基线方法，最多可减少72.1%的完工时间、提升36.8%的负载均衡性，并节省23.5%的成本。

Conclusion: PHWSOA在多目标任务调度中展现出优越性能，具备在实际云环境中高效管理资源的应用潜力。

Abstract: Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.

</details>


### [24] [SynthPix: A lightspeed PIV images generator](https://arxiv.org/abs/2512.09664)
*Antonio Terpin,Alan Bonomi,Francesco Banelli,Raffaello D'Andrea*

Main category: cs.DC

TL;DR: SynthPix 是一个基于 JAX 实现的高性能并行合成图像生成器，专为粒子图像测速（PIV）设计，显著提升了图像对生成速度，支持现有工具的配置参数，旨在加速数据密集型强化学习方法的训练和实时 PIV 反馈中的流场估计研究。


<details>
  <summary>Details</summary>
Motivation: 为支持数据密集型强化学习方法在流场估计中的训练，并缩短在实时 PIV 反馈控制研究中开发快速流场估计方法的迭代时间，需要一种高效、可并行化的合成 PIV 图像生成工具。

Method: 使用 JAX 实现 SynthPix，利用其在加速器上的高性能和并行能力，生成符合现有 PIV 工具配置参数的合成图像对。

Result: SynthPix 在每秒生成图像对的数量上比现有工具高出几个数量级，显著提升生成效率。

Conclusion: SynthPix 为流体力学社区提供了一个高效、灵活的合成 PIV 图像生成工具，有助于推动基于机器学习的流场估计和实时控制研究。

Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.

</details>


### [25] [Straggler Tolerant and Resilient DL Training on Homogeneous GPUs](https://arxiv.org/abs/2512.09685)
*Zeyu Zhang,Haiying Shen*

Main category: cs.DC

TL;DR: 本文研究了同构GPU深度学习训练中拖慢节点（straggler）的普遍性、成因及其影响，发现CPU与带宽使用不均衡是主要原因，并指出现有从同步SGD切换到异步SGD的缓解方法可能适得其反。为此，作者提出了STAR系统，通过新型同步模式、启发式与机器学习方法选择最优同步策略，并主动避免资源过载，显著降低训练时间（TTA）且保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 当前对同构GPU深度学习训练中拖慢节点的成因、影响及现有缓解方法的有效性缺乏深入理解，亟需系统性研究以填补这一空白。

Method: 提出STAR系统，包含新的参数更新同步模式、基于启发式和机器学习的同步模式选择机制、资源重分配策略，以及在参数服务器分配和梯度传输中主动避免CPU与带宽过载的机制。

Result: 在AWS上的实验表明，STAR在参数服务器架构和AllReduce架构下分别比现有最先进系统降低48-84%和51-70%的TTA，同时保持与同步SGD相同的收敛精度。

Conclusion: 拖慢节点在同构GPU训练中仍普遍存在，主要由CPU和带宽不平衡引起；简单切换至异步SGD未必有效，而STAR通过智能同步与资源管理可显著提升训练效率。

Abstract: Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.

</details>


### [26] [Recoverable Lock-Free Locks](https://arxiv.org/abs/2512.09710)
*Hagit Attiya,Panagiota Fatourou,Eleftherios Kosmas,Yuanhao Wei*

Main category: cs.DC

TL;DR: 本文提出首个同时实现无锁性和可恢复性的转换方法，将基于锁的实现转化为可恢复且无锁的版本。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时保证无锁性和系统崩溃后的可恢复性，作者旨在填补这一空白。

Method: 从基于锁的实现出发，对加锁和释放锁操作进行替换，构造出可恢复且无锁的替代方案，并支持嵌套锁以增强通用性。

Result: 该转换在不损害原有锁实现正确性的前提下，成功实现了可恢复性和无锁性。

Conclusion: 所提出的转换方法有效结合了无锁与可恢复特性，为并发数据结构的容错设计提供了新思路。

Abstract: This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.

</details>
