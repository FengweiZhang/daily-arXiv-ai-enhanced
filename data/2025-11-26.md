<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 8]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [CAMformer: Associative Memory is All You Need](https://arxiv.org/abs/2511.19740)
*Tergel Molom-Ochir,Benjamin F. Morris,Mark Horton,Chiyue Wei,Cong Guo,Brady Taylor,Peter Liu,Shan X. Wang,Deliang Fan,Hai Helen Li,Yiran Chen*

Main category: cs.AR

TL;DR: CAMformer 是一种新型加速器，通过将注意力机制重构为联想记忆操作，并利用电压域二值注意力内容可寻址存储器（BA-CAM）实现常数时间的相似性搜索，从而显著提升能效、吞吐量并减小面积，同时保持几乎无损的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统 Transformer 中注意力机制的计算复杂度为二次方，导致在扩展性方面面临挑战，亟需更高效的硬件加速方案。

Method: 提出 CAMformer 架构，将注意力计算转化为联想记忆操作，采用 BA-CAM 在模拟电压域中通过电荷共享实现常数时间相似性搜索，并结合两阶段 top-k 过滤、流水线执行和高精度上下文建模。

Result: 在 BERT 和 Vision Transformer 任务上，CAMformer 相比现有先进加速器实现了超过 10 倍的能效提升、最高 4 倍的吞吐量提升以及 6–8 倍的面积缩减，同时保持接近无损的模型准确率。

Conclusion: CAMformer 通过硬件-算法协同设计，有效解决了注意力机制的可扩展性瓶颈，在性能、能效与面积方面均取得显著优势，为大规模 Transformer 模型部署提供了高效可行的加速方案。

Abstract: Transformers face scalability challenges due to the quadratic cost of attention, which involves dense similarity computations between queries and keys. We propose CAMformer, a novel accelerator that reinterprets attention as an associative memory operation and computes attention scores using a voltage-domain Binary Attention Content Addressable Memory (BA-CAM). This enables constant-time similarity search through analog charge sharing, replacing digital arithmetic with physical similarity sensing. CAMformer integrates hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization to achieve both algorithmic accuracy and architectural efficiency. Evaluated on BERT and Vision Transformer workloads, CAMformer achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators--while maintaining near-lossless accuracy.

</details>


### [2] [Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher](https://arxiv.org/abs/2511.19973)
*Hoa Nguyen,Pongstorn Maidee,Jason Lowe-Power,Alireza Kaviani*

Main category: cs.AR

TL;DR: 本文提出了Pickle Prefetcher，一种可编程且可扩展的末级缓存（LLC）预取器，通过软件定义预取策略来高效处理不规则内存访问模式，在图应用等场景中显著优于传统预取技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于预测的预取器难以有效处理现代应用中普遍存在的不规则内存访问模式，而大型末级缓存对这类工作负载反而可能增加延迟。

Method: Pickle Prefetcher放弃复杂的硬件预测逻辑，提供一个简单的软件编程接口，允许软件自定义预取策略，无需扩展指令集架构（ISA），从而将硬件资源集中于及时调度和发出预取请求。

Result: 在gem5全系统模拟中，Pickle Prefetcher在GAPBS的BFS实现上相比基线系统最高提速1.74倍；与私有缓存预取器结合使用时，相比仅使用私有缓存预取器的系统最高提速1.40倍。

Conclusion: Pickle Prefetcher通过软硬件协同设计，以软件可编程性替代复杂硬件预测，在不显著增加硬件开销的前提下，有效提升了不规则内存访问模式下的性能。

Abstract: Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.

</details>


### [3] [R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation](https://arxiv.org/abs/2511.20090)
*Zizhang Luo,Fan Cui,Kexing Zhou,Runlin Guo,Mile Xia,Hongyuan Hou,Yun Lian*

Main category: cs.AR

TL;DR: 本文提出R3A，一种基于大语言模型（LLM）的自动RTL程序修复框架，通过随机思维树方法和多智能体故障定位机制提升修复可靠性，在RTL-repair数据集上修复了90.6%的bug，显著优于传统方法和其他LLM方法。


<details>
  <summary>Details</summary>
Motivation: 传统自动程序修复方法依赖固定模板，适用范围有限；而现有大语言模型在RTL修复中因输入上下文长、结果随机性高而可靠性不足。

Method: R3A引入随机思维树（stochastic Tree-Of-Thoughts）方法控制补丁生成智能体，并结合启发式函数平衡探索与利用；同时采用多智能体故障定位方法确定修复起点。

Result: 在RTL-repair数据集上，R3A在时限内修复了90.6%的bug，比传统方法及其他LLM方法多覆盖45%的bug，平均pass@5率达86.7%。

Conclusion: R3A显著提升了基于LLM的RTL自动修复的可靠性与修复能力，验证了其在硬件设计与验证中的有效性。

Abstract: Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution](https://arxiv.org/abs/2511.19445)
*Luca Accorsi,Demetrio Laganà,Federico Michelotto,Roberto Musmanno,Daniele Vigo*

Main category: cs.DC

TL;DR: 本文提出了一种并行共享内存框架 FILO2$^x$，用于高效求解带容量约束的车辆路径问题（CVRP），在几乎无需同步且不显式分解问题的前提下，通过多线程并发优化同一解的不同局部区域，显著缩短求解时间，同时保持与原单线程算法 FILO2 相当的解质量。


<details>
  <summary>Details</summary>
Motivation: 现有单线程算法 FILO2 虽高效，但未能充分利用现代多核计算资源；作者旨在设计一种低同步开销、无需显式问题分解的并行策略，以加速 CVRP 求解过程。

Method: 基于 FILO2 算法的局部优化特性，设计其并行版本 FILO2$^x$：多个求解器异步、并发地对同一底层解的不同局部区域进行优化，形成基于迭代的并行搜索轨迹，利用共享内存实现协作优化。

Result: 在数百至数十万客户规模的 CVRP 实例上，FILO2$^x$ 显著优于原始 FILO2 的求解速度，同时保持相近的最终解质量。

Conclusion: FILO2$^x$ 成功实现了高效的并行 CVRP 优化，在几乎不牺牲解质量的前提下大幅缩短计算时间，验证了基于局部性与异步协作的并行策略的有效性。

Abstract: We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.

</details>


### [5] [AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains](https://arxiv.org/abs/2511.19450)
*M. Zeeshan Haider,Tayyaba Noreen,M. D. Assuncao,Kaiwen Zhang*

Main category: cs.DC

TL;DR: 本文提出了一种名为预测性分片分配协议（PSAP）的动态智能分片方法，通过结合时序工作负载预测模型与安全约束强化学习控制器，实现前瞻性账户与交易分配，从而显著提升区块链系统的吞吐量、降低延迟并减少跨分片开销。


<details>
  <summary>Details</summary>
Motivation: 传统区块链分片技术常采用静态或启发式分片策略，易导致负载不均、拥塞及过多跨分片通信，削弱了分片带来的可扩展性优势。因此，亟需一种能动态、智能地分配负载并保障安全性的分片机制。

Method: PSAP协议整合了时序工作负载预测（TWF）模型与安全约束的近端策略优化（Safe-PPO）强化学习控制器，支持多区块前瞻预测与自适应分片重配置；并通过同步量化运行时和安全门限机制（限制质押集中度、迁移Gas与利用率）确保验证者间确定性推理与拜占庭安全性。

Result: 在Ethereum、NEAR和Hyperledger Fabric等异构数据集上的实验表明，相比现有动态分片方案，PSAP可实现最高2倍吞吐量提升、35%延迟降低以及20%跨分片开销减少。

Conclusion: 预测性、确定性且安全感知的分片分配机制是构建下一代高可扩展区块链系统的重要方向。

Abstract: Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\% lower latency, and 20\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.

</details>


### [6] [AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles](https://arxiv.org/abs/2511.19453)
*Yuxin Wang,Yuankai He,Weisong Shi*

Main category: cs.DC

TL;DR: 本文提出AVS，一种面向自动驾驶车辆的新型车载存储系统，通过计算与存储协同设计，实现高效的数据写入、快速选择性检索和显著的存储空间压缩。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶车辆产生的海量异构数据（如每天14TB）缺乏高效的通用存储与查询系统，现有车载数据记录器和存储栈无法满足新兴第三方应用对高效存取的需求。

Method: AVS采用分层布局协同计算：包括模态感知的数据缩减与压缩、基于冷热数据分层的每日归档机制，以及轻量级元数据索引层；并通过系统级基准测试和真实L4自动驾驶轨迹在嵌入式硬件上验证设计。

Result: 原型系统在有限资源下实现了可预测的实时数据写入、快速的选择性数据检索和显著的存储占用减少。

Conclusion: AVS展示了将存储作为自动驾驶系统一级组件的可行性，并为未来更可扩展、更长期部署的车载存储系统提供了观察与方向。

Abstract: Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.

</details>


### [7] [Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED](https://arxiv.org/abs/2511.19456)
*Anton Reinhard,Simeon Ehrig,René Widera,Michael Bussmann,Uwe Hernandez Acosta*

Main category: cs.DC

TL;DR: 本文提出了一种基于Julia语言的软件框架，通过结合有向无环图（DAG）调度与领域特定信息，实现对复杂科学计算任务的自动静态调度与编译优化。


<details>
  <summary>Details</summary>
Motivation: 科学计算中的复杂问题通常由多个具有不同计算需求的子任务组成，为提升效率，需将各子任务分配至最适合的硬件上执行，并兼顾并行性、任务依赖和数据传输等因素。

Method: 利用有向无环图表示计算任务，结合领域知识对DAG调度进行扩展，在Julia中构建一个能自动动态生成静态调度与编译代码的软件框架。

Result: 实现了针对量子电动力学中多外粒子散射过程矩阵元计算的示例应用，验证了所提方法的有效性和优化潜力。

Conclusion: 通过融合领域特定信息与DAG调度理论，该框架能够实现更高效的硬件利用和性能优化，适用于复杂的科学计算场景。

Abstract: Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.

</details>


### [8] [Systemic approach for modeling a generic smart grid](https://arxiv.org/abs/2511.19460)
*Sofiane Ben Amor,Guillaume Guerard,Loup-Noé Levy*

Main category: cs.DC

TL;DR: 本文提出了一种智能电网的骨干模型，通过分布式优化实现多系统协同仿真，以支持在实际部署前验证不同场景假设。


<details>
  <summary>Details</summary>
Motivation: 传统计算方法难以应对智能电网日益复杂的跨学科建模与仿真挑战，亟需一种系统性集成建模方法。

Method: 构建智能电网骨干模型，采用分布式子系统优化方法，对电力系统、能源市场、需求侧管理等进行集成仿真。

Result: 实现了生产与消费调度，在保持灵活性和可扩展性的同时，能够测试多种电网运行场景。

Conclusion: 所提出的模型为智能电网的系统级仿真和方案验证提供了一个有效工具，有助于在大规模部署前评估不同策略的可行性。

Abstract: Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.

</details>


### [9] [Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna](https://arxiv.org/abs/2511.19463)
*Aldo Canfora,Eleonora Bergamaschi,Riccardo Mioli,Federico Battini,Mirko Degli Esposti,Giorgio Pedrazzi,Chiara Dellacasa*

Main category: cs.DC

TL;DR: 本文提出了一种结合EnergyPlus模拟、高性能计算（HPC）和开放地理空间数据的城市建筑能耗建模（UBEM）流程，用于估算意大利博洛尼亚约25,000栋建筑的能耗，整个模拟在Cineca超级计算机Leonardo上不到30分钟完成。


<details>
  <summary>Details</summary>
Motivation: 城市尺度的建筑能耗建模对理解和预测能源消耗至关重要，但传统方法在数据获取和计算效率方面存在挑战。因此，需要一种高效且可扩展的UBEM方法。

Method: 该研究整合了EnergyPlus模拟、高性能计算（HPC）和开放地理空间数据集。建筑几何信息来自博洛尼亚开放数据门户并辅以LiDAR测量；非几何属性（如建筑材料、保温性能和窗户性能）则依据地区建筑法规和欧洲TABULA数据库获取。计算在Cineca的Leonardo超级计算机上执行。

Result: 成功在不到30分钟内完成了对约25,000栋建筑的能耗模拟，验证了所提UBEM流程的高效性和可扩展性。

Conclusion: 结合开放数据、物理模拟与高性能计算的城市建筑能耗建模方法能够高效、准确地支持城市级能源规划与政策制定。

Abstract: Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.

</details>


### [10] [Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments](https://arxiv.org/abs/2511.19464)
*Marcio Pohlmann,Alex Severo,Gefté Almeida,Diego Kreutz,Tiago Heinrich,Lourenço Pereira*

Main category: cs.DC

TL;DR: 本文研究了本地部署的小型语言模型（SLM）是否可用于安全运营中心（SOC）和计算机安全事件响应团队（CSIRT）的自动化事件分类任务，评估了21个不同规模的模型，发现模型参数量和GPU性能是关键因素，而温度超参数影响甚微。


<details>
  <summary>Details</summary>
Motivation: 由于使用云端大语言模型（LLM）进行安全事件分类存在成本、延迟和保密性风险，因此探索在本地运行的小型语言模型（SLM）是否能有效替代。

Method: 评估了21个参数规模从1B到20B不等的本地SLM，在两种不同架构下调整温度超参数，测量其执行时间和分类精度。

Result: 实验结果表明，温度对模型性能影响很小，而模型参数数量和GPU容量是决定性能的关键因素。

Conclusion: 本地部署的SLM在适当参数规模和硬件支持下，有望满足SOC和CSIRT对自动化事件分类的需求，同时规避云LLM带来的风险。

Abstract: SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.

</details>


### [11] [Towards a future space-based, highly scalable AI infrastructure system design](https://arxiv.org/abs/2511.19468)
*Blaise Agüera y Arcas,Travis Beals,Maria Biggs,Jessica V. Bloom,Thomas Fischbacher,Konstantin Gromov,Urs Köster,Rishiraj Pravahan,James Manyika*

Main category: cs.DC

TL;DR: 本文提出了一种基于卫星群的太空机器学习计算系统，利用太阳能供电、自由空间光通信和抗辐射TPU芯片，以应对未来AI对算力与能源不断增长的需求。


<details>
  <summary>Details</summary>
Motivation: 由于AI作为基础性通用技术将带来对算力和能源需求的持续增长，而太阳是太阳系中最丰富的能源，因此探索如何高效利用太阳能支持未来的AI基础设施具有重要意义。

Method: 设计由配备太阳能阵列、星间自由空间光链路和Google TPU加速芯片的卫星组成的近距编队集群；采用高精度机器学习模型控制大规模星座，并对Trillium TPU进行辐射测试以验证其在轨可靠性。

Result: 提出了一个半径1公里、包含81颗卫星的集群构型；TPU可承受相当于5年任务寿命的总电离剂量且无永久故障，并量化了位翻转错误；预计到2030年代中期，LEO发射成本可能降至约200美元/公斤。

Conclusion: 构建基于太阳能的太空AI计算系统在技术上具备可行性，有望成为满足未来AI能源与算力需求的一种高效方案。

Abstract: If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\lesssim$\$200/kg by the mid-2030s.

</details>


### [12] [Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments](https://arxiv.org/abs/2511.19479)
*Sangam Ghimire,Paribartan Timalsina,Nirjal Bhurtel,Bishal Neupane,Bigyan Byanju Shrestha,Subarna Bhattarai,Prajwal Gaire,Jessica Thapa,Sudan Jha*

Main category: cs.DC

TL;DR: 本文提出了一种适用于混合高性能计算（HPC）与云环境的联邦学习框架，有效应对系统异构性、通信开销和资源调度等挑战，在保障模型精度与数据隐私的同时，展现出良好的可扩展性、容错性和收敛性。


<details>
  <summary>Details</summary>
Motivation: 随着对可扩展且注重隐私的AI系统需求增长，联邦学习成为在不移动原始数据的前提下实现去中心化训练的有效方法；然而，将联邦学习部署于混合HPC与云环境中面临硬件异构、通信限制和数据分布不均等新挑战。

Method: 设计并实现了一个专为混合HPC与云环境优化的联邦学习框架，重点解决系统异构性、通信开销和资源调度问题，同时确保模型准确性和数据隐私。

Result: 在混合测试平台上进行的实验表明，该框架在非独立同分布（non-IID）数据和多样化硬件条件下仍具有优异的可扩展性、容错能力和收敛性能。

Conclusion: 联邦学习在现代分布式计算环境中具备实际应用潜力，能够有效构建可扩展的AI系统。

Abstract: As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heteroge- neous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system het- erogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.

</details>


### [13] [Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures](https://arxiv.org/abs/2511.19832)
*Aurelio Vivas,Harold Castro*

Main category: cs.DC

TL;DR: 本文提出了nFlows，一个面向NUMA架构的高性能计算（HPC）工作流执行运行时系统，用于建模、裸机执行、仿真和验证数据密集型工作流的调度算法。


<details>
  <summary>Details</summary>
Motivation: 当前大多数工作流调度策略针对网格或云环境设计，未充分考虑HPC系统中普遍存在的非统一内存访问（NUMA）架构所带来的数据局部性挑战。

Method: 设计并实现nFlows系统，支持构建仿真模型并在物理系统上直接执行，以研究NUMA对调度的影响、设计NUMA感知算法、分析数据移动行为等。

Result: nFlows能够有效支持NUMA感知调度算法的研究与验证，识别性能瓶颈，并探索内存中工作流执行的优化空间。

Conclusion: nFlows为在NUMA架构HPC系统上高效执行数据密集型科学工作流提供了关键工具和方法，填补了现有调度策略在NUMA感知方面的空白。

Abstract: Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.

</details>


### [14] [PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases](https://arxiv.org/abs/2511.19949)
*Qingda Hu,Xinjun Yang,Feifei Li,Junru Li,Ya Lin,Yuqi Zhou,Yicong Zhu,Junwei Zhang,Rongbiao Xie,Ling Zhou,Bin Wu,Wenchao Zhou*

Main category: cs.DC

TL;DR: PolarStore 是一种面向云原生关系型数据库的压缩共享存储系统，通过软硬件协同的双层压缩机制，在显著降低存储成本（约60%）的同时保持与未压缩集群相当的性能。


<details>
  <summary>Details</summary>
Motivation: 云原生RDBMS虽通过计算与存储分离实现了计算弹性，但存储成本仍是用户关注重点。现有压缩方法在性能开销与灵活性之间存在明显权衡：软件压缩带来较大性能损耗，硬件压缩则缺乏对多样化数据库负载的适应性。

Method: 提出 PolarStore 系统，采用双层压缩机制：结合 PolarCSD 硬件中的存储内压缩与软件中的轻量级压缩；引入面向数据库的 I/O 路径优化；基于大规模部署经验，改进 PolarCSD 硬件以提升主机稳定性，并设计压缩感知调度策略以提高集群空间效率。

Result: PolarStore 已在 PolarDB 中部署于数千台存储服务器，管理超 100 PB 数据，实现 3.55 倍压缩率，降低约 60% 存储成本，同时性能与未压缩集群相当。

Conclusion: 通过软硬件协同设计和数据库感知优化，PolarStore 成功在云原生 RDBMS 中实现了高性价比的存储压缩，在大幅节省成本的同时保障了关键路径的高性能。

Abstract: In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.

</details>


### [15] [QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation](https://arxiv.org/abs/2511.20100)
*Xinguo Zhu,Shaohui Peng,Jiaming Guo,Yunji Chen,Qi Guo,Yuanbo Wen,Hang Qin,Ruizhi Chen,Qirui Zhou,Ke Gao,Yanjun Wu,Chen Zhao,Ling Li*

Main category: cs.DC

TL;DR: 本文提出了一种名为“宏观思考微观编码”（MTMC）的分层框架，用于高效、正确地自动生成高性能GPU内核。该方法将优化策略与具体实现解耦，通过强化学习引导轻量级LLM探索高层优化策略，并利用通用LLM逐步实现这些策略，从而在准确性和运行速度上显著优于现有大模型和专家手工优化方法。


<details>
  <summary>Details</summary>
Motivation: 开发高性能GPU内核对AI和科学计算至关重要，但传统方法依赖专家手工编写，可移植性差。现有基于大语言模型（LLM）的方法在生成完整底层代码时面临正确性与效率难以兼顾的问题，根源在于需同时探索庞大的优化策略空间和实现细节空间。

Method: 提出MTMC框架：1）Macro Thinking阶段使用强化学习指导轻量级LLM学习高层语义优化策略以最大化硬件利用率；2）Micro Coding阶段利用通用LLM根据Macro Thinking提出的逐步优化建议增量式生成代码，避免一次性生成整个内核带来的错误。

Result: 在KernelBench上，MTMC在Level 1-2达到近100%准确率，Level 3达70%，比当前最优通用和领域微调LLM高出50%以上，并实现最高7.3倍于LLM、2.2倍于PyTorch Eager专家内核的速度提升；在更具挑战性的TritonBench上，准确率达59.64%，速度提升达34倍。

Conclusion: MTMC通过分层解耦优化策略与代码实现，有效解决了LLM在GPU内核生成中正确性与效率难以兼顾的问题，在多个基准测试中展现出显著优于现有方法的性能和准确性。

Abstract: Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

</details>


### [16] [Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management](https://arxiv.org/abs/2511.20172)
*Xinjun Yang,Qingda Hu,Junru Li,Feifei Li,Yuqi Zhou,Yicong Zhu,Qiuru Lin,Jian Dai,Yang Kong,Jiayu Zhang,Guoqiang Xu,Qiang Liu*

Main category: cs.DC

TL;DR: 本文提出Beluga，一种基于CXL的新型内存架构，使GPU和CPU可通过CXL交换机共享大规模内存池，显著降低LLM推理中KVCache访问的延迟并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）规模扩大和长上下文推理需求增长，GPU高带宽内存（HBM）容量不足，需依赖主机内存（CPU DRAM）存储KVCache；然而CPU内存通道数量有限，现有基于RDMA的解耦内存方案存在高延迟、协议复杂和同步开销大等问题。

Method: 利用新兴CXL技术构建共享内存池架构Beluga，支持GPU通过CXL交换机以本地访存语义直接访问大规模内存，并在此基础上实现面向LLM推理的KVCache管理系统Beluga-KVCache。

Result: 在vLLM推理引擎中，相比基于RDMA的方案，Beluga-KVCache将首Token生成时间（TTFT）减少89.6%，吞吐量提升7.35倍。

Conclusion: Beluga是首个支持GPU通过CXL交换机直接访问大规模内存池的系统，为实现低延迟、高效率的GPU共享内存访问提供了新路径。

Abstract: The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [Updatable Balanced Index for Fast On-device Search with Auto-selection Model](https://arxiv.org/abs/2511.20049)
*Yushuai Ji,Sheng Wang,Zhiyu Chen,Yuan Sun,Zhiyong Peng*

Main category: cs.DB

TL;DR: 本文提出UnIS系统，通过预测分割超平面加速BMKD-tree构建、采用选择性子树重建支持高效插入，并引入自动搜索策略选择机制提升查询性能，在多种边缘数据搜索任务中显著优于传统BMKD-tree。


<details>
  <summary>Details</summary>
Motivation: 现有基于BMKD-tree的边缘设备索引方法存在构建开销大、不支持实时插入及查询性能不稳定等问题，限制了其在边缘计算场景下的实用性。

Method: UnIS通过三方面改进BMKD-tree：1）利用数据分布预测分割超平面以加速构建；2）设计选择性子树重建机制以高效处理动态插入；3）构建自动选择模型为不同查询任务选取最优搜索策略。

Result: 实验表明，UnIS相较BMKD-tree在索引构建、插入、kNN搜索和半径搜索上分别提速17.96倍、1.60倍、7.15倍和1.09倍，并在边缘设备数据集简化任务中比Lloyd算法快217倍。

Conclusion: UnIS有效解决了BMKD-tree在边缘设备上的效率瓶颈，显著提升了动态数据索引与查询的整体性能，适用于实时边缘数据分析与学习任务。

Abstract: Diverse types of edge data, such as 2D geo-locations and 3D point clouds, are collected by sensors like lidar and GPS receivers on edge devices. On-device searches, such as k-nearest neighbor (kNN) search and radius search, are commonly used to enable fast analytics and learning technologies, such as k-means dataset simplification using kNN. To maintain high search efficiency, a representative approach is to utilize a balanced multi-way KD-tree (BMKD-tree). However, the index has shown limited gains, mainly due to substantial construction overhead, inflexibility to real-time insertion, and inconsistent query performance. In this paper, we propose UnIS to address the above limitations. We first accelerate the construction process of the BMKD-tree by utilizing the dataset distribution to predict the splitting hyperplanes. To make the continuously generated data searchable, we propose a selective sub-tree rebuilding scheme to accelerate rebalancing during insertion by reducing the number of data points involved. We then propose an auto-selection model to improve query performance by automatically selecting the optimal search strategy among multiple strategies for an arbitrary query task. Experimental results show that UnIS achieves average speedups of 17.96x in index construction, 1.60x in insertion, 7.15x in kNN search, and 1.09x in radius search compared to the BMKD-tree. We further verify its effectiveness in accelerating dataset simplification on edge devices, achieving a speedup of 217x over Lloyd's algorithm.

</details>


### [18] [Mobility Stream Processing on NebulaStream and MEOS](https://arxiv.org/abs/2511.20084)
*Mariana M. Garcez Duarte,Dwi P. A. Nugroho,Georges Tod,Evert Bevernage,Pieter Moelans,Emine Tas,Esteban Zimanyi,Mahmoud Sakr,Steffen Zeuch,Volker Markl*

Main category: cs.DB

TL;DR: 本文提出了NebulaMEOS系统，将时空处理库MEOS集成到物联网流数据管理系统NebulaStream中，以支持对移动物体（如火车）产生的时空流数据进行实时分析，并通过SNCB列车数据展示了地理围栏和复杂事件处理等应用。


<details>
  <summary>Details</summary>
Motivation: 当前面向物联网的流处理系统普遍缺乏时空处理能力，而现有时空库主要针对历史数据，难以满足对移动对象产生的时空流数据进行实时分析的需求。

Method: 将开源时空处理库MEOS与可扩展的物联网数据管理系统NebulaStream集成，构建名为NebulaMEOS的新系统，以支持实时时空流数据处理。

Result: 通过SNCB列车边缘设备传来的数据，成功演示了NebulaMEOS在地理围栏和地理空间复杂事件处理方面的实时分析与可视化能力。

Conclusion: NebulaMEOS有效填补了物联网流处理系统在实时时空分析方面的空白，为移动对象的实时监控与环境影响评估提供了可行方案。

Abstract: The increasing use of Internet-of-Things (IoT) sensors in moving objects has resulted in vast amounts of spatiotemporal streaming data. To analyze this data in situ, real-time spatiotemporal processing is needed. However, current stream processing systems designed for IoT environments often lack spatiotemporal processing capabilities, and existing spatiotemporal libraries primarily focus on analyzing historical data. This gap makes performing real-time spatiotemporal analytics challenging. In this demonstration, we present NebulaMEOS, which combines MEOS (Mobility Engine Open Source), a spatiotemporal processing library, with NebulaStream, a scalable data management system for IoT applications. By integrating MEOS into NebulaStream, NebulaMEOS utilizes spatiotemporal functionalities to process and analyze streaming data in real-time. We demonstrate NebulaMEOS by querying data streamed from edge devices on trains by the Société Nationale des Chemins de fer Belges (SNCB). Visitors can experience demonstrations of geofencing and geospatial complex event processing, visualizing real-time train operations and environmental impacts.

</details>


### [19] [N2E: A General Framework to Reduce Node-Differential Privacy to Edge-Differential Privacy for Graph Analytics](https://arxiv.org/abs/2511.20125)
*Yihua Hu,Hao Ding,Wei Dong*

Main category: cs.DB

TL;DR: 本文提出了一个名为N2E的通用框架，将节点差分隐私（node-DP）图分析任务转化为边差分隐私（edge-DP）任务，通过引入保距裁剪机制和首个节点DP最大度近似机制，在边缘计数和度分布估计等任务上显著降低了误差。


<details>
  <summary>Details</summary>
Motivation: 节点差分隐私（node-DP）相比边差分隐私（edge-DP）提供更强的隐私保护，但由于技术挑战，研究较少。现有基于群隐私的简单方法在设定保守的大度数上限时会导致效用低下，因此需要一种更高效且实用的通用转换框架。

Method: 提出N2E框架，包含两项关键技术：一是保距裁剪机制，控制裁剪后相邻图之间的边距离；二是首个满足节点DP的最大度近似机制，用于生成紧致且保护隐私的裁剪阈值。该框架可与现有edge-DP机制结合使用。

Result: 在边缘计数任务中，理论误差达到当前最优水平；在度分布估计任务中，显著优于现有方法。实验表明，边缘计数误差最多降低2.5倍，度分布估计误差最多降低80倍。

Conclusion: N2E框架有效弥合了node-DP与edge-DP之间的差距，为多种图分析任务提供了首个实用且高精度的node-DP解决方案，兼具理论保证与实际性能优势。

Abstract: Differential privacy (DP) has been widely adopted to protect sensitive information in graph analytics. While edge-DP, which protects privacy at the edge level, has been extensively studied, node-DP, offering stronger protection for entire nodes and their incident edges, remains largely underexplored due to its technical challenges. A natural way to bridge this gap is to develop a general framework for reducing node-DP graph analytical tasks to edge-DP ones, enabling the reuse of existing edge-DP mechanisms. A straightforward solution based on group privacy divides the privacy budget by a given degree upper bound, but this leads to poor utility when the bound is set conservatively large to accommodate worst-case inputs. To address this, we propose node-to-edge (N2E), a general framework that reduces any node-DP graph analytical task to an edge-DP one, with the error dependency on the graph's true maximum degree. N2E introduces two novel techniques: a distance-preserving clipping mechanism that bounds edge distance between neighboring graphs after clipping, and the first node-DP mechanism for maximum degree approximation, enabling tight, privacy-preserving clipping thresholds. By instantiating N2E with existing edge-DP mechanisms, we obtain the first node-DP solutions for tasks such as maximum degree estimation. For edge counting, our method theoretically matches the error of the state-of-the-art, which is provably optimal, and significantly outperforms existing approaches for degree distribution estimation. Experimental results demonstrate that our framework achieves up to a 2.5x reduction in error for edge counting and up to an 80x reduction for degree distribution estimation.

</details>


### [20] [An experimental study of existing tools for outlier detection and cleaning in trajectories](https://arxiv.org/abs/2511.20139)
*Mariana M Garcez Duarte,Mahmoud Sakr*

Main category: cs.DB

TL;DR: 本文评估了十个开源库在单轨迹内异常点检测与清洗方面的效率和准确性，提出了建立真实标签的方法，并对现有异常检测算法进行了分类和综述，以指导用户选择合适工具。


<details>
  <summary>Details</summary>
Motivation: 确保数据分析的完整性与有效性，需要在数据预处理阶段有效识别和清理轨迹中的异常点；然而目前缺乏对现有开源工具在真实使用场景下的系统性评估和指导。

Method: 实验评估十个开源异常检测库在真实应用场景下的表现，提出一种构建真实标签（ground-truth）的方法，并对现有异常检测算法进行分类：基于统计、滑动窗口、基于聚类、基于图和基于启发式五类。

Result: 对各开源库在效率与准确性方面进行了比较，提供了其性能洞察，并为用户根据具体需求选择合适工具提供了指导。

Conclusion: 本研究不仅有助于理解现有异常检测库的实际表现，还推动了数据预处理和异常检测方法的发展，为轨迹数据清洗提供了实用参考。

Abstract: Outlier detection and cleaning are essential steps in data preprocessing to ensure the integrity and validity of data analyses. This paper focuses on outlier points within individual trajectories, i.e., points that deviate significantly inside a single trajectory. We experiment with ten open-source libraries to comprehensively evaluate available tools, comparing their efficiency and accuracy in identifying and cleaning outliers. This experiment considers the libraries as they are offered to end users, with real-world applicability. We compare existing outlier detection libraries, introduce a method for establishing ground-truth, and aim to guide users in choosing the most appropriate tool for their specific outlier detection needs. Furthermore, we survey the state-of-the-art algorithms for outlier detection and classify them into five types: Statistic-based methods, Sliding window algorithms, Clustering-based methods, Graph-based methods, and Heuristic-based methods. Our research provides insights into these libraries' performance and contributes to developing data preprocessing and outlier detection methodologies.

</details>


### [21] [Forgetting by Pruning: Data Deletion in Join Cardinality Estimation](https://arxiv.org/abs/2511.20293)
*Chaowei He,Yuanjun Liu,Qingzhi Ma,Shenyuan Ren,Xizhao Luo,Lei Zhao,An Liu*

Main category: cs.DB

TL;DR: 本文提出了首个面向多表学习型基数估计系统的机器遗忘框架CEP，通过分布敏感剪枝和域剪枝有效应对数据删除带来的挑战，在保持极低计算开销的同时显著优于完全重训练。


<details>
  <summary>Details</summary>
Motivation: 在学习型基数估计系统中，由于多表关系数据存在复杂的分布依赖性，机器遗忘（尤其是数据删除）面临属性级敏感性、跨表传播以及值域消失导致的多表连接严重高估三大挑战。

Method: 提出Cardinality Estimation Pruning（CEP）框架，包含分布敏感剪枝（构建半连接删除结果并计算敏感度分数以指导参数剪枝）和域剪枝（移除因删除而完全消失的值域支持）。

Result: 在IMDB和TPC-H数据集上对NeuroCard和FACE模型的实验表明，CEP在多表场景下始终取得最低Q-error，尤其在高删除比例下常优于完全重训练，并将收敛迭代次数大幅减少，计算开销仅为微调时间的0.3%-2.5%。

Conclusion: CEP是首个专为多表学习型基数估计设计的机器遗忘框架，能高效、准确地处理数据删除问题，在性能和效率上均显著优于现有方法。

Abstract: Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time.

</details>


### [22] [The Case for Intent-Based Query Rewriting](https://arxiv.org/abs/2511.20419)
*Gianna Lisa Nicolai,Patrick Hansert,Sebastian Michel*

Main category: cs.DB

TL;DR: 本文提出了一种基于意图的查询重写方法INQURE，利用大语言模型理解查询意图并生成语义等效但结构不同的替代查询，以应对数据访问受限、隐私或成本等问题。


<details>
  <summary>Details</summary>
Motivation: 传统查询重写主要关注通过等价变换优化执行效率，但在数据因访问控制、隐私或高成本而不可用时无法发挥作用。作者旨在探索一种保留原始查询“可获得洞察”但允许结构和语法变化的新型重写方式。

Method: 提出系统INQURE，围绕大语言模型（LLM）构建，结合前置表过滤、候选重写生成、剪枝与排序机制，实现基于意图的查询重写。

Result: 在包含900多个数据库表模式的基准测试集上评估了INQURE，并通过用户研究比较了不同方法在重写质量和运行时间方面的优劣。

Conclusion: 基于意图的查询重写是一种可行的新范式，INQURE展示了利用LLM实现该目标的有效性，为解决数据访问限制问题提供了新思路。

Abstract: With this work, we describe the concept of intent-based query rewriting and present a first viable solution. The aim is to allow rewrites to alter the structure and syntactic outcome of an original query while keeping the obtainable insights intact. This drastically differs from traditional query rewriting, which typically aims to decrease query evaluation time by using strict equivalence rules and optimization heuristics on the query plan. Rewriting queries to queries that only provide a similar insight but otherwise can be entirely different can remedy inaccessible original data tables due to access control, privacy, or expensive data access regarding monetary cost or remote access. In this paper, we put forward INQURE, a system designed for INtent-based QUery REwriting. It uses access to a large language model (LLM) for the query understanding and human-like derivation of alternate queries. Around the LLM, INQURE employs upfront table filtering and subsequent candidate rewrite pruning and ranking. We report on the results of an evaluation using a benchmark set of over 900 database table schemas and discuss the pros and cons of alternate approaches regarding runtime and quality of the rewrites of a user study.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [23] [SARA: A Stall-Aware Memory Allocation Strategy for Mixed-Criticality Systems](https://arxiv.org/abs/2511.19991)
*Meng-Chia Lee,Wen Sheng Lim,Yuan-Hao Chang,Tei-Wei Kuo*

Main category: cs.OS

TL;DR: 本文提出了一种名为SARA的内存分配器，用于在内存受限的边缘设备中平衡软实时任务与非实时应用的性能，通过精准分配内存以满足软实时任务截止期限，并优化剩余内存提升非实时应用吞吐量。


<details>
  <summary>Details</summary>
Motivation: 边缘设备因成本、尺寸和功耗限制导致内存容量有限，引发频繁页面交换和系统级长停顿（long stalls），破坏软实时任务的截止期限保障并降低非实时应用吞吐量。

Method: SARA基于PSI指标量化内存不足引起的延迟对软实时任务各周期作业执行时间的影响，动态分配刚好满足截止期限的内存；同时定义并检测长停顿，主动丢弃受影响作业以减少执行中断。

Result: 实验表明，在内存仅为峰值需求60%的情况下，SARA使软实时任务平均截止命中率达97.13%，非实时应用吞吐量最高提升22.32倍。

Conclusion: SARA有效解决了内存受限混合关键性边缘设备中的内存分配问题，在保障软实时任务性能的同时显著提升非实时应用效率。

Abstract: The memory capacity in edge devices is often limited due to constraints on cost, size, and power. Consequently, memory competition leads to inevitable page swapping in memory-constrained mixed-criticality edge devices, causing slow storage I/O and thus performance degradation. In such scenarios, inefficient memory allocation disrupts the balance between application performance, causing soft real-time (soft RT) tasks to miss deadlines or preventing non-real-time (non-RT) applications from optimizing throughput. Meanwhile, we observe unpredictable, long system-level stalls (called long stalls) under high memory and I/O pressure, which further degrade performance. In this work, we propose a Stall-Aware Real-Time Memory Allocator (SARA), which discovers opportunities for performance balance by allocating just enough memory to soft RT tasks to meet deadlines and, at the same time, optimizing the remaining memory for non-RT applications. To minimize the memory usage of soft RT tasks while meeting real-time requirements, SARA leverages our insight into how latency, caused by memory insufficiency and measured by our proposed PSI-based metric, affects the execution time of each soft RT job, where a job runs per period and a soft RT task consists of multiple periods. Moreover, SARA detects long stalls using our definition and proactively drops affected jobs, minimizing stalls in task execution. Experiments show that SARA achieves an average of 97.13% deadline hit ratio for soft RT tasks and improves non-RT application throughput by up to 22.32x over existing approaches, even with memory capacity limited to 60% of peak demand.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [24] [Building Browser Agents: Architecture, Security, and Practical Solutions](https://arxiv.org/abs/2511.19477)
*Aram Vardanyan*

Main category: cs.SE

TL;DR: 该论文指出，浏览器智能体在生产环境中面临可靠性和安全性挑战，其性能瓶颈并非模型能力，而是架构设计；作者通过混合上下文管理、全面的浏览器工具和智能提示工程，在WebGames基准上实现了约85%的成功率，并主张采用受程序约束的专用工具替代通用自主浏览。


<details>
  <summary>Details</summary>
Motivation: 当前浏览器智能体在实际部署中存在可靠性与安全风险，尤其是提示注入攻击使通用自主操作本质上不安全，亟需重新思考其架构与设计原则。

Method: 采用混合上下文管理（结合无障碍树快照与选择性视觉）、匹配人类交互能力的完整浏览器工具集，以及智能提示工程；同时强调通过代码而非大语言模型推理来实施安全边界，转向专用工具而非通用智能。

Result: 在包含53个多样化任务的WebGames基准测试中，该智能体达到约85%的成功率，显著优于此前约50%的浏览器智能体表现，接近95.7%的人类基线。

Conclusion: 模型能力并非限制浏览器智能体性能的关键因素，架构设计才是决定成败的核心；为确保安全性与可靠性，应放弃通用浏览智能，转而发展具有程序化约束的专用工具。

Abstract: Browser agents enable autonomous web interaction but face critical reliability and security challenges in production. This paper presents findings from building and operating a production browser agent. The analysis examines where current approaches fail and what prevents safe autonomous operation. The fundamental insight: model capability does not limit agent performance; architectural decisions determine success or failure. Security analysis of real-world incidents reveals prompt injection attacks make general-purpose autonomous operation fundamentally unsafe. The paper argues against developing general browsing intelligence in favor of specialized tools with programmatic constraints, where safety boundaries are enforced through code instead of large language model (LLM) reasoning. Through hybrid context management combining accessibility tree snapshots with selective vision, comprehensive browser tooling matching human interaction capabilities, and intelligent prompt engineering, the agent achieved approximately 85% success rate on the WebGames benchmark across 53 diverse challenges (compared to approximately 50% reported for prior browser agents and 95.7% human baseline).

</details>


### [25] [Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation](https://arxiv.org/abs/2511.19483)
*Qingsong He,Jing Nan,Jiayu Jiao,Liangjie Tang,Xiaodong Xu,Mengmeng Sun,Qingyao Wang,Minghui Yan*

Main category: cs.SE

TL;DR: 本文提出Z-Space框架，通过多智能体协同与融合子空间加权算法（FSWW），在企业级大规模工具调用场景中显著降低LLM的token消耗并提升工具调用准确率。


<details>
  <summary>Details</summary>
Motivation: 随着企业级MCP服务快速增长，在数千个异构工具中高效准确地匹配目标功能成为限制系统实用性的核心挑战；现有方法存在语义脱节、上下文膨胀和推理延迟高等问题。

Method: Z-Space框架包含三个核心组件：(1) 基于意图解析模型对用户查询进行结构化语义理解；(2) 基于融合子空间加权算法（FSWW）的工具过滤模块，实现无需参数调优的细粒度语义对齐；(3) 支持动态规划与容错执行的推理执行智能体。

Result: 在饿了么平台技术部门部署后，系统在淘天、高德、盒马等多个业务单元的大规模测试数据生成场景中，将工具推理平均token消耗降低96.26%，工具调用准确率达92%。

Conclusion: Z-Space框架有效提升了智能测试数据生成系统的效率与可靠性，为企业级复杂任务自动化提供了可行解决方案。

Abstract: Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\% while achieving a 92\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.

</details>


### [26] [stable-pretraining-v1: Foundation Model Research Made Simple](https://arxiv.org/abs/2511.19484)
*Randall Balestriero,Hugues Van Assel,Sami BuGhanem,Lucas Maes*

Main category: cs.SE

TL;DR: stable-pretraining 是一个基于 PyTorch 等框架构建的模块化、可扩展且性能优化的自监督预训练库，旨在简化基础模型研究，提升实验迭代速度与可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型与自监督学习研究受限于复杂代码库、重复实现和繁重的工程负担，亟需一个灵活高效的研究工具。

Method: 开发了 stable-pretraining 库，整合探针、崩溃检测指标、增强流水线和可扩展评估流程，并强调全面日志记录以支持调试与复现。

Result: 该库能以较低开销生成新研究洞见，例如深度表示探针和 CLIP 在合成数据微调下的退化分析。

Conclusion: stable-pretraining 通过降低研究门槛并支持大规模实验，有望加速基础模型领域的发现与创新。

Abstract: Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.

</details>


### [27] [Evolution without an Oracle: Driving Effective Evolution with LLM Judges](https://arxiv.org/abs/2511.19489)
*Zhe Zhao,Yuheng Yang,Haibin Wen,Xiaojie Qiu,Zaixi Zhang,Qingfu Zhang*

Main category: cs.SE

TL;DR: 本文提出MADE框架，通过将模糊指令分解为可验证的子需求，利用大语言模型（LLM）作为主观评判者驱动进化计算，在无客观适应度函数的情况下显著提升复杂任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有结合大语言模型与进化计算的方法依赖于可计算的客观适应度函数（Oracle），限制了其在开放域问题中的应用；本文旨在探索仅依靠LLM主观评判进行进化的可行性。

Method: 提出MADE（Multi-Agent Decomposed Evolution）框架，通过“问题规范”将模糊目标分解为具体、可验证的子需求，从而降低LLM主观评估的噪声，形成稳定的选择压力。

Result: 在DevAI和InfoBench等复杂基准上，MADE在软件需求满足率上从39.9%提升至61.9%，并在复杂指令遵循任务中达到95%的完美通过率，显著优于强基线方法。

Conclusion: 该研究验证了从优化“可计算指标”转向优化“可描述品质”的范式转变，为缺乏真实标签的开放域问题提供了可行的进化优化路径。

Abstract: The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through "Problem Specification." By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing "computable metrics" to "describable qualities," thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.

</details>


### [28] [CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem](https://arxiv.org/abs/2511.19510)
*Asif Zaman,Kallol Naha,Khalid Belhajjame,Hasan M. Jamil*

Main category: cs.SE

TL;DR: 本文提出了一种名为CodeR³的新型遗留工作流迁移系统，利用生成式AI将已失效的Taverna工作流自动转换为现代工作流技术（如Snakemake和VisFlow），并结合可视化分析、服务自动替换与人工验证，显著减少人工解析负担，同时强调关键步骤仍需领域专家参与。


<details>
  <summary>Details</summary>
Motivation: 科学工作流蕴含宝贵的领域知识和计算方法，但大量已发表的工作流随时间推移而失效，尤其在Taverna等已停用的旧系统中更为严重，亟需有效手段恢复其可用性。

Method: 开发CodeR³系统，结合生成式AI分析失效工作流特征，将其自动迁移至现代工作流平台，并集成逐步分析可视化、自动化服务替换及人机协同验证机制。

Result: 通过多个Taverna工作流复兴案例，验证了该方法的可行性；自动化显著降低了工作流解析与服务识别的人工成本，但服务替换与数据验证仍依赖人类专家。计划构建众包平台支持社区协作复兴与验证。

Conclusion: 本文提出了一个兼顾自动化效率与人类判断的工作流复兴框架，为解决科学工作流衰减问题提供了可行路径。

Abstract: Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.

</details>


### [29] [Agint: Agentic Graph Compilation for Software Engineering Agents](https://arxiv.org/abs/2511.19635)
*Abhi Chivukula,Jay Somasundaram,Vijay Somasundaram*

Main category: cs.SE

TL;DR: Agint 是一个基于图的智能体编译器与运行时系统，通过将自然语言指令逐步转换为类型化、效应感知的代码 DAG，解决了 LLM 编码智能体在上下文管理、延迟、可靠性、可复现性和可扩展性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的编码智能体在实际应用中仍面临上下文管理困难、延迟高、可靠性不足、结果不可复现以及难以扩展等问题，亟需一种结合编译器技术与智能体架构的新范式来提升开发效率与系统性能。

Method: Agint 采用分层增量式编译策略，将自然语言指令转化为具有显式类型层级（文本 → 数据 → 规约 → 代码）的语义图，并结合混合式 LLM 与函数驱动的即时（JIT）运行时。其核心组件包括 dagify（DAG 编译器）、dagent（混合 JIT 运行时）、schemagin（模式生成器）和 datagin（数据转换器），支持动态图优化、推测执行、并行生成及与现有开发工具的互操作。

Result: Agint 实现了更低延迟、更高吞吐量、更高效上下文利用，并支持使用更小更快的模型进行可靠、可组合的并发代码生成。其图结构保障了可复现性，且通过 CLI 与 GUI（Agint Flow）支持技术人员与非技术人员协同进行原型设计到生产部署的全流程。

Conclusion: Agint 通过融合自然语言理解、类型化图表示与编译器技术，构建了一个可组合、团队协作导向的编码智能体基础设施，为下一代可扩展、可靠且高效的智能编程系统提供了可行路径。

Abstract: LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.

</details>


### [30] [CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection](https://arxiv.org/abs/2511.19875)
*Qingyu Zhang,Puzhuo Liu,Peng Di,Chenxiong Qian*

Main category: cs.SE

TL;DR: 本文提出了首个用于检测提交信息与代码变更不一致（MCI）的基准CODEFUSE-COMMITEVAL，基于ApacheCM数据集构建包含七类不一致样本，并评估了六种开源大语言模型在不同增强策略下的表现，发现模型对不一致样本识别效果优于一致样本，且不同类型不一致性检测难度存在差异。


<details>
  <summary>Details</summary>
Motivation: 提交信息常与代码变更不一致（MCI），误导审查者、阻碍维护、污染研究数据并可能掩盖安全补丁，但目前缺乏专门用于评估MCI检测能力的基准。

Method: 基于ApacheCM数据集，通过规则引导变异生成七类不一致提交信息，并采用双重验证确保正负样本质量；在此基础上，评估六种开源大语言模型在原始设置及三种增强策略（少样本提示、思维链、扩展上下文）下的MCI检测性能。

Result: 模型对不一致提交的检测更可靠（平均召回率85.95%，精确率80.28%，特异性63.8%）；gpt-oss-20B整体表现最佳但token消耗超两倍；增强策略效果各异：邻近上下文对大模型有益但对小模型引入噪声，少样本提升准确率但增加错误预测，思维链提高精确率和特异性但降低召回率；组件、文件路径和操作类不一致性更易检测，而意图层面的“目的”不一致性检测准确率低且token成本高。

Conclusion: CODEFUSE-COMMITEVAL为MCI检测提供了严谨的评估基础，揭示了当前模型在捕捉高层语义差距方面的不足，强调需引入更丰富的上下文和平衡的数据以推动该领域发展。

Abstract: Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level "purpose" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.

</details>


### [31] [LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework](https://arxiv.org/abs/2511.20403)
*Andrea Lops,Fedelucio Narducci,Azzurra Ragone,Michelantonio Trizio,Claudio Barto*

Main category: cs.SE

TL;DR: 本文提出了 AgoneTest，一个用于评估大语言模型（LLM）生成 Java 单元测试的自动化框架，并引入 Classes2Test 数据集和综合评估指标，在现实条件下比较不同 LLM 和提示策略的效果。


<details>
  <summary>Details</summary>
Motivation: 单元测试是软件开发中关键但耗费资源的环节。当前缺乏标准化的评估框架来系统比较不同大语言模型及其提示策略在生成单元测试方面的表现。

Method: 构建 AgoneTest 框架，整合 Classes2Test 数据集（包含 Java 被测类与其对应测试类的映射），并采用变异分数、测试异味等高级评估指标，建立端到端的标准化评估流程。

Result: 实验表明，在可编译的测试子集中，LLM 生成的测试在覆盖率和缺陷检测能力上可媲美甚至超越人工编写的测试；增强型提示策略能显著提升测试质量。

Conclusion: AgoneTest 揭示了 LLM 在软件测试中的潜力，为未来模型设计、提示工程和测试实践的改进提供了依据。

Abstract: Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.

</details>
