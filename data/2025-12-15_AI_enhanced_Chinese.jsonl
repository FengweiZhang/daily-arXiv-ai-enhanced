{"id": "2512.11550", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.11550", "abs": "https://arxiv.org/abs/2512.11550", "authors": ["Yifan Zhang", "Zhiheng Chen", "Ye Qiao", "Sitao Huang"], "title": "PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration", "comment": null, "summary": "Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPD-Swap\uff0c\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u5c40\u90e8\u91cd\u914d\u7f6e\uff08DPR\uff09\u7684\u9884\u586b\u5145-\u89e3\u7801\u5206\u79bb\u5f0fLLM\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5728\u8fb9\u7f18FPGA\u4e0a\u65f6\u5206\u590d\u7528\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5728\u4e0d\u589e\u52a0\u9762\u79ef\u6210\u672c\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u541e\u5410\u91cf\u3002", "motivation": "\u8fb9\u7f18FPGA\u90e8\u7f72\u8d85\u4f4e\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\uff08\u59821.58-bit BitNet\uff09\u65f6\uff0c\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u957f\uff0c\u9884\u586b\u5145\u9636\u6bb5\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u548cKV\u7f13\u5b58\u5e26\u5bbd\u9700\u6c42\u5bfc\u81f4\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff1b\u800c\u4f20\u7edf\u9759\u6001\u52a0\u901f\u5668\u96be\u4ee5\u517c\u987e\u8ba1\u7b97\u5bc6\u96c6\u7684\u9884\u586b\u5145\u4e0e\u5185\u5b58\u5e26\u5bbd\u53d7\u9650\u7684\u89e3\u7801\u9636\u6bb5\uff0c\u9020\u6210\u8d44\u6e90\u6d6a\u8d39\u548c\u6a21\u578b\u89c4\u6a21\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1PD-Swap\u52a0\u901f\u5668\uff0c\u5c06\u6ce8\u610f\u529b\u6a21\u5757\u5212\u5206\u4e3a\u53ef\u91cd\u914d\u7f6e\u533a\u57df\uff0c\u5305\u542b\u4e24\u4e2a\u4e13\u7528\u67b6\u6784\uff1a\u9762\u5411\u9884\u586b\u5145\u7684\u9ad8\u8ba1\u7b97\u5e76\u884c\u5f15\u64ce\u548c\u9762\u5411\u89e3\u7801\u7684KV\u7f13\u5b58\u4f18\u5316\u5f15\u64ce\uff1b\u6838\u5fc3\u4e09\u503c\u77e9\u9635\u4e58\u6cd5\u4e0e\u6743\u91cd\u7f13\u51b2\u4fdd\u6301\u9759\u6001\uff1b\u5229\u7528\u5c4b\u9876\u7ebf\u6a21\u578b\u4e0e\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u8054\u5408\u4f18\u5316\u91cd\u914d\u7f6e\u533a\u57df\u5927\u5c0f\u3001\u5e76\u884c\u5ea6\u53ca\u5e03\u7ebf\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u63a9\u76d6\u91cd\u914d\u7f6e\u5ef6\u8fdf\u3002", "result": "PD-Swap\u5728\u8fb9\u7f18FPGA\u4e0a\u5b9e\u73b0\u6700\u9ad827 tokens/s\u7684\u89e3\u7801\u541e\u5410\u91cf\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6848\u63d0\u53471.3\u20132.1\u500d\uff08\u4e0a\u4e0b\u6587\u8d8a\u957f\u589e\u76ca\u8d8a\u5927\uff09\uff0c\u4e14\u672a\u589e\u52a0\u989d\u5916\u786c\u4ef6\u9762\u79ef\u3002", "conclusion": "\u901a\u8fc7\u9884\u586b\u5145\u4e0e\u89e3\u7801\u9636\u6bb5\u7684\u67b6\u6784\u89e3\u8026\u548c\u52a8\u6001\u91cd\u914d\u7f6e\uff0cPD-Swap\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u4e0b\u8fb9\u7f18LLM\u52a0\u901f\u5668\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u63a8\u7406\u3002"}}
{"id": "2512.10977", "categories": ["cs.DC", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10977", "abs": "https://arxiv.org/abs/2512.10977", "authors": ["Alec M. Hammond", "Aram Markosyan", "Aman Dontula", "Simon Mahns", "Zacharias Fisches", "Dmitrii Pedchenko", "Keyur Muzumdar", "Natacha Supper", "Mark Saroufim", "Joe Isaacson", "Laura Wang", "Warren Hunt", "Kaustubh Gondkar", "Roman Levenstein", "Gabriel Synnaeve", "Richard Li", "Jacob Kahn", "Ajit Mathews"], "title": "Agentic Operator Generation for ML ASICs", "comment": null, "summary": "We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.", "AI": {"tldr": "TritorX \u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u53ef\u5927\u89c4\u6a21\u751f\u6210\u529f\u80fd\u6b63\u786e\u7684 Triton PyTorch ATen \u5185\u6838\uff0c\u8986\u76d6 481 \u4e2a\u7b97\u5b50\u5e76\u901a\u8fc7\u8d85\u8fc7 2 \u4e07\u4e2a OpInfo \u6d4b\u8bd5\uff0c\u5f3a\u8c03\u6b63\u786e\u6027\u4e0e\u901a\u7528\u6027\uff0c\u9002\u7528\u4e8e\u65b0\u5174\u52a0\u901f\u5668\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709\u5185\u6838\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5c11\u6570\u9ad8\u9891\u7b97\u5b50\u7684\u6027\u80fd\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4e2a\u7b97\u5b50\u96c6\u7684\u8986\u76d6\u548c\u6b63\u786e\u6027\u4fdd\u969c\u3002\u4e3a\u652f\u6301\u65b0\u52a0\u901f\u5668\u5e73\u53f0\u5feb\u901f\u90e8\u7f72\u5b8c\u6574\u7684 PyTorch \u540e\u7aef\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u517c\u987e\u5e7f\u6cdb\u7b97\u5b50\u3001\u6570\u636e\u7c7b\u578b\u3001\u5f20\u91cf\u5f62\u72b6\u548c\u53c2\u6570\u6a21\u5f0f\u7684\u81ea\u52a8\u5316\u5185\u6838\u751f\u6210\u65b9\u6848\u3002", "method": "TritorX \u7ed3\u5408\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u5b9a\u4e49\u4ee3\u7801\u68c0\u67e5\u5668\uff08linter\uff09\u3001JIT \u7f16\u8bd1\u5668\u4ee5\u53ca\u57fa\u4e8e PyTorch OpInfo \u7684\u6d4b\u8bd5\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u4e00\u5957\u7aef\u5230\u7aef\u7684\u5185\u6838\u751f\u6210\u4e0e\u9a8c\u8bc1\u6d41\u6c34\u7ebf\uff0c\u517c\u5bb9\u771f\u5b9e MTIA \u786c\u4ef6\u53ca\u4e0b\u4e00\u4ee3\u8bbe\u5907\u7684\u4eff\u771f\u73af\u5883\u3002", "result": "\u5728\u5b9e\u9a8c\u4e2d\uff0cTritorX \u6210\u529f\u4e3a 481 \u4e2a\u552f\u4e00\u7684 ATen \u7b97\u5b50\u751f\u6210\u4e86\u901a\u8fc7\u5168\u90e8\u5bf9\u5e94 OpInfo \u6d4b\u8bd5\uff08\u603b\u8ba1\u8d85 20,000 \u9879\uff09\u7684\u5185\u6838\u53ca\u5c01\u88c5\u4ee3\u7801\u3002", "conclusion": "TritorX \u5c55\u793a\u4e86\u5728\u4e00\u591c\u4e4b\u95f4\u4e3a\u65b0\u578b\u52a0\u901f\u5668\u5e73\u53f0\u81ea\u52a8\u751f\u6210\u5b8c\u6574 PyTorch ATen \u540e\u7aef\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u786c\u4ef6\u751f\u6001\u7684\u5feb\u901f\u8f6f\u4ef6\u9002\u914d\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2512.11273", "categories": ["cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11273", "abs": "https://arxiv.org/abs/2512.11273", "authors": ["Qi Deng", "Yuxuan Linghu", "Zhiyuan Liu"], "title": "Integrated Prediction and Multi-period Portfolio Optimization", "comment": "23 pages, 6 figures, and 4 tables", "summary": "Multi-period portfolio optimization is important for real portfolio management, as it accounts for transaction costs, path-dependent risks, and the intertemporal structure of trading decisions that single-period models cannot capture. Classical methods usually follow a two-stage framework: machine learning algorithms are employed to produce forecasts that closely fit the realized returns, and the predicted values are then used in a downstream portfolio optimization problem to determine the asset weights. This separation leads to a fundamental misalignment between predictions and decision outcomes, while also ignoring the impact of transaction costs. To bridge this gap, recent studies have proposed the idea of end-to-end learning, integrating the two stages into a single pipeline. This paper introduces IPMO (Integrated Prediction and Multi-period Portfolio Optimization), a model for multi-period mean-variance portfolio optimization with turnover penalties. The predictor generates multi-period return forecasts that parameterize a differentiable convex optimization layer, which in turn drives learning via portfolio performance. For scalability, we introduce a mirror-descent fixed-point (MDFP) differentiation scheme that avoids factorizing the Karush-Kuhn-Tucker (KKT) systems, which thus yields stable implicit gradients and nearly scale-insensitive runtime as the decision horizon grows. In experiments with real market data and two representative time-series prediction models, the IPMO method consistently outperforms the two-stage benchmarks in risk-adjusted performance net of transaction costs and achieves more coherent allocation paths. Our results show that integrating machine learning prediction with optimization in the multi-period setting improves financial outcomes and remains computationally tractable.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faIPMO\u6a21\u578b\uff0c\u5c06\u591a\u671f\u6536\u76ca\u9884\u6d4b\u4e0e\u5e26\u6362\u624b\u60e9\u7f5a\u7684\u5747\u503c-\u65b9\u5dee\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u7aef\u5230\u7aef\u96c6\u6210\uff0c\u901a\u8fc7\u53ef\u5fae\u51f8\u4f18\u5316\u5c42\u548c\u9ad8\u6548\u7684MDFP\u68af\u5ea6\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u5e02\u573a\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u4e24\u9636\u6bb5\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u591a\u671f\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5148\u9884\u6d4b\u6536\u76ca\u518d\u8fdb\u884c\u4f18\u5316\uff0c\u5bfc\u81f4\u9884\u6d4b\u4e0e\u51b3\u7b56\u76ee\u6807\u4e0d\u4e00\u81f4\u4e14\u5ffd\u7565\u4ea4\u6613\u6210\u672c\uff0c\u96be\u4ee5\u83b7\u5f97\u6700\u4f18\u6295\u8d44\u8868\u73b0\u3002", "method": "\u63d0\u51faIPMO\u6a21\u578b\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u4e0e\u542b\u6362\u624b\u60e9\u7f5a\u7684\u591a\u671f\u5747\u503c-\u65b9\u5dee\u4f18\u5316\u95ee\u9898\u6574\u5408\u4e3a\u7aef\u5230\u7aef\u53ef\u5fae\u67b6\u6784\uff1b\u5f15\u5165\u955c\u50cf\u4e0b\u964d\u4e0d\u52a8\u70b9\uff08MDFP\uff09\u5fae\u5206\u65b9\u6848\uff0c\u907f\u514dKKT\u7cfb\u7edf\u5206\u89e3\uff0c\u5b9e\u73b0\u7a33\u5b9a\u68af\u5ea6\u4e0e\u826f\u597d\u6269\u5c55\u6027\u3002", "result": "\u5728\u771f\u5b9e\u5e02\u573a\u6570\u636e\u548c\u4e24\u79cd\u4ee3\u8868\u6027\u9884\u6d4b\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cIPMO\u5728\u6263\u9664\u4ea4\u6613\u6210\u672c\u540e\u7684\u98ce\u9669\u8c03\u6574\u6536\u76ca\u548c\u8d44\u4ea7\u914d\u7f6e\u8def\u5f84\u4e00\u81f4\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u4e24\u9636\u6bb5\u57fa\u51c6\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u4e0e\u591a\u671f\u6295\u8d44\u7ec4\u5408\u4f18\u5316\u7aef\u5230\u7aef\u96c6\u6210\u80fd\u6709\u6548\u63d0\u5347\u91d1\u878d\u7ee9\u6548\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u3002"}}
{"id": "2512.10974", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10974", "abs": "https://arxiv.org/abs/2512.10974", "authors": ["Sohan Kumar Pande", "Sanjaya Kumar Panda", "Preeti Ranjan Sahu"], "title": "An Efficient Approach for Energy Conservation in Cloud Computing Environment", "comment": null, "summary": "Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u7efc\u5408\u8003\u8651CPU\u3001\u78c1\u76d8\u548cI/O\u8d44\u6e90\u5229\u7528\u7387\u53ca\u4efb\u52a1\u5904\u7406\u65f6\u95f4\u6765\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u4ece\u800c\u964d\u4f4e\u4e91\u670d\u52a1\u7684\u80fd\u8017\uff1b\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u6bd4\u73b0\u6709MaxUtil\u7b97\u6cd5\u66f4\u8282\u80fd\u3002", "motivation": "\u5f53\u524d\u4e91\u8ba1\u7b97\u670d\u52a1\u80fd\u8017\u9ad8\uff0c\u800c\u80fd\u6e90\u6709\u9650\u4e14\u5bf9\u73af\u5883\u6709\u6e29\u5ba4\u6548\u5e94\uff0c\u4e9f\u9700\u5f00\u53d1\u80fd\u6548\u66f4\u9ad8\u7684\u7b97\u6cd5\uff1b\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5e73\u5747\u8d44\u6e90\u5229\u7528\u7387\u6216\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u4f46\u5ffd\u7565\u4e86\u7269\u7406\u673a\u4e2d\u591a\u79cd\u8d44\u6e90\u7c7b\u578b\u7684\u534f\u540c\u5229\u7528\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5f15\u5165\u4e00\u4e2a\u7efc\u5408CPU\u3001\u78c1\u76d8\u3001I/O\u5229\u7528\u7387\u53ca\u4efb\u52a1\u5904\u7406\u65f6\u95f4\u7684\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u4ee5\u663e\u5f0f\u63d0\u5347\u5404\u7c7b\u8d44\u6e90\u7684\u5229\u7528\u7387\uff0c\u8fdb\u800c\u63d0\u9ad8\u6d3b\u8dc3\u8d44\u6e90\u7684\u6574\u4f53\u4f7f\u7528\u6548\u7387\u3002", "result": "\u901a\u8fc7\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5bf9\u6240\u63d0\u7b97\u6cd5\u4e0eMaxUtil\u7b97\u6cd5\u8fdb\u884c\u4eff\u771f\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u7b97\u6cd5\u5728\u80fd\u8017\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5177\u6709\u66f4\u597d\u7684\u80fd\u6548\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u591a\u7ef4\u8d44\u6e90\u5229\u7528\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e91\u6570\u636e\u4e2d\u5fc3\u7684\u80fd\u8017\uff0c\u4f18\u4e8e\u73b0\u6709MaxUtil\u7b97\u6cd5\u3002"}}
{"id": "2512.11001", "categories": ["cs.DB", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.11001", "abs": "https://arxiv.org/abs/2512.11001", "authors": ["Zoi Kaoudi", "Ioana Giurgiu"], "title": "Query Optimization Beyond Data Systems: The Case for Multi-Agent Systems", "comment": null, "summary": "The proliferation of large language models (LLMs) has accelerated the adoption of agent-based workflows, where multiple autonomous agents reason, invoke functions, and collaborate to compose complex data pipelines. However, current approaches to building such agentic architectures remain largely ad hoc, lacking generality, scalability, and systematic optimization. Existing systems often rely on fixed models and single execution engines and are unable to efficiently optimize multiple agents operating over heterogeneous data sources and query engines. This paper presents a vision for a next-generation query optimization framework tailored to multi-agent workflows. We argue that optimizing these workflows can benefit from redesigning query optimization principles to account for new challenges: orchestration of diverse agents, cost efficiency under expensive LLM calls and across heterogeneous engines, and redundancy across tasks. Led by a real-world example and building on an analysis of multi-agent workflows, we outline our envisioned architecture and the main research challenges of building a multi-agent query optimization framework, which aims at enabling automated model selection, workflow composition, and execution across heterogeneous engines. This vision establishes the groundwork for query optimization in emerging multi-agent architectures and opens up a set of future research directions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u65b0\u578b\u67e5\u8be2\u4f18\u5316\u6846\u67b6\u6784\u60f3\uff0c\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u667a\u80fd\u4f53\u67b6\u6784\u5728\u901a\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u7cfb\u7edf\u6027\u4f18\u5316\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u6784\u5efa\u65b9\u6cd5\u8f83\u4e3a\u968f\u610f\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3001\u53ef\u6269\u5c55\u6027\u53ca\u7cfb\u7edf\u6027\u4f18\u5316\u80fd\u529b\uff0c\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u5f02\u6784\u6570\u636e\u6e90\u548c\u67e5\u8be2\u5f15\u64ce\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u534f\u540c\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u7279\u70b9\uff0c\u7ed3\u5408\u73b0\u5b9e\u6848\u4f8b\uff0c\u91cd\u65b0\u8bbe\u8ba1\u67e5\u8be2\u4f18\u5316\u539f\u5219\uff0c\u63d0\u51fa\u4e00\u4e2a\u652f\u6301\u81ea\u52a8\u5316\u6a21\u578b\u9009\u62e9\u3001\u5de5\u4f5c\u6d41\u7ec4\u5408\u4e0e\u8de8\u5f02\u6784\u5f15\u64ce\u6267\u884c\u7684\u591a\u667a\u80fd\u4f53\u67e5\u8be2\u4f18\u5316\u6846\u67b6\u6784\u60f3\u3002", "result": "\u8bba\u6587\u52fe\u52d2\u51fa\u8be5\u6846\u67b6\u7684\u6838\u5fc3\u67b6\u6784\uff0c\u5e76\u8bc6\u522b\u4e86\u5b9e\u73b0\u8fc7\u7a0b\u4e2d\u9762\u4e34\u7684\u4e3b\u8981\u7814\u7a76\u6311\u6218\uff0c\u5305\u62ec\u591a\u6837\u5316\u667a\u80fd\u4f53\u7684\u7f16\u6392\u3001\u6602\u8d35LLM\u8c03\u7528\u4e0b\u7684\u6210\u672c\u6548\u7387\u4ee5\u53ca\u4efb\u52a1\u5197\u4f59\u7b49\u95ee\u9898\u3002", "conclusion": "\u8be5\u6784\u60f3\u4e3a\u65b0\u5174\u591a\u667a\u80fd\u4f53\u67b6\u6784\u4e2d\u7684\u67e5\u8be2\u4f18\u5316\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u6307\u660e\u4e86\u82e5\u5e72\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.11223", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11223", "abs": "https://arxiv.org/abs/2512.11223", "authors": ["Sasara Shimizu", "Yoshiki Higo"], "title": "Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests", "comment": null, "summary": "The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u4e0e\u624b\u52a8\u7f16\u5199\u6d4b\u8bd5\u7528\u4f8b\u5728\u5206\u652f\u8986\u76d6\u7387\u548c\u57fa\u4e8e\u9891\u8c31\u7684\u6545\u969c\u5b9a\u4f4d\uff08SBFL\uff09\u5f97\u5206\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u8986\u76d6\u66f4\u9ad8\u4f46SBFL\u5f97\u5206\u8f83\u4f4e\uff0c\u5c24\u5176\u5728\u6df1\u5c42\u5d4c\u5957\u4ee3\u7801\u4e2d\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6d4b\u8bd5\u7528\u4f8b\u7684\u8986\u76d6\u7387\uff0c\u8f83\u5c11\u8bc4\u4f30\u5176\u5728\u6545\u969c\u5b9a\u4f4d\uff08\u7279\u522b\u662f\u901a\u8fc7\u53d8\u5f02\u6d4b\u8bd5\u5f15\u5165\u7684\u4eba\u5de5\u6545\u969c\uff09\u4e2d\u7684\u6709\u6548\u6027\uff1b\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165SBFL\u5f97\u5206\u8fd9\u4e00\u8f83\u5c11\u4f7f\u7528\u7684\u6307\u6807\uff0c\u6df1\u5165\u6bd4\u8f83\u4e24\u7c7b\u6d4b\u8bd5\u65b9\u6cd5\u7684\u4f18\u52a3\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u81ea\u52a8\u751f\u6210\u6d4b\u8bd5\u4e0e\u624b\u52a8\u6d4b\u8bd5\u5728\u5206\u652f\u8986\u76d6\u7387\u548cSBFL\u5f97\u5206\u4e0a\u7684\u5dee\u5f02\uff0c\u4f7f\u7528SBFL\u5f97\u5206\u4f5c\u4e3a\u8861\u91cf\u6545\u969c\u5b9a\u4f4d\u51c6\u786e\u6027\u7684\u4e3b\u8981\u6307\u6807\u3002", "result": "\u81ea\u52a8\u751f\u6210\u7684\u6d4b\u8bd5\u7528\u4f8b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5206\u652f\u8986\u76d6\u7387\uff0c\u4f46\u5728SBFL\u5f97\u5206\u4e0a\u4f4e\u4e8e\u624b\u52a8\u6d4b\u8bd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u6df1\u5c42\u5d4c\u5957\u7ed3\u6784\u7684\u4ee3\u7801\u65f6\u8868\u73b0\u66f4\u5dee\u3002", "conclusion": "\u81ea\u52a8\u751f\u6210\u548c\u624b\u52a8\u521b\u5efa\u7684\u6d4b\u8bd5\u5404\u6709\u4f18\u52bf\uff0c\u5e94\u7ed3\u5408\u4f7f\u7528\u4ee5\u63d0\u5347\u6574\u4f53\u6d4b\u8bd5\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u9ad8\u6545\u969c\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2512.11412", "categories": ["cs.CE", "cs.AI", "cs.CL", "cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2512.11412", "abs": "https://arxiv.org/abs/2512.11412", "authors": ["Kwun Sy Lee", "Jiawei Chen", "Fuk Sheng Ford Chung", "Tianyu Zhao", "Zhenyuan Chen", "Debby D. Wang"], "title": "Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models", "comment": "6 pages, 4 figures", "summary": "Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5171\u4eab\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u4e0a\u5f15\u5165\u5e26L1\u7a00\u758f\u60e9\u7f5a\u7684\u4efb\u52a1\u7279\u5b9a\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5728\u63d0\u5347\u6bd2\u6027\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5206\u5b50\u6bd2\u6027\u9884\u6d4b\u6a21\u578b\u591a\u4e3a\u9ed1\u7bb1\u6a21\u578b\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u96be\u4ee5\u6ee1\u8db3\u836f\u7269\u7814\u53d1\u4e2d\u5bf9\u9ad8\u98ce\u9669\u5b89\u5168\u51b3\u7b56\u6240\u9700\u7684\u7ed3\u6784\u5316\u4f9d\u636e\uff0c\u56e0\u6b64\u4e9f\u9700\u517c\u5177\u9ad8\u51c6\u786e\u7387\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u5171\u4eab\u7684\u5316\u5b66\u8bed\u8a00\u6a21\u578b\u4e0e\u4efb\u52a1\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u5e76\u5728\u6ce8\u610f\u529b\u6743\u91cd\u4e0a\u65bd\u52a0L1\u7a00\u758f\u6027\u7ea6\u675f\uff0c\u4f7f\u6a21\u578b\u805a\u7126\u4e8e\u6bcf\u4e2a\u6bd2\u6027\u7ec8\u70b9\u6700\u5173\u952e\u7684\u5206\u5b50\u7247\u6bb5\u3002", "result": "\u5728ClinTox\u3001SIDER\u548cTox21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u5355\u4efb\u52a1\u548c\u6807\u51c6\u591a\u4efb\u52a1\u5b66\u4e60\u57fa\u7ebf\uff0c\u540c\u65f6\u751f\u6210\u7684\u7a00\u758f\u6ce8\u610f\u529b\u6743\u91cd\u80fd\u63d0\u4f9b\u76f4\u89c2\u7684\u5316\u5b66\u7247\u6bb5\u53ef\u89c6\u5316\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7a00\u758f\u6ce8\u610f\u529b\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\u6709\u6548\u5e73\u8861\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u6bd2\u6027\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u900f\u660e\u7684\u5de5\u5177\u3002"}}
{"id": "2512.11067", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11067", "abs": "https://arxiv.org/abs/2512.11067", "authors": ["Guorui Xiao", "Enhao Zhang", "Nicole Sullivan", "Will Hansen", "Magdalena Balazinska"], "title": "KathDB: Explainable Multimodal Database Management System with Human-AI Collaboration", "comment": null, "summary": "Traditional DBMSs execute user- or application-provided SQL queries over relational data with strong semantic guarantees and advanced query optimization, but writing complex SQL is hard and focuses only on structured tables. Contemporary multimodal systems (which operate over relations but also text, images, and even videos) either expose low-level controls that force users to use (and possibly create) machine learning UDFs manually within SQL or offload execution entirely to black-box LLMs, sacrificing usability or explainability. We propose KathDB, a new system that combines relational semantics with the reasoning power of foundation models over multimodal data. Furthermore, KathDB includes human-AI interaction channels during query parsing, execution, and result explanation, such that users can iteratively obtain explainable answers across data modalities.", "AI": {"tldr": "KathDB \u662f\u4e00\u4e2a\u878d\u5408\u5173\u7cfb\u578b\u6570\u636e\u5e93\u8bed\u4e49\u4e0e\u57fa\u7840\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u65b0\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u67e5\u8be2\uff0c\u5e76\u5728\u67e5\u8be2\u89e3\u6790\u3001\u6267\u884c\u548c\u7ed3\u679c\u89e3\u91ca\u8fc7\u7a0b\u4e2d\u5f15\u5165\u4eba\u673a\u4ea4\u4e92\u673a\u5236\uff0c\u4ee5\u63d0\u4f9b\u53ef\u89e3\u91ca\u4e14\u8de8\u6a21\u6001\u7684\u7b54\u6848\u3002", "motivation": "\u4f20\u7edf DBMS \u867d\u7136\u5bf9\u7ed3\u6784\u5316\u6570\u636e\u63d0\u4f9b\u5f3a\u8bed\u4e49\u4fdd\u8bc1\u548c\u9ad8\u7ea7\u67e5\u8be2\u4f18\u5316\uff0c\u4f46\u96be\u4ee5\u7f16\u5199\u590d\u6742 SQL\uff1b\u800c\u73b0\u6709\u652f\u6301\u591a\u6a21\u6001\u6570\u636e\u7684\u7cfb\u7edf\u8981\u4e48\u4f9d\u8d56\u7528\u6237\u624b\u52a8\u7f16\u5199\u673a\u5668\u5b66\u4e60 UDF\uff0c\u8981\u4e48\u5b8c\u5168\u4f9d\u8d56\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u727a\u7272\u4e86\u53ef\u7528\u6027\u6216\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa KathDB \u7cfb\u7edf\uff0c\u5c06\u5173\u7cfb\u578b\u8bed\u4e49\u4e0e\u57fa\u7840\u6a21\u578b\u5728\u591a\u6a21\u6001\u6570\u636e\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5e76\u5728\u67e5\u8be2\u89e3\u6790\u3001\u6267\u884c\u548c\u7ed3\u679c\u89e3\u91ca\u9636\u6bb5\u5d4c\u5165\u4eba\u673a\u4ea4\u4e92\u901a\u9053\u3002", "result": "\u7528\u6237\u80fd\u591f\u901a\u8fc7\u8fed\u4ee3\u65b9\u5f0f\u83b7\u5f97\u8de8\u6a21\u6001\u3001\u53ef\u89e3\u91ca\u7684\u67e5\u8be2\u7ed3\u679c\uff0c\u517c\u987e\u53ef\u7528\u6027\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "KathDB \u6709\u6548\u5f25\u5408\u4e86\u4f20\u7edf\u5173\u7cfb\u578b\u6570\u636e\u5e93\u4e0e\u73b0\u4ee3\u591a\u6a21\u6001\u7cfb\u7edf\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5728\u4fdd\u6301\u8bed\u4e49\u4e25\u8c28\u6027\u7684\u540c\u65f6\u63d0\u5347\u4e86\u591a\u6a21\u6001\u67e5\u8be2\u7684\u4ea4\u4e92\u6027\u4e0e\u900f\u660e\u5ea6\u3002"}}
{"id": "2512.11473", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.11473", "abs": "https://arxiv.org/abs/2512.11473", "authors": ["Fan Gu", "Xiangyu Hu"], "title": "Contiguous Storage of Grid Data for Heterogeneous Computing", "comment": "26 pages, 6 figures and 1 table", "summary": "Structured Cartesian grids are a fundamental component in numerical simulations. Although these grids facilitate straightforward discretization schemes, their na\u00efve use in sparse domains leads to excessive memory overhead and inefficient computation. Existing frameworks address are primarily optimized for CPU execution and exhibit performance bottlenecks on GPU architectures due to limited parallelism and high memory access latency. This work presents a redesigned storage architecture optimized for GPU compatibility and efficient execution across heterogeneous platforms. By abstracting low-level GPU-specific details and adopting a unified programming model based on SYCL, the proposed data structure enables seamless integration across host and device environments. This architecture simplifies GPU programming for end-users while improving scalability and portability in sparse-grid and gird-particle coupling numerical simulations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411GPU\u4f18\u5316\u7684\u7ed3\u6784\u5316\u7b1b\u5361\u5c14\u7f51\u683c\u5b58\u50a8\u67b6\u6784\uff0c\u901a\u8fc7SYCL\u7edf\u4e00\u7f16\u7a0b\u6a21\u578b\u63d0\u5347\u7a00\u758f\u7f51\u683c\u53ca\u7f51\u683c-\u7c92\u5b50\u8026\u5408\u6a21\u62df\u5728\u5f02\u6784\u5e73\u53f0\u4e0a\u7684\u6027\u80fd\u4e0e\u53ef\u79fb\u690d\u6027\u3002", "motivation": "\u73b0\u6709\u7ed3\u6784\u5316\u7b1b\u5361\u5c14\u7f51\u683c\u6846\u67b6\u4e3b\u8981\u9488\u5bf9CPU\u4f18\u5316\uff0c\u5728GPU\u4e0a\u56e0\u5e76\u884c\u6027\u53d7\u9650\u548c\u5185\u5b58\u8bbf\u95ee\u5ef6\u8fdf\u9ad8\u800c\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u9ad8\u6548\u652f\u6301\u7a00\u758f\u57df\u4e2d\u7684\u6570\u503c\u6a21\u62df\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684GPU\u517c\u5bb9\u5b58\u50a8\u67b6\u6784\uff0c\u62bd\u8c61\u5e95\u5c42GPU\u7ec6\u8282\uff0c\u91c7\u7528\u57fa\u4e8eSYCL\u7684\u7edf\u4e00\u7f16\u7a0b\u6a21\u578b\uff0c\u5b9e\u73b0\u4e3b\u673a\u4e0e\u8bbe\u5907\u73af\u5883\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u6240\u63d0\u67b6\u6784\u7b80\u5316\u4e86\u7528\u6237GPU\u7f16\u7a0b\uff0c\u540c\u65f6\u5728\u7a00\u758f\u7f51\u683c\u548c\u7f51\u683c-\u7c92\u5b50\u8026\u5408\u6570\u503c\u6a21\u62df\u4e2d\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8de8\u5e73\u53f0\u53ef\u79fb\u690d\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u7ed3\u6784\u5316\u7f51\u683c\u5728GPU\u4e0a\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4e3a\u5f02\u6784\u8ba1\u7b97\u73af\u5883\u4e0b\u7684\u9ad8\u6027\u80fd\u6570\u503c\u6a21\u62df\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u79fb\u690d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10979", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10979", "abs": "https://arxiv.org/abs/2512.10979", "authors": ["Sima Attar-Khorasani", "Lincoln Sherpa", "Matthias Lieber", "Siavash Ghiasvand"], "title": "Seamless Transitions: A Comprehensive Review of Live Migration Technologies", "comment": "35 pages, 0 figures", "summary": "Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5b9e\u65f6\u8fc1\u79fb\u6280\u672f\uff0c\u6574\u5408\u73b0\u6709\u7814\u7a76\u5e76\u6df1\u5165\u5206\u6790\u5bb9\u5668\u4e0e\u865a\u62df\u673a\u8fc1\u79fb\u65b9\u6cd5\u3001\u8fc1\u79fb\u5355\u5143\u53ca\u57fa\u7840\u8bbe\u65bd\u7279\u6027\uff0c\u63a2\u8ba8\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u4e0e\u9002\u7528\u6027\u5dee\u5f02\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7efc\u8ff0\u5e38\u5ffd\u7565\u5b9e\u65f6\u8fc1\u79fb\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9762\u4e34\u7684\u5173\u952e\u6280\u672f\u7ec6\u8282\u548c\u5b9e\u8df5\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u7efc\u5408\u73b0\u6709\u6587\u732e\uff0c\u4ece\u8fc1\u79fb\u6280\u672f\u3001\u8fc1\u79fb\u5355\u5143\u548c\u57fa\u7840\u8bbe\u65bd\u7279\u5f81\u7b49\u591a\u4e2a\u7ef4\u5ea6\u5bf9\u57fa\u4e8e\u5bb9\u5668\u548c\u865a\u62df\u673a\u7684\u5b9e\u65f6\u8fc1\u79fb\u6280\u672f\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u76ee\u6807\u548c\u7ea6\u675f\u4e0b\u7684\u9002\u7528\u6027\u3002", "result": "\u63ed\u793a\u4e86\u5bb9\u5668\u4e0e\u865a\u62df\u673a\u8fc1\u79fb\u6280\u672f\u5728\u91c7\u7eb3\u7a0b\u5ea6\u4e0a\u7684\u5dee\u8ddd\uff0c\u9610\u660e\u4e86\u7cfb\u7edf\u56e0\u7d20\u5bf9\u8fc1\u79fb\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u6307\u51fa\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fc1\u79fb\u7684\u590d\u6742\u6027\u548c\u8d44\u6e90\u5f00\u9500\u53ef\u80fd\u8d85\u8fc7\u5176\u6536\u76ca\u3002", "conclusion": "\u672c\u6587\u4e0d\u4ec5\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5173\u4e8e\u5b9e\u65f6\u8fc1\u79fb\u7684\u5168\u9762\u53c2\u8003\uff0c\u8fd8\u901a\u8fc7\u8bc6\u522b\u5f53\u524d\u6280\u672f\u74f6\u9888\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u4e0e\u5b9e\u9645\u90e8\u7f72\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.11129", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.11129", "abs": "https://arxiv.org/abs/2512.11129", "authors": ["Mahmoud Abo Khamis", "Alexandru-Mihai Hurjui", "Ahmet Kara", "Dan Olteanu", "Dan Suciu"], "title": "Acyclic Conjunctive Regular Path Queries are no Harder than Corresponding Conjunctive Queries", "comment": null, "summary": "We present an output-sensitive algorithm for evaluating an acyclic Conjunctive Regular Path Query (CRPQ). Its complexity is written in terms of the input size, the output size, and a well-known parameter of the query that is called the \"free-connex fractional hypertree width\". Our algorithm improves upon the complexity of the recently introduced output-sensitive algorithm for acyclic CRPQs. More notably, the complexity of our algorithm for a given acyclic CRPQ Q matches the best known output-sensitive complexity for the \"corresponding\" conjunctive query (CQ), that is the CQ that has the same structure as the CRPQ Q except that each RPQ is replaced with a binary atom (or a join of two binary atoms). This implies that it is not possible to improve upon our complexity for acyclic CRPQs without improving the state-of-the-art on output-sensitive evaluation for acyclic CQs. Our result is surprising because RPQs, and by extension CRPQs, are equivalent to recursive Datalog programs, which are generally poorly understood from a complexity standpoint. Yet, our result implies that the recursion aspect of acyclic CRPQs does not add any extra complexity on top of the corresponding (non-recursive) CQs, at least as far as output-sensitive analysis is concerned.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8bc4\u4f30\u65e0\u73af\u8fde\u63a5\u6b63\u5219\u8def\u5f84\u67e5\u8be2\uff08CRPQ\uff09\u7684\u8f93\u51fa\u654f\u611f\u7b97\u6cd5\uff0c\u5176\u590d\u6742\u5ea6\u4e0e\u5bf9\u5e94\u8fde\u63a5\u67e5\u8be2\uff08CQ\uff09\u7684\u6700\u4f73\u5df2\u77e5\u8f93\u51fa\u654f\u611f\u590d\u6742\u5ea6\u4e00\u81f4\uff0c\u8868\u660eCRPQ\u4e2d\u7684\u9012\u5f52\u7279\u6027\u5728\u8f93\u51fa\u654f\u611f\u5206\u6790\u4e0b\u672a\u5f15\u5165\u989d\u5916\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u65e0\u73afCRPQ\u7684\u8f93\u51fa\u654f\u611f\u7b97\u6cd5\u5728\u590d\u6742\u5ea6\u4e0a\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff1b\u540c\u65f6\uff0c\u7531\u4e8eCRPQ\u7b49\u4ef7\u4e8e\u9012\u5f52Datalog\u7a0b\u5e8f\uff0c\u5176\u590d\u6742\u6027\u5c1a\u4e0d\u6e05\u6670\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u63a2\u7a76\u5176\u662f\u5426\u6bd4\u5bf9\u5e94\u7684\u975e\u9012\u5f52CQ\u66f4\u590d\u6742\u3002", "method": "\u8bbe\u8ba1\u4e00\u79cd\u65b0\u7684\u8f93\u51fa\u654f\u611f\u7b97\u6cd5\uff0c\u5176\u590d\u6742\u5ea6\u4f9d\u8d56\u4e8e\u8f93\u5165\u5927\u5c0f\u3001\u8f93\u51fa\u5927\u5c0f\u4ee5\u53ca\u67e5\u8be2\u7684\u201cfree-connex fractional hypertree width\u201d\u53c2\u6570\uff0c\u5e76\u5c06\u5176\u4e0e\u5bf9\u5e94CQ\u7684\u590d\u6742\u5ea6\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u6240\u63d0\u7b97\u6cd5\u5728\u590d\u6742\u5ea6\u4e0a\u4f18\u4e8e\u8fd1\u671f\u63d0\u51fa\u7684\u540c\u7c7b\u7b97\u6cd5\uff0c\u4e14\u5bf9\u4e8e\u4efb\u610f\u65e0\u73afCRPQ Q\uff0c\u5176\u590d\u6742\u5ea6\u4e0e\u7ed3\u6784\u5bf9\u5e94\u7684CQ\u7684\u6700\u4f73\u8f93\u51fa\u654f\u611f\u590d\u6742\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u65e0\u73afCRPQ\u5728\u8f93\u51fa\u654f\u611f\u8bc4\u4f30\u4e2d\u5e76\u4e0d\u6bd4\u5176\u5bf9\u5e94\u7684\u975e\u9012\u5f52CQ\u66f4\u590d\u6742\uff0c\u8868\u660e\u9012\u5f52\u7279\u6027\u5728\u6b64\u7c7b\u67e5\u8be2\u4e2d\u672a\u5e26\u6765\u989d\u5916\u590d\u6742\u5ea6\uff1b\u4efb\u4f55\u5bf9CRPQ\u590d\u6742\u5ea6\u7684\u8fdb\u4e00\u6b65\u6539\u8fdb\u90fd\u5c06\u4f9d\u8d56\u4e8eCQ\u9886\u57df\u7684\u65b0\u7a81\u7834\u3002"}}
{"id": "2512.11402", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11402", "abs": "https://arxiv.org/abs/2512.11402", "authors": ["Aryan Gupta", "Y. Raghu Reddy"], "title": "REMODEL-LLM: Transforming C code to Java using LLMs", "comment": null, "summary": "The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e8619\u4e2a\u53c2\u6570\u5c0f\u4e8e200\u4ebf\u7684\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728C\u5230Java\u4ee3\u7801\u81ea\u52a8\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u4e0e\u89c4\u5219\u7ea6\u675f\u63d0\u793a\u7684\u6df7\u5408\u6d41\u7a0b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7edd\u5927\u591a\u6570\u6a21\u578b\u65e0\u6cd5\u751f\u6210\u53ef\u8fd0\u884c\u7684Java\u4ee3\u7801\uff0c\u4ec5\u4e09\u4e2a\u6a21\u578b\uff08phi4\u3001deepseek-coder-v2\u3001codeqwen\uff09\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u5904\u7406\u51fd\u6570\u6307\u9488\u3001sizeof\u548c\u679a\u4e3e\u7b49\u590d\u6742C\u8bed\u8a00\u7279\u6027\u65f6\u4ecd\u5b58\u5728\u660e\u663e\u5c40\u9650\u3002", "motivation": "C\u5230Java\u7684\u81ea\u52a8\u7ffb\u8bd1\u9762\u4e34\u8303\u5f0f\u5dee\u5f02\uff08\u8fc7\u7a0b\u5f0f vs \u9762\u5411\u5bf9\u8c61\uff09\u3001\u5185\u5b58\u6a21\u578b\u4e0d\u540c\uff08\u624b\u52a8\u6307\u9488 vs \u5783\u573e\u56de\u6536\uff09\u53ca\u6570\u636e\u7c7b\u578b\u4e0d\u517c\u5bb9\u7b49\u6839\u672c\u6027\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u4e9f\u9700\u63a2\u7d22\u8f7b\u91cf\u7ea7\u91cf\u5316LLM\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u4e0e\u74f6\u9888\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u6d41\u6c34\u7ebf\u65b9\u6cd5\uff1a\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u8fdb\u884c\u8bed\u4e49\u5206\u89e3\uff0c\u5e76\u7ed3\u5408\u9ad8\u5ea6\u7ea6\u675f\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5bf919\u4e2a\u5c0f\u578b\u91cf\u5316LLM\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u8868\u73b0\u5448\u73b0\u660e\u663e\u4e09\u7ea7\u5206\u5316\uff1a\u5927\u591a\u6570\u6a21\u578b\uff08Tier 3\uff09\u5b8c\u5168\u5931\u8d25\uff1b\u5c11\u6570\u4e2d\u7b49\u6a21\u578b\uff08Tier 2\uff09\u53ef\u751f\u6210\u53ef\u8fd0\u884c\u4ee3\u7801\u4f46\u5b58\u5728\u4e25\u91cd\u8bed\u4e49\u9519\u8bef\uff1b\u4ec5\u4e09\u4e2a\u6a21\u578b\uff08Tier 1\uff09\u901a\u8fc7\u8d8550%\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4f46\u4ecd\u65e0\u6cd5\u6b63\u786e\u5904\u7406\u51fd\u6570\u6307\u9488\u3001sizeof\u548cenum\u7b49\u590d\u6742C\u7279\u6027\u3002", "conclusion": "\u5f53\u524d\u7684\u5c0f\u578b\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728C\u5230Java\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u80fd\u529b\u6709\u9650\uff0c\u5373\u4f7f\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u4e5f\u96be\u4ee5\u5904\u7406C\u8bed\u8a00\u4e2d\u7684\u9ad8\u7ea7\u7279\u6027\uff0c\u63ed\u793a\u4e86\u5176\u5728\u590d\u6742\u4ee3\u7801\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u4e0a\u9650\u3002"}}
{"id": "2512.11748", "categories": ["cs.CE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11748", "abs": "https://arxiv.org/abs/2512.11748", "authors": ["Mohammed El Fallaki Idrissi", "Jad Mounayer", "Sebastian Rodriguez", "Fodil Meraghni", "Francisco Chinesta"], "title": "Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation", "comment": null, "summary": "This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u751f\u6210\u5f0f\u53c2\u6570\u5316\u8bbe\u8ba1\uff08GPD\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u4e2a\u79e9\u964d\u7ef4\u81ea\u7f16\u7801\u5668\uff08RRAE\uff09\u5206\u522b\u5904\u7406\u51e0\u4f55\u8bbe\u8ba1\u4e0e\u7a00\u758fPGD\u6a21\u6001\u89e3\uff0c\u5e76\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u56de\u5f52\u6280\u672f\u5b9e\u73b0\u4e8c\u8005\u5173\u8054\uff0c\u4ece\u800c\u652f\u6301\u9ad8\u6548\u7684\u8bbe\u8ba1\u63a2\u7d22\u3001\u4f18\u5316\u53ca\u6570\u5b57/\u6df7\u5408\u5b6a\u751f\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u4eff\u771f\u9a71\u52a8\u7684\u5de5\u7a0b\u8bbe\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u591a\u53c2\u6570\u8bbe\u8ba1\u4e0e\u5b9e\u65f6\u51b3\u7b56\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u7f3a\u4e4f\u5c06\u51e0\u4f55\u8bbe\u8ba1\u4e0e\u5176\u5bf9\u5e94\u7684\u53c2\u6570\u5316\u89e3\u9ad8\u6548\u8026\u5408\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u8bbe\u8ba1\u63a2\u7d22\u548c\u6570\u5b57\u5b6a\u751f\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faGPD\u6846\u67b6\uff0c\u5229\u7528\u4e24\u4e2aRRAE\u5206\u522b\u7f16\u7801\u751f\u6210\u51e0\u4f55\u8bbe\u8ba1\u548csPGD\u6a21\u6001\u89e3\uff0c\u5e76\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u901a\u8fc7\u56de\u5f52\u6280\u672f\u5efa\u7acb\u4e24\u8005\u6620\u5c04\u5173\u7cfb\uff0c\u5b9e\u73b0\u4ece\u8bbe\u8ba1\u5230\u53c2\u6570\u5316\u89e3\u7684\u5feb\u901f\u751f\u6210\u3002", "result": "\u8be5\u6846\u67b6\u6210\u529f\u5e94\u7528\u4e8e\u53cc\u76f8\u5fae\u7ed3\u6784\u95ee\u9898\uff0c\u80fd\u6709\u6548\u5904\u7406\u4e24\u4e2a\u5173\u952e\u6750\u6599\u53c2\u6570\u53d8\u5316\u4e0b\u7684\u591a\u53c2\u6570\u89e3\uff0c\u652f\u6301\u9ad8\u6548\u8bbe\u8ba1\u751f\u6210\u4e0e\u5b9e\u65f6\u9884\u6d4b\u3002", "conclusion": "GPD\u6846\u67b6\u4e3a\u4eff\u771f\u9a71\u52a8\u7684\u5de5\u7a0b\u79d1\u5b66\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bbe\u8ba1\u63a2\u7d22\u3001\u4f18\u5316\u80fd\u529b\u4ee5\u53ca\u6570\u5b57/\u6df7\u5408\u5b6a\u751f\u7cfb\u7edf\u7684\u9884\u6d4b\u4e0e\u51b3\u7b56\u6027\u80fd\u3002"}}
{"id": "2512.10980", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10980", "abs": "https://arxiv.org/abs/2512.10980", "authors": ["Akhmadillo Mamirov"], "title": "Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling", "comment": null, "summary": "GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9GPU\u96c6\u7fa4\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e09\u79cd\u52a8\u6001\u8c03\u5ea6\u5668\uff08HPS\u3001PBS\u3001SBS\uff09\uff0c\u5728\u771f\u5b9e\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u663e\u8457\u63d0\u5347\u5229\u7528\u7387\u3001\u541e\u5410\u91cf\u4e0e\u516c\u5e73\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u8c03\u5ea6\u7b56\u7565\u3002", "motivation": "\u73b0\u4ee3AI\u7cfb\u7edf\u4f9d\u8d56GPU\u96c6\u7fa4\uff0c\u4f46\u5b9e\u9645\u90e8\u7f72\u4e2d\u5e73\u5747\u5229\u7528\u7387\u4ec5\u7ea650%\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8d44\u6e90\u788e\u7247\u3001\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u53ca\u9759\u6001\u8c03\u5ea6\u7b56\u7565\u7684\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u5e76\u8bc4\u4f30\u4e09\u79cd\u4e13\u7528\u52a8\u6001\u8c03\u5ea6\u5668\uff1a\u6df7\u5408\u4f18\u5148\u7ea7\u8c03\u5ea6\u5668\uff08HPS\uff09\u3001\u9884\u6d4b\u56de\u586b\u8c03\u5ea6\u5668\uff08PBS\uff09\u548c\u667a\u80fd\u6279\u5904\u7406\u8c03\u5ea6\u5668\uff08SBS\uff09\uff0c\u5728\u5305\u542b\u8bad\u7ec3\u3001\u63a8\u7406\u548c\u7814\u7a76\u4efb\u52a1\u768464-GPU\u30018\u8282\u70b9\u96c6\u7fa4\u4e0a\u8fdb\u884c1000\u4e2aAI\u4f5c\u4e1a\u7684\u4eff\u771f\u6d4b\u8bd5\u3002", "result": "\u52a8\u6001\u8c03\u5ea6\u5668\u663e\u8457\u4f18\u4e8e\u9759\u6001\u57fa\u7ebf\uff08FIFO\u7b49\uff09\uff1aHPS\u5b9e\u73b0\u6700\u9ad8\u5229\u7528\u7387\uff0878.2%\uff09\u3001\u6700\u9ad8\u541e\u5410\u91cf\uff0825.8\u4f5c\u4e1a/\u5c0f\u65f6\uff09\u548c\u6700\u4f4e\u516c\u5e73\u6027\u65b9\u5dee\uff08457\uff09\uff0c\u9965\u997f\u4f5c\u4e1a\u4ece156\u964d\u81f312\uff1bPBS\u548cSBS\u5206\u522b\u5728\u788e\u7247\u5904\u7406\u548c\u7ed3\u6784\u76f8\u4f3c\u4f5c\u4e1a\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u9762\u5411\u5f02\u6784AI\u96c6\u7fa4\u7684\u591a\u76ee\u6807\u52a8\u6001\u8c03\u5ea6\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347GPU\u5229\u7528\u6548\u7387\uff0c\u5728\u541e\u5410\u91cf\u3001\u7b49\u5f85\u65f6\u95f4\u3001\u516c\u5e73\u6027\u548c\u9965\u997f\u63a7\u5236\u65b9\u9762\u5168\u9762\u4f18\u4e8e\u5355\u76ee\u6807\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u751f\u4ea7\u8c03\u5ea6\u6846\u67b6\u63d0\u4f9b\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2512.11161", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.11161", "abs": "https://arxiv.org/abs/2512.11161", "authors": ["Guanli Liu", "Renata Borovica-Gajic", "Hai Lan", "Zhifeng Bao"], "title": "Benchmarking RL-Enhanced Spatial Indices Against Traditional, Advanced, and Learned Counterparts", "comment": "Author accepted manuscript. Accepted at ICDE 2026. Publisher version will appear in the ICDE 2026 proceedings", "summary": "Reinforcement learning has recently been used to enhance index structures, giving rise to reinforcement learning-enhanced spatial indices (RLESIs) that aim to improve query efficiency during index construction. However, their practical benefits remain unclear due to the lack of unified implementations and comprehensive evaluations, especially in disk-based settings.\n  We present the first modular and extensible benchmark for RLESIs. Built on top of an existing spatial index library, our framework decouples index training from building, supports parameter tuning, and enables consistent comparison with traditional, advanced, and learned spatial indices.\n  We evaluate 12 representative spatial indices across six datasets and diverse workloads, including point, range, kNN, spatial join, and mixed read/write queries. Using latency, I/O, and index statistics as metrics, we find that while RLESIs can reduce query latency with tuning, they consistently underperform learned spatial indices and advanced variants in both query efficiency and index build cost. These findings highlight that although RLESIs offer promising architectural compatibility, their high tuning costs and limited generalization hinder practical adoption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7a7a\u95f4\u7d22\u5f15\uff08RLESIs\uff09\u7684\u6a21\u5757\u5316\u57fa\u51c6\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5168\u9762\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5c3d\u7ba1RLESIs\u5728\u8c03\u4f18\u540e\u53ef\u964d\u4f4e\u67e5\u8be2\u5ef6\u8fdf\uff0c\u4f46\u5728\u67e5\u8be2\u6548\u7387\u548c\u6784\u5efa\u5f00\u9500\u65b9\u9762\u666e\u904d\u4e0d\u5982\u5148\u8fdb\u4f20\u7edf\u7d22\u5f15\u548c\u5b66\u4e60\u578b\u7d22\u5f15\u3002", "motivation": "\u73b0\u6709RLESIs\u7f3a\u4e4f\u7edf\u4e00\u5b9e\u73b0\u4e0e\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5c24\u5176\u5728\u57fa\u4e8e\u78c1\u76d8\u7684\u73af\u5883\u4e2d\u5176\u5b9e\u9645\u4f18\u52bf\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5e73\u53f0\u4ee5\u5398\u6e05\u5176\u6027\u80fd\u8fb9\u754c\u4e0e\u9002\u7528\u573a\u666f\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u57fa\u4e8e\u73b0\u6709\u7a7a\u95f4\u7d22\u5f15\u5e93\uff0c\u5c06\u7d22\u5f15\u8bad\u7ec3\u4e0e\u6784\u5efa\u89e3\u8026\uff0c\u652f\u6301\u53c2\u6570\u8c03\u4f18\uff0c\u5e76\u5b9e\u73b0\u4e0e\u4f20\u7edf\u3001\u5148\u8fdb\u53ca\u5b66\u4e60\u578b\u7a7a\u95f4\u7d22\u5f15\u7684\u4e00\u81f4\u6027\u5bf9\u6bd4\uff1b\u5728\u516d\u4e2a\u6570\u636e\u96c6\u548c\u591a\u79cd\u67e5\u8be2\u8d1f\u8f7d\u4e0b\uff0c\u4f7f\u7528\u5ef6\u8fdf\u3001I/O\u548c\u7d22\u5f15\u7edf\u8ba1\u6307\u6807\u5bf912\u79cd\u4ee3\u8868\u6027\u7d22\u5f15\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRLESIs\u867d\u53ef\u901a\u8fc7\u8c03\u4f18\u51cf\u5c11\u67e5\u8be2\u5ef6\u8fdf\uff0c\u4f46\u5728\u67e5\u8be2\u6548\u7387\u548c\u7d22\u5f15\u6784\u5efa\u6210\u672c\u4e0a\u59cb\u7ec8\u900a\u4e8e\u5b66\u4e60\u578b\u548c\u5148\u8fdb\u4f20\u7edf\u7d22\u5f15\uff1b\u5176\u9ad8\u8c03\u4f18\u6210\u672c\u4e0e\u6cdb\u5316\u80fd\u529b\u6709\u9650\u5236\u7ea6\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "RLESIs\u867d\u5177\u5907\u826f\u597d\u7684\u67b6\u6784\u517c\u5bb9\u6027\uff0c\u4f46\u5f53\u524d\u5728\u6027\u80fd\u548c\u5b9e\u7528\u6027\u65b9\u9762\u5c1a\u672a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u672a\u6765\u9700\u964d\u4f4e\u8c03\u4f18\u5f00\u9500\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u4ee5\u63a8\u52a8\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.11482", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.11482", "abs": "https://arxiv.org/abs/2512.11482", "authors": ["Melih Catal", "Pooja Rani", "Harald C. Gall"], "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models", "comment": null, "summary": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u5728\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08CodeLLMs\uff09\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u53d1\u73b0DP\u80fd\u663e\u8457\u964d\u4f4e\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u8bb0\u5fc6\u98ce\u9669\uff0c\u540c\u65f6\u57fa\u672c\u4fdd\u6301\u751a\u81f3\u7565\u5fae\u63d0\u5347\u5176\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u4e14\u5bf9\u8bad\u7ec3\u6548\u7387\u548c\u80fd\u8017\u5f71\u54cd\u751a\u5fae\u3002", "motivation": "CodeLLMs\u53ef\u80fd\u65e0\u610f\u4e2d\u8bb0\u5fc6\u5e76\u590d\u73b0\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u5e26\u6765\u9690\u79c1\u6cc4\u9732\u548c\u77e5\u8bc6\u4ea7\u6743\u4fb5\u72af\u7684\u98ce\u9669\uff0c\u9650\u5236\u4e86\u5176\u5728\u654f\u611f\u9886\u57df\u7684\u90e8\u7f72\uff1b\u56e0\u6b64\u9700\u8981\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\u3002", "method": "\u5e94\u7528\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u6280\u672f\uff0c\u5728CodeLLMs\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u6821\u51c6\u566a\u58f0\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30DP\u5bf9\u51cf\u5c11\u8bb0\u5fc6\u884c\u4e3a\u548c\u4fdd\u7559\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "DP\u663e\u8457\u51cf\u5c11\u4e86\u5404\u7c7b\u4ee3\u7801\u7247\u6bb5\u7684\u8bb0\u5fc6\u73b0\u8c61\uff0c\u5c24\u5176\u5bf9\u6700\u6613\u88ab\u8bb0\u5fc6\u7684\u7247\u6bb5\u6548\u679c\u6700\u597d\uff1b\u867d\u7136\u7565\u5fae\u589e\u52a0\u56f0\u60d1\u5ea6\uff0c\u4f46\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5f97\u4ee5\u4fdd\u7559\u751a\u81f3\u589e\u5f3a\uff1b\u540c\u65f6DP\u5bf9\u8bad\u7ec3\u65f6\u95f4\u548c\u80fd\u8017\u5f71\u54cd\u4e0d\u5927\u3002", "conclusion": "\u5dee\u5206\u9690\u79c1\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u4fdd\u62a4\u9690\u79c1\u3001\u9632\u6b62\u77e5\u8bc6\u4ea7\u6743\u4fb5\u6743\u7684\u540c\u65f6\uff0c\u7ef4\u6301CodeLLMs\u7684\u5b9e\u7528\u6027\u80fd\uff0c\u9002\u5408\u7528\u4e8e\u9690\u79c1\u4fdd\u62a4\u573a\u666f\u4e0b\u7684\u6a21\u578b\u8bad\u7ec3\u3002"}}
{"id": "2512.10987", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10987", "abs": "https://arxiv.org/abs/2512.10987", "authors": ["Sumit Chongder"], "title": "Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems", "comment": "13 pages, 14 figures, 2 tables", "summary": "In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u5206\u5c42\u8054\u90a6\u5b66\u4e60\uff08HFL\uff09\u4e0e\u4e24\u79cd\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\u2014\u2014\u805a\u5408\u8054\u90a6\u5b66\u4e60\uff08AFL\uff09\u548c\u6301\u7eed\u8054\u90a6\u5b66\u4e60\uff08CFL\uff09\uff0c\u53d1\u73b0AFL\u548cCFL\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8eHFL\u3002", "motivation": "\u96c6\u4e2d\u5f0fHFL\u5b58\u5728\u901a\u4fe1\u74f6\u9888\u548c\u9690\u79c1\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u53bb\u4e2d\u5fc3\u5316\u7684\u66ff\u4ee3\u65b9\u6848\u4ee5\u63d0\u5347\u8054\u90a6\u5b66\u4e60\u7684\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u3002", "method": "\u5728Fashion MNIST\u548cMNIST\u6570\u636e\u96c6\u4e0a\u5bf9HFL\u3001AFL\u548cCFL\u4e09\u79cd\u67b6\u6784\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6bd4\u8f83\u5176\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548c\u5e73\u8861\u51c6\u786e\u7387\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "AFL\u548cCFL\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u4f18\u4e8eHFL\uff0c\u8868\u660e\u53bb\u4e2d\u5fc3\u5316\u805a\u5408\u673a\u5236\u80fd\u66f4\u6709\u6548\u5730\u652f\u6301\u5206\u5e03\u5f0f\u8bbe\u5907\u95f4\u7684\u534f\u540c\u6a21\u578b\u8bad\u7ec3\u3002", "conclusion": "\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982AFL\u548cCFL\uff09\u5728\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2512.11363", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.11363", "abs": "https://arxiv.org/abs/2512.11363", "authors": ["Junyi Fan", "Li Sun"], "title": "A Cross-Chain Event-Driven Data Infrastructure for Aave Protocol Analytics and Applications", "comment": "12 pages", "summary": "Decentralized lending protocols, exemplified by Aave V3, have transformed financial intermediation by enabling permissionless, multi-chain borrowing and lending without intermediaries. Despite managing over $10 billion in total value locked, empirical research remains severely constrained by the lack of standardized, cross-chain event-level datasets.\n  This paper introduces the first comprehensive, event-driven data infrastructure for Aave V3 spanning six major EVM-compatible chains (Ethereum, Arbitrum, Optimism, Polygon, Avalanche, and Base) from respective deployment blocks through October 2025. We collect and fully decode eight core event types -- Supply, Borrow, Withdraw, Repay, LiquidationCall, FlashLoan, ReserveDataUpdated, and MintedToTreasury -- producing over 50 million structured records enriched with block metadata and USD valuations.\n  Using an open-source Python pipeline with dynamic batch sizing and automatic sharding (each file less than or equal to 1 million rows), we ensure strict chronological ordering and full reproducibility. The resulting publicly available dataset enables granular analysis of capital flows, interest rate dynamics, liquidation cascades, and cross-chain user behavior, providing a foundational resource for future studies on decentralized lending markets and systemic risk.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u9996\u4e2a\u8986\u76d6\u516d\u6761\u4e3b\u6d41EVM\u94fe\u7684Aave V3\u4e8b\u4ef6\u7ea7\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc75000\u4e07\u6761\u7ed3\u6784\u5316\u8bb0\u5f55\uff0c\u652f\u6301\u5bf9\u53bb\u4e2d\u5fc3\u5316\u501f\u8d37\u5e02\u573a\u7684\u7ec6\u7c92\u5ea6\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u53bb\u4e2d\u5fc3\u5316\u501f\u8d37\u534f\u8bae\uff08\u5982Aave V3\uff09\u7684\u5b9e\u8bc1\u7814\u7a76\u53d7\u9650\u4e8e\u7f3a\u4e4f\u6807\u51c6\u5316\u3001\u8de8\u94fe\u7684\u4e8b\u4ef6\u7ea7\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u5bf9\u8d44\u672c\u6d41\u52a8\u3001\u5229\u7387\u673a\u5236\u548c\u7cfb\u7edf\u6027\u98ce\u9669\u7b49\u5173\u952e\u95ee\u9898\u7684\u6df1\u5165\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5f00\u6e90Python\u7ba1\u9053\uff0c\u4ece\u5404\u94fe\u90e8\u7f72\u533a\u5757\u81f32025\u5e7410\u6708\u91c7\u96c6\u5e76\u5b8c\u6574\u89e3\u7801Aave V3\u7684\u516b\u7c7b\u6838\u5fc3\u4e8b\u4ef6\uff0c\u7ed3\u5408\u52a8\u6001\u6279\u5904\u7406\u4e0e\u81ea\u52a8\u5206\u7247\u7b56\u7565\uff0c\u751f\u6210\u5e26\u533a\u5757\u5143\u6570\u636e\u548c\u7f8e\u5143\u4f30\u503c\u7684\u7ed3\u6784\u5316\u6570\u636e\u96c6\u3002", "result": "\u6210\u529f\u6784\u5efa\u6db5\u76d6\u4ee5\u592a\u574a\u3001Arbitrum\u3001Optimism\u3001Polygon\u3001Avalanche\u548cBase\u516d\u6761\u94fe\u7684\u9ad8\u8d28\u91cf\u3001\u53ef\u590d\u73b0\u3001\u516c\u5f00\u53ef\u7528\u7684Aave V3\u4e8b\u4ef6\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d855000\u4e07\u6761\u8bb0\u5f55\u3002", "conclusion": "\u8be5\u6570\u636e\u57fa\u7840\u8bbe\u65bd\u4e3a\u7814\u7a76\u53bb\u4e2d\u5fc3\u5316\u501f\u8d37\u5e02\u573a\u4e2d\u7684\u8d44\u91d1\u6d41\u3001\u5229\u7387\u52a8\u6001\u3001\u6e05\u7b97\u7ea7\u8054\u53ca\u8de8\u94fe\u7528\u6237\u884c\u4e3a\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u6709\u671b\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u5b9e\u8bc1\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2512.11403", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.11403", "abs": "https://arxiv.org/abs/2512.11403", "authors": ["Jacques Chabin", "Mirian Halfeld Ferrari", "Nicolas Hiot"], "title": "Bridging Textual Data and Conceptual Models: A Model-Agnostic Structuring Approach", "comment": "Awarded Best Paper Award from BDA 2025 committee", "summary": "We introduce an automated method for structuring textual data into a model-agnostic schema, enabling alignment with any database model. It generates both a schema and its instance. Initially, textual data is represented as semantically enriched syntax trees, which are then refined through iterative tree rewriting and grammar extraction, guided by the attribute grammar meta-model \\metaG. The applicability of this approach is demonstrated using clinical medical cases as a proof of concept.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u5c06\u6587\u672c\u6570\u636e\u7ed3\u6784\u5316\u4e3a\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u6a21\u5f0f\uff0c\u5e76\u540c\u65f6\u751f\u6210\u8be5\u6a21\u5f0f\u53ca\u5176\u5b9e\u4f8b\uff0c\u4ee5\u4e34\u5e8a\u533b\u5b66\u6848\u4f8b\u4f5c\u4e3a\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u6570\u636e\u7f3a\u4e4f\u901a\u7528\u4e14\u81ea\u52a8\u5316\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u96be\u4ee5\u9002\u914d\u4e0d\u540c\u6570\u636e\u5e93\u6a21\u578b\uff1b\u4f5c\u8005\u65e8\u5728\u6784\u5efa\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u65b9\u6848\uff0c\u5b9e\u73b0\u6587\u672c\u5230\u7ed3\u6784\u5316\u6570\u636e\u7684\u901a\u7528\u8f6c\u6362\u3002", "method": "\u5c06\u6587\u672c\u6570\u636e\u8868\u793a\u4e3a\u8bed\u4e49\u589e\u5f3a\u7684\u8bed\u6cd5\u6811\uff0c\u901a\u8fc7\u57fa\u4e8e\u5c5e\u6027\u6587\u6cd5\u5143\u6a21\u578b \\metaG \u7684\u8fed\u4ee3\u6811\u91cd\u5199\u548c\u6587\u6cd5\u63d0\u53d6\uff0c\u9010\u6b65\u7cbe\u70bc\u751f\u6210\u901a\u7528\u6a21\u5f0f\u53ca\u5176\u5b9e\u4f8b\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u5e94\u7528\u4e8e\u4e34\u5e8a\u533b\u5b66\u6848\u4f8b\uff0c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u751f\u6210\u4e0e\u4efb\u610f\u6570\u636e\u5e93\u6a21\u578b\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u6a21\u5f0f\u53ca\u5176\u5b9e\u4f8b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u81ea\u52a8\u5316\u7684\u6587\u672c\u7ed3\u6784\u5316\u9014\u5f84\uff0c\u5177\u5907\u826f\u597d\u7684\u6a21\u578b\u65e0\u5173\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5982\u533b\u7597\u7b49\u590d\u6742\u9886\u57df\u3002"}}
{"id": "2512.11200", "categories": ["cs.DC", "cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.11200", "abs": "https://arxiv.org/abs/2512.11200", "authors": ["Adilet Metinov", "Gulida M. Kudakeeva", "Gulnara D. Kabaeva"], "title": "Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration", "comment": "9 pages , 2 tables", "summary": "Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e09\u79cdGPU\u539f\u751f\u7f16\u8bd1\u65b9\u6cd5\u4ee5\u89e3\u51b3AI\u4ee3\u7801\u751f\u6210\u4e2d\u7684CPU-GPU\u6570\u636e\u4f20\u8f93\u5ef6\u8fdf\u95ee\u9898\uff0c\u5305\u62ec\u5e76\u884c\u4f20\u7edf\u7f16\u8bd1\u3001\u795e\u7ecf\u7f16\u8bd1\u548c\u6df7\u5408\u67b6\u6784\uff0c\u7406\u8bba\u5206\u6790\u8868\u660e\u53ef\u5b9e\u73b010-100\u500d\u7684\u8fed\u4ee3\u52a0\u901f\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7801\u751f\u6210\u7cfb\u7edf\u5728\u7f16\u8bd1\u3001\u6267\u884c\u548c\u6d4b\u8bd5\u9636\u6bb5\u56e0CPU\u4e0eGPU\u95f4\u7684\u6570\u636e\u4f20\u8f93\u800c\u9762\u4e34\u663e\u8457\u5ef6\u8fdf\u74f6\u9888\uff0c\u4e9f\u9700\u6d88\u9664\u6b64\u7c7b\u5f00\u9500\u4ee5\u63d0\u5347\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u4e92\u8865\u7684GPU\u539f\u751f\u7f16\u8bd1\u7b56\u7565\uff1a(1) \u9002\u914dGPU\u6267\u884c\u7684\u5e76\u884c\u4f20\u7edf\u7f16\u8bd1\uff1b(2) \u57fa\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u5b66\u4e60\u4e0e\u6982\u7387\u9a8c\u8bc1\u7684\u795e\u7ecf\u7f16\u8bd1\uff1b(3) \u878d\u5408\u524d\u4e24\u8005\u7684\u6df7\u5408\u67b6\u6784\uff0c\u5e76\u5efa\u7acb\u76f8\u5e94\u7684\u7406\u8bba\u6a21\u578b\u4e0e\u9a8c\u8bc1\u6846\u67b6\u3002", "result": "\u7406\u8bba\u63a8\u5bfc\u663e\u793a\uff0c\u4f20\u7edfGPU\u7f16\u8bd1\u901a\u8fc7\u6d88\u9664\u6570\u636e\u4f20\u8f93\u83b7\u5f972-5\u500d\u52a0\u901f\uff0c\u795e\u7ecf\u7f16\u8bd1\u501f\u52a9\u5927\u89c4\u6a21\u5e76\u884c\u5b9e\u73b010-100\u500d\u52a0\u901f\uff0c\u6df7\u5408\u65b9\u6cd5\u5219\u517c\u987e\u5b9e\u7528\u6027\u4e0e\u6b63\u786e\u6027\u4fdd\u969c\u3002", "conclusion": "GPU\u539f\u751f\u7f16\u8bd1\u80fd\u663e\u8457\u964d\u4f4eAI\u4ee3\u7801\u751f\u6210\u7684\u5ef6\u8fdf\u4e0e\u80fd\u8017\uff0c\u6240\u63d0\u51fa\u7684\u6982\u7387\u9a8c\u8bc1\u6846\u67b6\u652f\u6301\u5728\u7f16\u8bd1\u7cbe\u5ea6\u4e0e\u5e76\u884c\u63a2\u7d22\u4e4b\u95f4\u6743\u8861\uff0c\u4e3a\u81ea\u6539\u8fdbAI\u7cfb\u7edf\u53ca\u672a\u6765\u7c7b\u6bd4\u8ba1\u7b97\u5e73\u53f0\u63d0\u4f9b\u65b0\u8def\u5f84\u3002"}}
{"id": "2512.11306", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.11306", "abs": "https://arxiv.org/abs/2512.11306", "authors": ["Tianyuan Wu", "Lunxi Cao", "Yining Wei", "Wei Gao", "Yuheng Zhao", "Dakai An", "Shaopan Xiong", "Zhiqiang Lv", "Ju Huang", "Siran Yang", "Yinghao Yu", "Jiamang Wang", "Lin Qu", "Wei Wang"], "title": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training", "comment": "17 pages, 15 figures", "summary": "Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable \"warm-star\" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRollMux\uff0c\u4e00\u79cd\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u8de8\u96c6\u7fa4\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u540c\u8c03\u5ea6rollout\u4e0e\u8bad\u7ec3\u4efb\u52a1\uff0c\u6709\u6548\u5229\u7528\u56e0\u540c\u6b65\u7b49\u5f85\u9020\u6210\u7684\u7a7a\u95f2\u8d44\u6e90\uff0c\u663e\u8457\u63d0\u5347\u786c\u4ef6\u5229\u7528\u7387\u548c\u6210\u672c\u6548\u7387\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u4e2d\uff0con-policy\u7b97\u6cd5\u8981\u6c42\u4e25\u683c\u7684\u540c\u6b65\uff0c\u5bfc\u81f4rollout\u96c6\u7fa4\u4e0e\u8bad\u7ec3\u96c6\u7fa4\u4e4b\u95f4\u51fa\u73b0\u4e25\u91cd\u7684\u4f9d\u8d56\u7a7a\u6ce1\uff08dependency bubbles\uff09\uff0c\u9020\u6210\u4e00\u65b9\u957f\u65f6\u95f4\u95f2\u7f6e\uff0c\u964d\u4f4e\u6574\u4f53\u786c\u4ef6\u6548\u7387\u3002", "method": "RollMux\u5f15\u5165\u201c\u534f\u540c\u6267\u884c\u7ec4\u201d\uff08co-execution group\uff09\u62bd\u8c61\uff0c\u5c06\u96c6\u7fa4\u5212\u5206\u4e3a\u9694\u79bb\u7684\u5c40\u90e8\u6027\u57df\uff0c\u5e76\u91c7\u7528\u4e24\u5c42\u8c03\u5ea6\u67b6\u6784\uff1a\u7ec4\u95f4\u8c03\u5ea6\u5668\u4f7f\u7528\u4fdd\u5b88\u968f\u673a\u89c4\u5212\u4f18\u5316\u4f5c\u4e1a\u653e\u7f6e\uff0c\u7ec4\u5185\u8c03\u5ea6\u5668\u6267\u884c\u53ef\u8bc1\u660e\u6700\u4f18\u7684\u8f6e\u8be2\u8c03\u5ea6\uff1b\u540c\u65f6\u901a\u8fc7\u9a7b\u7559\u7ea6\u675f\u4fdd\u6301\u6a21\u578b\u72b6\u6001\u7f13\u5b58\uff0c\u5b9e\u73b0\u201c\u70ed\u542f\u52a8\u201d\u4e0a\u4e0b\u6587\u5207\u6362\u3002", "result": "\u5728\u5305\u542b328\u4e2aH20\u548c328\u4e2aH800 GPU\u7684\u751f\u4ea7\u7ea7\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0cRollMux\u76f8\u6bd4\u6807\u51c6\u89e3\u8026\u67b6\u6784\u63d0\u53471.84\u500d\u6210\u672c\u6548\u7387\uff0c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u540c\u7f6e\u57fa\u7ebf\u63d0\u53471.38\u500d\uff0c\u4e14100%\u6ee1\u8db3\u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\u3002", "conclusion": "RollMux\u901a\u8fc7\u8de8\u96c6\u7fa4\u534f\u540c\u8c03\u5ea6\u6709\u6548\u56de\u6536\u56e0\u540c\u6b65\u7b49\u5f85\u9020\u6210\u7684\u8d44\u6e90\u7a7a\u95f2\uff0c\u5728\u4fdd\u8bc1SLO\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u6210\u672c\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2512.11532", "categories": ["cs.DC", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11532", "abs": "https://arxiv.org/abs/2512.11532", "authors": ["Chong Tang", "Hao Dai", "Jagmohan Chauhan"], "title": "Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems", "comment": null, "summary": "The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.", "AI": {"tldr": "Parallax \u662f\u4e00\u4e2a\u65e0\u9700\u6a21\u578b\u91cd\u6784\u6216\u81ea\u5b9a\u4e49\u7b97\u5b50\u5373\u53ef\u52a0\u901f\u79fb\u52a8\u7aef\u52a8\u6001 DNN \u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u56fe\u5212\u5206\u3001\u5206\u652f\u611f\u77e5\u5185\u5b58\u7ba1\u7406\u548c\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u5728\u591a\u79cd\u8bbe\u5907\u4e0a\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u3001\u63a7\u5236\u5185\u5b58\u5f00\u9500\u5e76\u8282\u7701\u80fd\u8017\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u65f6 DNN \u5e94\u7528\u5bf9\u4f4e\u5ef6\u8fdf\u63a8\u7406\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u73b0\u6709\u6846\u67b6\u5728\u5904\u7406\u52a8\u6001\u63a7\u5236\u6d41\u548c\u4e0d\u652f\u6301\u7684\u7b97\u5b50\u56de\u9000\u5230 CPU \u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u5185\u5b58\u5cf0\u503c\u3002", "method": "Parallax \u901a\u8fc7\u5212\u5206\u8ba1\u7b97 DAG \u66b4\u9732\u5e76\u884c\u6027\uff0c\u91c7\u7528\u5206\u652f\u611f\u77e5\u7684\u5185\u5b58\u7ba1\u7406\uff08\u4e13\u7528\u5185\u5b58\u533a\u4e0e\u7f13\u51b2\u590d\u7528\uff09\uff0c\u5e76\u7ed3\u5408\u81ea\u9002\u5e94\u8c03\u5ea6\u5668\u6839\u636e\u8bbe\u5907\u5185\u5b58\u7ea6\u675f\u6267\u884c\u5206\u652f\uff0c\u540c\u65f6\u5229\u7528\u7ec6\u7c92\u5ea6\u5b50\u56fe\u63a7\u5236\u5b9e\u73b0\u5f02\u6784\u63a8\u7406\u3002", "result": "\u5728\u4e09\u79cd\u79fb\u52a8\u8bbe\u5907\u4e0a\u5bf9\u4e94\u4e2a\u5178\u578b DNN \u7684\u8bc4\u4f30\u8868\u660e\uff0cParallax \u76f8\u6bd4\u6700\u5148\u8fdb\u6846\u67b6\u6700\u591a\u964d\u4f4e 46% \u5ef6\u8fdf\uff0c\u5e73\u5747\u5185\u5b58\u5f00\u9500\u4ec5\u589e\u52a0 26.5%\uff0c\u5e76\u6700\u591a\u8282\u7701 30% \u80fd\u8017\u3002", "conclusion": "Parallax \u6709\u6548\u63d0\u5347\u4e86\u79fb\u52a8\u7aef\u52a8\u6001 DNN \u63a8\u7406\u7684\u6548\u7387\uff0c\u5728\u5ef6\u8fdf\u3001\u5185\u5b58\u548c\u80fd\u8017\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u6ee1\u8db3\u4e86\u5b9e\u65f6\u63a8\u7406\u7684\u54cd\u5e94\u6027\u9700\u6c42\u3002"}}
{"id": "2512.11727", "categories": ["cs.DC", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.11727", "abs": "https://arxiv.org/abs/2512.11727", "authors": ["Yuze He", "Ferdi Kossmann", "Srinivasan Seshan", "Peter Steenkiste"], "title": "ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning", "comment": null, "summary": "Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.", "AI": {"tldr": "ECCO is a resource-efficient video analytics framework that reduces compute and communication costs by grouping cameras with similar data drift and retraining shared models, achieving higher accuracy or supporting more cameras than existing methods.", "motivation": "Current video analytics systems retrain separate lightweight DNN models per camera to handle data drift, but this approach incurs high compute and communication overhead, limiting scalability.", "method": "ECCO leverages temporal and spatial correlations in data drift across nearby cameras. It includes: (i) a dynamic camera grouping algorithm, (ii) a GPU allocator for fair and accurate retraining across groups, and (iii) a per-camera transmission controller that adjusts frame sampling and bandwidth based on allocated GPU resources.", "result": "Evaluations on three datasets and two vision tasks show ECCO improves retraining accuracy by 6.7%\u201318.1% under the same resource budget, or supports 3.3\u00d7 more concurrent cameras at the same accuracy compared to state-of-the-art baselines.", "conclusion": "By exploiting correlated data drift among cameras and enabling shared model retraining, ECCO offers a scalable and efficient solution for continuous learning in video analytics systems."}}
