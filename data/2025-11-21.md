<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Optimizing Communication in Byzantine Agreement Protocols with Slim-HBBFT](https://arxiv.org/abs/2511.15957)
*Nasit S Sony,Xianzhong Ding*

Main category: cs.DC

TL;DR: 本文提出了一种名为Slim-HBBFT的新型原子广播协议，通过仅处理部分参与方的请求并引入优先可证明广播（P-PB）机制，将通信复杂度降低了O(n)倍，同时保证了异步通用子集协议所需的安全性。


<details>
  <summary>Details</summary>
Motivation: 传统异步拜占庭容错协议要求所有参与方广播其请求，即使最终只就一个参与方的请求达成一致，也导致高昂的通信开销。在原子广播场景中，若请求内容重复或变化不大，这种高成本并不合理，因此需要一种更高效的协议。

Method: 设计了一种优先可证明广播（P-PB）协议，仅为选定的部分参与方生成广播证明，并以此为基础构建Slim-HBBFT原子广播协议，从而减少整体通信量。

Result: Slim-HBBFT在保持安全性的前提下，将通信复杂度相较于传统方案降低了O(n)倍，并满足异步通用子集协议的各项属性。

Conclusion: Slim-HBBFT是一种高效且安全的原子广播协议，适用于请求冗余较高的异步拜占庭环境，显著提升了可扩展性。

Abstract: Byzantine agreement protocols in asynchronous networks have received renewed interest because they do not rely on network behavior to achieve termination. Conventional asynchronous Byzantine agreement protocols require every party to broadcast its requests (e.g., transactions), and at the end of the protocol, parties agree on one party's request. If parties agree on one party's requests while exchanging every party's request, the protocol becomes expensive. These protocols are used to design an atomic broadcast (ABC) protocol where parties agree on $\langle n-f \rangle$ parties' requests (assuming $n=3f+1$, where $n$ is the total number of parties, and $f$ is the number of Byzantine parties). Although the parties agree on a subset of requests in the ABC protocol, if the requests do not vary (are duplicated), investing in a costly protocol is not justified. We propose Slim-HBBFT, an atomic broadcast protocol that considers requests from a fraction of $n$ parties and improves communication complexity by a factor of $O(n)$. At the core of our design is a prioritized provable-broadcast (P-PB) protocol that generates proof of broadcast only for selected parties. We use the P-PB protocol to design the Slim-HBBFT atomic broadcast protocol. Additionally, we conduct a comprehensive security analysis to demonstrate that Slim-HBBFT satisfies the properties of the Asynchronous Common Subset protocol, ensuring robust security and reliability.

</details>


### [2] [Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows](https://arxiv.org/abs/2511.15977)
*Daniel Mas Montserrat,Ray Verma,Míriam Barrabés,Francisco M. de la Vega,Carlos D. Bustamante,Alexander G. Ioannidis*

Main category: cs.DC

TL;DR: 本文提出多种自适应、内存高效的染色体级生物信息学工作流并行化机制，包括基于符号回归的内存预测模型、动态调度器和静态调度策略，以优化大规模基因组工作流的内存使用与执行效率。


<details>
  <summary>Details</summary>
Motivation: 大规模基因组工作流在精准医疗中常因样本数据量大而引发高内存峰值、密集磁盘I/O及内存溢出错误；传统静态资源分配方法难以应对各染色体任务间内存需求的差异，导致资源利用率低和运行时间长。

Method: 1）构建符号回归模型估算每条染色体任务的内存消耗，并引入插值偏差以保守减少过度分配；2）设计动态调度器，利用多项式回归预测内存使用，将任务打包视为背包问题以最优批处理作业；3）提出静态调度器，优化染色体处理顺序以降低峰值内存同时保持吞吐量。

Result: 在仿真和真实基因组流程上的评估表明，所提方法能有效减少内存溢出、均衡线程负载，并显著加快端到端执行速度。

Conclusion: 通过自适应内存预测与调度策略，本文为大规模基因组工作流提供了高效、鲁棒的资源优化方案，展现出在实际应用中的潜力。

Abstract: Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.

</details>


### [3] [Can Asymmetric Tile Buffering Be Beneficial?](https://arxiv.org/abs/2511.16041)
*Chengyue Wang,Wesley Pang,Xinrui Wu,Gregory Jun,Luis Romero,Endri Taka,Diana Marculescu,Tony Nowatzki,Pranathi Vasireddy,Joseph Melber,Deming Chen,Jason Cong*

Main category: cs.DC

TL;DR: 本文提出了一种名为非对称平铺缓冲（ATB）的新技术，通过解耦输入和输出操作数的平铺维度，显著提升了通用矩阵乘法（GEMM）在AI硬件上的性能，在AMD XDNA2 AI Engine上实现了最高4.54倍的加速。


<details>
  <summary>Details</summary>
Motivation: 传统GEMM实现采用对称平铺缓冲策略，限制了算术强度与内存效率的进一步优化。作者旨在探索更灵活的平铺方法以提升AI工作负载的计算效率。

Method: 提出非对称平铺缓冲（ATB）方法，并构建性能模型综合考虑其带来的算术强度增益与核切换开销，指导有效平铺因子的选择。

Result: 在AMD最新的XDNA2 AI Engine上，ATB将混合精度BFP16–BF16 GEMM性能从4.8 TFLOPS提升至24.6 TFLOPS，提速达4.54倍，创下该平台新纪录。

Conclusion: 非对称平铺缓冲是一种实用且高效的优化技术，能显著提升GEMM在专用AI硬件上的性能，值得在更多架构中推广。

Abstract: General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.
  In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE.

</details>


### [4] [Mitigating Shared Storage Congestion Using Control Theory](https://arxiv.org/abs/2511.16177)
*Thomas Collignon,Kouds Halitim,Raphaël Bleuse,Sophie Cerf,Bogdan Robu,Éric Rutten,Lionel Seinturier,Alexandre van Kempen*

Main category: cs.DC

TL;DR: 本文提出一种基于控制理论的自适应方法，通过动态调节客户端I/O速率来缓解HPC系统中的I/O拥塞问题，在真实测试平台上验证了其可减少最多20%的总运行时间并降低尾部延迟。


<details>
  <summary>Details</summary>
Motivation: 传统HPC系统中的I/O栈优化通常针对特定工作负载，依赖专家知识，难以泛化；同时共享环境中资源争用导致性能不稳定，出现减速甚至超时。

Method: 基于控制理论设计自适应控制器，利用少量运行时系统负载指标动态调节客户端I/O速率，以减轻拥塞并提升性能稳定性。

Result: 在多节点集群的真实测试平台上评估表明，该方法有效缓解I/O拥塞，总运行时间最多减少20%，尾部延迟降低，且性能更稳定。

Conclusion: 所提出的自适应I/O调控方法在不依赖复杂配置的前提下，显著提升了HPC系统中I/O性能的稳定性与效率。

Abstract: Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance.

</details>


### [5] [Fast LLM Post-training via Decoupled and Best-of-N Speculation](https://arxiv.org/abs/2511.16193)
*Rongxin Cheng,Kai Zhou,Xingda Wei,Siyuan Liu,Mingcong Han,Mingjing Ai,Yeju Zhou,Baoquan Zhong,Wencong Xiao,Xin Liu,Rong Chen,Haibo Chen*

Main category: cs.DC

TL;DR: SpecActor通过动态解耦推测和动态Best-of-N推测方法，优化了大语言模型后训练阶段的rollout过程，在不增加额外计算资源的情况下，相比基线方法提速1.3–1.7倍。


<details>
  <summary>Details</summary>
Motivation: 大语言模型后训练中的rollout阶段耗时严重，而传统推测解码在大批量训练场景下效率低下，难以发挥加速效果。

Method: 提出SpecActor框架，包含（1）动态解耦推测执行方法，提升GPU计算效率以适应大批量场景；（2）动态Best-of-N推测方法，根据rollout进度动态选择和组合不同草稿模型，提高推测准确性。

Result: SpecActor相比常规后训练基线提速1.3–1.7倍，相比直接应用推测解码提速1.3–1.5倍。

Conclusion: SpecActor有效解决了推测rollout在大批量训练中的效率与准确性问题，显著加速了大语言模型后训练过程。

Abstract: Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\sys} is {1.3--1.7}\,$\times$ faster than common post-training baselines, and is {1.3--1.5}\,$\times$ faster compared to naively adopting speculative decoding for rollout.

</details>


### [6] [Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming](https://arxiv.org/abs/2511.16450)
*Ziyue Xu,Zhihong Zhang,Holger R. Roth,Chester Chen,Yan Cheng,Andrew Feng*

Main category: cs.DC

TL;DR: 本文提出通过消息量化和容器/文件流式传输两种技术，优化NVIDIA FLARE框架在大语言模型（LLM）联邦学习中的通信效率与内存管理，从而提升其可扩展性与实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型参数量庞大，在联邦学习中带来严重的通信开销与本地资源限制问题，亟需高效传输与处理机制以支持实际部署。

Method: 在NVIDIA FLARE开源SDK基础上，引入消息量化以减小传输数据量，并采用容器/文件流式传输技术优化内存使用。

Result: 所提方法显著提升了大语言模型在联邦学习中的通信效率、内存管理能力及整体工作流的可扩展性。

Conclusion: 通过量化与流式传输技术，有效缓解了LLM在联邦学习中的通信与资源瓶颈，增强了系统在真实场景下的鲁棒性与性能。

Abstract: Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [7] [Technique to Baseline QE Artefact Generation Aligned to Quality Metrics](https://arxiv.org/abs/2511.15733)
*Eitan Farchi,Kiran Nayak,Papia Ghosh Majumdar,Saritha Route*

Main category: cs.SE

TL;DR: 本文提出一种结合大语言模型（LLM）生成、反向生成和基于评分标准的迭代优化方法，用于系统化评估和提升质量工程（QE）工件（如需求、测试用例、BDD场景）的质量，并在12个项目中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在自动生成质量工程工件方面展现出潜力，但如何确保这些生成内容的质量仍是一个关键挑战。

Method: 采用LLM驱动的生成、反向生成以及由评分标准引导的迭代优化方法，从清晰性、完整性、一致性和可测试性四个维度对QE工件进行评估与改进。

Result: 在12个项目的实验中，反向生成的工件在输入质量较低时能显著提升输出质量，在输入质量较高时也能保持高标准。

Conclusion: 该框架实现了可扩展且可靠的QE工件验证机制，有效连接了自动化生成与质量问责。

Abstract: Large Language Models (LLMs) are transforming Quality Engineering (QE) by automating the generation of artefacts such as requirements, test cases, and Behavior Driven Development (BDD) scenarios. However, ensuring the quality of these outputs remains a challenge. This paper presents a systematic technique to baseline and evaluate QE artefacts using quantifiable metrics. The approach combines LLM-driven generation, reverse generation , and iterative refinement guided by rubrics technique for clarity, completeness, consistency, and testability. Experimental results across 12 projects show that reverse-generated artefacts can outperform low-quality inputs and maintain high standards when inputs are strong. The framework enables scalable, reliable QE artefact validation, bridging automation with accountability.

</details>


### [8] [Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym](https://arxiv.org/abs/2511.15757)
*Kareem Shehada,Yifan Wu,Wyatt D. Feng,Adithya Iyer,Gryphon Kumfert,Yangruibo Ding,Zhiyun Qian*

Main category: cs.SE

TL;DR: 本文提出了RGym，一个轻量级、跨平台的Linux内核自动程序修复（APR）评估框架，并构建了一个基于专用定位技术（如调用栈和问题提交）的高效修复流程，在本地普通硬件上实现了高达43.36%的修复成功率，单个漏洞修复成本低于0.2美元。


<details>
  <summary>Details</summary>
Motivation: 现有自动程序修复（APR）基准（如SWE-Bench）主要关注用户空间应用，忽视了内核空间调试与修复的复杂性；而Linux内核因其单体内核结构、并发性和底层硬件交互等特点，给APR带来独特挑战。此前方法如KGym和CrashFixer在该领域成功率低或依赖昂贵复杂的云基础设施。

Method: 作者设计了RGym框架，可在本地普通硬件上运行，并提出一种结合调用栈和问题提交等专用定位技术的轻量级APR流程，避免KGym中不切实际的预言机使用。实验基于143个经过筛选验证的内核漏洞数据集，采用GPT-5 Thinking模型进行修复，并通过消融实验分析定位策略、提示结构和模型选择的影响，同时探索基于反馈重试机制对成功率的提升效果。

Result: 所提方法在143个内核漏洞上达到最高43.36%的通过率，单个漏洞修复成本低于0.20美元；消融研究表明定位策略、提示结构和模型选择均对性能有显著影响，且基于反馈的重试机制可进一步提高成功率。

Conclusion: RGym为Linux内核自动程序修复提供了一个高效、低成本、可本地部署的评估与修复方案，显著优于以往依赖昂贵资源的方法，推动了内核级APR研究的可及性与实用性。

Abstract: Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.

</details>


### [9] [A Causal Perspective on Measuring, Explaining and Mitigating Smells in \llm-Generated Code](https://arxiv.org/abs/2511.15817)
*Alejandro Velasco,Daniel Rodriguez-Cardenas,Dipin Khati,David N. Palacio,Luftar Rahman Alif,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文系统研究了大语言模型（LLM）生成代码中的“代码异味”问题，提出并验证了一种名为PSC（Propensity Smelly Score）的概率度量方法，用于评估生成代码中特定异味出现的可能性，并通过因果分析揭示了生成策略、模型规模、架构和提示设计对异味倾向的影响。研究发现提示设计和模型架构对异味倾向起决定性作用，并提出了有效的缓解策略。用户研究表明PSC有助于开发者理解模型行为并评估代码质量。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件工程中被广泛应用，但其生成的代码常包含不良编程实践（即“代码异味”），影响代码可读性、可维护性和设计完整性。现有研究多聚焦于异味检测或修复，缺乏对其在生成代码中如何及何时产生的系统理解。

Method: 作者基于Propensity Smelly Score（PSC）这一概率度量指标，系统测量LLM生成代码中的异味倾向；利用PSC作为因果分析工具，考察生成策略、模型大小、模型架构和提示形式等因素对代码结构质量的影响；并通过用户研究验证PSC在辅助开发者判断代码质量方面的有效性。

Result: 研究发现提示设计和模型架构对异味倾向具有决定性影响；提出的缓解策略能有效降低异味发生率；用户研究表明PSC能帮助开发者更好地理解和评估模型生成代码的质量。

Conclusion: 该工作为将质量感知评估整合到大语言模型代码生成的评估与部署流程中奠定了基础，强调了在模型应用中考虑结构性代码质量的重要性。

Abstract: Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.
  This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.

</details>


### [10] [AI-Enabled Orchestration of Event-Driven Business Processes in Workday ERP for Healthcare Enterprises](https://arxiv.org/abs/2511.15852)
*Monu Sharma*

Main category: cs.SE

TL;DR: 本文提出了一种在Workday ERP中嵌入AI能力的事件驱动编排框架，通过机器学习触发器、异常检测和流程挖掘技术，实现医疗环境中财务与供应链工作流的智能同步，显著提升运营效率与决策准确性。


<details>
  <summary>Details</summary>
Motivation: 传统ERP系统的工作流逻辑难以适应医疗领域高度事件驱动和数据密集的运营环境，亟需更灵活、智能的集成方案。

Method: 在Workday ERP平台中构建AI驱动的事件驱动编排框架，结合机器学习触发器、异常检测和流程挖掘分析，自动响应库存耗尽、付款延迟及患者需求波动等运营事件。

Result: 多机构案例分析表明，该框架显著提升了流程效率、成本可见性和决策准确性，并增强了系统的运营韧性、治理能力和可扩展性。

Conclusion: 将AI能力嵌入Workday的事件驱动架构可有效推动医疗企业智能ERP集成，为下一代自动化策略提供参考模型。

Abstract: The adoption of cloud-based Enterprise Resource Planning (ERP) platforms such as Workday has transformed healthcare operations by integrating financial, supply-chain, and workforce processes into a unified ecosystem. However, traditional workflow logic in ERP systems often lacks the adaptability required to manage event-driven and data-intensive healthcare environments.
  This study proposes an AI-enabled event-driven orchestration framework within Workday ERP that intelligently synchronizes financial and supply-chain workflows across distributed healthcare entities. The framework employs machine-learning triggers, anomaly detection, and process mining analytics to anticipate and automate responses to operational events such as inventory depletion, payment delays, or patient demand fluctuations. A multi-organization case analysis demonstrates measurable gains in process efficiency, cost visibility, and decision accuracy.
  Results confirm that embedding AI capabilities into Workday's event-based architecture enhances operational resilience, governance, and scalability. The proposed model contributes to the broader understanding of intelligent ERP integration and establishes a reference for next-generation automation strategies in healthcare enterprises.

</details>


### [11] [RE for AI in Practice: Managing Data Annotation Requirements for AI Autonomous Driving Systems](https://arxiv.org/abs/2511.15859)
*Hina Saeeda,Mazen Mohamad,Eric Knauss,Jennifer Horkoff,Ali Nouri*

Main category: cs.SE

TL;DR: 该研究通过19次访谈揭示了自动驾驶AI感知系统中数据标注需求存在的五大挑战（模糊性、边缘案例复杂性、需求演变、不一致性、资源限制）和三类最佳实践，并首次提供了基于实证的标注需求改进建议，以提升系统可靠性与合规性。


<details>
  <summary>Details</summary>
Motivation: 高质量的数据标注需求对开发安全可靠的自动驾驶AI感知系统至关重要，但其制定与管理尚缺乏系统研究，导致实践中存在不一致、安全隐患及合规问题。

Method: 研究者对来自6家国际公司和4个研究机构的19名从业者进行了半结构化访谈，并采用主题分析法提炼关键挑战与最佳实践。

Result: 识别出五大关键挑战和三类最佳实践，揭示了标注需求、标注实践、标注数据质量与AI感知系统性能之间的紧密关联，表明需求缺陷会沿开发流程传播。

Conclusion: 本研究首次提供基于实证的标注需求改进指导，为提升标注质量、合规性和系统可靠性提供可操作洞见，并推动需求工程与AI软件工程的融合。

Abstract: High-quality data annotation requirements are crucial for the development of safe and reliable AI-enabled perception systems (AIePS) in autonomous driving. Although these requirements play a vital role in reducing bias and enhancing performance, their formulation and management remain underexplored, leading to inconsistencies, safety risks, and regulatory concerns. Our study investigates how annotation requirements are defined and used in practice, the challenges in ensuring their quality, practitioner-recommended improvements, and their impact on AIePS development and performance. We conducted $19$ semi-structured interviews with participants from six international companies and four research organisations. Our thematic analysis reveals five main key challenges: ambiguity, edge case complexity, evolving requirements, inconsistencies, and resource constraints and three main categories of best practices, including ensuring compliance with ethical standards, improving data annotation requirements guidelines, and embedded quality assurance for data annotation requirements. We also uncover critical interrelationships between annotation requirements, annotation practices, annotated data quality, and AIePS performance and development, showing how requirement flaws propagate through the AIePS development pipeline. To the best of our knowledge, this study is the first to offer empirically grounded guidance on improving annotation requirements, offering actionable insights to enhance annotation quality, regulatory compliance, and system reliability. It also contributes to the emerging fields of Software Engineering (SE for AI) and Requirements Engineering (RE for AI) by bridging the gap between RE and AI in a timely and much-needed manner.

</details>


### [12] [InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution](https://arxiv.org/abs/2511.16004)
*KeFan Li,Mengfei Wang,Hengzhi Zhang,Zhichao Li,Yuan Yuan,Mu Li,Xiang Gao,Hailong Sun,Chunming Hu,Weifeng Lv*

Main category: cs.SE

TL;DR: InfCode 是一个对抗式多智能体框架，通过测试与补丁的迭代优化，在 SWE-bench Verified 上达到 79.4% 的新 SOTA 性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于智能体或流水线的方法在解决真实软件问题时依赖不充分的测试，导致生成的补丁虽通过验证却未真正修复缺陷；因此需要一种能进行仓库级推理、准确诊断并提供强验证信号的新方法。

Method: 提出 InfCode 框架，包含测试补丁生成器、代码补丁生成器和选择器三个智能体，在容器化环境中通过对抗交互迭代优化测试与补丁，并选出最可靠的修复方案。

Result: 在 SWE-bench Lite 和 SWE-bench Verified 上使用 DeepSeek-V3 和 Claude 4.5 Sonnet 等模型进行实验，InfCode 显著优于强基线方法，在 SWE-bench Verified 上达到 79.4% 的性能。

Conclusion: InfCode 通过对抗式多智能体机制有效提升了仓库级软件问题自动修复的能力，实现了当前最优性能，并已开源。

Abstract: Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.

</details>


### [13] [The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report](https://arxiv.org/abs/2511.16092)
*Xing Hu,Raula Gaikovina Kula,Christoph Treude*

Main category: cs.SE

TL;DR: 本文报告了Shonan Meeting 222中33位专家对生成式人工智能（GenAI）在集成开发环境（IDE）中影响的讨论，探讨其带来的挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI在代码生成、测试、审查和修复等任务中的出色表现，其有望改变开发者与AI在IDE中的交互方式，因此需要深入探讨其对IDE的影响。

Method: 组织来自软件工程、人工智能和人机交互领域的33位专家召开Shonan Meeting 222，通过研讨形式分析GenAI在IDE中的应用前景与问题。

Result: 会议识别出GenAI在提升抽象层次、改变人机协作模式方面的潜力，并总结了当前面临的关键挑战与未来机会。

Conclusion: GenAI正在重塑IDE中的人机交互范式，但需跨学科合作以应对技术、工具和用户体验等方面的挑战。

Abstract: Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report

</details>


### [14] [Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions](https://arxiv.org/abs/2511.16123)
*Linyi Han,Shidong Pan,Zhenchang Xing,Sofonias Yitagesu,Xiaowang Zhang,Zhiyong Feng,Jiamou Sun,Qing Huang*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型的领域约束合成框架，用于统一来自不同来源的文本漏洞描述（TVDs）中的关键信息，通过提取、自评估和融合三个阶段提升合成效果，并开发了可视化工具Digest Labels以增强可用性。


<details>
  <summary>Details</summary>
Motivation: 不同漏洞库中的文本漏洞描述（TVDs）存在关键信息不一致的问题，影响安全分析师对漏洞的全面理解；现有方法在对齐外部知识库时往往丢失有价值的信息，难以生成综合性的表示。

Method: 提出一个三阶段的领域约束大语言模型合成框架：1）基于规则模板的提取阶段，确保捕获所有关键细节；2）使用领域锚词进行语义变异性自评估；3）利用信息熵融合多源信息，解决不一致性并优先保留相关细节。同时开发了可视化工具Digest Labels。

Result: 该框架将关键方面增强的F1分数从0.82提升至0.87，并使理解和处理效率提升超过30%；人工评估表明Digest Labels显著提高了TVDs的可用性。

Conclusion: 所提出的框架有效解决了TVDs中的不一致性问题，提升了漏洞信息的综合表示能力与实用性，为安全分析提供了更高效、准确的支持。

Abstract: Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.

</details>


### [15] [Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts](https://arxiv.org/abs/2511.16224)
*Francesco Salzano,Simone Scalabrino,Rocco Oliveto,Simone Scalabrino*

Main category: cs.SE

TL;DR: 该论文评估了大语言模型（LLM）在生成Solidity智能合约代码时的功能正确性与非功能属性（如Gas消耗、复杂度等），发现尽管生成代码在语义上与真实合约相似，但功能正确率低（仅20%-26%），且常因省略验证逻辑而过于简化；引入检索增强生成（RAG）可显著提升正确率至最高45%，但仍难以达到生产级可靠性，需专家人工验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLM生成的Solidity智能合约在功能性（如正确性）和非功能性（如Gas消耗、安全性、确定性）方面的全面评估，而这些属性对智能合约至关重要。

Method: 在零样本和检索增强生成（RAG）两种设置下，对四种前沿大语言模型生成的500个真实函数进行多维度评估，包括代码相似性、语义嵌入、自动化测试、Gas分析以及认知与圈复杂度分析。

Result: LLM生成的代码语义相似度高但功能正确率低（20%-26%）；代码普遍更简单、Gas消耗更低，常因缺失验证逻辑；RAG显著提升功能正确率（最高达45%），并生成更简洁高效的代码。

Conclusion: 尽管RAG能有效提升LLM生成智能合约的质量，但当前方法仍难以确保生成代码的鲁棒性和生产就绪性，必须结合专家人工验证。

Abstract: Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.

</details>


### [16] [Data Annotation Quality Problems in AI-Enabled Perception System Development](https://arxiv.org/abs/2511.16410)
*Hina Saeeda,Tommy Johansson,Mazen Mohamad,Eric Knauss*

Main category: cs.SE

TL;DR: 本研究通过多组织案例分析，识别并分类了自动驾驶AI感知系统数据标注中的18种常见错误类型，涵盖完整性、准确性和一致性三个维度，并验证了该分类法在工业实践中的实用性。


<details>
  <summary>Details</summary>
Motivation: 数据标注对自动驾驶AI感知系统的开发至关重要但极易出错，而行业缺乏对标注错误如何在多组织汽车供应链中产生和传播的实证洞察。

Method: 开展涉及六家公司和四家研究机构的多组织案例研究，基于19次半结构化访谈（20位专家，共50小时转录文本），采用六阶段主题分析方法构建标注错误分类体系。

Result: 提出了包含18种标注错误类型的分类法，覆盖完整性（如属性遗漏、边缘案例缺失）、准确性（如误标、边界框不准）和一致性（如标注者间分歧、指令模糊）三个数据质量维度，并经行业从业者验证其在根本原因分析、供应商评估等方面的实用价值。

Conclusion: 将标注质量视为生命周期与供应链问题，为可信AI感知系统开发提供了共享术语、诊断工具和可操作指导，推动了面向AI的软件工程（SE4AI）发展。

Abstract: Data annotation is essential but highly error-prone in the development of AI-enabled perception systems (AIePS) for automated driving, and its quality directly influences model performance, safety, and reliability. However, the industry lacks empirical insights into how annotation errors emerge and spread across the multi-organisational automotive supply chain. This study addresses this gap through a multi-organisation case study involving six companies and four research institutes across Europe and the UK. Based on 19 semi-structured interviews with 20 experts (50 hours of transcripts) and a six-phase thematic analysis, we develop a taxonomy of 18 recurring annotation error types across three data-quality dimensions: completeness (e.g., attribute omission, missing feedback loops, edge-case omissions, selection bias), accuracy (e.g., mislabelling, bounding-box inaccuracies, granularity mismatches, bias-driven errors), and consistency (e.g., inter-annotator disagreement, ambiguous instructions, misaligned hand-offs, cross-modality inconsistencies). The taxonomy was validated with industry practitioners, who reported its usefulness for root-cause analysis, supplier quality reviews, onboarding, and improving annotation guidelines. They described it as a failure-mode catalogue similar to FMEA. By conceptualising annotation quality as a lifecycle and supply-chain issue, this study contributes to SE4AI by offering a shared vocabulary, diagnostic toolset, and actionable guidance for building trustworthy AI-enabled perception systems.

</details>


### [17] [Green Resilience of Cyber-Physical Systems: Doctoral Dissertation](https://arxiv.org/abs/2511.16593)
*Diaeddin Rimawi*

Main category: cs.SE

TL;DR: 本文研究在线协作人工智能系统（OL-CAIS）在面对干扰事件时如何在恢复性能（韧性）与降低能耗（绿色性）之间取得平衡，提出了GResilience框架及相应的多智能体策略，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: OL-CAIS作为一类信息物理系统，在与人类协作学习过程中易受干扰事件影响，导致性能下降；决策者需在恢复性能的同时控制能源消耗，因此需要建模并优化韧性与绿色性之间的权衡。

Method: 提出包含稳态、干扰态和终态的三状态韧性模型；构建GResilience框架，分别采用单智能体多目标优化、双智能体博弈决策和强化学习（RL-agent）策略实现绿色恢复；设计衡量韧性和绿色性的指标体系；通过真实与模拟实验（协作机器人从人类示范中学习物体分类）进行评估。

Result: 韧性模型能有效捕捉干扰期间的性能变化；GResilience策略显著缩短恢复时间、稳定性能并减少对人类依赖，其中RL-agent效果最佳，但CO₂排放略有增加；重复干扰会导致灾难性遗忘，而所提策略有助于维持系统稳定性；容器化执行可将CO₂排放减半。

Conclusion: 本研究为OL-CAIS提供了韧性建模、绿色恢复策略与评估指标，有效支持其在干扰后实现兼顾性能恢复与低碳运行的绿色韧性目标。

Abstract: Cyber-physical systems (CPS) combine computational and physical components. Online Collaborative AI System (OL-CAIS) is a type of CPS that learn online in collaboration with humans to achieve a common goal, which makes it vulnerable to disruptive events that degrade performance. Decision-makers must therefore restore performance while limiting energy impact, creating a trade-off between resilience and greenness. This research addresses how to balance these two properties in OL-CAIS. It aims to model resilience for automatic state detection, develop agent-based policies that optimize the greenness-resilience trade-off, and understand catastrophic forgetting to maintain performance consistency. We model OL-CAIS behavior through three operational states: steady, disruptive, and final. To support recovery during disruptions, we introduce the GResilience framework, which provides recovery strategies through multi-objective optimization (one-agent), game-theoretic decision-making (two-agent), and reinforcement learning (RL-agent). We also design a measurement framework to quantify resilience and greenness. Empirical evaluation uses real and simulated experiments with a collaborative robot learning object classification from human demonstrations. Results show that the resilience model captures performance transitions during disruptions, and that GResilience policies improve green recovery by shortening recovery time, stabilizing performance, and reducing human dependency. RL-agent policies achieve the strongest results, although with a marginal increase in CO2 emissions. We also observe catastrophic forgetting after repeated disruptions, while our policies help maintain steadiness. A comparison with containerized execution shows that containerization cuts CO2 emissions by half. Overall, this research provides models, metrics, and policies that ensure the green recovery of OL-CAIS.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [18] [Benchmarking Table Extraction from Heterogeneous Scientific Extraction Documents](https://arxiv.org/abs/2511.16134)
*Marijan Soric,Cécile Gracianne,Ioana Manolescu,Pierre Senellart*

Main category: cs.DB

TL;DR: 本文提出了一个用于端到端表格提取（TE）方法的新基准，包含对评估指标的分析、严谨的评估流程设计以及两个包含37k样本的新异构数据集。实验表明现有方法在面对异构数据时泛化能力不足，且在鲁棒性和可解释性方面存在局限。


<details>
  <summary>Details</summary>
Motivation: 由于现有表格提取工具种类繁多、技术差异大，用户难以选择合适的方法，因此需要一个统一且全面的基准来评估不同TE方法的性能。

Method: 作者设计了一个新的端到端表格提取评估基准，包括对现有评估指标的分析、能够分别评估各子任务和整体性能的严谨评估流程，并引入两个新的异构数据集；在该基准上测试了多种模型，包括现成库、软件工具、大视觉语言模型和计算机视觉方法。

Result: 实验结果显示当前表格提取方法在异构数据上泛化能力差，且在鲁棒性和可解释性方面仍有明显不足。

Conclusion: 表格提取仍然是一个具有挑战性的任务，亟需在泛化性、鲁棒性和可解释性方面进行改进；所提出的基准为未来研究提供了可靠的评估框架。

Abstract: Table Extraction (TE) consists in extracting tables from PDF documents, in a structured format which can be automatically processed. While numerous TE tools exist, the variety of methods and techniques makes it difficult for users to choose an appropriate one. We propose a novel benchmark for assessing end-to-end TE methods (from PDF to the final table). We contribute an analysis of TE evaluation metrics, and the design of a rigorous evaluation process, which allows scoring each TE sub-task as well as end-to-end TE, and captures model uncertainty. Along with a prior dataset, our benchmark comprises two new heterogeneous datasets of 37k samples. We run our benchmark on diverse models, including off-the-shelf libraries, software tools, large vision language models, and approaches based on computer vision. The results demonstrate that TE remains challenging: current methods suffer from a lack of generalizability when facing heterogeneous data, and from limitations in robustness and interpretability.

</details>


### [19] [[Experiment, Analysis, and Benchmark] Systematic Evaluation of Plan-based Adaptive Query Processing](https://arxiv.org/abs/2511.16455)
*Pei Mu,Anderson Chaves Carniel,Antonio Barbalace,Amir Shaikhha*

Main category: cs.DB

TL;DR: 本文首次系统分析了基于计划的自适应查询处理（AQP）在磁盘和内存数据库中的性能表现，发现其在两类系统中性能提升的来源截然不同：在 PostgreSQL 中主要源于查询计划重排而非基数更新，在 DuckDB 中则主要依赖基数精化。


<details>
  <summary>Details</summary>
Motivation: 传统数据库管理系统中不可靠的基数估计严重影响查询性能，而现有对基于计划的自适应查询处理（AQP）机制的理解，特别是在不同存储架构（磁盘 vs 内存）下的有效性差异，尚不清晰。

Method: 在 PostgreSQL（磁盘型）和 DuckDB（内存型）两个数据库系统上实现并评估当前最先进的基于计划的 AQP 策略，并在两个基准测试中进行对比实验，同时与一种先进的基于相关性的 AQP 方法进行比较。

Result: 实验表明，基于计划的 AQP 在两类系统中均带来整体加速，但提升来源不同：PostgreSQL 的收益主要来自计划重排，基数更新反而引入开销；DuckDB 的性能提升则主要由基数精化驱动。此外，该方法显著优于对比的基于相关性的 AQP 方法。

Conclusion: 研究揭示了基于计划的 AQP 在不同存储架构下有效性的根本原因，为研究人员理解其适用场景提供了关键依据，并为数据库开发者在实现成本与性能收益之间做出权衡提供了实践指导。

Abstract: Unreliable cardinality estimation remains a critical performance bottleneck in database management systems (DBMSs). Adaptive Query Processing (AQP) strategies address this limitation by providing a more robust query execution mechanism. Specifically, plan-based AQP achieves this by incrementally refining cardinality using feedback from the execution of sub-plans. However, the actual reason behind the improvements of plan-based AQP, especially across different storage architectures (on-disk vs. in-memory DBMSs), remains unexplored.
  This paper presents the first comprehensive analysis of state-of-the-art plan-based AQP. We implement and evaluate this strategy on both on-disk and in-memory DBMSs across two benchmarks. Our key findings reveal that while plan-based AQP provides overall speedups in both environments, the sources of improvement differ significantly. In the on-disk DBMS, PostgreSQL, performance gains primarily come from the query plan reorderings, but not the cardinality updating mechanism; in fact, updating cardinalities introduces measurable overhead. Conversely, in the in-memory DBMS, DuckDB, cardinality refinement drives significant performance improvements for most queries. We also observe significant performance benefits of the plan-based AQP compared to a state-of-the-art related-based AQP method. These observations provide crucial insights for researchers on when and why plan-based AQP is effective, and ultimately guide database system developers on the tradeoffs between the implementation effort and performance improvements.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [20] [Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring](https://arxiv.org/abs/2511.16628)
*Tammam Bakeer,Max Herbers,Steffen Marx*

Main category: cs.CE

TL;DR: 本文提出一种贝叶斯反演框架，用于结构健康监测中从稀疏数据恢复分布参数（如弯曲刚度），并结合Fisher信息量化传感器布局对参数可识别性与空间分辨率的影响，从而统一参数识别与不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 在结构健康监测中，从稀疏数据恢复分布力学参数的问题通常是不适定的，传统确定性正则化方法虽能稳定反演，但难以揭示解的空间分辨率极限和内在不确定性。

Method: 采用贝叶斯反演方法，以旋转影响线数据识别分布弯曲刚度为案例，利用Fisher信息评估传感器的信息量，并通过可信区间揭示实际不可识别区域。

Result: 在德累斯顿工业大学openLAB实桥上验证了该方法，结果表明测量数据对目标参数具有高信息量，但其有效性在空间上不均且受实验设计严格限制。

Conclusion: 所提贝叶斯框架能够有效融合参数识别与不确定性量化，为传感器优化布置和结构健康监测诊断结果的可信度解释提供理论基础。

Abstract: In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.
  The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [21] [Green Distributed AI Training: Orchestrating Compute Across Renewable-Powered Micro Datacenters](https://arxiv.org/abs/2511.16182)
*Giuseppe Tomei,Andrea Mayer,Giuseppe Alcini,Stefano Salsano*

Main category: cs.NI

TL;DR: 本文提出一种基于可再生能源的分布式微数据中心架构，通过可行性感知的动态迁移机制，在保障AI工作负载性能的同时显著减少非可再生能源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前集中式数据中心在能耗比例性和地理灵活性方面难以匹配以间歇性可再生能源为主导的能源格局，导致大量零碳能源被弃用。

Method: 构建一个形式化的可行性域模型，将检查点大小、广域带宽和可再生窗口时长关联起来，并在此基础上设计可行性感知的编排框架，实现AI工作负载的动态迁移。

Result: 基于真实轨迹的评估表明，该方法能同时降低非可再生能源使用并提升性能稳定性，优于纯能耗驱动策略；研究还探讨了面向未来的可再生能源感知AI基础设施架构。

Conclusion: 可行性感知的迁移机制是构建与可再生能源供应相协调、地理自适应且灵活流动的下一代AI计算范式的关键基础。

Abstract: The accelerating expansion of AI workloads is colliding with an energy landscape increasingly dominated by intermittent renewable generation. While vast quantities of zero-carbon energy are routinely curtailed, today's centralized datacenter architectures remain poorly matched to this reality in both energy proportionality and geographic flexibility. This work envisions a shift toward a distributed fabric of renewable-powered micro-datacenters that dynamically follow the availability of surplus green energy through live workload migration.
  At the core of this vision lies a formal feasibility-domain model that delineates when migratory AI computation is practically achievable. By explicitly linking checkpoint size, wide-area bandwidth, and renewable-window duration, the model reveals that migration is almost always energetically justified, and that time-not energy-is the dominant constraint shaping feasibility. This insight enables the design of a feasibility-aware orchestration framework that transforms migration from a best-effort heuristic into a principled control mechanism. Trace-driven evaluation shows that such orchestration can simultaneously reduce non-renewable energy use and improve performance stability, overcoming the tradeoffs of purely energy-driven strategies.
  Beyond the immediate feasibility analysis, the extended version explores the architectural horizon of renewable-aware AI infrastructures. It examines the role of emerging ultra-efficient GPU-enabled edge platforms, anticipates integration with grid-level control and demand-response ecosystems, and outlines paths toward supporting partially migratable and distributed workloads. The work positions feasibility-aware migration as a foundational building block for a future computing paradigm in which AI execution becomes fluid, geographically adaptive, and aligned with renewable energy availability.

</details>
