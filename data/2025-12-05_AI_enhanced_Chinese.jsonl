{"id": "2512.04751", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.04751", "abs": "https://arxiv.org/abs/2512.04751", "authors": ["Junhao Wei", "Yanzhao Gu", "Ran Zhang", "Mingjing Huang", "Jinhong Song", "Yanxiao Li", "Wenxuan Zhu", "Yapeng Wang", "Zikun Li", "Zhiwen Wang", "Xu Yang", "Ngai Cheong"], "title": "NAWOA-XGBoost: A Novel Model for Early Prediction of Academic Potential in Computer Science Students", "comment": null, "summary": "Whale Optimization Algorithm (WOA) suffers from limited global search ability, slow convergence, and tendency to fall into local optima, restricting its effectiveness in hyperparameter optimization for machine learning models. To address these issues, this study proposes a Nonlinear Adaptive Whale Optimization Algorithm (NAWOA), which integrates strategies such as Good Nodes Set initialization, Leader-Followers Foraging, Dynamic Encircling Prey, Triangular Hunting, and a nonlinear convergence factor to enhance exploration, exploitation, and convergence stability. Experiments on 23 benchmark functions demonstrate NAWOA's superior optimization capability and robustness. Based on this optimizer, an NAWOA-XGBoost model was developed to predict academic potential using data from 495 Computer Science undergraduates at Macao Polytechnic University (2009-2019). Results show that NAWOA-XGBoost outperforms traditional XGBoost and WOA-XGBoost across key metrics, including Accuracy (0.8148), Macro F1 (0.8101), AUC (0.8932), and G-Mean (0.8172), demonstrating strong adaptability on multi-class imbalanced datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5NAWOA\uff0c\u5e76\u5c06\u5176\u7528\u4e8e\u4f18\u5316XGBoost\u6a21\u578b\u7684\u8d85\u53c2\u6570\uff0c\u4ee5\u9884\u6d4b\u5b66\u751f\u7684\u5b66\u672f\u6f5c\u529b\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u539f\u59cb\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\uff08WOA\uff09\u5b58\u5728\u5168\u5c40\u641c\u7d22\u80fd\u529b\u6709\u9650\u3001\u6536\u655b\u901f\u5ea6\u6162\u53ca\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u5b66\u4e60\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u975e\u7ebf\u6027\u81ea\u9002\u5e94\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\uff08NAWOA\uff09\uff0c\u878d\u5408\u4f18\u8d28\u8282\u70b9\u96c6\u521d\u59cb\u5316\u3001\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u89c5\u98df\u3001\u52a8\u6001\u5305\u56f4\u730e\u7269\u3001\u4e09\u89d2\u72e9\u730e\u7b56\u7565\u53ca\u975e\u7ebf\u6027\u6536\u655b\u56e0\u5b50\uff1b\u5e76\u57fa\u4e8eNAWOA\u6784\u5efaNAWOA-XGBoost\u6a21\u578b\u7528\u4e8e\u5b66\u672f\u6f5c\u529b\u9884\u6d4b\u3002", "result": "\u572823\u4e2a\u57fa\u51c6\u51fd\u6570\u4e0a\u9a8c\u8bc1\u4e86NAWOA\u7684\u4f18\u8d8a\u6027\u80fd\uff1b\u5728495\u540d\u8ba1\u7b97\u673a\u79d1\u5b66\u672c\u79d1\u751f\u6570\u636e\u96c6\u4e0a\uff0cNAWOA-XGBoost\u5728Accuracy\uff080.8148\uff09\u3001Macro F1\uff080.8101\uff09\u3001AUC\uff080.8932\uff09\u548cG-Mean\uff080.8172\uff09\u7b49\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfXGBoost\u548cWOA-XGBoost\u3002", "conclusion": "NAWOA\u663e\u8457\u63d0\u5347\u4e86\u4f18\u5316\u6027\u80fd\u4e0e\u7a33\u5b9a\u6027\uff0c\u6240\u6784\u5efa\u7684NAWOA-XGBoost\u6a21\u578b\u5728\u591a\u7c7b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u9884\u6d4b\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2512.04776", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.04776", "abs": "https://arxiv.org/abs/2512.04776", "authors": ["Joaquin Luque", "Alejandro Carrasco", "Enrique Personal", "Francisco Perez", "Carlos Leon"], "title": "Customer Identification for Electricity Retailers Based on Monthly Demand Profiles by Activity Sectors and Locations", "comment": null, "summary": "The increasing competition in the electric sector is challenging retail companies as they must assign its commercial efforts to attract the most profitable customers. Those are whose energy demand best fit certain target profiles, which usually depend on generation or cost policies. But, even when the demand profile is available, it is in an anonymous way, preventing its association to a particular client. In this paper, we explore a large dataset containing several millions of monthly demand profiles in Spain and use the available information about the associated economic sector and location for an indirect identification of the customers. The distance of the demand profile from the target is used to define a key performance indicator (KPI) which is used as the main driver of the proposed marketing strategy. The combined use of activity and location has been revealed as a powerful tool for indirect identification of customers, as 100,000 customers are uniquely identified, while about 300,000 clients are identifiable in small sets containing 10 or less consumers. To assess the proposed marketing strategy, it has been compared to the random attraction of new clients, showing a reduction of distance from the target of 40% for 10,000 new customers.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u897f\u73ed\u7259\u6570\u767e\u4e07\u7528\u6237\u7684\u6708\u5ea6\u7528\u7535\u6570\u636e\uff0c\u7ed3\u5408\u5176\u6240\u5c5e\u7ecf\u6d4e\u90e8\u95e8\u548c\u5730\u7406\u4f4d\u7f6e\u4fe1\u606f\uff0c\u95f4\u63a5\u8bc6\u522b\u51fa\u9ad8\u4ef7\u503c\u5ba2\u6237\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7528\u7535\u66f2\u7ebf\u4e0e\u76ee\u6807\u66f2\u7ebf\u8ddd\u79bb\u7684\u8425\u9500\u7b56\u7565\uff0c\u5728\u5438\u5f15\u65b0\u5ba2\u6237\u65f6\u6bd4\u968f\u673a\u7b56\u7565\u51cf\u5c1140%\u7684\u8ddd\u79bb\u504f\u5dee\u3002", "motivation": "\u7535\u529b\u96f6\u552e\u4f01\u4e1a\u9762\u4e34\u6fc0\u70c8\u7ade\u4e89\uff0c\u9700\u7cbe\u51c6\u8bc6\u522b\u6700\u6709\u5229\u53ef\u56fe\u7684\u5ba2\u6237\uff1b\u7136\u800c\u5ba2\u6237\u7528\u7535\u6570\u636e\u901a\u5e38\u533f\u540d\uff0c\u96be\u4ee5\u76f4\u63a5\u5173\u8054\u5230\u5177\u4f53\u7528\u6237\u3002", "method": "\u5229\u7528\u5305\u542b\u6570\u767e\u4e07\u6708\u5ea6\u7528\u7535\u66f2\u7ebf\u7684\u5927\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5ba2\u6237\u7684\u7ecf\u6d4e\u90e8\u95e8\u548c\u5730\u7406\u4f4d\u7f6e\u4fe1\u606f\u8fdb\u884c\u95f4\u63a5\u8bc6\u522b\uff1b\u5b9a\u4e49\u7528\u7535\u66f2\u7ebf\u4e0e\u76ee\u6807\u66f2\u7ebf\u8ddd\u79bb\u4f5c\u4e3a\u5173\u952e\u7ee9\u6548\u6307\u6807\uff08KPI\uff09\uff0c\u9a71\u52a8\u8425\u9500\u7b56\u7565\u3002", "result": "\u6210\u529f\u552f\u4e00\u8bc6\u522b\u51fa10\u4e07\u5ba2\u6237\uff0c\u53e6\u6709\u7ea630\u4e07\u5ba2\u6237\u88ab\u5f52\u5165\u4e0d\u8d85\u8fc710\u4eba\u7684\u5c0f\u96c6\u5408\u4e2d\uff1b\u76f8\u6bd4\u968f\u673a\u5438\u5f15\u5ba2\u6237\uff0c\u8be5\u7b56\u7565\u5728\u83b7\u53d61\u4e07\u540d\u65b0\u5ba2\u6237\u65f6\u4f7f\u4e0e\u76ee\u6807\u66f2\u7ebf\u7684\u5e73\u5747\u8ddd\u79bb\u51cf\u5c1140%\u3002", "conclusion": "\u7ed3\u5408\u7ecf\u6d4e\u6d3b\u52a8\u7c7b\u578b\u4e0e\u5730\u7406\u4f4d\u7f6e\u80fd\u6709\u6548\u5b9e\u73b0\u5ba2\u6237\u95f4\u63a5\u8bc6\u522b\uff0c\u6240\u63d0\u51fa\u7684\u57fa\u4e8eKPI\u7684\u8425\u9500\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u968f\u673a\u7b56\u7565\uff0c\u6709\u52a9\u4e8e\u7535\u529b\u96f6\u552e\u5546\u66f4\u9ad8\u6548\u5730\u5438\u5f15\u9ad8\u4ef7\u503c\u5ba2\u6237\u3002"}}
{"id": "2512.04088", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04088", "abs": "https://arxiv.org/abs/2512.04088", "authors": ["Kolichala Rajashekar", "Nafiseh Sharghivand", "Radu Prodan", "Reza Farahani"], "title": "Toward Sustainability-Aware LLM Inference on Edge Clusters", "comment": "4 pages, 5 figures, 3 tables, conference paper", "summary": "Large language models (LLMs) require substantial computational resources, leading to significant carbon emissions and operational costs. Although training is energy-intensive, the long-term environmental burden arises from inference, amplified by the massive global query volume. Cloud-based inference offers scalability but suffers from latency and bandwidth constraints due to centralized processing and continuous data transfer. Edge clusters instead can mitigate these limitations by enabling localized execution, yet they face trade-offs between performance, energy efficiency, and device constraints. This short paper presents a sustainability-aware LLM inference for edge clusters comprising NVIDIA Jetson Orin NX (8GB) and Nvidia Ada 2000 (16GB) devices. It aims to balance inference latency and carbon footprint through carbon- and latency-aware routing strategies, guided by empirical benchmarking of energy consumption and execution time across diverse prompts and batch (i.e., group of prompts) configurations. We compared baseline greedy strategies to carbon-aware and latency-aware strategies in prompt routing to specific hardware based on benchmarking information. Experimental evaluation shows that a batch size of four prompts achieves a trade-off between throughput, energy efficiency, while larger batches risk GPU memory saturation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u8fb9\u7f18\u96c6\u7fa4\u7684\u53ef\u6301\u7eedLLM\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u78b3\u611f\u77e5\u548c\u5ef6\u8fdf\u611f\u77e5\u7684\u8def\u7531\u7b56\u7565\uff0c\u5728NVIDIA Jetson Orin NX\u4e0eAda 2000\u8bbe\u5907\u4e0a\u5e73\u8861\u63a8\u7406\u5ef6\u8fdf\u4e0e\u78b3\u8db3\u8ff9\uff0c\u5e76\u57fa\u4e8e\u5b9e\u8bc1\u8bc4\u4f30\u786e\u5b9a\u6279\u91cf\u5927\u5c0f\u4e3a4\u65f6\u53ef\u517c\u987e\u541e\u5410\u91cf\u4e0e\u80fd\u6548\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u5728\u4e91\u7aef\u90e8\u7f72\u5b58\u5728\u9ad8\u5ef6\u8fdf\u4e0e\u5e26\u5bbd\u9650\u5236\uff0c\u800c\u8fb9\u7f18\u8ba1\u7b97\u867d\u80fd\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5374\u9700\u6743\u8861\u6027\u80fd\u3001\u80fd\u6548\u4e0e\u8bbe\u5907\u8d44\u6e90\u7ea6\u675f\uff1b\u540c\u65f6\uff0c\u5927\u89c4\u6a21\u63a8\u7406\u5e26\u6765\u7684\u78b3\u6392\u653e\u95ee\u9898\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u5728\u7531Jetson Orin NX\uff088GB\uff09\u548cAda 2000\uff0816GB\uff09\u7ec4\u6210\u7684\u8fb9\u7f18\u96c6\u7fa4\u4e0a\uff0c\u57fa\u4e8e\u5bf9\u4e0d\u540c\u63d0\u793a\u548c\u6279\u5904\u7406\u914d\u7f6e\u4e0b\u7684\u80fd\u8017\u4e0e\u6267\u884c\u65f6\u95f4\u7684\u5b9e\u8bc1\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bbe\u8ba1\u5e76\u6bd4\u8f83\u4e86\u78b3\u611f\u77e5\u3001\u5ef6\u8fdf\u611f\u77e5\u53ca\u8d2a\u5a6a\u57fa\u7ebf\u7684\u63d0\u793a\u8def\u7531\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6279\u91cf\u5927\u5c0f\u4e3a4\u53ef\u5728\u541e\u5410\u91cf\u4e0e\u80fd\u6548\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u800c\u66f4\u5927\u7684\u6279\u91cf\u6613\u5bfc\u81f4GPU\u5185\u5b58\u9971\u548c\uff1b\u78b3\u611f\u77e5\u4e0e\u5ef6\u8fdf\u611f\u77e5\u7b56\u7565\u6709\u6548\u4f18\u5316\u4e86\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u78b3\u8db3\u8ff9\u4e0e\u54cd\u5e94\u5ef6\u8fdf\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u786c\u4ef6\u7279\u6027\u548c\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\uff0c\u91c7\u7528\u53ef\u6301\u7eed\u5bfc\u5411\u7684\u8def\u7531\u7b56\u7565\u53ef\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u4e14\u4f4e\u78b3\u7684LLM\u63a8\u7406\u3002"}}
{"id": "2512.04320", "categories": ["cs.DC", "cs.OS"], "pdf": "https://arxiv.org/pdf/2512.04320", "abs": "https://arxiv.org/abs/2512.04320", "authors": ["Yineng Yan", "William Ruys", "Hochan Lee", "Ian Henriksen", "Arthur Peters", "Sean Stephens", "Bozhi You", "Henrique Fingler", "Martin Burtscher", "Milos Gligoric", "Keshav Pingali", "Mattan Erez", "George Biros", "Christopher J. Rossbach"], "title": "VLCs: Managing Parallelism with Virtualized Libraries", "comment": "Research Paper accepted to the ACM Symposium on Cloud Computing (SoCC'25)", "summary": "As the complexity and scale of modern parallel machines continue to grow, programmers increasingly rely on composition of software libraries to encapsulate and exploit parallelism. However, many libraries are not designed with composition in mind and assume they have exclusive access to all resources. Using such libraries concurrently can result in contention and degraded performance. Prior solutions involve modifying the libraries or the OS, which is often infeasible.\n  We propose Virtual Library Contexts (VLCs), which are process subunits that encapsulate sets of libraries and associated resource allocations. VLCs control the resource utilization of these libraries without modifying library code. This enables the user to partition resources between libraries to prevent contention, or load multiple copies of the same library to allow parallel execution of otherwise thread-unsafe code within the same process.\n  In this paper, we describe and evaluate C++ and Python prototypes of VLCs. Experiments show VLCs enable a speedup up to 2.85x on benchmarks including applications using OpenMP, OpenBLAS, and LibTorch.", "AI": {"tldr": "\u63d0\u51fa\u865a\u62df\u5e93\u4e0a\u4e0b\u6587\uff08VLCs\uff09\u673a\u5236\uff0c\u65e0\u9700\u4fee\u6539\u5e93\u4ee3\u7801\u5373\u53ef\u5728\u5355\u8fdb\u7a0b\u4e2d\u5bf9\u5e76\u884c\u5e93\u8fdb\u884c\u8d44\u6e90\u9694\u79bb\u4e0e\u591a\u5b9e\u4f8b\u52a0\u8f7d\uff0c\u4ece\u800c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u5e76\u884c\u7a0b\u5e8f\u5e38\u7ec4\u5408\u4f7f\u7528\u591a\u4e2a\u5e76\u884c\u5e93\uff0c\u4f46\u8fd9\u4e9b\u5e93\u901a\u5e38\u5047\u5b9a\u72ec\u5360\u7cfb\u7edf\u8d44\u6e90\uff0c\u5bfc\u81f4\u5e76\u53d1\u4f7f\u7528\u65f6\u4ea7\u751f\u8d44\u6e90\u4e89\u7528\u548c\u6027\u80fd\u4e0b\u964d\uff1b\u800c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u4fee\u6539\u5e93\u6216\u64cd\u4f5c\u7cfb\u7edf\uff0c\u53ef\u884c\u6027\u4f4e\u3002", "method": "\u5f15\u5165\u865a\u62df\u5e93\u4e0a\u4e0b\u6587\uff08VLCs\uff09\uff0c\u4f5c\u4e3a\u8fdb\u7a0b\u5185\u7684\u5b50\u5355\u5143\uff0c\u5c01\u88c5\u5e93\u53ca\u5176\u8d44\u6e90\u5206\u914d\uff0c\u63a7\u5236\u5176\u8d44\u6e90\u4f7f\u7528\uff0c\u652f\u6301\u8d44\u6e90\u5212\u5206\u548c\u540c\u4e00\u5e93\u7684\u591a\u526f\u672c\u52a0\u8f7d\u3002", "result": "\u5728C++\u548cPython\u4e2d\u5b9e\u73b0VLC\u539f\u578b\uff0c\u5728\u5305\u542bOpenMP\u3001OpenBLAS\u548cLibTorch\u7684\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6700\u9ad8\u83b7\u5f972.85\u500d\u52a0\u901f\u3002", "conclusion": "VLCs\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u73b0\u6709\u5e76\u884c\u5e93\u5373\u53ef\u6709\u6548\u7ba1\u7406\u8d44\u6e90\u4e89\u7528\u548c\u63d0\u5347\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.04527", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04527", "abs": "https://arxiv.org/abs/2512.04527", "authors": ["Xingyu Liu", "Jiawei Liang", "Linfeng Du", "Yipu Zhang", "Chaofang Ma", "Hanwei Fan", "Jiang Xu", "Wei Zhang"], "title": "FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration", "comment": null, "summary": "In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FLEX\uff0c\u4e00\u79cd\u7528\u4e8e\u6df7\u5408\u5355\u5143\u9ad8\u5ea6\u5408\u6cd5\u5316\u4efb\u52a1\u7684FPGA-CPU\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u4f18\u5316\u4efb\u52a1\u5206\u914d\u3001\u91c7\u7528\u591a\u7c92\u5ea6\u6d41\u6c34\u7ebf\u6280\u672f\u4ee5\u53ca\u9488\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u578b\u5355\u5143\u79fb\u52a8\u8fc7\u7a0b\u8fdb\u884c\u4f18\u5316\uff0c\u5728\u901f\u5ea6\u548c\u5408\u6cd5\u5316\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709CPU-GPU\u548c\u591a\u7ebf\u7a0bCPU\u65b9\u6848\u3002", "motivation": "\u6df7\u5408\u5355\u5143\u9ad8\u5ea6\u5408\u6cd5\u5316\u662f\u7269\u7406\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u4f46\u5176\u8ba1\u7b97\u5bc6\u96c6\u4e14\u8017\u65f6\u3002\u73b0\u6709\u57fa\u4e8eCPU\u6216GPU\u7684\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u6765\u63d0\u5347\u5408\u6cd5\u5316\u901f\u5ea6\u4e0e\u8d28\u91cf\u3002", "method": "\u4f5c\u8005\u63d0\u51faFLEX\u52a0\u901f\u5668\uff0c\u7ed3\u5408FPGA\u4e0eCPU\u7684\u4f18\u52bf\uff1a\u9996\u5148\u4f18\u5316\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u9ad8\u6548\u5212\u5206FPGA\u4e0eCPU\u7684\u4efb\u52a1\uff1b\u5176\u6b21\u91c7\u7528\u591a\u7c92\u5ea6\u6d41\u6c34\u7ebf\u6280\u672f\u52a0\u901f\u6700\u8017\u65f6\u7684\u201c\u5bfb\u627e\u6700\u4f18\u653e\u7f6e\u4f4d\u7f6e\uff08FOP\uff09\u201d\u6b65\u9aa4\uff1b\u6700\u540e\u4e13\u95e8\u4f18\u5316FOP\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u7684\u5355\u5143\u79fb\u52a8\u8fc7\u7a0b\uff0c\u4f7f\u5176\u4e0e\u591a\u7c92\u5ea6\u6d41\u6c34\u7ebf\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFLEX\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684CPU-GPU\u548c\u591a\u7ebf\u7a0bCPU\u5408\u6cd5\u5316\u5668\uff0c\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad818.3\u500d\u548c5.4\u500d\u7684\u52a0\u901f\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u5c06\u5408\u6cd5\u5316\u8d28\u91cf\u63d0\u5347\u4e864%\u548c1%\u3002", "conclusion": "FLEX\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u5355\u5143\u9ad8\u5ea6\u5408\u6cd5\u5316\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5728\u901f\u5ea6\u548c\u8d28\u91cf\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86FPGA-CPU\u5f02\u6784\u67b6\u6784\u5728\u7535\u5b50\u8bbe\u8ba1\u81ea\u52a8\u5316\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.04106", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.04106", "abs": "https://arxiv.org/abs/2512.04106", "authors": ["Fouad Trad", "Ali Chehab"], "title": "Retrieval-Augmented Few-Shot Prompting Versus Fine-Tuning for Code Vulnerability Detection", "comment": "Accepted in the 3rd International Conference on Foundation and Large Language Models (FLLM2025)", "summary": "Few-shot prompting has emerged as a practical alternative to fine-tuning for leveraging the capabilities of large language models (LLMs) in specialized tasks. However, its effectiveness depends heavily on the selection and quality of in-context examples, particularly in complex domains. In this work, we examine retrieval-augmented prompting as a strategy to improve few-shot performance in code vulnerability detection, where the goal is to identify one or more security-relevant weaknesses present in a given code snippet from a predefined set of vulnerability categories. We perform a systematic evaluation using the Gemini-1.5-Flash model across three approaches: (1) standard few-shot prompting with randomly selected examples, (2) retrieval-augmented prompting using semantically similar examples, and (3) retrieval-based labeling, which assigns labels based on retrieved examples without model inference. Our results show that retrieval-augmented prompting consistently outperforms the other prompting strategies. At 20 shots, it achieves an F1 score of 74.05% and a partial match accuracy of 83.90%. We further compare this approach against zero-shot prompting and several fine-tuned models, including Gemini-1.5-Flash and smaller open-source models such as DistilBERT, DistilGPT2, and CodeBERT. Retrieval-augmented prompting outperforms both zero-shot (F1 score: 36.35%, partial match accuracy: 20.30%) and fine-tuned Gemini (F1 score: 59.31%, partial match accuracy: 53.10%), while avoiding the training time and cost associated with model fine-tuning. On the other hand, fine-tuning CodeBERT yields higher performance (F1 score: 91.22%, partial match accuracy: 91.30%) but requires additional training, maintenance effort, and resources.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u63d0\u793a\uff08retrieval-augmented prompting\uff09\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5c11\u6837\u672c\u5b66\u4e60\u6027\u80fd\u7684\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u968f\u673a\u793a\u4f8b\u7684\u5c11\u6837\u672c\u63d0\u793a\u3001\u96f6\u6837\u672c\u63d0\u793a\u4ee5\u53ca\u5fae\u8c03\u540e\u7684Gemini\u6a21\u578b\uff0c\u572820\u4e2a\u793a\u4f8b\u4e0b\u8fbe\u523074.05%\u7684F1\u5206\u6570\u548c83.90%\u7684\u90e8\u5206\u5339\u914d\u51c6\u786e\u7387\uff0c\u4e14\u65e0\u9700\u5fae\u8c03\u6210\u672c\u3002", "motivation": "\u5c11\u6837\u672c\u63d0\u793a\u7684\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u4e0a\u4e0b\u6587\u793a\u4f8b\u7684\u8d28\u91cf\u4e0e\u9009\u62e9\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u5982\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u5e0c\u671b\u63a2\u7d22\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7b56\u7565\u6765\u63d0\u5347\u63d0\u793a\u6548\u679c\uff0c\u4ee5\u907f\u514d\u5fae\u8c03\u5e26\u6765\u7684\u9ad8\u6210\u672c\u3002", "method": "\u4f5c\u8005\u5728Gemini-1.5-Flash\u6a21\u578b\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u4e09\u79cd\u7b56\u7565\uff1a(1) \u968f\u673a\u9009\u62e9\u793a\u4f8b\u7684\u6807\u51c6\u5c11\u6837\u672c\u63d0\u793a\uff1b(2) \u4f7f\u7528\u8bed\u4e49\u76f8\u4f3c\u793a\u4f8b\u7684\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\uff1b(3) \u57fa\u4e8e\u68c0\u7d22\u793a\u4f8b\u76f4\u63a5\u6253\u6807\u7b7e\u7684\u65e0\u6a21\u578b\u63a8\u7406\u65b9\u6cd5\u3002\u540c\u65f6\u4e0e\u96f6\u6837\u672c\u63d0\u793a\u53ca\u591a\u4e2a\u5fae\u8c03\u6a21\u578b\uff08\u5305\u62ecGemini\u3001DistilBERT\u3001DistilGPT2\u3001CodeBERT\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u5728\u6240\u6709\u63d0\u793a\u7b56\u7565\u4e2d\u8868\u73b0\u6700\u4f73\uff1a20-shot\u4e0bF1\u8fbe74.05%\uff0c\u90e8\u5206\u5339\u914d\u51c6\u786e\u738783.90%\uff1b\u4f18\u4e8e\u96f6\u6837\u672c\uff08F1: 36.35%\uff09\u548c\u5fae\u8c03Gemini\uff08F1: 59.31%\uff09\u3002\u867d\u7136\u5fae\u8c03CodeBERT\u6027\u80fd\u66f4\u9ad8\uff08F1: 91.22%\uff09\uff0c\u4f46\u9700\u989d\u5916\u8bad\u7ec3\u8d44\u6e90\u3002", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u63d0\u793a\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u4f4e\u6210\u672c\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u5728\u4e0d\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5c11\u6837\u672c\u6027\u80fd\u3002"}}
{"id": "2512.04086", "categories": ["cs.DB", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04086", "abs": "https://arxiv.org/abs/2512.04086", "authors": ["Sepideh Masoudi", "Sebastian Werner", "Pierluigi Plebani", "Stefan Tai"], "title": "Energy Profiling of Data-Sharing Pipelines: Modeling, Estimation, and Reuse Strategies", "comment": null, "summary": "Data-sharing pipelines involve a series of stages that apply policy-based data transformations to enable secure and effective data exchange among organizations. Although numerous tools and platforms exist to manage governance and enforcement in these pipelines, energy efficiency in data exchange has received limited attention. This paper introduces a novel method to model and estimate the energy consumption of different execution configurations in data-sharing pipelines. Additionally, this method identifies reuse potential in shared stages across pipelines that hold the key to reducing energy in large data-sharing federations. We validate this method through simulation experiments, revealing promising potential for cross-organizational pipeline optimization and laying a foundation for energy-conscious execution strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u548c\u4f30\u7b97\u6570\u636e\u5171\u4eab\u7ba1\u9053\u4e2d\u4e0d\u540c\u6267\u884c\u914d\u7f6e\u7684\u80fd\u8017\uff0c\u5e76\u8bc6\u522b\u8de8\u7ba1\u9053\u5171\u4eab\u9636\u6bb5\u7684\u590d\u7528\u6f5c\u529b\uff0c\u4ece\u800c\u964d\u4f4e\u5927\u578b\u6570\u636e\u5171\u4eab\u8054\u76df\u4e2d\u7684\u80fd\u8017\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5171\u4eab\u7ba1\u9053\u7684\u7814\u7a76\u591a\u805a\u7126\u4e8e\u6cbb\u7406\u4e0e\u7b56\u7565\u6267\u884c\uff0c\u800c\u5bf9\u80fd\u6e90\u6548\u7387\u7684\u5173\u6ce8\u4e0d\u8db3\uff1b\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u5347\u6570\u636e\u4ea4\u6362\u8fc7\u7a0b\u4e2d\u7684\u80fd\u6548\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5efa\u6a21\u4e0e\u4f30\u7b97\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u6267\u884c\u914d\u7f6e\u7684\u80fd\u8017\uff0c\u5e76\u8bc6\u522b\u591a\u4e2a\u7ba1\u9053\u4e2d\u53ef\u590d\u7528\u7684\u5171\u4eab\u9636\u6bb5\u4ee5\u4f18\u5316\u80fd\u6e90\u4f7f\u7528\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u8de8\u7ec4\u7ec7\u7ba1\u9053\u4f18\u5316\u5728\u8282\u80fd\u65b9\u9762\u7684\u663e\u8457\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u8282\u80fd\u578b\u6570\u636e\u5171\u4eab\u7ba1\u9053\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9762\u5411\u80fd\u6548\u7684\u6570\u636e\u4ea4\u6362\u7b56\u7565\u53d1\u5c55\u3002"}}
{"id": "2512.04089", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04089", "abs": "https://arxiv.org/abs/2512.04089", "authors": ["Mario Colosi", "Reza Farahani", "Lauri Loven", "Radu Prodan", "Massimo Villari"], "title": "Serverless Everywhere: A Comparative Analysis of WebAssembly Workflows Across Browser, Edge, and Cloud", "comment": "7 pages, 8 Figures, 2 Tables, conference paper", "summary": "WebAssembly (Wasm) is a binary instruction format that enables portable, sandboxed, and near-native execution across heterogeneous platforms, making it well-suited for serverless workflow execution on browsers, edge nodes, and cloud servers. However, its performance and stability depend heavily on factors such as startup overhead, runtime execution model (e.g., Ahead-of-Time (AOT) and Just-in-Time (JIT) compilation), and resource variability across deployment contexts. This paper evaluates a Wasm-based serverless workflow executed consistently from the browser to edge and cloud instances. The setup uses wasm32-wasi modules: in the browser, execution occurs within a web worker, while on Edge and Cloud, an HTTP shim streams frames to the Wasm runtime. We measure cold- and warm-start latency, per-step delays, workflow makespan, throughput, and CPU/memory utilization to capture the end-to-end behavior across environments. Results show that AOT compilation and instance warming substantially reduce startup latency. For workflows with small payloads, the browser achieves competitive performance owing to fully in-memory data exchanges. In contrast, as payloads grow, the workflow transitions into a compute- and memory-intensive phase where AOT execution on edge and cloud nodes distinctly surpasses browser performance.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86 WebAssembly\uff08Wasm\uff09\u5728\u6d4f\u89c8\u5668\u3001\u8fb9\u7f18\u548c\u4e91\u73af\u5883\u4e2d\u6267\u884c\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\uff0c\u53d1\u73b0 AOT \u7f16\u8bd1\u548c\u5b9e\u4f8b\u9884\u70ed\u663e\u8457\u964d\u4f4e\u542f\u52a8\u5ef6\u8fdf\uff0c\u4e14\u5728\u5904\u7406\u5927\u6570\u636e\u8d1f\u8f7d\u65f6\uff0c\u8fb9\u7f18\u548c\u4e91\u8282\u70b9\u7684 AOT \u6267\u884c\u660e\u663e\u4f18\u4e8e\u6d4f\u89c8\u5668\u3002", "motivation": "WebAssembly \u867d\u5177\u5907\u8de8\u5e73\u53f0\u3001\u6c99\u7bb1\u5316\u548c\u63a5\u8fd1\u539f\u751f\u6267\u884c\u7684\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u6d41\uff0c\u4f46\u5176\u6027\u80fd\u53d7\u542f\u52a8\u5f00\u9500\u3001\u8fd0\u884c\u65f6\u7f16\u8bd1\u6a21\u578b\u53ca\u90e8\u7f72\u73af\u5883\u8d44\u6e90\u5dee\u5f02\u5f71\u54cd\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528 wasm32-wasi \u6a21\u5757\u5728\u6d4f\u89c8\u5668\uff08\u901a\u8fc7 Web Worker\uff09\u3001\u8fb9\u7f18\u548c\u4e91\u8282\u70b9\uff08\u901a\u8fc7 HTTP shim \u6d41\u5f0f\u4f20\u8f93\u5e27\u81f3 Wasm \u8fd0\u884c\u65f6\uff09\u7edf\u4e00\u6267\u884c\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u6d41\uff0c\u5e76\u6d4b\u91cf\u51b7/\u70ed\u542f\u52a8\u5ef6\u8fdf\u3001\u6bcf\u6b65\u5ef6\u8fdf\u3001\u5de5\u4f5c\u6d41\u603b\u5b8c\u6210\u65f6\u95f4\u3001\u541e\u5410\u91cf\u53ca CPU/\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\u3002", "result": "AOT \u7f16\u8bd1\u548c\u5b9e\u4f8b\u9884\u70ed\u5927\u5e45\u51cf\u5c11\u542f\u52a8\u5ef6\u8fdf\uff1b\u5c0f\u8d1f\u8f7d\u4e0b\u6d4f\u89c8\u5668\u56e0\u5168\u5185\u5b58\u6570\u636e\u4ea4\u6362\u8868\u73b0\u826f\u597d\uff1b\u5927\u8d1f\u8f7d\u4e0b\u8fb9\u7f18\u548c\u4e91\u8282\u70b9\u7684 AOT \u6267\u884c\u5728\u8ba1\u7b97\u4e0e\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u6d4f\u89c8\u5668\u3002", "conclusion": "WebAssembly \u5728\u4e0d\u540c\u90e8\u7f72\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u5dee\u5f02\u5316\u6027\u80fd\u7279\u5f81\uff0c\u5408\u7406\u9009\u62e9\u7f16\u8bd1\u7b56\u7565\u548c\u90e8\u7f72\u4f4d\u7f6e\u53ef\u4f18\u5316\u65e0\u670d\u52a1\u5668\u5de5\u4f5c\u6d41\u7684\u6574\u4f53\u6548\u7387\u3002"}}
{"id": "2512.04111", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.04111", "abs": "https://arxiv.org/abs/2512.04111", "authors": ["Hanjun Luo", "Chiming Ni", "Jiaheng Wen", "Zhimu Huang", "Yiran Wang", "Bingduo Liao", "Sylvia Chung", "Yingbin Jin", "Xinfeng Li", "Wenyuan Xu", "XiaoFeng Wang", "Hanan Salam"], "title": "HAI-Eval: Measuring Human-AI Synergy in Collaborative Coding", "comment": null, "summary": "LLM-powered coding agents are reshaping the development paradigm. However, existing evaluation systems, neither traditional tests for humans nor benchmarks for LLMs, fail to capture this shift. They remain focused on well-defined algorithmic problems, which excludes problems where success depends on human-AI collaboration. Such collaborative problems not only require human reasoning to interpret complex contexts and guide solution strategies, but also demand AI efficiency for implementation. To bridge this gap, we introduce HAI-Eval, a unified benchmark designed to measure the synergy of human-AI partnership in coding. HAI-Eval's core innovation is its \"Collaboration-Necessary\" problem templates, which are intractable for both standalone LLMs and unaided humans, but solvable through effective collaboration. Specifically, HAI-Eval uses 45 templates to dynamically create tasks. It also provides a standardized IDE for human participants and a reproducible toolkit with 450 task instances for LLMs, ensuring an ecologically valid evaluation. We conduct a within-subject study with 45 participants and benchmark their performance against 5 state-of-the-art LLMs under 4 different levels of human intervention. Results show that standalone LLMs and unaided participants achieve poor pass rates (0.67% and 18.89%), human-AI collaboration significantly improves performance to 31.11%. Our analysis reveals an emerging co-reasoning partnership. This finding challenges the traditional human-tool hierarchy by showing that strategic breakthroughs can originate from either humans or AI. HAI-Eval establishes not only a challenging benchmark for next-generation coding agents but also a grounded, scalable framework for assessing core developer competencies in the AI era. Our benchmark and interactive demo will be openly accessible.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 HAI-Eval\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4eba\u7c7b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u534f\u4f5c\u6548\u80fd\u7684\u7edf\u4e00\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5305\u542b45\u4e2a\u201c\u5fc5\u987b\u534f\u4f5c\u201d\u95ee\u9898\u6a21\u677f\uff0c\u8fd9\u4e9b\u95ee\u9898\u5bf9\u5355\u72ec\u7684\u4eba\u7c7b\u6216LLM\u90fd\u96be\u4ee5\u89e3\u51b3\uff0c\u4f46\u901a\u8fc7\u6709\u6548\u534f\u4f5c\u53ef\u6210\u529f\u5b8c\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4eba\u673a\u534f\u4f5c\u663e\u8457\u4f18\u4e8e\u5404\u81ea\u72ec\u7acb\u8868\u73b0\uff0c\u5e76\u63ed\u793a\u4e86\u4e00\u79cd\u65b0\u5174\u7684\u5171\u540c\u63a8\u7406\u4f19\u4f34\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u4f53\u7cfb\u65e0\u6cd5\u8861\u91cf\u4eba\u7c7b\u4e0eLLM\u5728\u7f16\u7a0b\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u56e0\u5176\u805a\u7126\u4e8e\u5b9a\u4e49\u660e\u786e\u7684\u7b97\u6cd5\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u9700\u4eba\u673a\u534f\u540c\u89e3\u51b3\u7684\u590d\u6742\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u53cd\u6620\u771f\u5b9e\u5f00\u53d1\u573a\u666f\u4e2d\u4eba\u673a\u534f\u540c\u6548\u80fd\u7684\u65b0\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u4e86 HAI-Eval \u57fa\u51c6\uff0c\u5305\u542b45\u4e2a\u201c\u534f\u4f5c\u5fc5\u8981\u578b\u201d\u95ee\u9898\u6a21\u677f\uff0c\u53ef\u52a8\u6001\u751f\u6210\u4efb\u52a1\uff1b\u63d0\u4f9b\u6807\u51c6\u5316IDE\u4f9b\u4eba\u7c7b\u4f7f\u7528\uff0c\u5e76\u4e3aLLM\u63d0\u4f9b\u542b450\u4e2a\u4efb\u52a1\u5b9e\u4f8b\u7684\u53ef\u590d\u73b0\u5de5\u5177\u5305\u3002\u901a\u8fc7\u4e00\u9879\u5305\u542b45\u540d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\uff0c\u5728\u56db\u79cd\u4eba\u7c7b\u5e72\u9884\u7a0b\u5ea6\u4e0b\u5bf9\u6bd45\u4e2a\u5148\u8fdbLLM\u7684\u8868\u73b0\u3002", "result": "\u5355\u72ecLLM\u548c\u672a\u8f85\u52a9\u4eba\u7c7b\u7684\u901a\u8fc7\u7387\u6781\u4f4e\uff08\u5206\u522b\u4e3a0.67%\u548c18.89%\uff09\uff0c\u800c\u4eba\u673a\u534f\u4f5c\u5c06\u901a\u8fc7\u7387\u63d0\u5347\u81f331.11%\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6218\u7565\u7a81\u7834\u53ef\u6765\u81ea\u4eba\u7c7b\u6216AI\uff0c\u4f53\u73b0\u51fa\u5171\u63a8\u7406\u7684\u5408\u4f5c\u6a21\u5f0f\u3002", "conclusion": "HAI-Eval \u4e0d\u4ec5\u4e3a\u4e0b\u4e00\u4ee3\u7f16\u7801\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e5f\u4e3aAI\u65f6\u4ee3\u5f00\u53d1\u8005\u6838\u5fc3\u80fd\u529b\u7684\u8bc4\u4f30\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u3001\u8d34\u8fd1\u5b9e\u9645\u7684\u6846\u67b6\u3002"}}
{"id": "2512.04380", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.04380", "abs": "https://arxiv.org/abs/2512.04380", "authors": ["Kitae Kim", "Yan Kyaw Tun", "Md. Shirajum Munir", "Chirsto Kurisummoottil Thomas", "Walid Saad", "Choong Seon Hong"], "title": "Vision and Causal Learning Based Channel Estimation for THz Communications", "comment": "Submitted to IEEE Transactions on Mobile Computing on Mar. 20, 2025 (18 pages, 9 figures)", "summary": "The use of terahertz (THz) communications with massive multiple input multiple output (MIMO) systems in 6G can potentially provide high data rates and low latency communications. However, accurate channel estimation in THz frequencies presents significant challenges due to factors such as high propagation losses, sensitivity to environmental obstructions, and strong atmospheric absorption. These challenges are par- ticularly pronounced in urban environments, where traditional channel estimation methods often fail to deliver reliable results, particularly in complex non-line-of-sight (NLoS) scenarios. This paper introduces a novel vision-based channel estimation tech- nique that integrates causal reasoning into urban THz communi- cation systems. The proposed method combines computer vision algorithms with variational causal dynamics (VCD) to analyze real-time images of the urban environment, allowing for a deeper understanding of the physical factors that influence THz signal propagation. By capturing the complex, dynamic interactions between physical objects (such as buildings, trees, and vehicles) and the transmitted signals, the model can predict the channel with up to twice the accuracy of conventional methods. This model improves estimation accuracy and demonstrates supe- rior generalization performance. Hence, it can provide reliable predictions even in previously unseen urban environments. The effectiveness of the proposed method is particularly evident in NLoS conditions, where it significantly outperforms traditional methods such as by accounting for indirect signal paths, such as reflections and diffractions. Simulation results confirm that the proposed vision-based approach surpasses conventional artificial intelligence (AI)-based estimation techniques in accuracy and robustness, showing a substantial improvement across various dynamic urban scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u592a\u8d6b\u5179\uff08THz\uff09\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u53d8\u5206\u56e0\u679c\u52a8\u529b\u5b66\uff08VCD\uff09\uff0c\u5229\u7528\u5b9e\u65f6\u57ce\u5e02\u73af\u5883\u56fe\u50cf\u63d0\u5347\u4fe1\u9053\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u975e\u89c6\u8ddd\uff08NLoS\uff09\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u592a\u8d6b\u5179\u901a\u4fe1\u57286G\u4e2d\u5177\u6709\u9ad8\u6570\u636e\u901f\u7387\u548c\u4f4e\u5ef6\u8fdf\u6f5c\u529b\uff0c\u4f46\u5176\u4fe1\u9053\u4f30\u8ba1\u9762\u4e34\u9ad8\u4f20\u64ad\u635f\u8017\u3001\u73af\u5883\u906e\u6321\u548c\u5927\u6c14\u5438\u6536\u7b49\u6311\u6218\uff0c\u5c24\u5176\u5728\u57ce\u5e02\u590d\u6742\u975e\u89c6\u8ddd\u73af\u5883\u4e2d\u4f20\u7edf\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u7ed3\u5408\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u4e0e\u53d8\u5206\u56e0\u679c\u52a8\u529b\u5b66\uff08VCD\uff09\uff0c\u901a\u8fc7\u5206\u6790\u57ce\u5e02\u73af\u5883\u7684\u5b9e\u65f6\u56fe\u50cf\uff0c\u5efa\u6a21\u7269\u7406\u5bf9\u8c61\u4e0eTHz\u4fe1\u53f7\u95f4\u7684\u52a8\u6001\u56e0\u679c\u5173\u7cfb\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4fe1\u9053\u4f30\u8ba1\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u591a\u79cd\u52a8\u6001\u57ce\u5e02\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfAI\u65b9\u6cd5\uff0c\u4fe1\u9053\u9884\u6d4b\u7cbe\u5ea6\u6700\u9ad8\u53ef\u8fbe\u4f20\u7edf\u65b9\u6cd5\u7684\u4e24\u500d\uff0c\u5e76\u5728\u975e\u89c6\u8ddd\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u57fa\u4e8e\u89c6\u89c9\u4e0e\u56e0\u679c\u63a8\u7406\u7684\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u592a\u8d6b\u5179\u901a\u4fe1\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u4e3a6G THz-MIMO\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2512.04735", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.04735", "abs": "https://arxiv.org/abs/2512.04735", "authors": ["Herbert Jordan", "Kamil Jezek", "Pavle Subotic", "Bernhard Scholz"], "title": "A Fast Ethereum-Compatible Forkless Database", "comment": null, "summary": "The State Database of a blockchain stores account data and enables authentication. Modern blockchains use fast consensus protocols to avoid forking, improving throughput and finality. However, Ethereum's StateDB was designed for a forking chain that maintains multiple state versions. While newer blockchains adopt Ethereum's standard for DApp compatibility, they do not require multiple state versions, making legacy Ethereum databases inefficient for fast, non-forking blockchains. Moreover, existing StateDB implementations have been built on key-value stores (e.g., LevelDB), which make them less efficient.\n  This paper introduces a novel state database that is a native database implementation and maintains Ethereum compatibility while being specialized for non-forking blockchains. Our database delivers ten times speedups and 99% space reductions for validators, and a threefold decrease in storage requirements for archive nodes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u4e3a\u65e0\u5206\u53c9\u533a\u5757\u94fe\u8bbe\u8ba1\u7684\u65b0\u578b\u72b6\u6001\u6570\u636e\u5e93\uff0c\u5728\u4fdd\u6301\u4ee5\u592a\u574a\u517c\u5bb9\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u5b58\u50a8\u5f00\u9500\u3002", "motivation": "\u4ee5\u592a\u574a\u7684\u72b6\u6001\u6570\u636e\u5e93\uff08StateDB\uff09\u4e3a\u652f\u6301\u5206\u53c9\u94fe\u800c\u8bbe\u8ba1\uff0c\u9700\u7ef4\u62a4\u591a\u4e2a\u72b6\u6001\u7248\u672c\uff0c\u8fd9\u5728\u91c7\u7528\u5feb\u901f\u5171\u8bc6\u534f\u8bae\u3001\u65e0\u9700\u5206\u53c9\u7684\u73b0\u4ee3\u533a\u5757\u94fe\u4e2d\u9020\u6210\u6548\u7387\u4f4e\u4e0b\uff1b\u6b64\u5916\uff0c\u73b0\u6709StateDB\u57fa\u4e8e\u952e\u503c\u5b58\u50a8\uff08\u5982LevelDB\uff09\uff0c\u8fdb\u4e00\u6b65\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e00\u79cd\u539f\u751f\u7684\u72b6\u6001\u6570\u636e\u5e93\uff0c\u4e13\u95e8\u9488\u5bf9\u65e0\u5206\u53c9\u533a\u5757\u94fe\u8fdb\u884c\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4ee5\u592a\u574a\u7684\u517c\u5bb9\u6027\u3002", "result": "\u8be5\u6570\u636e\u5e93\u4e3a\u9a8c\u8bc1\u8282\u70b9\u5e26\u676510\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c99%\u7684\u7a7a\u95f4\u8282\u7701\uff0c\u5e76\u4f7f\u5f52\u6863\u8282\u70b9\u7684\u5b58\u50a8\u9700\u6c42\u51cf\u5c11\u4e09\u5206\u4e4b\u4e8c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b0\u578b\u72b6\u6001\u6570\u636e\u5e93\u5728\u517c\u5bb9\u4ee5\u592a\u574a\u6807\u51c6\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfStateDB\u5728\u65e0\u5206\u53c9\u9ad8\u6027\u80fd\u533a\u5757\u94fe\u4e2d\u7684\u6548\u7387\u74f6\u9888\uff0c\u663e\u8457\u4f18\u5316\u4e86\u901f\u5ea6\u4e0e\u5b58\u50a8\u5f00\u9500\u3002"}}
{"id": "2512.04093", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04093", "abs": "https://arxiv.org/abs/2512.04093", "authors": ["Ali Akbar Vali", "Sadoon Azizi", "Mohammad Shojafar", "Rajkumar Buyya"], "title": "Energy-Efficient Resource Management in Microservices-based Fog and Edge Computing: State-of-the-Art and Future Directions", "comment": "46 pages", "summary": "The exponential growth of Internet of Things (IoT) devices has intensified the demand for efficient and responsive services. To address this demand, fog and edge computing have emerged as distributed paradigms that bring computational resources closer to end users, reducing latency, bandwidth limitations, and energy consumption. However, these paradigms present challenges in resource management due to resource constraints, computational heterogeneity, dynamic workloads, and diverse Quality of Service (QoS) requirements. This paper presents a comprehensive survey of state-of-the-art resource management strategies in microservices-based fog and edge computing, focusing on energy-efficient solutions. We systematically review and classify more than 136 studies (2020-2024) into five key subdomains: service placement, resource provisioning, task scheduling and offloading, resource allocation, and instance selection. Our categorization is based on optimization techniques, targeted objectives, and the strengths and limitations of each approach. In addition, we examine existing surveys and identify unresolved challenges and gaps in the literature. By highlighting the lack of synergy among fundamental resource management components, we outline promising research directions leveraging AI-driven optimization, quantum computing, and serverless computing. This survey serves as a comprehensive reference for researchers and practitioners by providing a unified and energy-aware perspective on resource management in microservices-based fog and edge computing, paving the way for more integrated, efficient, and sustainable future solutions.", "AI": {"tldr": "\u672c\u6587\u5bf92020\u20132024\u5e74\u95f4136\u9879\u4ee5\u4e0a\u5173\u4e8e\u5fae\u670d\u52a1\u5316\u96fe\u8ba1\u7b97\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u8d44\u6e90\u7ba1\u7406\u7684\u7814\u7a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u805a\u7126\u4e8e\u80fd\u6548\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u5c06\u5176\u5212\u5206\u4e3a\u670d\u52a1\u653e\u7f6e\u3001\u8d44\u6e90\u4f9b\u5e94\u3001\u4efb\u52a1\u8c03\u5ea6\u4e0e\u5378\u8f7d\u3001\u8d44\u6e90\u5206\u914d\u548c\u5b9e\u4f8b\u9009\u62e9\u4e94\u5927\u5b50\u9886\u57df\uff0c\u540c\u65f6\u6307\u51fa\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u7ec4\u4ef6\u95f4\u534f\u540c\uff0c\u5e76\u63d0\u51fa\u7ed3\u5408AI\u3001\u91cf\u5b50\u8ba1\u7b97\u548c\u65e0\u670d\u52a1\u5668\u67b6\u6784\u7684\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u8bbe\u5907\u6fc0\u589e\uff0c\u96fe\u8ba1\u7b97\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u867d\u53ef\u964d\u4f4e\u5ef6\u8fdf\u4e0e\u80fd\u8017\uff0c\u4f46\u5176\u8d44\u6e90\u53d7\u9650\u3001\u5f02\u6784\u6027\u5f3a\u3001\u8d1f\u8f7d\u52a8\u6001\u4e14QoS\u9700\u6c42\u591a\u6837\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u8282\u80fd\u7684\u8d44\u6e90\u7ba1\u7406\u673a\u5236\uff1b\u73b0\u6709\u7814\u7a76\u5206\u6563\uff0c\u7f3a\u4e4f\u7edf\u4e00\u89c6\u89d2\u4e0e\u7ec4\u4ef6\u534f\u540c\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf92020\u20132024\u5e74\u95f4136\u7bc7\u4ee5\u4e0a\u76f8\u5173\u8bba\u6587\u8fdb\u884c\u5206\u7c7b\u4e0e\u5206\u6790\uff0c\u4f9d\u636e\u4f18\u5316\u6280\u672f\u3001\u76ee\u6807\u51fd\u6570\u53ca\u65b9\u6cd5\u4f18\u7f3a\u70b9\uff0c\u5c06\u7814\u7a76\u5212\u5206\u4e3a\u4e94\u4e2a\u5173\u952e\u5b50\u9886\u57df\uff0c\u5e76\u5bf9\u6bd4\u73b0\u6709\u7efc\u8ff0\uff0c\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u6db5\u76d6\u670d\u52a1\u653e\u7f6e\u3001\u8d44\u6e90\u4f9b\u5e94\u3001\u4efb\u52a1\u8c03\u5ea6\u4e0e\u5378\u8f7d\u3001\u8d44\u6e90\u5206\u914d\u548c\u5b9e\u4f8b\u9009\u62e9\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u80fd\u6548\u4f18\u5316\u65b9\u9762\u7684\u8fdb\u5c55\u4e0e\u5c40\u9650\uff0c\u5e76\u660e\u786e\u4e86AI\u9a71\u52a8\u3001\u91cf\u5b50\u8ba1\u7b97\u548c\u65e0\u670d\u52a1\u5668\u7b49\u65b0\u5174\u6280\u672f\u7684\u878d\u5408\u6f5c\u529b\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3a\u5fae\u670d\u52a1\u5316\u96fe/\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u6ce8\u91cd\u80fd\u6548\u7684\u53c2\u8003\u6846\u67b6\uff0c\u5f3a\u8c03\u9700\u52a0\u5f3a\u5404\u7ba1\u7406\u7ec4\u4ef6\u95f4\u7684\u534f\u540c\uff0c\u5e76\u547c\u5401\u672a\u6765\u7814\u7a76\u63a2\u7d22AI\u3001\u91cf\u5b50\u4e0e\u65e0\u670d\u52a1\u5668\u7b49\u524d\u6cbf\u6280\u672f\u4ee5\u5b9e\u73b0\u66f4\u96c6\u6210\u3001\u9ad8\u6548\u548c\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.04910", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04910", "abs": "https://arxiv.org/abs/2512.04910", "authors": ["Fang Li"], "title": "Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming", "comment": "Accepted by the 43rd IEEE International Conference on Computer Design (ICCD 2025)", "summary": "This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u7684\u81ea\u52a8\u6761\u5f62\u677f\u7535\u8def\u5e03\u5c40\u8bbe\u8ba1\u65b0\u65b9\u6cd5\uff0c\u5c06\u5e03\u5c40\u95ee\u9898\u5efa\u6a21\u4e3a\u5408\u6210\u4e0e\u591a\u76ee\u6807\u4f18\u5316\u4efb\u52a1\uff0c\u5728\u4fdd\u8bc1\u53ef\u884c\u6027\u7684\u524d\u63d0\u4e0b\u751f\u6210\u7d27\u51d1\u3001\u53ef\u5236\u9020\u7684\u5e03\u5c40\u3002", "motivation": "\u4f20\u7edf\u6761\u5f62\u677f\u5e03\u5c40\u8bbe\u8ba1\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u6613\u51fa\u9519\uff1b\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u51e0\u4f55\u4e0e\u7535\u6c14\u7ea6\u675f\u65f6\u5b58\u5728\u5c40\u9650\uff0c\u7f3a\u4e4f\u517c\u987e\u5e03\u5c40\u7d27\u51d1\u6027\u4e0e\u53ef\u5236\u9020\u6027\u7684\u6709\u6548\u65b9\u6848\u3002", "method": "\u5229\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u7684\u58f0\u660e\u5f0f\u7279\u6027\uff0c\u5c06\u6761\u5f62\u677f\u5e03\u5c40\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u540c\u65f6\u6ee1\u8db3\u53ef\u884c\u6027\u7ea6\u675f\u5e76\u4f18\u5316\u677f\u9762\u79ef\u4e0e\u5143\u4ef6\u8de8\u6761\u6570\u7684\u591a\u76ee\u6807\u95ee\u9898\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u6c42\u89e3\u7b56\u7565\uff1a\u5148\u786e\u4fdd\u5e03\u5c40\u53ef\u884c\uff0c\u518d\u4f18\u5316\u5e03\u5c40\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u4e3a\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u7535\u8def\u81ea\u52a8\u751f\u6210\u7d27\u51d1\u4e14\u53ef\u5236\u9020\u7684\u5e03\u5c40\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u4e0e\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u63a8\u8fdb\u4e86\u6761\u5f62\u677f\u81ea\u52a8\u5e03\u5c40\u6280\u672f\uff0c\u4e0d\u4ec5\u4e3a\u7535\u5b50\u539f\u578b\u5236\u4f5c\u548c\u6559\u80b2\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u4e5f\u5c55\u793a\u4e86\u58f0\u660e\u5f0f\u7f16\u7a0b\u5728\u590d\u6742\u8bbe\u8ba1\u81ea\u52a8\u5316\u95ee\u9898\u4e2d\u7684\u5f3a\u5927\u80fd\u529b\u3002"}}
{"id": "2512.04117", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.04117", "abs": "https://arxiv.org/abs/2512.04117", "authors": ["Joost Mertens", "Joachim Denil"], "title": "Reusing Model Validation Methods for the Continuous Validation of Digital Twins of Cyber-Physical Systems", "comment": null, "summary": "One of the challenges in twinned systems is ensuring the digital twin remains a valid representation of the system it twins. Depending on the type of twinning occurring, it is either trivial, such as in dashboarding/visualizations that mirror the system with real-time data, or challenging, in case the digital twin is a simulation model that reflects the behavior of a physical twinned system. The challenge in this latter case comes from the fact that in contrast to software systems, physical systems are not immutable once deployed, but instead they evolve through processes like maintenance, wear and tear or user error. It is therefore important to detect when changes occur in the physical system to evolve the twin alongside it. We employ and reuse validation techniques from model-based design for this goal. Model validation is one of the steps used to gain trust in the representativeness of a simulation model. In this work, we provide two contributions: (i) we provide a generic approach that, through the use of validation metrics, is able to detect anomalies in twinned systems, and (ii) we demonstrate these techniques with the help of an academic yet industrially relevant case study of a gantry crane such as found in ports. Treating anomalies also means correcting the error in the digital twin, which we do with a parameter estimation based on the historical data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u9a8c\u8bc1\u6307\u6807\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u4f30\u8ba1\u4fee\u6b63\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u4ee5\u4fdd\u6301\u5176\u4e0e\u7269\u7406\u7cfb\u7edf\u7684\u540c\u6b65\uff1b\u8be5\u65b9\u6cd5\u5728\u6e2f\u53e3\u95e8\u5f0f\u8d77\u91cd\u673a\u6848\u4f8b\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u7269\u7406\u7cfb\u7edf\u5728\u90e8\u7f72\u540e\u4f1a\u56e0\u7ef4\u62a4\u3001\u78e8\u635f\u6216\u4eba\u4e3a\u9519\u8bef\u7b49\u56e0\u7d20\u4e0d\u65ad\u6f14\u5316\uff0c\u800c\u6570\u5b57\u5b6a\u751f\u82e5\u4e3a\u4eff\u771f\u6a21\u578b\uff0c\u5219\u9700\u968f\u4e4b\u66f4\u65b0\u4ee5\u7ef4\u6301\u5176\u6709\u6548\u6027\uff1b\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u673a\u5236\u6765\u68c0\u6d4b\u7269\u7406\u7cfb\u7edf\u7684\u53d8\u5316\u5e76\u76f8\u5e94\u8c03\u6574\u6570\u5b57\u5b6a\u751f\u3002", "method": "\u590d\u7528\u57fa\u4e8e\u6a21\u578b\u8bbe\u8ba1\u4e2d\u7684\u9a8c\u8bc1\u6280\u672f\uff0c\u5229\u7528\u9a8c\u8bc1\u6307\u6807\u68c0\u6d4b\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\uff0c\u5e76\u57fa\u4e8e\u5386\u53f2\u6570\u636e\u8fdb\u884c\u53c2\u6570\u4f30\u8ba1\uff0c\u4ee5\u4fee\u6b63\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u4e2d\u7684\u8bef\u5dee\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5728\u4e00\u4e2a\u5177\u6709\u5de5\u4e1a\u76f8\u5173\u6027\u7684\u95e8\u5f0f\u8d77\u91cd\u673a\u5b66\u672f\u6848\u4f8b\u4e2d\u6210\u529f\u6f14\u793a\u4e86\u8be5\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u9a8c\u8bc1\u4e0e\u53c2\u6570\u4f30\u8ba1\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u5e76\u4fee\u6b63\u6570\u5b57\u5b6a\u751f\u4e0e\u7269\u7406\u7cfb\u7edf\u4e4b\u95f4\u7684\u504f\u5dee\uff0c\u4ece\u800c\u7ef4\u6301\u6570\u5b57\u5b6a\u751f\u7684\u4ee3\u8868\u6027\u4e0e\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2512.04096", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04096", "abs": "https://arxiv.org/abs/2512.04096", "authors": ["Sushant Kumar Gupta", "Anil Raghunath Iyer", "Chang Yu", "Neel Bagora", "Olivier Pomerleau", "Vivek Kumar", "Prunthaban Kanthakumar"], "title": "Formal Specification for Fast ACS: Low-Latency File-Based Ordered Message Delivery at Scale", "comment": null, "summary": "Low-latency message delivery is crucial for real-time systems. Data originating from a producer must be delivered to consumers, potentially distributed in clusters across metropolitan and continental boundaries. With the growing scale of computing, there can be several thousand consumers of the data. Such systems require a robust messaging system capable of transmitting messages containing data across clusters and efficiently delivering them to consumers. The system must offer guarantees like ordering and at-least-once delivery while avoiding overload on consumers, allowing them to consume messages at their own pace.\n  This paper presents the design of Fast ACS (an abbreviation for Ads Copy Service), a file-based ordered message delivery system that leverages a combination of two-sided (inter-cluster) and one-sided (intra-cluster) communication primitives - namely, Remote Procedure Call and Remote Memory Access, respectively - to deliver messages. The system has been successfully deployed to dozens of production clusters and scales to accommodate several thousand consumers within each cluster, which amounts to Tbps-scale intra-cluster consumer traffic at peak. Notably, Fast ACS delivers messages to consumers across the globe within a few seconds or even sub-seconds (p99) based on the message volume and consumer scale, at a low resource cost.", "AI": {"tldr": "Fast ACS \u662f\u4e00\u79cd\u57fa\u4e8e\u6587\u4ef6\u7684\u6709\u5e8f\u6d88\u606f\u4f20\u9012\u7cfb\u7edf\uff0c\u7ed3\u5408 RPC \u4e0e\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\u6280\u672f\uff0c\u5728\u5927\u89c4\u6a21\u8de8\u5730\u57df\u96c6\u7fa4\u4e2d\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u541e\u5410\u3001\u6709\u5e8f\u4e14\u81f3\u5c11\u4e00\u6b21\u7684\u6d88\u606f\u6295\u9012\u3002", "motivation": "\u5b9e\u65f6\u7cfb\u7edf\u9700\u8981\u5728\u5927\u89c4\u6a21\u3001\u8de8\u5730\u57df\u5206\u5e03\u7684\u6d88\u8d39\u8005\u4e4b\u95f4\u9ad8\u6548\u3001\u53ef\u9760\u5730\u4f20\u9012\u6d88\u606f\uff0c\u540c\u65f6\u4fdd\u8bc1\u987a\u5e8f\u6027\u3001\u81f3\u5c11\u4e00\u6b21\u6295\u9012\uff0c\u5e76\u907f\u514d\u6d88\u8d39\u8005\u8fc7\u8f7d\u3002", "method": "Fast ACS \u5229\u7528\u4e24\u65b9\u901a\u4fe1\uff08\u8de8\u96c6\u7fa4\u7684\u8fdc\u7a0b\u8fc7\u7a0b\u8c03\u7528\uff09\u548c\u5355\u65b9\u901a\u4fe1\uff08\u96c6\u7fa4\u5185\u7684\u8fdc\u7a0b\u5185\u5b58\u8bbf\u95ee\uff09\u76f8\u7ed3\u5408\u7684\u65b9\u5f0f\uff0c\u6784\u5efa\u57fa\u4e8e\u6587\u4ef6\u7684\u6709\u5e8f\u6d88\u606f\u4f20\u9012\u673a\u5236\u3002", "result": "\u7cfb\u7edf\u5df2\u5728\u6570\u5341\u4e2a\u751f\u4ea7\u96c6\u7fa4\u4e2d\u90e8\u7f72\uff0c\u652f\u6301\u6bcf\u4e2a\u96c6\u7fa4\u6570\u5343\u6d88\u8d39\u8005\uff0c\u5cf0\u503c\u65f6\u5b9e\u73b0 Tbps \u7ea7\u522b\u7684\u96c6\u7fa4\u5185\u6d41\u91cf\uff0c\u5168\u7403\u8303\u56f4\u5185 p99 \u5ef6\u8fdf\u4e3a\u79d2\u7ea7\u751a\u81f3\u4e9a\u79d2\u7ea7\uff0c\u8d44\u6e90\u5f00\u9500\u4f4e\u3002", "conclusion": "Fast ACS \u80fd\u591f\u6709\u6548\u6ee1\u8db3\u5927\u89c4\u6a21\u5b9e\u65f6\u7cfb\u7edf\u5bf9\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6d88\u606f\u4f20\u9012\u7684\u9700\u6c42\uff0c\u5177\u5907\u826f\u597d\u7684\u751f\u4ea7\u9002\u7528\u6027\u3002"}}
{"id": "2512.04250", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.04250", "abs": "https://arxiv.org/abs/2512.04250", "authors": ["Shubham Somani", "Vanish Talwar", "Madhura Parikh", "Eduardo Hernandez", "Jimmy Wang", "Shreya Shah", "Chinmay Gandhi", "Sanjay Sundarajan", "Neeru Sharma", "Srikanth Kamath", "Nitin Gupta", "Benjamin Renard", "Ohad Yahalom", "Chris Davis"], "title": "DrP: Meta's Efficient Investigations Platform at Scale", "comment": null, "summary": "Investigations are a significant step in the operational workflows for large scale systems across multiple domains such as services, data, AI/ML, mobile. Investigation processes followed by on-call engineers are often manual or rely on ad-hoc scripts. This leads to inefficient investigations resulting in increased time to mitigate and isolate failures/SLO violations. It also contributes to on-call toil and poor productivity leading to multiple hours/days spent in triaging/debugging incidents. In this paper, we present DrP, an end-to-end framework and system to automate investigations that reduces the mean time to resolve incidents (MTTR) and reduces on-call toil. DrP consists of an expressive and flexible SDK to author investigation playbooks in code (called analyzers), a scalable backend system to execute these automated playbooks, plug-ins to integrate playbooks into mainstream workflows such as alerts and incident management tools, and a post-processing system to take actions on investigations including mitigation steps.\n  We have implemented and deployed DrP at large scale at Meta covering 300+ teams, 2000+ analyzers, across a large set of use cases across domains such as services, core infrastructure, AI/ML, hardware, mobile. DrP has been running in production for the past 5 years and executes 50K automated analyses per day. Overall, our results and experience show that DrP has been able to reduce average MTTR by 20 percent at large scale (with over 80 percent for some teams) and has significantly improved on-call productivity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86DrP\u2014\u2014\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u5316\u8c03\u67e5\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u7f16\u7a0b\u7684\u5206\u6790\u5668\uff08analyzers\uff09\u3001\u53ef\u6269\u5c55\u540e\u7aef\u3001\u4e0e\u544a\u8b66\u548c\u4e8b\u4ef6\u7ba1\u7406\u5de5\u5177\u96c6\u6210\u7684\u63d2\u4ef6\u4ee5\u53ca\u4e8b\u540e\u5904\u7406\u7cfb\u7edf\uff0c\u663e\u8457\u964d\u4f4e\u5e73\u5747\u6545\u969c\u4fee\u590d\u65f6\u95f4\uff08MTTR\uff09\u5e76\u51cf\u8f7b\u8fd0\u7ef4\u8d1f\u62c5\u3002", "motivation": "\u5f53\u524d\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u7684\u6545\u969c\u8c03\u67e5\u6d41\u7a0b\u591a\u4f9d\u8d56\u4eba\u5de5\u6216\u4e34\u65f6\u811a\u672c\uff0c\u6548\u7387\u4f4e\u4e0b\uff0c\u5bfc\u81f4\u6545\u969c\u4fee\u590d\u65f6\u95f4\u957f\u3001\u8fd0\u7ef4\u8d1f\u62c5\u91cd\u3001\u751f\u4ea7\u529b\u4f4e\u4e0b\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0DrP\u7cfb\u7edf\uff0c\u5305\u62ec\u7528\u4e8e\u7f16\u5199\u8c03\u67e5\u5267\u672c\u7684SDK\uff08\u79f0\u4e3aanalyzers\uff09\u3001\u53ef\u6269\u5c55\u6267\u884c\u540e\u7aef\u3001\u4e0e\u4e3b\u6d41\u5de5\u4f5c\u6d41\uff08\u5982\u544a\u8b66\u7cfb\u7edf\uff09\u96c6\u6210\u7684\u63d2\u4ef6\uff0c\u4ee5\u53ca\u652f\u6301\u81ea\u52a8\u7f13\u89e3\u63aa\u65bd\u7684\u4e8b\u540e\u5904\u7406\u7cfb\u7edf\u3002", "result": "DrP\u5df2\u5728Meta\u5927\u89c4\u6a21\u90e8\u7f725\u5e74\uff0c\u8986\u76d6300\u591a\u4e2a\u56e2\u961f\u548c2000\u591a\u4e2aanalyzers\uff0c\u65e5\u5747\u6267\u884c5\u4e07\u6b21\u81ea\u52a8\u5316\u5206\u6790\uff0c\u5e73\u5747MTTR\u964d\u4f4e20%\uff08\u90e8\u5206\u56e2\u961f\u8d85\u8fc780%\uff09\uff0c\u663e\u8457\u63d0\u5347\u8fd0\u7ef4\u6548\u7387\u3002", "conclusion": "DrP\u6709\u6548\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u7cfb\u7edf\u4e2d\u6545\u969c\u8c03\u67e5\u7684\u81ea\u52a8\u5316\uff0c\u663e\u8457\u7f29\u77ed\u6545\u969c\u6062\u590d\u65f6\u95f4\u5e76\u51cf\u8f7b\u8fd0\u7ef4\u8d1f\u62c5\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.04226", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04226", "abs": "https://arxiv.org/abs/2512.04226", "authors": ["Ryan Swann", "Muhammad Osama", "Xiaohu Guo", "Bryant Nelson", "Lixun Zhang", "Alex Brown", "Yen Ong", "Ali Yazdani", "Sean Siddens", "Ganesh Dasika", "Alex Underwood"], "title": "tritonBLAS: Triton-based Analytical Approach for GEMM Kernel Parameter Selection", "comment": null, "summary": "We present tritonBLAS, a fast and deterministic analytical model that uses architectural parameters like the cache hierarchy, and relative code and data placement to generate performant GPU GEMM kernels. tritonBLAS explicitly models the relationship between architectural topology, matrix shapes, and algorithmic blocking behavior to predict near-optimal configurations without runtime autotuning. Based on this model, we developed and implemented a lightweight GEMM framework entirely within Triton. We evaluate the performance of tritonBLAS across a diverse set of GEMM problem sizes on modern GPUs. tritonBLAS achieves over 95% of the performance of autotuning solutions, while reducing autotuning time to zero. This makes tritonBLAS a practical drop-in replacement for empirical tuning in production HPC and ML workloads.", "AI": {"tldr": "tritonBLAS \u662f\u4e00\u4e2a\u57fa\u4e8e\u67b6\u6784\u53c2\u6570\u7684\u5feb\u901f\u3001\u786e\u5b9a\u6027\u5206\u6790\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u6027\u80fd GPU GEMM \u5185\u6838\uff0c\u65e0\u9700\u8fd0\u884c\u65f6\u81ea\u52a8\u8c03\u4f18\u5373\u53ef\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf GEMM \u5185\u6838\u4f9d\u8d56\u8fd0\u884c\u65f6\u81ea\u52a8\u8c03\u4f18\u4ee5\u83b7\u5f97\u9ad8\u6027\u80fd\uff0c\u4f46\u8be5\u8fc7\u7a0b\u8017\u65f6\u4e14\u4e0d\u53ef\u9884\u6d4b\uff1b\u4f5c\u8005\u5e0c\u671b\u6784\u5efa\u4e00\u4e2a\u65e0\u9700\u81ea\u52a8\u8c03\u4f18\u3001\u80fd\u57fa\u4e8e\u786c\u4ef6\u67b6\u6784\u53c2\u6570\u76f4\u63a5\u9884\u6d4b\u8fd1\u4f18\u914d\u7f6e\u7684\u6a21\u578b\u3002", "method": "tritonBLAS \u5229\u7528\u7f13\u5b58\u5c42\u6b21\u7ed3\u6784\u3001\u4ee3\u7801\u4e0e\u6570\u636e\u76f8\u5bf9\u5e03\u5c40\u7b49\u67b6\u6784\u53c2\u6570\uff0c\u663e\u5f0f\u5efa\u6a21\u67b6\u6784\u62d3\u6251\u3001\u77e9\u9635\u5f62\u72b6\u4e0e\u7b97\u6cd5\u5206\u5757\u884c\u4e3a\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5728 Triton \u4e2d\u5b9e\u73b0\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7 GEMM \u6846\u67b6\u3002", "result": "\u5728\u591a\u79cd GEMM \u95ee\u9898\u89c4\u6a21\u548c\u73b0\u4ee3 GPU \u4e0a\u8bc4\u4f30\u8868\u660e\uff0ctritonBLAS \u8fbe\u5230\u81ea\u52a8\u8c03\u4f18\u65b9\u6848 95% \u4ee5\u4e0a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u81ea\u52a8\u8c03\u4f18\u65f6\u95f4\u964d\u81f3\u96f6\u3002", "conclusion": "tritonBLAS \u53ef\u4f5c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u751f\u4ea7\u73af\u5883\u4e2d\u7ecf\u9a8c\u8c03\u4f18\u65b9\u6cd5\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2512.04256", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.04256", "abs": "https://arxiv.org/abs/2512.04256", "authors": ["Qiaolin Qin", "Ronnie de Souza Santos", "Rodrigo Spinola"], "title": "On the Role and Impact of GenAI Tools in Software Engineering Education", "comment": "Accepted at IEEE/ACM ICSE Software Engineering Education and Training (ICSE SEET 2026)", "summary": "Context. The rise of generative AI (GenAI) tools like ChatGPT and GitHub Copilot has transformed how software is learned and written. In software engineering (SE) education, these tools offer new opportunities for support, but also raise concerns about over-reliance, ethical use, and impacts on learning. Objective. This study investigates how undergraduate SE students use GenAI tools, focusing on the benefits, challenges, ethical concerns, and instructional expectations that shape their experiences. Method. We conducted a survey with 130 undergraduate students from two universities. The survey combined structured Likert-scale items and open-ended questions to investigate five dimensions: usage context, perceived benefits, challenges, ethical and instructional perceptions. Results. Students most often use GenAI for incremental learning and advanced implementation, reporting benefits such as brainstorming support and confidence-building. At the same time, they face challenges including unclear rationales and difficulty adapting outputs. Students highlight ethical concerns around fairness and misconduct, and call for clearer instructional guidance. Conclusion. GenAI is reshaping SE education in nuanced ways. Our findings underscore the need for scaffolding, ethical policies, and adaptive instructional strategies to ensure that GenAI supports equitable and effective learning.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8c03\u67e5130\u540d\u672c\u79d1\u751f\uff0c\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u4e2d\u7684\u4f7f\u7528\u60c5\u51b5\uff0c\u53d1\u73b0\u5b66\u751f\u4e3b\u8981\u5c06\u5176\u7528\u4e8e\u6e10\u8fdb\u5f0f\u5b66\u4e60\u548c\u9ad8\u7ea7\u5b9e\u73b0\uff0c\u867d\u83b7\u76ca\u4e8e\u5934\u8111\u98ce\u66b4\u548c\u4fe1\u5fc3\u63d0\u5347\uff0c\u4f46\u4e5f\u9762\u4e34\u8f93\u51fa\u7406\u89e3\u56f0\u96be\u3001\u4f26\u7406\u62c5\u5fe7\u7b49\u95ee\u9898\uff0c\u5e76\u547c\u5401\u66f4\u6e05\u6670\u7684\u6559\u5b66\u6307\u5bfc\u3002", "motivation": "\u968f\u7740ChatGPT\u548cGitHub Copilot\u7b49\u751f\u6210\u5f0fAI\u5de5\u5177\u7684\u5174\u8d77\uff0c\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\u65e2\u8fce\u6765\u65b0\u673a\u9047\uff0c\u4e5f\u9762\u4e34\u5b66\u751f\u8fc7\u5ea6\u4f9d\u8d56\u3001\u4f26\u7406\u95ee\u9898\u53ca\u5b66\u4e60\u6548\u679c\u53d7\u635f\u7b49\u6311\u6218\uff0c\u4e9f\u9700\u4e86\u89e3\u5b66\u751f\u5b9e\u9645\u4f7f\u7528\u4f53\u9a8c\u4ee5\u4f18\u5316\u6559\u5b66\u7b56\u7565\u3002", "method": "\u5bf9\u6765\u81ea\u4e24\u6240\u5927\u5b66\u7684130\u540d\u8f6f\u4ef6\u5de5\u7a0b\u672c\u79d1\u751f\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\uff0c\u7ed3\u5408\u674e\u514b\u7279\u91cf\u8868\u4e0e\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u4ece\u4f7f\u7528\u60c5\u5883\u3001\u611f\u77e5\u6536\u76ca\u3001\u6311\u6218\u3001\u4f26\u7406\u8ba4\u77e5\u548c\u6559\u5b66\u671f\u671b\u4e94\u4e2a\u7ef4\u5ea6\u6536\u96c6\u6570\u636e\u3002", "result": "\u5b66\u751f\u4e3b\u8981\u5c06GenAI\u7528\u4e8e\u6e10\u8fdb\u5b66\u4e60\u548c\u9ad8\u7ea7\u7f16\u7801\uff0c\u8ba4\u4e3a\u5176\u6709\u52a9\u4e8e\u6784\u601d\u548c\u589e\u5f3a\u4fe1\u5fc3\uff1b\u4f46\u5e38\u9047\u5230\u8f93\u51fa\u903b\u8f91\u4e0d\u6e05\u3001\u96be\u4ee5\u9002\u914d\u7b49\u95ee\u9898\uff1b\u540c\u65f6\u5173\u6ce8\u516c\u5e73\u6027\u548c\u5b66\u672f\u4e0d\u7aef\u7b49\u4f26\u7406\u98ce\u9669\uff0c\u5e76\u5e0c\u671b\u83b7\u5f97\u66f4\u660e\u786e\u7684\u6559\u5b66\u6307\u5f15\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u6b63\u4ee5\u590d\u6742\u65b9\u5f0f\u91cd\u5851\u8f6f\u4ef6\u5de5\u7a0b\u6559\u80b2\uff0c\u9700\u901a\u8fc7\u6559\u5b66\u652f\u67b6\u8bbe\u8ba1\u3001\u4f26\u7406\u89c4\u8303\u5236\u5b9a\u548c\u9002\u5e94\u6027\u6559\u5b66\u7b56\u7565\uff0c\u786e\u4fdd\u5176\u4fc3\u8fdb\u516c\u5e73\u4e14\u6709\u6548\u7684\u5b66\u4e60\u3002"}}
{"id": "2512.04262", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.04262", "abs": "https://arxiv.org/abs/2512.04262", "authors": ["Nolan Platt", "Ethan Luchs", "Sehrish Nizamani"], "title": "Catching UX Flaws in Code: Leveraging LLMs to Identify Usability Flaws at the Development Stage", "comment": "7 pages. Published in Proceedings of the 2025 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC). DOI: 10.1109/VL-HCC65237.2025.00024", "summary": "Usability evaluations are essential for ensuring that modern interfaces meet user needs, yet traditional heuristic evaluations by human experts can be time-consuming and subjective, especially early in development. This paper investigates whether large language models (LLMs) can provide reliable and consistent heuristic assessments at the development stage. By applying Jakob Nielsen's ten usability heuristics to thirty open-source websites, we generated over 850 heuristic evaluations in three independent evaluations per site using a pipeline of OpenAI's GPT-4o. For issue detection, the model demonstrated moderate consistency, with an average pairwise Cohen's Kappa of 0.50 and an exact agreement of 84%. Severity judgments showed more variability: weighted Cohen's Kappa averaged 0.63, but exact agreement was just 56%, and Krippendorff's Alpha was near zero. These results suggest that while GPT-4o can produce internally consistent evaluations, especially for identifying the presence of usability issues, its ability to judge severity varies and requires human oversight in practice. Our findings highlight the feasibility and limitations of using LLMs for early-stage, automated usability testing, and offer a foundation for improving consistency in automated User Experience (UX) evaluation. To the best of our knowledge, our work provides one of the first quantitative inter-rater reliability analyses of automated heuristic evaluation and highlights methods for improving model consistency.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u65e9\u671f\u5f00\u53d1\u9636\u6bb5\u8fdb\u884c\u542f\u53d1\u5f0f\u53ef\u7528\u6027\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u4e0e\u4e00\u81f4\u6027\uff0c\u53d1\u73b0GPT-4o\u5728\u8bc6\u522b\u53ef\u7528\u6027\u95ee\u9898\u65b9\u9762\u8868\u73b0\u4e2d\u7b49\u4e00\u81f4\uff0c\u4f46\u5728\u4e25\u91cd\u6027\u5224\u65ad\u4e0a\u5dee\u5f02\u8f83\u5927\uff0c\u9700\u4eba\u5de5\u76d1\u7763\u3002", "motivation": "\u4f20\u7edf\u7531\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u7684\u542f\u53d1\u5f0f\u53ef\u7528\u6027\u8bc4\u4f30\u8017\u65f6\u4e14\u4e3b\u89c2\uff0c\u5c24\u5176\u5728\u5f00\u53d1\u65e9\u671f\u9636\u6bb5\uff1b\u56e0\u6b64\u63a2\u7d22\u662f\u5426\u53ef\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u53ef\u9760\u7684\u542f\u53d1\u5f0f\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8eJakob Nielsen\u7684\u5341\u5927\u53ef\u7528\u6027\u542f\u53d1\u5f0f\u539f\u5219\uff0c\u5bf930\u4e2a\u5f00\u6e90\u7f51\u7ad9\u4f7f\u7528GPT-4o\u8fdb\u884c\u4e09\u6b21\u72ec\u7acb\u8bc4\u4f30\uff0c\u5171\u751f\u6210850\u591a\u6761\u542f\u53d1\u5f0f\u8bc4\u4f30\u7ed3\u679c\uff0c\u5e76\u901a\u8fc7Cohen's Kappa\u548cKrippendorff's Alpha\u7b49\u6307\u6807\u5206\u6790\u4e00\u81f4\u6027\u3002", "result": "\u6a21\u578b\u5728\u95ee\u9898\u68c0\u6d4b\u4e0a\u8868\u73b0\u51fa\u4e2d\u7b49\u4e00\u81f4\u6027\uff08\u5e73\u5747Cohen's Kappa\u4e3a0.50\uff0c\u5b8c\u5168\u4e00\u81f4\u7387\u4e3a84%\uff09\uff0c\u4f46\u5728\u4e25\u91cd\u6027\u5224\u65ad\u4e0a\u4e00\u81f4\u6027\u8f83\u4f4e\uff08\u52a0\u6743Cohen's Kappa\u4e3a0.63\uff0c\u5b8c\u5168\u4e00\u81f4\u7387\u4ec556%\uff0cKrippendorff's Alpha\u63a5\u8fd1\u96f6\uff09\u3002", "conclusion": "GPT-4o\u53ef\u7528\u4e8e\u65e9\u671f\u81ea\u52a8\u5316\u53ef\u7528\u6027\u6d4b\u8bd5\uff0c\u5728\u8bc6\u522b\u95ee\u9898\u65b9\u9762\u5177\u6709\u4e00\u81f4\u6027\uff0c\u4f46\u4e25\u91cd\u6027\u8bc4\u4f30\u4ecd\u9700\u4eba\u5de5\u4ecb\u5165\uff1b\u672c\u7814\u7a76\u4e3a\u63d0\u5347LLM\u5728UX\u8bc4\u4f30\u4e2d\u7684\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u65b9\u6cd5\u3002"}}
{"id": "2512.04263", "categories": ["cs.SE", "cs.LG", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.04263", "abs": "https://arxiv.org/abs/2512.04263", "authors": ["Hoang Duc Nguyen", "Anh Van Pham", "Hien D. Nguyen"], "title": "Polynomiogram: An Integrated Framework for Root Visualization and Generative Art", "comment": null, "summary": "This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.", "AI": {"tldr": "Polynomiogram \u662f\u4e00\u4e2a\u7ed3\u5408\u79d1\u5b66\u8ba1\u7b97\u4e0e\u751f\u6210\u827a\u672f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u53c2\u6570\u91c7\u6837\u548c\u53cc\u5f15\u64ce\u6570\u503c\u6c42\u89e3\uff0c\u652f\u6301\u5bf9\u591a\u9879\u5f0f\u6839\u7cfb\u7edf\u7684\u53ef\u89c6\u5316\u5206\u6790\u4e0e\u4e2a\u6027\u5316\u827a\u672f\u521b\u4f5c\u3002", "motivation": "\u5c06\u591a\u9879\u5f0f\u6839\u7cfb\u7edf\u7684\u6570\u5b66\u7814\u7a76\u4e0e\u751f\u6210\u827a\u672f\u76f8\u7ed3\u5408\uff0c\u63d0\u4f9b\u4e00\u4e2a\u65e2\u80fd\u7528\u4e8e\u79d1\u7814\u6559\u80b2\u53c8\u80fd\u7528\u4e8e\u521b\u610f\u8868\u8fbe\u7684\u7edf\u4e00\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u7528\u6237\u81ea\u5b9a\u4e49\u57df\u4e2d\u62bd\u53d6\u4e24\u4e2a\u72ec\u7acb\u53c2\u6570\u5e76\u901a\u8fc7\u751f\u6210\u51fd\u6570\u6620\u5c04\u5230\u591a\u9879\u5f0f\u7cfb\u6570\u7684\u7075\u6d3b\u91c7\u6837\u65b9\u6848\uff1b\u96c6\u6210 NumPy \u4f34\u4fa3\u77e9\u9635\u6c42\u89e3\u5668\uff08\u5feb\u901f\u5927\u89c4\u6a21\u8ba1\u7b97\uff09\u548c MPSolve\uff08\u9ad8\u7cbe\u5ea6\u9a8c\u8bc1\uff09\u4e24\u79cd\u6570\u503c\u5f15\u64ce\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86 Kac \u548c Lucas \u591a\u9879\u5f0f\u7b49\u7ecf\u5178\u7cfb\u7efc\u7684\u6570\u503c\u51c6\u786e\u6027\uff1b\u5e94\u7528\u4e8e\u4e09\u6b21\u591a\u9879\u5f0f\u7cfb\u7edf\u63ed\u793a\u5176\u5206\u5c94\u7ed3\u6784\uff1b\u5e76\u751f\u6210\u4e86\u5982\u6728\u69ff\u82b1\u5f62\u6001\u53ca\u81f4\u656c AI \u7684\u4e2a\u6027\u5316\u827a\u672f\u4f5c\u54c1\u3002", "conclusion": "Polynomiogram \u6846\u67b6\u5728\u79d1\u5b66\u7814\u7a76\u3001\u6559\u80b2\u53ef\u89c6\u5316\u548c\u751f\u6210\u827a\u672f\u65b9\u9762\u5747\u5c55\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u5b9e\u73b0\u4e86\u6570\u5b66\u4e25\u8c28\u6027\u4e0e\u827a\u672f\u521b\u9020\u529b\u7684\u6709\u6548\u878d\u5408\u3002"}}
{"id": "2512.04355", "categories": ["cs.DC", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.04355", "abs": "https://arxiv.org/abs/2512.04355", "authors": ["Gregory Bolet", "Giorgis Georgakoudis", "Konstantinos Parasyris", "Harshitha Menon", "Niranjan Hasabnis", "Kirk W. Cameron", "Gal Oren"], "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity", "comment": "13 pages, 6 figures, MLSys 2026 Submission", "summary": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 gpuFLOPBench \u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u672a\u8fd0\u884c CUDA \u5185\u6838\u7684\u60c5\u51b5\u4e0b\u9884\u6d4b\u5176\u5355\u53cc\u7cbe\u5ea6\u6d6e\u70b9\u8fd0\u7b97\u6b21\u6570\uff08FLOPs\uff09\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u9690\u5f0f FLOPs \u65f6\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u73b0\u4ee3 GPU \u8f6f\u4ef6\u5f00\u53d1\u9700\u8981\u5728\u542f\u52a8\u5185\u6838\u524d\u9884\u5224\u6027\u80fd\u74f6\u9888\uff0c\u800c\u73b0\u6709 LLMs \u7f3a\u4e4f\u5bf9\u6b64\u7c7b\u524d\u77bb\u6027\u6027\u80fd\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b 577 \u4e2a\u6765\u81ea HeCBench \u7684 CUDA \u5185\u6838\u7684\u6570\u636e\u96c6\uff0c\u6bcf\u4e2a\u5185\u6838\u6807\u6ce8\u4e86\u771f\u5b9e FLOP \u6570\u91cf\u548c\u516b\u9879\u6267\u884c\u5c5e\u6027\uff0c\u5e76\u4ee5\u6b64\u8bc4\u4f30\u4e3b\u6d41\u95ed\u6e90 LLM \u5728\u201c\u4e0d\u8fd0\u884c\u5373\u8ba1\u6570\u201d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u6700\u65b0 LLM \u5728\u7b80\u5355\u5185\u6838\u4e0a\u53ef\u5b9e\u73b0\u5b8c\u7f8e\u5206\u7c7b\uff0c\u4f46\u5728\u6d89\u53ca\u9664\u6cd5\u3001\u5185\u7f6e\u6570\u5b66\u51fd\u6570\u6216\u516c\u5171\u5b50\u8868\u8fbe\u5f0f\u7b49\u9690\u5f0f FLOPs \u573a\u666f\u4e2d\u4ecd\u4f1a\u51fa\u73b0\u6570\u91cf\u7ea7\u7ea7\u522b\u7684\u9884\u6d4b\u9519\u8bef\u3002", "conclusion": "\u5f53\u524d\u4ee3\u7801\u8f85\u52a9\u5de5\u5177\u96be\u4ee5\u5185\u5316\u786c\u4ef6\u76f8\u5173\u7684\u5fae\u7801\u6548\u5e94\uff0cgpuFLOPBench \u53ef\u4f5c\u4e3a\u63a8\u52a8 LLM \u53d1\u5c55\u3001\u4f7f\u5176\u5177\u5907\u4e13\u4e1a GPU \u5f00\u53d1\u8005\u7ea7\u6027\u80fd\u63a8\u7406\u80fd\u529b\u7684\u91cd\u8981\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2512.04273", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04273", "abs": "https://arxiv.org/abs/2512.04273", "authors": ["Tyler Slater"], "title": "Quantitative Analysis of Technical Debt and Pattern Violation in Large Language Model Architectures", "comment": "Under review at the Journal of Systems and Software (Special Issue on Impactful Software Architecture)", "summary": "As Large Language Models (LLMs) transition from code completion tools to autonomous system architects, their impact on long-term software maintainability remains unquantified. While existing research benchmarks functional correctness (pass@k), this study presents the first empirical framework to measure \"Architectural Erosion\" and the accumulation of Technical Debt in AI-synthesized microservices. We conducted a comparative pilot study of three state-of-the-art models (GPT-5.1, Claude 4.5 Sonnet, and Llama 3 8B) by prompting them to implement a standardized Book Lending Microservice under strict Hexagonal Architecture constraints. Utilizing Abstract Syntax Tree (AST) parsing, we find that while proprietary models achieve high architectural conformance (0% violation rate for GPT-5.1), open-weights models exhibit critical divergence. Specifically, Llama 3 demonstrated an 80% Architectural Violation Rate, frequently bypassing interface adapters to create illegal circular dependencies between Domain and Infrastructure layers. Furthermore, we identified a phenomenon of \"Implementation Laziness,\" where open-weights models generated 60% fewer Logical Lines of Code (LLOC) than their proprietary counterparts, effectively omitting complex business logic to satisfy token constraints. These findings suggest that without automated architectural linting, utilizing smaller open-weights models for system scaffolding accelerates the accumulation of structural technical debt.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u8861\u91cfAI\u751f\u6210\u5fae\u670d\u52a1\u4e2d\u201c\u67b6\u6784\u4fb5\u8680\u201d\u548c\u201c\u6280\u672f\u503a\u52a1\u201d\u7684\u5b9e\u8bc1\u6846\u67b6\uff0c\u53d1\u73b0\u5f00\u6e90\u6743\u91cd\u6a21\u578b\uff08\u5982Llama 3\uff09\u5728\u9075\u5faa\u516d\u8fb9\u5f62\u67b6\u6784\u65b9\u9762\u8868\u73b0\u663e\u8457\u5dee\u4e8e\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-5.1\uff09\uff0c\u5b58\u5728\u9ad8\u67b6\u6784\u8fdd\u89c4\u7387\u548c\u903b\u8f91\u4ee3\u7801\u7f3a\u5931\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7814\u7a76\u591a\u805a\u7126\u4e8e\u529f\u80fd\u6b63\u786e\u6027\uff08\u5982pass@k\u6307\u6807\uff09\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u751f\u6210\u4ee3\u7801\u957f\u671f\u53ef\u7ef4\u62a4\u6027\u5f71\u54cd\u7684\u91cf\u5316\u5206\u6790\uff0c\u5c24\u5176\u662f\u67b6\u6784\u5c42\u9762\u7684\u6280\u672f\u503a\u52a1\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63d0\u793a\u4e09\u79cd\u524d\u6cbf\u6a21\u578b\uff08GPT-5.1\u3001Claude 4.5 Sonnet \u548c Llama 3 8B\uff09\u5728\u4e25\u683c\u516d\u8fb9\u5f62\u67b6\u6784\u7ea6\u675f\u4e0b\u5b9e\u73b0\u6807\u51c6\u5316\u7684\u56fe\u4e66\u501f\u9605\u5fae\u670d\u52a1\uff0c\u5e76\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\uff08AST\uff09\u89e3\u6790\u8bc4\u4f30\u5176\u67b6\u6784\u5408\u89c4\u6027\u4e0e\u903b\u8f91\u4ee3\u7801\u884c\u6570\uff08LLOC\uff09\u3002", "result": "\u95ed\u6e90\u6a21\u578b\uff08\u5982GPT-5.1\uff09\u67b6\u6784\u8fdd\u89c4\u7387\u4e3a0%\uff0c\u800cLlama 3\u7684\u67b6\u6784\u8fdd\u89c4\u7387\u8fbe80%\uff0c\u5e38\u5728\u9886\u57df\u5c42\u4e0e\u57fa\u7840\u8bbe\u65bd\u5c42\u95f4\u5f15\u5165\u975e\u6cd5\u5faa\u73af\u4f9d\u8d56\uff1b\u6b64\u5916\uff0c\u5f00\u6e90\u6a21\u578b\u751f\u6210\u7684\u903b\u8f91\u4ee3\u7801\u884c\u6570\u6bd4\u95ed\u6e90\u6a21\u578b\u5c1160%\uff0c\u8868\u73b0\u51fa\u201c\u5b9e\u73b0\u60f0\u6027\u201d\u3002", "conclusion": "\u82e5\u65e0\u81ea\u52a8\u5316\u67b6\u6784\u68c0\u67e5\u673a\u5236\uff0c\u4f7f\u7528\u8f83\u5c0f\u7684\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u811a\u624b\u67b6\u6784\u5efa\u4f1a\u52a0\u901f\u7ed3\u6784\u6027\u6280\u672f\u503a\u52a1\u7684\u79ef\u7d2f\uff0c\u5f71\u54cd\u8f6f\u4ef6\u957f\u671f\u53ef\u7ef4\u62a4\u6027\u3002"}}
{"id": "2512.04389", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04389", "abs": "https://arxiv.org/abs/2512.04389", "authors": ["Zhen Hu", "Dongliang Xiong", "Kai Huang", "Changjun Wu", "Xiaowen Jiang"], "title": "A Structure-Aware Irregular Blocking Method for Sparse LU Factorization", "comment": null, "summary": "In sparse LU factorization, nonzero elements after symbolic factorization tend to distribute in diagonal and right-bottom region of sparse matrices. However, regular 2D blocking on this non-uniform distribution structure may lead to workload imbalance across blocks. Besides, existing matrix features fail to guide us effectively in blocking. In this paper, we propose a structure-aware irregular blocking method for numerical factorization. A novel diagonal block-based feature is introduced to effectively characterize the local nonzero distribution of sparse matrices. Based on this, we further propose an irregular blocking method that adjusts block sizes according to the local distribution of nonzeros. The strategy utilizes fine-grained blocks in dense regions and coarse-grained blocks in sparse regions, adequately balancing the nonzeros of blocks both within the same level and across levels in the dependency tree. Experiments demonstrate that, on a single NVIDIA A100 GPU, our proposed irregular blocking method achieves average speedups of 1.50x and 3.32x over PanguLU and the latest SuperLU_DIST, respectively. In addition, it achieves speedups of 1.40x and 3.84x over PanguLU and SuperLU_DIST on 4 NVIDIA A100 GPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u4e0d\u89c4\u5219\u5206\u5757\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5bf9\u89d2\u5757\u7684\u65b0\u7279\u5f81\u6765\u523b\u753b\u7a00\u758f\u77e9\u9635\u4e2d\u975e\u96f6\u5143\u7684\u5c40\u90e8\u5206\u5e03\uff0c\u5e76\u636e\u6b64\u52a8\u6001\u8c03\u6574\u5206\u5757\u5927\u5c0f\uff0c\u5728\u5355GPU\u548c\u591aGPU\u73af\u5883\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7a00\u758fLU\u5206\u89e3\u5de5\u5177\u3002", "motivation": "\u5728\u7a00\u758fLU\u5206\u89e3\u4e2d\uff0c\u7b26\u53f7\u5206\u89e3\u540e\u7684\u975e\u96f6\u5143\u7d20\u901a\u5e38\u96c6\u4e2d\u5728\u77e9\u9635\u7684\u5bf9\u89d2\u7ebf\u548c\u53f3\u4e0b\u533a\u57df\uff0c\u4f20\u7edf\u4e8c\u7ef4\u89c4\u5219\u5206\u5757\u5728\u8fd9\u79cd\u975e\u5747\u5300\u5206\u5e03\u4e0b\u5bb9\u6613\u9020\u6210\u8d1f\u8f7d\u4e0d\u5747\u8861\uff1b\u540c\u65f6\uff0c\u73b0\u6709\u77e9\u9635\u7279\u5f81\u96be\u4ee5\u6709\u6548\u6307\u5bfc\u5206\u5757\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u7684\u4e0d\u89c4\u5219\u5206\u5757\u65b9\u6cd5\uff1a\u9996\u5148\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5bf9\u89d2\u5757\u7684\u7279\u5f81\u4ee5\u523b\u753b\u7a00\u758f\u77e9\u9635\u5c40\u90e8\u975e\u96f6\u5206\u5e03\uff1b\u7136\u540e\u6839\u636e\u8be5\u7279\u5f81\u52a8\u6001\u8c03\u6574\u5206\u5757\u7c92\u5ea6\u2014\u2014\u5728\u7a20\u5bc6\u533a\u57df\u4f7f\u7528\u7ec6\u7c92\u5ea6\u5757\uff0c\u5728\u7a00\u758f\u533a\u57df\u4f7f\u7528\u7c97\u7c92\u5ea6\u5757\uff0c\u4ece\u800c\u5728\u4f9d\u8d56\u6811\u7684\u540c\u5c42\u53ca\u8de8\u5c42\u95f4\u5b9e\u73b0\u975e\u96f6\u5143\u6570\u91cf\u7684\u5747\u8861\u3002", "result": "\u5728\u5355\u5757NVIDIA A100 GPU\u4e0a\uff0c\u6240\u63d0\u65b9\u6cd5\u76f8\u6bd4PanguLU\u548cSuperLU_DIST\u5206\u522b\u83b7\u5f971.50\u500d\u548c3.32\u500d\u7684\u5e73\u5747\u52a0\u901f\uff1b\u57284\u5757A100 GPU\u4e0a\uff0c\u52a0\u901f\u6bd4\u5206\u522b\u4e3a1.40\u500d\u548c3.84\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e0d\u89c4\u5219\u5206\u5757\u65b9\u6cd5\u80fd\u6709\u6548\u5e94\u5bf9\u7a00\u758fLU\u5206\u89e3\u4e2d\u975e\u96f6\u5143\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u5e76\u884c\u6027\u80fd\uff0c\u4f18\u4e8e\u5f53\u524d\u4e3b\u6d41\u7a00\u758f\u6c42\u89e3\u5668\u3002"}}
{"id": "2512.04319", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04319", "abs": "https://arxiv.org/abs/2512.04319", "authors": ["Zixiao Zhao", "Fatemeh H. Fard", "Jie JW Wu"], "title": "MANTRA: a Framework for Multi-stage Adaptive Noise TReAtment During Training", "comment": null, "summary": "The reliable application of deep learning models to software engineering tasks hinges on high-quality training data. Yet, large-scale repositories inevitably introduce noisy or mislabeled examples that degrade both accuracy and robustness. While Noise Label Learning (NLL) has been extensively studied in other fields, there are a few works that investigate NLL in Software Engineering (SE) and Large Language Models (LLMs) for SE tasks. In this work, we propose MANTRA, a Multi-stage Adaptive Noise TReAtment framework that embeds noise diagnosis and mitigation directly into the fine-tuning process of code-Pretrained Language Models (PTM) and code-LLMs. We first investigate the effect of noise at varying levels on convergence and loss trajectories of the models. Then we apply an adaptive dropout strategy guided by per-sample loss dynamics and Gaussian Mixture Model clustering to exclude persistently noisy points while preserving clean data. Applying to code summarization and commit intent classification, our experiments reveal that some LLMs are more sensitive to noise than others. However, with MANTRA, the performance of all models in both tasks is improved. MANTRA enables researchers and practitioners to reduce the impact of errors introduced by the dataset in training, saves time in data cleaning and processing, while maximizing the effect of fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMANTRA\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4ee3\u7801\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5d4c\u5165\u566a\u58f0\u8bca\u65ad\u4e0e\u7f13\u89e3\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u542b\u566a\u8f6f\u4ef6\u5de5\u7a0b\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u4f9d\u8d56\u4e8e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u5927\u89c4\u6a21\u4ee3\u7801\u4ed3\u5e93\u5e38\u5305\u542b\u566a\u58f0\u6216\u9519\u8bef\u6807\u7b7e\uff0c\u5f71\u54cd\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u9c81\u68d2\u6027\uff1b\u76ee\u524d\u9488\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51faMANTRA\u591a\u9636\u6bb5\u81ea\u9002\u5e94\u566a\u58f0\u5904\u7406\u6846\u67b6\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7ed3\u5408\u6837\u672c\u635f\u5931\u52a8\u6001\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u805a\u7c7b\uff0c\u91c7\u7528\u81ea\u9002\u5e94dropout\u7b56\u7565\u5254\u9664\u6301\u7eed\u566a\u58f0\u6837\u672c\uff0c\u4fdd\u7559\u5e72\u51c0\u6570\u636e\u3002", "result": "\u5728\u4ee3\u7801\u6458\u8981\u548c\u63d0\u4ea4\u610f\u56fe\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u566a\u58f0\u654f\u611f\u5ea6\u4e0d\u540c\uff0c\u4f46\u4f7f\u7528MANTRA\u540e\u6240\u6709\u6a21\u578b\u6027\u80fd\u5747\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "MANTRA\u80fd\u6709\u6548\u51cf\u8f7b\u8bad\u7ec3\u6570\u636e\u4e2d\u566a\u58f0\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u51cf\u5c11\u6570\u636e\u6e05\u6d17\u65f6\u95f4\uff0c\u5e76\u6700\u5927\u5316\u5fae\u8c03\u6548\u679c\uff0c\u4e3a\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u7684\u566a\u58f0\u6807\u7b7e\u95ee\u9898\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.04449", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04449", "abs": "https://arxiv.org/abs/2512.04449", "authors": ["Suyeon Lee", "Kangkyu Park", "Kwangsik Shin", "Ada Gavrilovska"], "title": "Offloading to CXL-based Computational Memory", "comment": null, "summary": "CXL-based Computational Memory (CCM) enables near-memory processing within expanded remote memory, presenting opportunities to address data movement costs associated with disaggregated memory systems and to accelerate overall performance. However, existing operation offloading mechanisms are not capable of leveraging the trade-offs of different models based on different CXL protocols. This work first examines these tradeoffs and demonstrates their impact on end-to-end performance and system efficiency for workloads with diverse data and processing requirements. We propose a novel 'Asynchronous Back-Streaming' protocol by carefully layering data and control transfer operations on top of the underlying CXL protocols. We design KAI, a system that realizes the asynchronous back-streaming model that supports asynchronous data movement and lightweight pipelining in host-CCM interactions. Overall, KAI reduces end-to-end runtime by up to 50.4%, and CCM and host idle times by average 22.11x and 3.85x, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKAI\u7684\u65b0\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u5f02\u6b65\u56de\u6d41\u201d\u534f\u8bae\uff0c\u5728CXL\u8ba1\u7b97\u5185\u5b58\uff08CCM\uff09\u4e2d\u5b9e\u73b0\u5f02\u6b65\u6570\u636e\u4f20\u8f93\u4e0e\u8f7b\u91cf\u7ea7\u6d41\u6c34\u7ebf\uff0c\u663e\u8457\u63d0\u5347\u7aef\u5230\u7aef\u6027\u80fd\u5e76\u51cf\u5c11\u4e3b\u673a\u4e0eCCM\u7684\u7a7a\u95f2\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709CXL\u8ba1\u7b97\u5185\u5b58\u4e2d\u7684\u4efb\u52a1\u5378\u8f7d\u673a\u5236\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4e0d\u540cCXL\u534f\u8bae\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u96be\u4ee5\u9002\u5e94\u5177\u6709\u591a\u6837\u5316\u6570\u636e\u548c\u5904\u7406\u9700\u6c42\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86\u4e0d\u540cCXL\u534f\u8bae\u4e0b\u7684\u6027\u80fd\u6743\u8861\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u4e86\u201c\u5f02\u6b65\u56de\u6d41\u201d\u534f\u8bae\uff0c\u5c06\u6570\u636e\u4e0e\u63a7\u5236\u4f20\u8f93\u64cd\u4f5c\u5206\u5c42\u53e0\u52a0\u4e8e\u5e95\u5c42CXL\u534f\u8bae\u4e4b\u4e0a\uff0c\u6784\u5efa\u4e86\u652f\u6301\u5f02\u6b65\u6570\u636e\u79fb\u52a8\u548c\u8f7b\u91cf\u7ea7\u6d41\u6c34\u7ebf\u7684KAI\u7cfb\u7edf\u3002", "result": "KAI\u7cfb\u7edf\u6700\u591a\u53ef\u5c06\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1150.4%\uff0c\u5e73\u5747\u964d\u4f4eCCM\u548c\u4e3b\u673a\u7a7a\u95f2\u65f6\u95f4\u5206\u522b\u8fbe22.11\u500d\u548c3.85\u500d\u3002", "conclusion": "\u901a\u8fc7\u5408\u7406\u5229\u7528CXL\u534f\u8bae\u7279\u6027\u5e76\u5f15\u5165\u5f02\u6b65\u673a\u5236\uff0cKAI\u6709\u6548\u63d0\u5347\u4e86\u8ba1\u7b97\u5185\u5b58\u7cfb\u7edf\u7684\u6027\u80fd\u4e0e\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4e3a\u672a\u6765\u5f02\u6784\u5185\u5b58\u67b6\u6784\u4e2d\u7684\u9ad8\u6548\u4efb\u52a1\u5378\u8f7d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.04344", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.04344", "abs": "https://arxiv.org/abs/2512.04344", "authors": ["Zitong Zhou", "Ben Limpanukorn", "Hong Jin Kang", "Jiyuan Wang", "Yaoxuan Wu", "Akos Kiss", "Renata Hodovan", "Miryung Kim"], "title": "Targeted Testing of Compiler Optimizations via Grammar-Level Composition Styles", "comment": null, "summary": "Ensuring the correctness of compiler optimizations is critical, but existing fuzzers struggle to test optimizations effectively. First, most fuzzers use optimization pipelines (heuristics-based, fixed sequences of passes) as their harness. The phase-ordering problem can enable or preempt transformations, so pipelines inevitably miss optimization interactions; moreover, many optimizations are not scheduled, even at aggressive levels. Second, optimizations typically fire only when inputs satisfy specific structural relationships, which existing generators and mutations struggle to produce. We propose targeted fuzzing of individual optimizations to complement pipeline-based testing. Our key idea is to exploit composition styles - structural relations over program constructs (adjacency, nesting, repetition, ordering) - that optimizations look for. We build a general-purpose, grammar-based mutational fuzzer, TargetFuzz, that (i) mines composition styles from an optimization-relevant corpus, then (ii) rebuilds them inside different contexts offered by a larger, generic corpus via synthesized mutations to test variations of optimization logic. TargetFuzz is adaptable to a new programming language by lightweight, grammar-based, construct annotations - and it automatically synthesizes mutators and crossovers to rebuild composition styles. No need for hand-coded generators or language-specific mutators, which is particularly useful for modular frameworks such as MLIR, whose dialect-based, rapidly evolving ecosystem makes optimizations difficult to fuzz. Our evaluation on LLVM and MLIR shows that TargetFuzz improves coverage by 8% and 11% and triggers optimizations 2.8$\\times$ and 2.6$\\times$, compared to baseline fuzzers under the targeted fuzzing mode. We show that targeted fuzzing is complementary: it effectively tests all 37 sampled LLVM optimizations, while pipeline-fuzzing missed 12.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTargetFuzz\u7684\u65b0\u578b\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u5355\u4e2a\u7f16\u8bd1\u5668\u4f18\u5316\u8fdb\u884c\u5b9a\u5411\u6a21\u7cca\u6d4b\u8bd5\uff0c\u5229\u7528\u7a0b\u5e8f\u7ed3\u6784\u4e2d\u7684\u7ec4\u5408\u6a21\u5f0f\uff08\u5982\u76f8\u90bb\u3001\u5d4c\u5957\u7b49\uff09\u6765\u6709\u6548\u89e6\u53d1\u4f18\u5316\u903b\u8f91\u3002\u76f8\u6bd4\u4f20\u7edf\u57fa\u4e8e\u4f18\u5316\u6d41\u6c34\u7ebf\u7684\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\uff0cTargetFuzz\u5728LLVM\u548cMLIR\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u8986\u76d6\u7387\u548c\u4f18\u5316\u89e6\u53d1\u6b21\u6570\uff0c\u5e76\u80fd\u8986\u76d6\u66f4\u591a\u539f\u672c\u88ab\u9057\u6f0f\u7684\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u6a21\u7cca\u6d4b\u8bd5\u5de5\u5177\u96be\u4ee5\u6709\u6548\u6d4b\u8bd5\u7f16\u8bd1\u5668\u4f18\u5316\uff1a\u4e00\u65b9\u9762\u4f9d\u8d56\u56fa\u5b9a\u987a\u5e8f\u7684\u4f18\u5316\u6d41\u6c34\u7ebf\uff0c\u53d7\u76f8\u4f4d\u6392\u5e8f\u95ee\u9898\u5f71\u54cd\uff0c\u65e0\u6cd5\u5145\u5206\u66b4\u9732\u4f18\u5316\u4e4b\u95f4\u7684\u4ea4\u4e92\uff1b\u53e6\u4e00\u65b9\u9762\u96be\u4ee5\u751f\u6210\u6ee1\u8db3\u7279\u5b9a\u7ed3\u6784\u5173\u7cfb\u7684\u8f93\u5165\u4ee5\u89e6\u53d1\u4f18\u5316\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u51c6\u3001\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u63d0\u51faTargetFuzz\uff0c\u4e00\u79cd\u57fa\u4e8e\u6587\u6cd5\u7684\u53d8\u5f02\u578b\u6a21\u7cca\u6d4b\u8bd5\u5668\u3002\u5b83\u9996\u5148\u4ece\u4e0e\u4f18\u5316\u76f8\u5173\u7684\u8bed\u6599\u5e93\u4e2d\u6316\u6398\u201c\u7ec4\u5408\u98ce\u683c\u201d\uff08\u5373\u4f18\u5316\u6240\u4f9d\u8d56\u7684\u7ed3\u6784\u5173\u7cfb\uff09\uff0c\u7136\u540e\u5728\u66f4\u5927\u7684\u901a\u7528\u8bed\u6599\u5e93\u4e2d\u901a\u8fc7\u5408\u6210\u53d8\u5f02\u5c06\u8fd9\u4e9b\u7ec4\u5408\u98ce\u683c\u91cd\u5efa\u5230\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\uff0c\u4ece\u800c\u6d4b\u8bd5\u5404\u79cd\u4f18\u5316\u903b\u8f91\u3002\u8be5\u65b9\u6cd5\u53ea\u9700\u8f7b\u91cf\u7ea7\u7684\u6587\u6cd5\u6ce8\u89e3\u5373\u53ef\u9002\u914d\u65b0\u8bed\u8a00\uff0c\u5e76\u81ea\u52a8\u5408\u6210\u53d8\u5f02\u548c\u4ea4\u53c9\u64cd\u4f5c\uff0c\u65e0\u9700\u624b\u5de5\u7f16\u5199\u751f\u6210\u5668\u6216\u8bed\u8a00\u7279\u5b9a\u7684\u53d8\u5f02\u7b56\u7565\u3002", "result": "\u5728LLVM\u548cMLIR\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTargetFuzz\u76f8\u6bd4\u57fa\u7ebf\u6a21\u7cca\u5668\uff0c\u5728\u5b9a\u5411\u6a21\u7cca\u6a21\u5f0f\u4e0b\u5206\u522b\u63d0\u53478%\u548c11%\u7684\u8986\u76d6\u7387\uff0c\u89e6\u53d1\u4f18\u5316\u6b21\u6570\u63d0\u9ad82.8\u500d\u548c2.6\u500d\u3002\u6b64\u5916\uff0cTargetFuzz\u6210\u529f\u6d4b\u8bd5\u4e86\u5168\u90e837\u4e2a\u91c7\u6837\u7684LLVM\u4f18\u5316\uff0c\u800c\u6d41\u6c34\u7ebf\u6a21\u7cca\u6d4b\u8bd5\u9057\u6f0f\u4e86\u5176\u4e2d12\u4e2a\u3002", "conclusion": "TargetFuzz\u901a\u8fc7\u5b9a\u5411\u6a21\u7cca\u6d4b\u8bd5\u6709\u6548\u5f25\u8865\u4e86\u4f20\u7edf\u6d41\u6c34\u7ebf\u6a21\u7cca\u6d4b\u8bd5\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u5982MLIR\u8fd9\u7c7b\u6a21\u5757\u5316\u3001\u5feb\u901f\u6f14\u5316\u7684\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u4e3a\u7f16\u8bd1\u5668\u4f18\u5316\u7684\u6b63\u786e\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.04984", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.04984", "abs": "https://arxiv.org/abs/2512.04984", "authors": ["O. Tansel Baydas", "Ozgur B. Akan"], "title": "Federated Learning for Terahertz Wireless Communication", "comment": "10 pages, 4 figures", "summary": "The convergence of Terahertz (THz) communications and Federated Learning (FL) promises ultra-fast distributed learning, yet the impact of realistic wideband impairments on optimization dynamics remains theoretically uncharacterized. This paper bridges this gap by developing a multicarrier stochastic framework that explicitly couples local gradient updates with frequency-selective THz effects, including beam squint, molecular absorption, and jitter. Our analysis uncovers a critical diversity trap: under standard unbiased aggregation, the convergence error floor is driven by the harmonic mean of subcarrier SNRs. Consequently, a single spectral hole caused by severe beam squint can render the entire bandwidth useless for reliable model updates. We further identify a fundamental bandwidth limit, revealing that expanding the spectrum beyond a critical point degrades convergence due to the integration of thermal noise and gain collapse at band edges. Finally, we demonstrate that an SNR-weighted aggregation strategy is necessary to suppress the variance singularity at these spectral holes, effectively recovering convergence in high-squint regimes where standard averaging fails. Numerical results validate the expected impact of the discussed physical layer parameters' on performance of THz-FL systems.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u592a\u8d6b\u5179\uff08THz\uff09\u901a\u4fe1\u4e2d\u5bbd\u5e26\u635f\u4f24\u5bf9\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4f18\u5316\u52a8\u6001\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u591a\u8f7d\u6ce2\u968f\u673a\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6807\u51c6\u805a\u5408\u4e0b\u56e0\u9891\u8c31\u7a7a\u6d1e\u5bfc\u81f4\u7684\u201c\u591a\u6837\u6027\u9677\u9631\u201d\u548c\u5e26\u5bbd\u6269\u5c55\u7684\u6536\u655b\u6027\u6076\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51faSNR\u52a0\u6743\u805a\u5408\u7b56\u7565\u4ee5\u6062\u590d\u9ad8\u6ce2\u675f\u504f\u659c\u4e0b\u7684\u6536\u655b\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u592a\u8d6b\u5179\u901a\u4fe1\u4e2d\u5b9e\u9645\u5bbd\u5e26\u635f\u4f24\uff08\u5982\u6ce2\u675f\u504f\u659c\u3001\u5206\u5b50\u5438\u6536\u548c\u6296\u52a8\uff09\u5982\u4f55\u5f71\u54cd\u8054\u90a6\u5b66\u4e60\u4f18\u5316\u52a8\u6001\u7684\u7406\u8bba\u523b\u753b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u8f7d\u6ce2\u968f\u673a\u6846\u67b6\uff0c\u5c06\u672c\u5730\u68af\u5ea6\u66f4\u65b0\u4e0e\u9891\u7387\u9009\u62e9\u6027THz\u6548\u5e94\u663e\u5f0f\u8026\u5408\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u805a\u5408\u7b56\u7565\u4e0b\u7684\u6536\u655b\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u6807\u51c6\u65e0\u504f\u805a\u5408\u4e0b\u6536\u655b\u8bef\u5dee\u7531\u5b50\u8f7d\u6ce2\u4fe1\u566a\u6bd4\u7684\u8c03\u548c\u5e73\u5747\u51b3\u5b9a\uff0c\u5355\u4e2a\u9891\u8c31\u7a7a\u6d1e\u53ef\u4f7f\u6574\u4e2a\u5e26\u5bbd\u5931\u6548\uff1b\u8bc6\u522b\u51fa\u5e26\u5bbd\u5b58\u5728\u4e34\u754c\u4e0a\u9650\uff0c\u8d85\u8fc7\u540e\u56e0\u70ed\u566a\u58f0\u548c\u8fb9\u7f18\u589e\u76ca\u5d29\u6e83\u53cd\u800c\u6076\u5316\u6536\u655b\uff1b\u63d0\u51fa\u7684SNR\u52a0\u6743\u805a\u5408\u7b56\u7565\u80fd\u6709\u6548\u6291\u5236\u9891\u8c31\u7a7a\u6d1e\u5904\u7684\u65b9\u5dee\u5947\u5f02\u6027\uff0c\u5728\u9ad8\u6ce2\u675f\u504f\u659c\u573a\u666f\u4e0b\u6062\u590d\u6536\u655b\u3002", "conclusion": "\u5728\u592a\u8d6b\u5179\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\uff0c\u5fc5\u987b\u8003\u8651\u7269\u7406\u5c42\u5bbd\u5e26\u635f\u4f24\u5bf9\u5b66\u4e60\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u91c7\u7528SNR\u611f\u77e5\u7684\u805a\u5408\u673a\u5236\u662f\u5b9e\u73b0\u53ef\u9760\u6536\u655b\u7684\u5173\u952e\u3002"}}
{"id": "2512.04445", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.04445", "abs": "https://arxiv.org/abs/2512.04445", "authors": ["Yanbin Zhang", "Hanhui Ye", "Yue Bai", "Qiming Zhang", "Liao Xiang", "Wu Mianzhi", "Renjun Hu"], "title": "Automating Complex Document Workflows via Stepwise and Rollback-Enabled Operation Orchestration", "comment": "9 pages, 3 figures, accepted by AAAI-2026", "summary": "Workflow automation promises substantial productivity gains in everyday document-related tasks. While prior agentic systems can execute isolated instructions, they struggle with automating multi-step, session-level workflows due to limited control over the operational process. To this end, we introduce AutoDW, a novel execution framework that enables stepwise, rollback-enabled operation orchestration. AutoDW incrementally plans API actions conditioned on user instructions, intent-filtered API candidates, and the evolving states of the document. It further employs robust rollback mechanisms at both the argument and API levels, enabling dynamic correction and fault tolerance. These designs together ensure that the execution trajectory of AutoDW remains aligned with user intent and document context across long-horizon workflows. To assess its effectiveness, we construct a comprehensive benchmark of 250 sessions and 1,708 human-annotated instructions, reflecting realistic document processing scenarios with interdependent instructions. AutoDW achieves 90% and 62% completion rates on instruction- and session-level tasks, respectively, outperforming strong baselines by 40% and 76%. Moreover, AutoDW also remains robust for the decision of backbone LLMs and on tasks with varying difficulty. Code and data will be open-sourced. Code: https://github.com/YJett/AutoDW", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAutoDW\uff0c\u4e00\u79cd\u652f\u6301\u9010\u6b65\u6267\u884c\u4e0e\u56de\u6eda\u673a\u5236\u7684\u6587\u6863\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5728\u5305\u542b\u591a\u6b65\u9aa4\u3001\u76f8\u4e92\u4f9d\u8d56\u6307\u4ee4\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u7cfb\u7edf\u867d\u80fd\u6267\u884c\u5355\u6761\u6307\u4ee4\uff0c\u4f46\u5728\u5904\u7406\u591a\u6b65\u9aa4\u3001\u4f1a\u8bdd\u7ea7\u6587\u6863\u5de5\u4f5c\u6d41\u65f6\u7f3a\u4e4f\u5bf9\u64cd\u4f5c\u6d41\u7a0b\u7684\u6709\u6548\u63a7\u5236\uff0c\u96be\u4ee5\u4fdd\u8bc1\u957f\u671f\u4efb\u52a1\u4e2d\u4e0e\u7528\u6237\u610f\u56fe\u548c\u6587\u6863\u72b6\u6001\u7684\u4e00\u81f4\u6027\u3002", "method": "AutoDW\u901a\u8fc7\u9010\u6b65\u89c4\u5212API\u8c03\u7528\uff0c\u7ed3\u5408\u7528\u6237\u6307\u4ee4\u3001\u610f\u56fe\u8fc7\u6ee4\u540e\u7684API\u5019\u9009\u96c6\u53ca\u6587\u6863\u72b6\u6001\u6f14\u5316\u8fdb\u884c\u51b3\u7b56\uff0c\u5e76\u5728\u53c2\u6570\u548cAPI\u5c42\u9762\u5f15\u5165\u56de\u6eda\u673a\u5236\uff0c\u5b9e\u73b0\u52a8\u6001\u4fee\u6b63\u4e0e\u5bb9\u9519\u3002", "result": "\u5728\u5305\u542b250\u4e2a\u4f1a\u8bdd\u30011,708\u6761\u4eba\u5de5\u6807\u6ce8\u6307\u4ee4\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoDW\u5728\u6307\u4ee4\u7ea7\u548c\u4f1a\u8bdd\u7ea7\u4efb\u52a1\u4e0a\u5206\u522b\u8fbe\u523090%\u548c62%\u7684\u5b8c\u6210\u7387\uff0c\u8f83\u5f3a\u57fa\u7ebf\u63d0\u534740%\u548c76%\uff0c\u4e14\u5bf9\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4efb\u52a1\u96be\u5ea6\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "AutoDW\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6b65\u9aa4\u6587\u6863\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u4e2d\u7684\u63a7\u5236\u4e0e\u5bb9\u9519\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u548c\u4e00\u81f4\u6027\uff0c\u5177\u5907\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.04474", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.04474", "abs": "https://arxiv.org/abs/2512.04474", "authors": ["Jiaqi Sun", "Wei Li", "Heng Zhang", "Chutong Ding", "Shiyou Qian", "Jian Cao", "Guangtao Xue"], "title": "LLM-SrcLog: Towards Proactive and Unified Log Template Extraction via Large Language Models", "comment": null, "summary": "Log parsing transforms raw logs into structured templates containing constants and variables. It underpins anomaly detection, failure diagnosis, and other AIOps tasks. Current parsers are mostly reactive and log-centric. They only infer templates from logs, mostly overlooking the source code. This restricts their capacity to grasp dynamic log structures or adjust to evolving systems. Moreover, per-log LLM inference is too costly for practical deployment. In this paper, we propose LLM-SrcLog, a proactive and unified framework for log template parsing. It extracts templates directly from source code prior to deployment and supplements them with data-driven parsing for logs without available code. LLM-SrcLog integrates a cross-function static code analyzer to reconstruct meaningful logging contexts, an LLM-based white-box template extractor with post-processing to distinguish constants from variables, and a black-box template extractor that incorporates data-driven clustering for remaining unmatched logs. Experiments on two public benchmarks (Hadoop and Zookeeper) and a large-scale industrial system (Sunfire-Compute) show that, compared to two LLM-based baselines, LLM-SrcLog improves average F1-score by 2-17% and 8-35%. Meanwhile, its online parsing latency is comparable to data-driven methods and about 1,000 times faster than per-log LLM parsing. LLM-SrcLog achieves a near-ideal balance between speed and accuracy. Finally, we further validate the effectiveness of LLM-SrcLog through practical case studies in a real-world production environment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLLM-SrcLog\uff0c\u4e00\u79cd\u7ed3\u5408\u6e90\u4ee3\u7801\u5206\u6790\u4e0e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u65e5\u5fd7\u6a21\u677f\u89e3\u6790\u6846\u67b6\uff0c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u65e5\u5fd7\u89e3\u6790\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u65e5\u5fd7\u672c\u8eab\uff0c\u5ffd\u7565\u6e90\u4ee3\u7801\u4fe1\u606f\uff0c\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u65e5\u5fd7\u7ed3\u6784\u548c\u7cfb\u7edf\u6f14\u5316\uff1b\u540c\u65f6\uff0c\u9010\u6761\u65e5\u5fd7\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002", "method": "LLM-SrcLog\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u8de8\u51fd\u6570\u9759\u6001\u4ee3\u7801\u5206\u6790\u5668\u4ee5\u91cd\u5efa\u65e5\u5fd7\u4e0a\u4e0b\u6587\u3001\u57fa\u4e8eLLM\u7684\u767d\u76d2\u6a21\u677f\u63d0\u53d6\u5668\uff08\u542b\u540e\u5904\u7406\u4ee5\u533a\u5206\u5e38\u91cf\u4e0e\u53d8\u91cf\uff09\u3001\u4ee5\u53ca\u7528\u4e8e\u5904\u7406\u65e0\u6e90\u7801\u65e5\u5fd7\u7684\u9ed1\u76d2\u6570\u636e\u9a71\u52a8\u805a\u7c7b\u89e3\u6790\u5668\u3002", "result": "\u5728Hadoop\u3001Zookeeper\u548cSunfire-Compute\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cLLM-SrcLog\u76f8\u6bd4\u4e24\u79cd\u57fa\u4e8eLLM\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cF1\u5206\u6570\u63d0\u53472\u201335%\uff0c\u5728\u7ebf\u89e3\u6790\u5ef6\u8fdf\u4e0e\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u76f8\u5f53\uff0c\u6bd4\u9010\u6761LLM\u89e3\u6790\u5feb\u7ea61000\u500d\u3002", "conclusion": "LLM-SrcLog\u901a\u8fc7\u878d\u5408\u6e90\u7801\u5148\u9a8c\u4e0e\u6570\u636e\u9a71\u52a8\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u4e0e\u9ad8\u6548\u7387\u7684\u7edf\u4e00\uff0c\u5e76\u5728\u771f\u5b9e\u751f\u4ea7\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2512.04538", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.04538", "abs": "https://arxiv.org/abs/2512.04538", "authors": ["Xinkui Zhao", "Rongkai Liu", "Yifan Zhang", "Chen Zhi", "Lufei Zhang", "Guanjie Cheng", "Yueshen Xu", "Shuiguang Deng", "Jianwei Yin"], "title": "Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding", "comment": null, "summary": "As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.", "AI": {"tldr": "CoCo is a novel code completion framework that leverages multi-granularity structured context from large-scale repositories via static analysis and graph-based selection, significantly outperforming existing methods by up to 20.2% EM on benchmark datasets.", "motivation": "Existing RAG-based code completion methods treat code as plain text, ignoring structural semantics and code-specific dependencies, which limits their ability to understand control flow and developer intent.", "method": "CoCo uses static code analysis to extract structured context at function, file, and project levels, applies a graph-based multi-granularity context selection mechanism to filter noise, converts the context into natural language prompts, and employs a structure-aware re-ranker to align generated code semantically and structurally.", "result": "CoCo achieves consistent improvements over state-of-the-art baselines, with up to 20.2% gains in exact match (EM) on CrossCodeEval and RepoEval benchmarks, and is model-agnostic for easy integration.", "conclusion": "By incorporating structured, multi-granularity context and preserving code semantics, CoCo significantly enhances repository-level code completion quality and offers a flexible, plug-and-play solution for existing systems."}}
{"id": "2512.04673", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.04673", "abs": "https://arxiv.org/abs/2512.04673", "authors": ["Gunjan Das", "Paheli Bhattacharya", "Rishabh Gupta"], "title": "Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u901a\u7528\u548c\u4ee3\u7801\u4e13\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u3001\u63a8\u7406\u548c\u4ee3\u7801\u7406\u89e3\u7b49\u591a\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4ee3\u7801\u4f18\u5316\u6a21\u578b\uff08\u5982CodeLLaMA\uff09\u5728\u975e\u7f16\u7801\u4efb\u52a1\u4e2d\u4e5f\u5c55\u73b0\u51fa\u4f18\u4e8e\u901a\u7528\u6a21\u578b\uff08\u5982Mistral-7B\u3001Llama-3-8B\uff09\u7684\u63a8\u7406\u4e0e\u8bed\u6cd5\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u4e8e\u5355\u4e2a\u6a21\u578b\u80fd\u529b\uff0c\u7f3a\u4e4f\u5bf9\u8bed\u8a00\u3001\u63a8\u7406\u4e0e\u4ee3\u7801\u7406\u89e3\u80fd\u529b\u7684\u7edf\u4e00\u8de8\u9886\u57df\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "method": "\u5728\u516d\u4e2a\u6db5\u76d6\u8bed\u8a00\u80fd\u529b\u3001\u6570\u5b66\u63a8\u7406\u548c\u53ef\u4fe1\u5ea6\u7684\u57fa\u51c6\u4e0a\uff0c\u5bf9\u4e94\u79cd\u901a\u7528\u548c\u4e09\u79cd\u4ee3\u7801\u4e13\u7528\u7684\u6700\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u5728CoNaLa\u6570\u636e\u96c6\u4e0a\u5206\u6790\u5176\u4ee3\u7801\u89e3\u91ca\u884c\u4e3a\u3002", "result": "\u4ee3\u7801\u4f18\u5316\u6a21\u578b\uff08\u5982CodeLLaMA\u53d8\u4f53\uff09\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u63a8\u7406\u80fd\u529b\u548c\u8bed\u6cd5\u7cbe\u786e\u6027\uff0c\u5373\u4f7f\u5728\u975e\u7f16\u7801\u4efb\u52a1\u4e2d\u4e5f\u4f18\u4e8e\u901a\u7528\u6a21\u578b\u3002", "conclusion": "\u4e13\u4e3a\u4ee3\u7801\u8bbe\u8ba1\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u9886\u57df\u80fd\u529b\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u8868\u660e\u4ee3\u7801\u8bad\u7ec3\u5bf9\u63d0\u5347\u7efc\u5408\u8bed\u8a00\u4e0e\u63a8\u7406\u80fd\u529b\u5177\u6709\u79ef\u6781\u5f71\u54cd\u3002"}}
{"id": "2512.04680", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.04680", "abs": "https://arxiv.org/abs/2512.04680", "authors": ["Jialong Li", "Mingyue Zhang", "Nianyu Li", "Danny Weyns", "Zhi Jin", "Kenji Tei"], "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap", "comment": "Accepted by ACM Transactions on Autonomous and Adaptive Systems", "summary": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5728\u81ea\u9002\u5e94\u7cfb\u7edf\uff08SASs\uff09\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\u4e0e\u6311\u6218\uff0c\u56f4\u7ed5MAPE-K\u53cd\u9988\u73af\u529f\u80fd\u589e\u5f3a\u548c\u4eba\u673a\u534f\u540c\u4e24\u4e2a\u65b9\u9762\u8fdb\u884c\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u6574\u5408GenAI\u5230SAS\u4e2d\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u53ca\u5e94\u5bf9\u7b56\u7565\u3002", "motivation": "\u5c3d\u7ba1GenAI\u5728\u6570\u636e\u7406\u89e3\u4e0e\u903b\u8f91\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u5176\u80fd\u529b\u4e0eSAS\u7684\u6838\u5fc3\u529f\u80fd\u9ad8\u5ea6\u5951\u5408\uff0c\u4f46\u5176\u5728SAS\u4e2d\u5e94\u7528\u7684\u5177\u4f53\u4f18\u52bf\u4e0e\u6311\u6218\u5c1a\u4e0d\u660e\u786e\u3002\u7531\u4e8e\u76f8\u5173\u6587\u732e\u6709\u9650\u3001SAS\u6280\u672f\u4e0e\u5e94\u7528\u573a\u666f\u591a\u6837\u4ee5\u53caGenAI\u6280\u672f\u5feb\u901f\u6f14\u8fdb\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u68b3\u7406\u4ee5\u6307\u5bfc\u7814\u7a76\u4e0e\u5b9e\u8df5\u3002", "method": "\u4f5c\u8005\u4ece\u56db\u4e2a\u4e0d\u540c\u7814\u7a76\u9886\u57df\u6536\u96c6\u3001\u7b5b\u9009\u5e76\u5206\u6790\u76f8\u5173\u6587\u732e\uff0c\u5c06GenAI\u5bf9SAS\u7684\u6f5c\u5728\u76ca\u5904\u5f52\u7eb3\u4e3a\u4e24\u7c7b\uff1a\u4e00\u662f\u63d0\u5347SAS\u5728MAPE-K\u53cd\u9988\u73af\u5404\u73af\u8282\u4e2d\u7684\u81ea\u4e3b\u6027\uff0c\u4e8c\u662f\u6539\u5584\u4eba\u5728\u73af\u8def\u4e2d\u4e0eSAS\u7684\u4ea4\u4e92\u3002\u57fa\u4e8e\u6b64\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u6574\u5408GenAI\u5230SAS\u7684\u7814\u7a76\u8def\u7ebf\u56fe\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u51faGenAI\u5728\u589e\u5f3aSAS\u81ea\u4e3b\u6027\u548c\u4eba\u673a\u534f\u4f5c\u65b9\u9762\u7684\u5177\u4f53\u6f5c\u529b\uff0c\u5e76\u7cfb\u7edf\u603b\u7ed3\u4e86\u5f53\u524d\u9762\u4e34\u7684\u5173\u952e\u7814\u7a76\u6311\u6218\uff0c\u5982\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u3001\u5b9e\u65f6\u6027\u7b49\u3002\u540c\u65f6\u6307\u51fa\u4e86GenAI\u81ea\u8eab\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u53ef\u80fd\u7684\u7f13\u89e3\u7b56\u7565\u3002", "conclusion": "GenAI\u4e3aSAS\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u4f46\u5176\u6709\u6548\u6574\u5408\u4ecd\u9762\u4e34\u591a\u91cd\u6311\u6218\u3002\u672c\u6587\u901a\u8fc7\u63d0\u4f9b\u5168\u9762\u7684\u73b0\u72b6\u5feb\u7167\u548c\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u5728\u8be5\u4ea4\u53c9\u9886\u57df\u7684\u63a2\u7d22\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.04702", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.04702", "abs": "https://arxiv.org/abs/2512.04702", "authors": ["Divyansh Pandey", "Vyakhya Gupta", "Prakhar Singhal", "Karthik Vaidhyanathan"], "title": "POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?", "comment": "Accepted as a short paper at SEAMS 2026", "summary": "The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPOLARIS\uff0c\u4e00\u79cd\u4e09\u5c42\u591a\u667a\u80fd\u4f53\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4f4e\u5ef6\u8fdf\u9002\u914d\u3001\u53ef\u89e3\u91ca\u63a8\u7406\u4e0e\u5143\u5b66\u4e60\u80fd\u529b\uff0c\u5b9e\u73b0\u5bf9\u672a\u77e5\u73af\u5883\u7684\u9884\u6d4b\u6027\u3001\u4e3b\u52a8\u5f0f\u81ea\u9002\u5e94\uff0c\u6807\u5fd7\u7740\u8fc8\u5411\u201c\u81ea\u9002\u5e943.0\u201d\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u4f20\u7edf\u81ea\u9002\u5e94\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7531\u89c4\u6a21\u3001\u590d\u6742\u6027\u548c\u81ea\u4e3b\u6027\u5e26\u6765\u7684\u672a\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6848\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u548c\u8de8\u5b50\u7cfb\u7edf\u534f\u540c\u673a\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7a81\u53d1\u7684\u201c\u672a\u77e5\u7684\u672a\u77e5\u201d\u95ee\u9898\u3002", "method": "\u63d0\u51faPOLARIS\u6846\u67b6\uff0c\u5305\u542b\u4e09\u5c42\uff1a\uff081\uff09\u4f4e\u5ef6\u8fdf\u9002\u914d\u5668\u5c42\u7528\u4e8e\u76d1\u63a7\u4e0e\u5b89\u5168\u6267\u884c\uff1b\uff082\uff09\u900f\u660e\u63a8\u7406\u5c42\u5229\u7528\u5de5\u5177\u611f\u77e5\u3001\u53ef\u89e3\u91ca\u7684\u667a\u80fd\u4f53\u751f\u6210\u5e76\u9a8c\u8bc1\u9002\u5e94\u8ba1\u5212\uff1b\uff083\uff09\u5143\u5b66\u4e60\u5c42\u8bb0\u5f55\u7ecf\u9a8c\u5e76\u6301\u7eed\u4f18\u5316\u9002\u5e94\u7b56\u7565\u3002", "result": "\u5728SWIM\u548cSWITCH\u4e24\u4e2a\u81ea\u9002\u5e94\u57fa\u51c6\u4e0a\u7684\u521d\u6b65\u8bc4\u4f30\u8868\u660e\uff0cPOLARIS consistently\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "POLARIS\u4ee3\u8868\u4e86\u5411Self-Adaptation 3.0\u7684\u6f14\u8fdb\uff0c\u5373\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u4ece\u73af\u5883\u4e2d\u5b66\u4e60\uff0c\u8fd8\u80fd\u63a8\u7406\u5e76\u81ea\u6211\u6f14\u5316\u5176\u9002\u5e94\u673a\u5236\uff0c\u6301\u7eed\u63d0\u5347\u5e94\u5bf9\u65b0\u6311\u6218\u7684\u80fd\u529b\u3002"}}
{"id": "2512.05062", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.05062", "abs": "https://arxiv.org/abs/2512.05062", "authors": ["Yue Zhang", "Uchswas Paul", "Marcelo d'Amorim", "Akond Rahman"], "title": "Configuration Defects in Kubernetes", "comment": null, "summary": "Kubernetes is a tool that facilitates rapid deployment of software. Unfortunately, configuring Kubernetes is prone to errors. Configuration defects are not uncommon and can result in serious consequences. This paper reports an empirical study about configuration defects in Kubernetes with the goal of helping practitioners detect and prevent these defects. We study 719 defects that we extract from 2,260 Kubernetes configuration scripts using open source repositories. Using qualitative analysis, we identify 15 categories of defects. We find 8 publicly available static analysis tools to be capable of detecting 8 of the 15 defect categories. We find that the highest precision and recall of those tools are for defects related to data fields. We develop a linter to detect two categories of defects that cause serious consequences, which none of the studied tools are able to detect. Our linter revealed 26 previously-unknown defects that have been confirmed by practitioners, 19 of which have already been fixed. We conclude our paper by providing recommendations on how defect detection and repair techniques can be used for Kubernetes configuration scripts. The datasets and source code used for the paper are publicly available online.", "AI": {"tldr": "\u672c\u6587\u5bf9Kubernetes\u914d\u7f6e\u7f3a\u9677\u8fdb\u884c\u4e86\u5b9e\u8bc1\u7814\u7a76\uff0c\u8bc6\u522b\u51fa15\u7c7b\u7f3a\u9677\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u9759\u6001\u5206\u6790\u5de5\u5177\u7684\u68c0\u6d4b\u80fd\u529b\uff1b\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684linter\u6765\u68c0\u6d4b\u4e24\u79cd\u4e25\u91cd\u4f46\u672a\u88ab\u73b0\u6709\u5de5\u5177\u8986\u76d6\u7684\u7f3a\u9677\u7c7b\u578b\uff0c\u53d1\u73b0\u4e8626\u4e2a\u672a\u77e5\u7f3a\u9677\uff0c\u5176\u4e2d19\u4e2a\u5df2\u88ab\u4fee\u590d\u3002", "motivation": "Kubernetes\u914d\u7f6e\u5bb9\u6613\u51fa\u9519\uff0c\u914d\u7f6e\u7f3a\u9677\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u8fd9\u4e9b\u7f3a\u9677\u5e76\u63d0\u5347\u68c0\u6d4b\u4e0e\u9884\u9632\u80fd\u529b\u3002", "method": "\u4ece\u5f00\u6e90\u4ed3\u5e93\u4e2d\u63d0\u53d62,260\u4e2aKubernetes\u914d\u7f6e\u811a\u672c\u4e2d\u7684719\u4e2a\u7f3a\u9677\uff0c\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u5c06\u5176\u5f52\u7c7b\u4e3a15\u79cd\u7c7b\u578b\uff1b\u8bc4\u4f308\u4e2a\u516c\u5f00\u9759\u6001\u5206\u6790\u5de5\u5177\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u5f00\u53d1\u4e00\u4e2a\u65b0\u7684linter\u4ee5\u68c0\u6d4b\u73b0\u6709\u5de5\u5177\u65e0\u6cd5\u53d1\u73b0\u7684\u4e24\u7c7b\u4e25\u91cd\u7f3a\u9677\u3002", "result": "\u73b0\u6709\u5de5\u5177\u4ec5\u80fd\u68c0\u6d4b15\u7c7b\u7f3a\u9677\u4e2d\u76848\u7c7b\uff0c\u4e14\u5bf9\u6570\u636e\u5b57\u6bb5\u76f8\u5173\u7f3a\u9677\u68c0\u6d4b\u6548\u679c\u6700\u597d\uff1b\u65b0\u5f00\u53d1\u7684linter\u6210\u529f\u53d1\u73b026\u4e2a\u6b64\u524d\u672a\u77e5\u7684\u7f3a\u9677\uff0c\u5176\u4e2d19\u4e2a\u5df2\u88ab\u5f00\u53d1\u8005\u786e\u8ba4\u5e76\u4fee\u590d\u3002", "conclusion": "\u5e94\u7ed3\u5408\u7f3a\u9677\u68c0\u6d4b\u4e0e\u4fee\u590d\u6280\u672f\u6765\u63d0\u5347Kubernetes\u914d\u7f6e\u811a\u672c\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u76f8\u5173\u5b9e\u8df5\u5efa\u8bae\uff1b\u7814\u7a76\u6570\u636e\u4e0e\u4ee3\u7801\u5df2\u516c\u5f00\u3002"}}
