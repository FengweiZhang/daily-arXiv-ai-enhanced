<div id=toc></div>

# Table of Contents

- [cs.CE](#cs.CE) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.DB](#cs.DB) [Total: 4]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [1] [A unified framework for equation discovery and dynamic prediction of hysteretic systems](https://arxiv.org/abs/2512.02408)
*Siyuan Yang,Wei Liu,Zhilu Lai*

Main category: cs.CE

TL;DR: 本文提出了一种统一框架，结合内变量学习与符号回归，从数据中自动发现滞回系统的显式控制方程，无需预定义模型库。


<details>
  <summary>Details</summary>
Motivation: 现有滞回建模方法（如Bouc-Wen模型）依赖理想化假设和参数校准，而现有方程发现方法多为系统特定或依赖预定义模型库，难以灵活捕捉未知机制。

Method: 整合内变量学习与符号回归，直接从数据中自动提取滞回内变量并发现显式控制方程，不依赖如SINDy等方法所需的预定义库。

Result: 所发现的方程可自然用于预测滞回系统的动态响应，实现了对滞回动力学的有效建模与表征。

Conclusion: 该研究为滞回系统的方程发现与动力学表征提供了一个系统性、统一的框架，提升了建模的灵活性与适用性。

Abstract: Hysteresis is a nonlinear phenomenon with memory effects, where a system's output depends on both its current state and past states. It is prevalent in various physical and mechanical systems, such as yielding structures under seismic excitation, ferromagnetic materials, and piezoelectric actuators. Analytical models like the Bouc-Wen model are often employed but rely on idealized assumptions and careful parameter calibration, limiting their applicability to diverse or mechanism-unknown behaviors. Existing equation discovery approaches for hysteresis are often system-specific or rely on predefined model libraries, which limit their flexibility and ability to capture the hidden mechanisms. To address these, this research develops a unified framework that integrates learning of internal variables (commonly used in modeling hysteresis) and symbolic regression to automatically extract internal hysteretic variable, and discover explicit governing equations directly from data without predefined libraries as required by methods such as sparse identification of nonlinear dynamics (SINDy). Solving the discovered equations naturally enables prediction of the dynamic responses of hysteretic systems. This work provides a systematic view and approach for both equation discovery and characterization of hysteretic dynamics, defining a unified framework for these types of problems.

</details>


### [2] [The Invisible Hand: Characterizing Generative AI Adoption and its Effects on An Online Freelancing Market](https://arxiv.org/abs/2512.02509)
*Yiming Zhu,Gareth Tyson,Pan Hui*

Main category: cs.CE

TL;DR: 本文通过分析 Freelancer.com 上超过180万条职位和380万用户的数据，研究了生成式人工智能（GenAI）对自由职业市场中职位发布、技能需求和用户行为的影响，特别聚焦于ChatGPT相关岗位的兴起及其特征。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情后自由职业平台迅速增长，而生成式AI的兴起引发了对其如何影响自由职业市场职位需求和工作者参与的疑问，但目前缺乏相关实证研究。

Method: 对 Freelancer.com 平台进行大规模数据分析，涵盖180多万个职位帖和380多万名用户，重点分析与ChatGPT相关的职位及其技能要求和任务类型。

Result: 发现ChatGPT在自由职业市场中占据领先地位，明确了GenAI相关职位的具体技能需求和任务内容，揭示了AI对就业结构、技能组合和用户行为的影响。

Conclusion: 生成式AI正在重塑自由职业市场的格局，本研究为理解AI时代下自由职业的演变提供了实证依据和全面视角。

Abstract: Since the COVID-19 pandemic, freelancing platforms have experienced significant growth in both worker registrations and job postings. However, the rise of generative AI (GenAI) technologies has raised questions about how it affect the job posting in freelancer market. Despite growing discussions, there is limited empirical research on the GenAI adoption and its effect on job demand and worker engagement. We present a large-scale analysis of Freelancer.com, utilizing over 1.8 million job posts and 3.8 million users. We investigate the emergence of jobs with the adoption of GenAI and identify leading position of ChatGPT in the freelancing market. With a focus on ChatGPT related jobs, we inspect their specific skill requirements, and the tasks that workers are asked to perform. Our findings provide insights into the evolving landscape of freelancing in the age of AI, offering a comprehensive profile of GenAI's effects on employment, skills, and user behaviors in freelancing market.

</details>


### [3] [Beyond N-grams: A Hierarchical Reward Learning Framework for Clinically-Aware Medical Report Generation](https://arxiv.org/abs/2512.02710)
*Yuan Wang,Shujian Gao,Jiaxiang Liu,Songtao Jiang,Haoxiang Xia,Xiaotian Zhang,Zhaolu Kang,Yemin Wang,Zuozhu Liu*

Main category: cs.CE

TL;DR: 本文提出HiMed-RL框架，通过三层层次化奖励机制（词元级、概念级、语义级）提升医学报告生成的临床准确性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动医学报告生成方法虽能生成流畅语句，但常存在临床幻觉问题，导致事实错误，难以在真实医疗场景中可靠部署。

Method: 提出HiMed-RL层次化医学奖励学习框架，包含词元级语言流畅性、概念级医学术语对齐专家知识、语义级诊断一致性验证，并采用类人动态奖励调整策略，由简到难训练模型。

Result: HiMed-3B在域内和域外基准上均达到SOTA，尤其在域外任务上比次优基线提升12.1%。

Conclusion: 该工作为生成兼具语言流畅性与临床细粒度质量的医学报告提供了可靠范式。

Abstract: Automatic medical report generation can greatly reduce the workload of doctors, but it is often unreliable for real-world deployment. Current methods can write formally fluent sentences but may be factually flawed, introducing serious medical errors known as clinical hallucinations, which make them untrustworthy for diagnosis. To bridge this gap, we introduce HiMed-RL, a Hierarchical Medical Reward Learning Framework designed to explicitly prioritize clinical quality. HiMed-RL moves beyond simple text matching by deconstructing reward learning into three synergistic levels: it first ensures linguistic fluency at the token-level, then enforces factual grounding at the concept-level by aligning key medical terms with expert knowledge, and finally assesses high-level diagnostic consistency at the semantic-level using a specialized LLM verifier. This hierarchical reward is implemented via a Human-inspired Dynamic Reward Adjustment, a strategy which first teaches the model to learn basic facts before progressing to more complex diagnostic reasoning. Experimentally, HiMed-3B achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks, particularly on the latter, with an improvement of 12.1% over the second-best baseline. Our work provides a robust paradigm for generating reports that not only improve fluency but clinical fine-grained quality.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [4] [ProtO-RU: An O-RAN Split-7.2 Radio Unit using SDRs](https://arxiv.org/abs/2512.02398)
*Zhiyu Zhou,Xin Zhe Khooi,Satis Kumar Permal,Mun Choon Chan*

Main category: cs.NI

TL;DR: ProtO-RU 是首个基于开源软件和通用硬件实现的 O-RAN Split-7.2 无线单元（RU），具备可编程性、兼容性和良好性能，为 O-RAN 研究提供了低成本平台。


<details>
  <summary>Details</summary>
Motivation: 当前商用 O-RU 多为专有硬件，缺乏开放性和可编程性，限制了 O-RAN 架构下 RU 层面的研究与创新。因此，亟需一个开源、灵活且可集成的 O-RU 实现方案。

Method: 基于开源 srsRAN 软件栈，在软件定义无线电（SDR）和通用 CPU 上构建 ProtO-RU，支持 TDD/FDD 双工模式，并与 srsRAN 和 OpenAirInterface5G 的 CU/DU 堆栈集成。

Result: ProtO-RU 能与商用 5G 用户设备互操作，在多用户持续负载下保持稳定，吞吐量表现与 Split-8 架构及商用 O-RU 相当。

Conclusion: ProtO-RU 成功实现了高性能、全功能的开源 Split-7.2 O-RU，显著降低了 O-RAN 端到端研究门槛，并为 RU 层创新提供了新平台。

Abstract: We present ProtO-RU, the first open source, software-defined O-RAN Split-7.2 Radio Unit built using SDRs and commodity CPUs. Unlike proprietary hardware-based commercial O-RUs, ProtO-RU is built on the open-source srsRAN software stack, and it is fully programmable. We demonstrate that ProtO-RU integrates with the srsRAN and OpenAirInterface5G CU/DU stacks, supports both TDD and FDD duplexing modes, and interoperates with commercial 5G UEs. Our evaluation shows that ProtO-RU remains stable under sustained load with multiple UEs and delivers throughput comparable to Split-8 and commercial O-RUs. ProtO-RU opens up new opportunities for RU-level innovations and lowers the barrier of entry for end-to-end O-RAN research.

</details>


### [5] [Wi-Fi Rate Adaptation for Moving Equipment in Industrial Environments](https://arxiv.org/abs/2512.02455)
*Pietro Chiavassa,Stefano Scanzio,Gianluca Cena*

Main category: cs.NI

TL;DR: 本文评估了Minstrel速率自适应算法在静态和移动场景下的性能，重点关注工业环境中关键的延迟和丢包率指标，为未来基于集中式数字孪生的增强型速率自适应算法开发提供初步依据。


<details>
  <summary>Details</summary>
Motivation: 工业环境中对无线通信（如Wi-Fi）提出了严格的可靠性要求，尤其是有界传输延迟，而现有速率自适应算法（如Minstrel）在此类场景下的性能尚不明确，需进行针对性评估以支持后续优化。

Method: 对Linux内核默认的开源速率自适应算法Minstrel在静态和移动两种场景下进行实验评估，重点测量其在工业应用中关注的延迟和丢包率等性能指标。

Result: 论文提供了Minstrel算法在不同场景下延迟和丢包率的具体表现数据，揭示了其在满足工业可靠性需求方面的局限性。

Conclusion: Minstrel算法的当前性能评估结果可作为基础，用于指导未来开发更适用于工业环境、基于集中式数字孪生的增强型速率自适应算法。

Abstract: Wi-Fi is currently considered one of the most promising solutions for interconnecting mobile equipment (e.g., autonomous mobile robots and active exoskeletons) in industrial environments. However, relability requirements imposed by the industrial context, such as ensuring bounded transmission latency, are a major challenge for over-the-air communication. One of the aspects of Wi-Fi technology that greatly affects the probability of a packet reaching its destination is the selection of the appropriate transmission rate. Rate adaptation algorithms are in charge of this operation, but their design and implementation are not regulated by the IEEE 802.11 standard. One of the most popular solutions, available as open source, is Minstrel, which is the default choice for the Linux Kernel. In this paper, Minstrel performance is evaluated for both static and mobility scenarios. Our analysis focuses on metrics of interest for industrial contexts, i.e., latency and packet loss ratio, and serves as a preliminary evaluation for the future development of enhanced rate adaptation algorithms based on centralized digital twins.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [6] [DOLMA: A Data Object Level Memory Disaggregation Framework for HPC Applications](https://arxiv.org/abs/2512.02300)
*Haoyu Zheng,Shouwei Gao,Jie Ren,Wenqian Dong*

Main category: cs.DC

TL;DR: DOLMA 是一种面向高性能计算（HPC）的数据对象级内存解耦框架，通过智能卸载数据对象、定量分析本地内存需求以及利用双缓冲机制进行远程内存预取，在显著降低本地内存使用的同时将性能损失控制在16%以内。


<details>
  <summary>Details</summary>
Motivation: 内存解耦虽能提升HPC系统内存容量和利用率，但远程内存访问带来的性能开销对计算密集型HPC应用影响显著，因其执行时间高度依赖数据局部性。

Method: DOLMA 框架通过识别并卸载数据对象至远程内存，提供本地内存大小的定量分析，并利用HPC应用中可预测的内存访问模式，采用双缓冲设计实现远程内存预取，同时兼顾多线程并发。

Result: 在8个HPC工作负载和计算核上的评估表明，DOLMA平均最多减少63%的本地内存使用，同时将性能下降控制在16%以内。

Conclusion: DOLMA 在最小化性能损失的前提下，为HPC领域提供了一种灵活高效的内存解耦解决方案。

Abstract: Memory disaggregation is promising to scale memory capacity and improves utilization in HPC systems. However, the performance overhead of accessing remote memory poses a significant chal- lenge, particularly for compute-intensive HPC applications where execution times are highly sensitive to data locality. In this work, we present DOLMA, a Data Object Level M emory dis Aggregation framework designed for HPC applications. DOLMA intelligently identifies and offloads data objects to remote memory, while pro- viding quantitative analysis to decide a suitable local memory size. Furthermore, DOLMA leverages the predictable memory access patterns typical in HPC applications and enables remote memory prefetch via a dual-buffer design. By carefully balancing local and remote memory usage and maintaining multi-thread concurrency, DOLMA provides a flexible and efficient solution for leveraging dis- aggregated memory in HPC domains while minimally compromis- ing application performance. Evaluating with eight HPC workloads and computational kernels, DOLMA limits performance degrada- tion to less than 16% while reducing local memory usage by up to 63%, on average.

</details>


### [7] [Offloading Artificial Intelligence Workloads across the Computing Continuum by means of Active Storage Systems](https://arxiv.org/abs/2512.02646)
*Alex Barceló,Sebastián A. Cajas Ordoñez,Jaydeep Samanta,Andrés L. Suárez-Cetrulo,Romila Ghosh,Ricardo Simón Carbajo,Anna Queralt*

Main category: cs.DC

TL;DR: 本文提出一种基于主动存储的软件架构，用于在计算连续体中高效分发AI工作负载，通过将计算嵌入存储系统减少数据传输开销，在保持模型准确率的同时显著提升内存效率和训练速度。


<details>
  <summary>Details</summary>
Motivation: 传统云架构难以应对AI工作负载带来的海量、高速数据处理需求，导致存储、计算和数据移动效率低下；现有分布式框架缺乏对设备异构性和算法快速演进的适应能力。

Method: 设计并实现了一种面向计算连续体的AI工作负载分发软件架构，利用主流Python库与dataClay主动存储平台，将计算任务嵌入存储层以减少数据迁移。

Result: 实验表明，通过主动存储卸载工作负载可显著提升内存效率和训练速度，同时在不同设备上保持模型准确性，并在存储需求、训练时间和执行效率方面展现出良好权衡。

Conclusion: 主动存储技术能有效优化AI工作负载管理，提升分布式AI部署的可扩展性与资源效率，且对领域专家和开发者具有低使用门槛。

Abstract: The increasing demand for artificial intelligence (AI) workloads across diverse computing environments has driven the need for more efficient data management strategies. Traditional cloud-based architectures struggle to handle the sheer volume and velocity of AI-driven data, leading to inefficiencies in storage, computation, and data movement. This paper explores the integration of active storage systems within the computing continuum to optimize AI workload distribution.
  By embedding computation directly into storage architectures, active storage is able to reduce data transfer overhead, enhancing performance and improving resource utilization. Other existing frameworks and architectures offer mechanisms to distribute certain AI processes across distributed environments; however, they lack the flexibility and adaptability that the continuum requires, both regarding the heterogeneity of devices and the rapid-changing algorithms and models being used by domain experts and researchers.
  This article proposes a software architecture aimed at seamlessly distributing AI workloads across the computing continuum, and presents its implementation using mainstream Python libraries and dataClay, an active storage platform. The evaluation shows the benefits and trade-offs regarding memory consumption, storage requirements, training times, and execution efficiency across different devices. Experimental results demonstrate that the process of offloading workloads through active storage significantly improves memory efficiency and training speeds while maintaining accuracy. Our findings highlight the potential of active storage to revolutionize AI workload management, making distributed AI deployments more scalable and resource-efficient with a very low entry barrier for domain experts and application developers.

</details>


### [8] [Distributed and Autonomic Minimum Spanning Trees](https://arxiv.org/abs/2512.02683)
*Luiz A. Rodrigues,Elias P. Duarte,Luciana Arantes*

Main category: cs.DC

TL;DR: 本文提出了一种自适应算法，用于在分布式系统中构建和维护具有对数深度和度的生成树，以支持可扩展且容错的广播通信。


<details>
  <summary>Details</summary>
Motivation: 传统的一对多广播方法在分布式系统中不可扩展，会给发送方带来沉重负载；因此需要一种更高效、可扩展且能容忍故障的通信机制。

Method: 利用VCube虚拟拓扑作为故障检测器，设计一种自适应算法，动态构建并维护一棵每个节点入度和树深度均不超过$ \log_2 n $的生成树，并基于该树实现尽力而为和可靠两种广播算法。

Result: 该算法可在最多$ n-1 $个进程失效的情况下，仍保持正确进程间的连通性；仿真结果表明其优于其他替代方案。

Conclusion: 所提出的自适应生成树算法有效提升了分布式系统中广播通信的可扩展性和容错能力。

Abstract: The most common strategy for enabling a process in a distributed system to broadcast a message is one-to-all communication. However, this approach is not scalable, as it places a heavy load on the sender. This work presents an autonomic algorithm that enables the $n$ processes in a distributed system to build and maintain a spanning tree connecting themselves. In this context, processes are the vertices of the spanning tree. By definition, a spanning tree connects all processes without forming cycles. The proposed algorithm ensures that every vertex in the spanning tree has both an in-degree and the tree depth of at most $log_2 n$. When all processes are correct, the degree of each process is exactly $log_2 n$. A spanning tree is dynamically created from any source process and is transparently reconstructed as processes fail or recover. Up to $n-1$ processes can fail, and the correct processes remain connected through a scalable, functioning spanning tree. To build and maintain the tree, processes use the VCube virtual topology, which also serves as a failure detector. Two broadcast algorithms based on the autonomic spanning tree algorithm are presented: one for best-effort broadcast and one for reliable broadcast. Simulation results are provided, including comparisons with other alternatives.

</details>


### [9] [Designing FAIR Workflows at OLCF: Building Scalable and Reusable Ecosystems for HPC Science](https://arxiv.org/abs/2512.02818)
*Sean R. Wilkinson,Patrick Widener,Sarp Oral,Rafael Ferreira da Silva*

Main category: cs.DC

TL;DR: 本文提出高性能计算（HPC）中心应积极构建支持跨学科研究的FAIR生态系统，通过将工作流拆解为可复用的FAIR组件，提升科研数字资源的共享与重用效率。


<details>
  <summary>Details</summary>
Motivation: HPC用户常因系统环境差异而重复开发与特定中心绑定的数字制品，造成资源浪费；现有FAIR实践多局限于单一学科，形成信息孤岛，阻碍跨领域协作。

Method: 借鉴欧洲开放科学云（EOSC）中EOSC-Life FAIR工作流协作平台的架构，设计一种面向HPC环境的组件化FAIR模型，强调对工作流中单个组件而非整体进行FAIR化。

Result: 该方法能更好地适应HPC用户的多样化和动态需求，提高数字制品的长期价值，并促进跨学科科研协作。

Conclusion: HPC中心应在推动跨学科FAIR生态建设中发挥核心作用，通过基础设施设计支持组件级FAIR实践，从而增强科研数字资源的可发现性、可共享性和可重用性。

Abstract: High Performance Computing (HPC) centers provide advanced infrastructure that enables scientific research at extreme scale. These centers operate with hardware configurations, software environments, and security requirements that differ substantially from most users' local systems. As a result, users often develop customized digital artifacts that are tightly coupled to a given HPC center. This practice can lead to significant duplication of effort as multiple users independently create similar solutions to common problems. The FAIR Principles offer a framework to address these challenges. Initially designed to improve data stewardship, the FAIR approach has since been extended to encompass software, workflows, models, and infrastructure. By encouraging the use of rich metadata and community standards, FAIR practices aim to make digital artifacts easier to share and reuse, both within and across scientific domains. Many FAIR initiatives have emerged within individual research communities, often aligned by discipline (e.g. bioinformatics, earth sciences). These communities have made progress in adopting FAIR practices, but their domain-specific nature can lead to silos that limit broader collaboration. Thus, we propose that HPC centers play a more active role in fostering FAIR ecosystems that support research across multiple disciplines. This requires designing infrastructure that enables researchers to discover, share, and reuse computational components more effectively. Here, we build on the architecture of the European Open Science Cloud (EOSC) EOSC-Life FAIR Workflows Collaboratory to propose a model tailored to the needs of HPC. Rather than focusing on entire workflows, we emphasize the importance of making individual workflow components FAIR. This component-based approach better supports the diverse and evolving needs of HPC users while maximizing the long-term value of their work.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Towards autonomous normative multi-agent systems for Human-AI software engineering teams](https://arxiv.org/abs/2512.02329)
*Hoa Khanh Dam,Geeta Mahala,Rashina Hoda,Xi Zheng,Cristina Conati*

Main category: cs.SE

TL;DR: 本文提出一种由具备类人推理能力的AI智能体主导的新型软件工程范式，通过大语言模型驱动、结合信念-愿望-意图架构与规范协调机制，实现高效、可靠且合规的软件开发。


<details>
  <summary>Details</summary>
Motivation: 当前软件开发流程在速度、可靠性和适应性方面存在局限，亟需引入更智能、自主的AI系统来提升开发效率与质量，并确保合规性。

Method: 构建基于大语言模型的软件工程智能体，赋予其信念、愿望、意图和记忆等认知能力，并通过道义模态（承诺、义务、禁止、许可）规范其与人类及其他智能体之间的协作。

Result: 实现了智能体在软件设计、实现、测试与部署中的高效协同，显著超越现有开发流程的性能，并建立了一个可扩展、透明且可信的人机协作框架。

Conclusion: 该研究为未来人机协同的软件工程团队奠定了基础，展示了AI智能体在提升开发效率、保障合规性及构建可信系统方面的巨大潜力。

Abstract: This paper envisions a transformative paradigm in software engineering, where Artificial Intelligence, embodied in fully autonomous agents, becomes the primary driver of the core software development activities. We introduce a new class of software engineering agents, empowered by Large Language Models and equipped with beliefs, desires, intentions, and memory to enable human-like reasoning. These agents collaborate with humans and other agents to design, implement, test, and deploy software systems with a level of speed, reliability, and adaptability far beyond the current software development processes. Their coordination and collaboration are governed by norms expressed as deontic modalities - commitments, obligations, prohibitions and permissions - that regulate interactions and ensure regulatory compliance. These innovations establish a scalable, transparent and trustworthy framework for future Human-AI software engineering teams.

</details>


### [11] [Process-Centric Analysis of Agentic Software Systems](https://arxiv.org/abs/2512.02393)
*Shuyang Liu,Yang Chen,Rahul Krishna,Saurabh Sinha,Jatin Ganhotra,Reyhan Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出Graphectory方法，用于对智能体系统的工作流进行结构化建模与过程导向评估，并通过分析4000条轨迹揭示了不同大语言模型和提示策略下智能体编程行为的差异与低效问题。


<details>
  <summary>Details</summary>
Motivation: 传统对智能体系统的评估仅关注最终成败，忽略了其推理、规划与策略演变等过程细节；作者受传统软件系统图表示启发，旨在建立一种能刻画智能体系统时序与语义关系的结构化表示方法。

Method: 提出Graphectory框架，将智能体系统的执行轨迹编码为包含时间与语义关系的图结构，从而支持不依赖最终结果的过程性指标设计与分析。

Result: 在SWE-agent和OpenHands两个主流智能体编程工作流上，结合四种大语言模型对4000条轨迹的自动分析表明：更强的模型或更丰富的提示产生更复杂的Graphectory；成功案例呈现有序策略，失败案例则混乱重复；即使成功，过程也常低效冗长。

Conclusion: Graphectory为智能体系统提供了细粒度、过程导向的评估视角，有助于理解其内部工作机制并识别优化空间，超越了传统仅以结果为导向的评价方式。

Abstract: Agentic systems are modern software systems: they consist of orchestrated modules, expose interfaces, and are deployed in software pipelines. Unlike conventional programs, their execution (i.e., trajectories) is inherently stochastic and adaptive to the problem they are solving. Evaluation of such systems is often outcome-centric, judging their performance based on success or failure at the final step. This narrow focus overlooks detailed insights about such systems, failing to explain how agents reason, plan, act, or change their strategies over time. Inspired by the structured representation of conventional software systems as graphs, we introduce Graphectory to systematically encode the temporal and semantic relations in such software systems. Graphectory facilitates the design of process-centric metrics and analyses to assess the quality of agentic workflows independent of final success.
  Using Graphectory, we analyze 4000 trajectories of two dominant agentic programming workflows, namely SWE-agent and OpenHands, with a combination of four backbone Large Language Models (LLMs), attempting to resolve SWE-bench Verified issues. Our fully automated analyses reveal that: (1) agents using richer prompts or stronger LLMs exhibit more complex Graphectory, reflecting deeper exploration, broader context gathering, and more thorough validation before patch submission; (2) agents' problem-solving strategies vary with both problem difficulty and the underlying LLM -- for resolved issues, the strategies often follow coherent localization-patching-validation steps, while unresolved ones exhibit chaotic, repetitive, or backtracking behaviors; (3) even when successful, agentic programming systems often display inefficient processes, leading to unnecessarily prolonged trajectories.

</details>


### [12] [Feedback Loops and Code Perturbations in LLM-based Software Engineering: A Case Study on a C-to-Rust Translation System](https://arxiv.org/abs/2512.02567)
*Martin Weiss,Jesko Hecking-Harbusch,Jochen Quante,Matthias Woehrle*

Main category: cs.SE

TL;DR: 本文研究了反馈循环、大语言模型选择和行为保持的代码变换对C到Rust自动翻译系统效果的影响，发现反馈循环能显著缩小不同模型间的性能差距，并提升系统鲁棒性甚至整体表现。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在软件工程任务中的广泛应用，自动化方法需更高可靠性才能用于工业实践。作者聚焦影响结果质量的三个关键因素：自动化反馈循环、大语言模型选择以及行为保持的代码变化，以C到Rust翻译为应用场景进行系统评估。

Method: 构建基于“生成-检查”模式的C-to-Rust自动翻译系统，利用大语言模型生成Rust代码，并通过自动检查其可编译性和与原C代码的行为等价性；若检查失败，则通过反馈循环重新提示模型修复输出。通过控制变量法分别评估三种因素对系统成功率的影响。

Result: 无反馈循环时，大语言模型的选择对翻译成功率影响显著；引入反馈循环后，不同模型间性能差异明显缩小，且系统在代码扰动下的鲁棒性增强。此外，由代码扰动引入的多样性甚至能提升系统整体性能。

Conclusion: 反馈循环是提升自动化代码翻译系统可靠性和一致性的关键机制，能够缓解模型选择带来的性能波动，并通过代码扰动进一步优化系统表现，为工业级应用提供更稳健的解决方案。

Abstract: The advent of strong generative AI has a considerable impact on various software engineering tasks such as code repair, test generation, or language translation. While tools like GitHub Copilot are already in widespread use in interactive settings, automated approaches require a higher level of reliability before being usable in industrial practice. In this paper, we focus on three aspects that directly influence the quality of the results: a) the effect of automated feedback loops, b) the choice of Large Language Model (LLM), and c) the influence of behavior-preserving code changes.
  We study the effect of these three variables on an automated C-to-Rust translation system. Code translation from C to Rust is an attractive use case in industry due to Rust's safety guarantees. The translation system is based on a generate-and-check pattern, in which Rust code generated by the LLM is automatically checked for compilability and behavioral equivalence with the original C code. For negative checking results, the LLM is re-prompted in a feedback loop to repair its output. These checks also allow us to evaluate and compare the respective success rates of the translation system when varying the three variables.
  Our results show that without feedback loops LLM selection has a large effect on translation success. However, when the translation system uses feedback loops the differences across models diminish. We observe this for the average performance of the system as well as its robustness under code perturbations. Finally, we also identify that diversity provided by code perturbations can even result in improved system performance.

</details>


### [13] [Integrative Analysis of Risk Management Methodologies in Data Science Projects](https://arxiv.org/abs/2512.02728)
*Sabrina Delmondes da Costa Feitosa*

Main category: cs.SE

TL;DR: 本文通过整合性文献综述，比较分析了数据科学项目中主流的风险管理方法，发现传统标准（如ISO 31000、PMBOK、NIST RMF）对新兴风险覆盖有限，而新兴框架（如DS EthiCo RMF）更强调伦理与社会技术维度；研究提出构建融合技术效率、组织协同与负责任数据实践的混合框架，并指明未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 数据科学项目常因技术限制、组织障碍和风险管理不足而失败，现有方法在应对伦理与社会技术风险方面存在明显缺口，亟需系统梳理与整合现有风险管理方法。

Method: 采用整合性文献综述方法，基于结构化筛选与内容分析协议，从索引数据库中选取并分析主流风险管理标准（ISO 31000、PMBOK、NIST RMF）及数据科学专用框架（CRISP-DM、DS EthiCo RMF）。

Result: 传统风险管理方法对数据科学中的新兴风险（尤其是伦理与社会技术风险）覆盖不足；而新近提出的DS EthiCo RMF等框架则展现出多维结构优势，能有效整合伦理监督、治理机制与持续监控。

Conclusion: 应发展融合传统与新兴要素的混合型风险管理框架，以兼顾技术效能、组织对齐与负责任的数据实践；同时，本研究识别出若干研究空白，可为后续工作提供理论指引。

Abstract: Data science initiatives frequently exhibit high failure rates, driven by technical constraints, organizational limitations and insufficient risk management practices. Challenges such as low data maturity, lack of governance, misalignment between technical and business teams, and the absence of structured mechanisms to address ethical and sociotechnical risks have been widely identified in the literature. In this context, the purpose of this study is to conduct a comparative analysis of the main risk management methodologies applied to data science projects, aiming to identify, classify, and synthesize their similarities, differences and existing gaps. An integrative literature review was performed using indexed databases and a structured protocol for selection and content analysis. The study examines widely adopted risk management standards ISO 31000, PMBOK Risk Management and NIST RMF, as well as frameworks specific to data science workflows, such as CRISP DM and the recently proposed DS EthiCo RMF, which incorporates ethical and sociotechnical dimensions into the project life cycle. The findings reveal that traditional approaches provide limited coverage of emerging risks, whereas contemporary models propose multidimensional structures capable of integrating ethical oversight, governance and continuous monitoring. As a contribution, this work offers theoretical support for the development of hybrid frameworks that balance technical efficiency, organizational alignment and responsible data practices, while highlighting research gaps that can guide future investigations.

</details>


### [14] ["Can you feel the vibes?": An exploration of novice programmer engagement with vibe coding](https://arxiv.org/abs/2512.02750)
*Kiev Gama,Filipe Calegario,Victoria Jackson,Alexander Nolte,Luiz Augusto Morais,Vinicius Garcia*

Main category: cs.SE

TL;DR: 本文通过在巴西一所公立大学举办的一场为期一天的“氛围编程”（vibe coding）黑客松，研究了新手程序员和混合经验团队如何利用自然语言提示进行软件开发。研究发现，这种方法促进了快速原型设计和跨学科协作，但也暴露出创意过早收敛、代码质量不均以及对核心软件工程实践参与不足等问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和AI辅助编程的兴起，“氛围编程”作为一种通过自然语言提示而非直接编写代码来创建软件的新范式，其教育潜力尚未被充分探索。作者旨在了解该方法在真实学习场景中对初学者和混合背景团队的影响。

Method: 研究者组织了一场包含31名来自计算与非计算专业的本科生参与的9小时黑客松活动，将其分为9个小组。通过现场观察、退出问卷和半结构化访谈，收集了关于创意过程、工具使用、协作动态和学习成果的数据。

Result: 参与者能够快速构建原型并开展跨学科合作，发展出提示工程技能，并在限定时间内完成可运行演示。然而也出现了创意阶段过早收敛、生成代码质量参差不齐需返工、以及较少运用软件工程核心实践等问题。团队普遍采用多AI工具串联的工作流，且人类判断在关键优化环节不可或缺。

Conclusion: “氛围编程”黑客松可作为低风险、高包容性的学习环境，尤其适合激发新手信心；但需配合明确的教学支架，以促进发散思维、批判性评估AI输出，并建立对产出质量的合理预期。

Abstract: Emerging alongside generative AI and the broader trend of AI-assisted coding, the term "vibe coding" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.

</details>


### [15] [Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior](https://arxiv.org/abs/2512.02795)
*Marcus Kessel*

Main category: cs.SE

TL;DR: 本文提出“观测湖仓”（Observation Lakehouse）架构，用于高效存储和查询代码执行行为数据（以Stimulus-Response Cubes形式），实现无需重执行即可进行行为聚类、多版本评估和共识预言，验证了在单机上大规模行为挖掘的可行性。


<details>
  <summary>Details</summary>
Motivation: 当前代码生成大模型主要基于静态代码训练，容易学习到错误或标签不准确的代码；而程序的真实语义行为只能通过动态执行观测获得。已有工作虽能表示行为（如SRM/SRC），但缺乏可扩展的持久化、演化和交互分析能力。

Method: 构建基于Apache Parquet + Iceberg + DuckDB的观测湖仓系统，将每次执行的刺激-响应-上下文三元组以追加方式存入统一观测表，并通过SQL按需物化SRC切片；数据来源于LASSO控制管道和CI流水线（如单元测试）。

Result: 在509个问题的基准上，系统仅用不到51MiB存储约860万条观测记录，并可在笔记本电脑上100毫秒内重建SRM/SRC视图与聚类结果，证明无需分布式集群即可高效支持行为感知任务。

Conclusion: 观测湖仓使运行时行为数据成为一等公民，为行为感知的模型评估与训练提供了可行的基础设施，并已作为开源项目公开发布。

Abstract: Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse

</details>


### [16] [Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits](https://arxiv.org/abs/2512.02898)
*Pedro Orvalho,Marta Kwiatkowska,Mikoláš Janota,Vasco Manquinho*

Main category: cs.SE

TL;DR: CFaults 是一种基于模型诊断（MBD）与 MaxSAT 的新型多故障定位工具，适用于 C 语言程序和布尔电路，在保证诊断一致性的同时仅生成子集最小的故障诊断，实验表明其在 C 程序上比现有方法更快，在电路数据集上仍具竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有基于公式的故障定位（FBFL）方法无法在所有失败测试中保证诊断集合的一致性，且常产生非子集最小的冗余诊断，尤其在多故障场景下表现不佳。

Method: CFaults 将多个失败测试用例统一建模为一个最大可满足性（MaxSAT）公式，结合基于模型的诊断（MBD）方法，从而确保跨观测的一致性并简化故障定位过程。

Result: 在 TCAS、C-Pack-IPAs 和 ISCAS85 三个基准集上的实验表明：CFaults 在 C 程序上比 BugAssist、SNIPER 和 HSD 更快；在 ISCAS85 电路上虽略慢于 HSD，但仅少定位 6% 的电路；且仅输出子集最小的诊断，避免了冗余。

Conclusion: CFaults 有效解决了多故障场景下诊断一致性与冗余问题，在软件和电路故障定位中均表现出良好性能与实用性。

Abstract: Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.
  This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [17] [Trinity: Disaggregating Vector Search from Prefill-Decode Disaggregation in LLM Serving](https://arxiv.org/abs/2512.02281)
*Yi Liu,Chen Qian*

Main category: cs.DB

TL;DR: Trinity 是一个面向 PD（Prefill-Decoding）解耦架构的实用框架，通过引入专用的共享向量检索 GPU 池、连续批处理机制和阶段感知调度策略，将向量检索任务高效整合进 LLM 服务流程，在满足 SLO 的前提下降低尾延迟。


<details>
  <summary>Details</summary>
Motivation: 在大规模 LLM 服务中，Prefill 和 Decode 阶段已普遍采用解耦部署，但向量检索任务仍与模型推理耦合，导致尾延迟升高。作者希望探索如何在 PD 解耦架构下，以专用部署方式协调向量检索，以应对多样化的检索负载并满足 SLO。

Method: 提出 Trinity 框架：(1) 设计新型 GPU 向量检索服务架构，适配 PD 解耦；(2) 引入面向异构查询的向量检索连续批处理机制，提升 GPU 利用率；(3) 实现阶段感知调度，在 Prefill 与 Decode 任务间抢占式调度向量检索请求。

Result: Trinity 成功将所有检索任务整合到统一的共享向量检索 GPU 池中，并与 PD 解耦的 LLM 服务协同工作，在多种检索负载下有效控制尾延迟，满足服务等级目标（SLO）。

Conclusion: Trinity 为在 PD 解耦架构中高效集成向量检索提供了一种实用且可扩展的解决方案，显著优化了 LLM 服务系统的整体性能与资源利用率。

Abstract: Prefill and decode (PD) disaggregation separates prompt prefill and token-by-token decode stages into distinct GPU pools and has become the dominant architecture for large-scale LLM serving in industry. Also, retrieval tasks via vector search remains entangled with the model inference process, like heterogeneous RAG requests and prompt answer caches, inflating tail latency. We are motivated to investigate how vector search should be orchestrated along with PD disaggregation with a dedicated deployment architecture without violating SLOs in various retrieval workloads. We present Trinity, a practical framework that consolidates all retrieval into a single, shared vector-search GPU pool and make it work with PD disaggregated LLM serving in match. Trinity introduces (1) a novel architecture for deploying GPU-based vector search service in PD disaggregation. (2) Continuous batching for vector search that make full used of GPUs under heterogeneous queries; (3) Stage-aware scheduling that preempts vector search requests between both decode and prefill tasks.

</details>


### [18] [Multi-Objective Agentic Rewrites for Unstructured Data Processing](https://arxiv.org/abs/2512.02289)
*Lindsey Linxi Wei,Shreya Shankar,Sepanta Zeighami,Yeounoh Chung,Fatma Ozcan,Aditya G. Parameswaran*

Main category: cs.DB

TL;DR: MOAR 是一个用于 DocETL 的新型多目标优化器，通过引入新重写指令和全局搜索算法，在提升准确性的同时显著降低成本。


<details>
  <summary>Details</summary>
Motivation: DocETL 虽能通过语义操作符和重写指令提升大语言模型（LLM）处理数据的准确性，但仅优化准确性而忽略成本；此外，局部优化难以应对操作符间不可预测的交互，导致整体效果不佳。

Method: 提出 MOAR 优化器，新增两类重写指令并扩展原有类别，使指令总数超30种；设计基于多臂老虎机框架的全局搜索算法，在整个管道上下文中探索重写策略，以兼顾准确性和成本。

Result: 在六个工作负载上，MOAR 比次优优化器 ABACUS 准确率高27%，并在达到 ABACUS 最佳准确率的同时仅用其55%的成本。

Conclusion: MOAR 成功实现了对 LLM 数据处理管道的多目标优化，兼顾准确性与成本，为声明式、LLM 驱动的数据处理系统提供了更高效的优化方案。

Abstract: One year ago, we open-sourced DocETL, a declarative system for LLM-powered data processing that, as of November 2025, has 3.2K GitHub stars and users across domains (e.g., journalism, law, medicine, policy, finance, and urban planning). In DocETL, users build pipelines by composing operators described in natural language, also known as semantic operators, with an LLM executing each operator's logic. However, due to complexity in the operator or the data it operates on, LLMs often give inaccurate results. To address this challenge, DocETL introduced rewrite directives, or abstract rules that guide LLM agents in rewriting pipelines by decomposing operators or data. For example, decomposing a single filter("is this email sent from an executive and discussing fraud?") into the conjunction of two separate semantic filters may improve accuracy. However, DocETL only optimizes for accuracy, not cost. How do we optimize for both?
  We present MOAR (Multi-Objective Agentic Rewrites), a new optimizer for DocETL. To target cost optimization, we introduce two new categories of directives and extend all three existing categories with new ones, bringing the total to over 30 directives -- more than doubling what DocETL originally had. Moreover, since operators can interact with each other unpredictably due to LLM behavior, optimizing operators or sub-pipelines individually can yield suboptimal overall plans. Recognizing this, we design a new global search algorithm that explores rewrites in the context of entire pipelines. Since the space of rewrites is infinite -- pipelines can be rewritten in many ways, and each rewritten pipeline can itself be rewritten -- our algorithm adapts a multi-armed bandit framework to prioritize which pipelines to rewrite. Across six workloads, MOAR achieves 27% higher accuracy than ABACUS, the next-best optimizer, while matching its best accuracy at 55% of its cost.

</details>


### [19] [QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning](https://arxiv.org/abs/2512.02444)
*Ning Wang,Sainyam Galhotra*

Main category: cs.DB

TL;DR: QJoin 是一个基于强化学习的框架，通过学习和复用转换策略来提升在异构数据源中发现连接（join）的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统等值连接发现方法在开放或联邦环境中表现不佳，因为这些环境中的标识符格式不一致、嵌入或分散在多个列中；近似连接虽能处理轻微字符串差异，但无法捕捉系统性转换。

Method: QJoin 使用强化学习训练智能体，在考虑唯一性的奖励机制下探索简洁高效的转换链，并引入两种复用机制：智能体迁移（从预训练模型初始化新策略）和转换复用（缓存成功操作序列用于相似列簇）。

Result: 在 AutoJoin Web 基准测试（31 对表）上，QJoin 平均 F1 分数达 91.0%；在 NYC+Chicago 开放数据集的 19,990 个连接任务中，通过复用机制最多减少 7.4%（13,747 秒）运行时间。

Conclusion: 转换学习与复用机制能显著提升连接发现的准确性与效率，适用于复杂、异构的数据集成场景。

Abstract: Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient.

</details>


### [20] [From Administrative Chaos to Analytical Cohorts: A Three-Stage Normalisation Pipeline for Longitudinal University Administrative Records](https://arxiv.org/abs/2512.02936)
*H. R. Paz*

Main category: cs.DB

TL;DR: 本文提出了一种针对拉美某公立大学工程专业学生四十年行政记录的三阶段数据标准化流程，有效处理了人口统计、身份识别及中学与地理信息的不一致性，并揭示了43.4%缺失数据源于结构性历史原因而非随机缺失，从而为学习分析和政策评估提供了透明、可复现的数据处理框架。


<details>
  <summary>Details</summary>
Motivation: 在利用高校纵向行政数据进行决策时，原始数据的标准化过程常被忽视。作者旨在解决数据不一致问题，尤其是身份重复、地理信息缺失和中学类型混乱等挑战，并应对因历史招生实践导致的结构性缺失数据问题。

Method: 构建三阶段标准化流程：(i) N1 CENSAL：统一人口统计信息；(ii) N1b 身份解析：合并重复标识符并保留审计轨迹；(iii) N1c 地理与中学标准化：建立参考表、分类中学类型，并标记不可恢复记录为DATA_MISSING。采用卡方检验和逻辑回归进行缺失机制的法证分析。

Result: 该流程保留全部24,133名学生记录，实现100%地理编码，56.6%的学生获得有效中学类型；其余43.4%被确认为结构性缺失，其缺失模式可通过入学年代和地理位置高度预测。

Conclusion: 文章提供了一个透明、可复现的高等教育数据标准化流程，提出了处理结构性缺失数据的框架，并指导如何定义分析上一致的群体（全样本 vs. 中学信息子样本），以支持后续的学习分析与政策评估。

Abstract: The growing use of longitudinal university administrative records in data-driven decision-making often overlooks a critical layer: how raw, inconsistent data are normalised before modelling. This article presents a three-stage normalisation pipeline for a dataset of 24,133 engineering students at a Latin American public university, spanning four decades (1980-2019). The pipeline comprises: (i) N1 CENSAL, harmonising demographics into a single person-level layer; (ii) N1b IDENTITY RESOLUTION, consolidating duplicate identifiers into a canonical ID while preserving an audit trail; and (iii) N1c GEO and SECONDARY-SCHOOL NORMALISATION, which builds reference tables, classifies school types (state national, state provincial, private secular, private religious), and flags irrecoverable cases as DATA_MISSING. The pipeline preserves 100% of students, achieves full geocoding, and yields valid school types for 56.6% of the population. The remaining 43.4% are identified as structurally missing due to legacy enrolment practices rather than stochastic non-response. Forensic analysis (chi-square, logistic regression) shows missingness is highly predictable from entry decade and geography, confirming a structural, historically induced mechanism. The article contributes: (a) a transparent, reproducible normalisation pipeline tailored to higher education; (b) a framework for treating structurally missing information without speculative imputation; and (c) guidance on defining analytically coherent cohorts (full population vs. secondary-school-informed subcohorts) for downstream learning analytics and policy evaluation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [21] [Microbenchmarking NVIDIA's Blackwell Architecture: An in-depth Architectural Analysis](https://arxiv.org/abs/2512.02189)
*Aaron Jarmusch,Sunita Chandrasekaran*

Main category: cs.AR

TL;DR: 本文提出了一套开源微基准测试套件，系统评估了NVIDIA Blackwell（B200）GPU相较于H200在张量核心、内存子系统和多种浮点精度下的性能提升，发现B200在混合精度吞吐量上提升1.56倍、能效提高42%，并显著降低缓存未命中时的内存访问延迟。


<details>
  <summary>Details</summary>
Motivation: 随着GPU架构快速演进以满足E级计算和机器学习需求，现有方法难以系统量化新架构特性对多样化工作负载的性能影响，亟需实用工具帮助开发者优化应用并指导未来硬件设计。

Method: 开发并开源一套微基准测试套件，对Blackwell（B200）与H200 GPU在内存子系统、张量核心流水线及多种浮点精度（FP32/FP16/FP8/FP6/FP4）下进行系统性对比评估，涵盖稠密/稀疏GEMM、Transformer推理与训练等工作负载。

Result: B200相比H200实现1.56倍的混合精度吞吐量提升和42%的能效增益；内存分析显示其在缓存未命中场景下内存访问延迟降低58%，显著影响算法设计策略。

Conclusion: 所提出的微基准套件为开发者提供了深入理解Blackwell架构优势的实用工具，有助于优化工作负载并为未来GPU架构设计提供数据支撑。

Abstract: As GPU architectures rapidly evolve to meet the overcoming demands of exascale computing and machine learning, the performance implications of architectural innovations remain poorly understood across diverse workloads. NVIDIA's Blackwell (B200) generation introduce significant architectural advances including the 5th generation tensor cores, tensor memory (TMEM), decompression engine (DE), and dual chips; however systematic methodologies for quantifying these improvements lag behind hardware development cycles. We contribute an open-source microbenchmark suite that offers practical insights into optimizing workloads to fully utilize the rich feature sets of the modern GPU architecture. This work aims to enable application developers make informed architectural decisions and guide future GPU design directions.
  Our work studies Blackwell GPUs, compares them to H200 generation with regards to the memory subsystem, tensor core pipeline and floating-point precisions (FP32, FP16, FP8, FP6, FP4). Our systematic evaluation of dense/sparse GEMM, transformer inference, and training workloads demonstrate that B200's tensor core enhancements achieves 1.56x higher mixed-precision throughput and 42% better energy efficiency than H200. Our memory analysis reveals 58% reduction in memory access latency in cache-misses, fundamentally changing optimal algorithm design strategies.

</details>


### [22] [Near-Memory Architecture for Threshold-Ordinal Surface-Based Corner Detection of Event Cameras](https://arxiv.org/abs/2512.02346)
*Hongyang Shang,An Guo,Shuai Dong,Junyi Yang,Ye Ke,Arindam Basu*

Main category: cs.AR

TL;DR: 本文提出了一种面向事件相机角点检测的近存计算架构NM-TOS，通过读写解耦SRAM、流水线更新、软硬件协同优化及DVFS技术，在65nm工艺下显著降低延迟与能耗，同时保持高检测精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于事件相机的角点检测算法（如TOS）在资源受限的边缘设备上存在高延迟问题，削弱了事件相机高速低功耗的优势，亟需高效硬件实现方案。

Method: 提出NM-TOS近存架构，采用读写解耦8T SRAM单元、流水线加速块更新，并结合软硬件协同优化的外围电路与动态电压频率调节（DVFS）策略。

Result: 在65nm CMOS工艺下，相比传统数字实现，NM-TOS在1.2V时延迟/能耗降低24.7倍/1.2倍，在0.6V时分别降低1.93倍/6.6倍；蒙特卡洛仿真显示在≥0.62V时无误码，角点检测AUC仅轻微下降0.015–0.027。

Conclusion: 所提NM-TOS架构有效解决了事件相机角点检测在边缘设备上的能效与延迟瓶颈，在保证鲁棒性和精度的同时显著提升硬件效率。

Abstract: Event-based Cameras (EBCs) are widely utilized in surveillance and autonomous driving applications due to their high speed and low power consumption. Corners are essential low-level features in event-driven computer vision, and novel algorithms utilizing event-based representations, such as Threshold-Ordinal Surface (TOS), have been developed for corner detection. However, the implementation of these algorithms on resource-constrained edge devices is hindered by significant latency, undermining the advantages of EBCs. To address this challenge, a near-memory architecture for efficient TOS updates (NM-TOS) is proposed. This architecture employs a read-write decoupled 8T SRAM cell and optimizes patch update speed through pipelining. Hardware-software co-optimized peripheral circuits and dynamic voltage and frequency scaling (DVFS) enable power and latency reductions. Compared to traditional digital implementations, our architecture reduces latency/energy by 24.7x/1.2x at Vdd = 1.2 V or 1.93x/6.6x at Vdd = 0.6 V based on 65nm CMOS process. Monte Carlo simulations confirm robust circuit operation, demonstrating zero bit error rate at operating voltages above 0.62 V, with only 0.2% at 0.61 V and 2.5% at 0.6 V. Corner detection evaluation using precision-recall area under curve (AUC) metrics reveals minor AUC reductions of 0.027 and 0.015 at 0.6 V for two popular EBC datasets.

</details>


### [23] [SAT-MapIt: A SAT-based Modulo Scheduling Mapper for Coarse Grain Reconfigurable Architectures](https://arxiv.org/abs/2512.02875)
*Cristian Tirelli,Lorenzo Ferretti,Laura Pozzi*

Main category: cs.AR

TL;DR: 本文提出一种基于SAT求解器的新方法SAT-MapIt，用于将计算密集型循环映射到粗粒度可重构阵列（CGRAs）上，相比现有技术在近一半的测试用例中取得了更优结果。


<details>
  <summary>Details</summary>
Motivation: 现有CGRAs编译技术依赖模调度和图算法（如最大团枚举），在映射质量上存在局限；作者希望通过SAT形式化方法更有效地探索映射解空间。

Method: 提出一种称为“核移动调度”（KMS）的新型调度方案，结合数据流图与CGRAs架构信息，构建布尔约束集，并利用SAT求解器在给定迭代间隔（II）下搜索有效映射；若无解则递增II并重复该过程。

Result: 在47.72%的基准测试中，SAT-MapIt优于现有技术：有时获得更低的II，有时甚至在先前方法无法找到有效映射的情况下成功生成映射。

Conclusion: 基于SAT的映射方法能更高效地探索CGRAs映射解空间，在多数情况下优于当前最先进的编译技术，展示了其在提升CGRAs加速效果方面的潜力。

Abstract: Coarse-Grain Reconfigurable Arrays (CGRAs) are emerging low-power architectures aimed at accelerating compute-intensive application loops. The acceleration that a CGRA can ultimately provide, however, heavily depends on the quality of the mapping, i.e. on how effectively the loop is compiled onto the given platform. State of the Art compilation techniques achieve mapping through modulo scheduling, a strategy which attempts to minimize the II (Iteration Interval) needed to execute a loop, and they do so usually through well known graph algorithms, such as Max-Clique Enumeration.
  We address the mapping problem through a SAT formulation, instead, and thus explore the solution space more effectively than current SoA tools. To formulate the SAT problem, we introduce an ad-hoc schedule called the \textit{kernel mobility schedule} (KMS), which we use in conjunction with the data-flow graph and the architectural information of the CGRA in order to create a set of boolean statements that describe all constraints to be obeyed by the mapping for a given II. We then let the SAT solver efficiently navigate this complex space. As in other SoA techniques, the process is iterative: if a valid mapping does not exist for the given II, the II is increased and a new KMS and set of constraints is generated and solved.
  Our experimental results show that SAT-MapIt obtains better results compared to SoA alternatives in $47.72\%$ of the benchmarks explored: sometimes finding a lower II, and others even finding a valid mapping when none could previously be found.

</details>


### [24] [Mapping code on Coarse Grained Reconfigurable Arrays using a SAT solver](https://arxiv.org/abs/2512.02884)
*Cristian Tirelli,Laura Pozzi*

Main category: cs.AR

TL;DR: 本文提出一种基于SAT的新型编译方法——Kernel Mobility Schedule，用于在粗粒度可重构阵列（CGRA）上为给定数据流图和迭代间隔（II）寻找最优映射，从而降低II、提升映射质量并缩短编译时间。


<details>
  <summary>Details</summary>
Motivation: 现有CGRA编译技术虽采用模调度以最小化迭代间隔（II），但在映射质量和编译效率方面仍有提升空间；作者旨在通过更优的调度与约束建模，实现对任意拓扑结构下最低II的高效求解。

Method: 将CGRA映射问题建模为可满足性（SAT）问题，提出名为Kernel Mobility Schedule的新调度方案，用于编码给定数据流图（DFG）和指定II下的所有可能映射，并结合CGRA架构信息生成完整约束集以求解有效映射。

Result: 实验表明，该方法相比现有最先进（SoA）技术，不仅平均编译时间更短，而且能获得更高质量的映射结果。

Conclusion: 所提出的基于SAT和Kernel Mobility Schedule的编译方法有效提升了CGRA上循环加速任务的映射质量与编译效率，为低功耗可重构架构的高效编译提供了新思路。

Abstract: Emerging low-powered architectures like Coarse-Grain Reconfigurable Arrays (CGRAs) are becoming more common. Often included as co-processors, they are used to accelerate compute-intensive workloads like loops. The speedup obtained is defined by the hardware design of the accelerator and by the quality of the compilation. State of the art (SoA) compilation techniques leverage modulo scheduling to minimize the Iteration Interval (II), exploit the architecture parallelism and, consequentially, reduce the execution time of the accelerated workload. In our work, we focus on improving the compilation process by finding the lowest II for any given topology, through a satisfiability (SAT) formulation of the mapping problem. We introduce a novel schedule, called Kernel Mobility Schedule, to encode all the possible mappings for a given Data Flow Graph (DFG) and for a given II. The schedule is used together with the CGRA architectural information to generate all the constraints necessary to find a valid mapping. Experimental results demonstrate that our method not only reduces compilation time on average but also achieves higher quality mappings compared to existing SoA techniques.

</details>
