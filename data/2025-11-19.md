<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 12]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.CE](#cs.CE) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Show and Tell: Prompt Strategies for Style Control in Multi-Turn LLM Code Generation](https://arxiv.org/abs/2511.13972)
*Jeremiah Bohr*

Main category: cs.SE

TL;DR: 该研究探讨了不同提示策略（指令型、示例型及组合型）对语言模型生成代码风格的控制效果，特别是在模型对初始代码进行功能增强时能否维持简洁性。结果表明，组合提示在初始压缩和扩展约束方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 语言模型生成的代码虽然功能正确，但往往过于冗长，偏离人类编写风格。如何在模型增强代码功能的同时保持预设的代码风格（如简洁性），是当前提示工程面临的核心问题。

Method: 研究采用配对两轮协议：首先让模型在四种系统提示条件（指令型、示例型、组合型、对照组）下生成Python任务的初始解决方案，然后在任务不变的情况下，根据通用改进指令对代码进行一轮增强（共160对程序）。通过比较不同提示策略在初始生成和增强阶段的代码长度变化，评估其风格控制能力。

Result: 组合提示在初始阶段实现最强的代码压缩，并在增强阶段展现出最佳的扩展约束；指令型提示初始效果显著但扩展约束中等；示例型提示初始效果微弱且无扩展约束。

Conclusion: 初始提示效果与扩展约束是提示设计中两个独立的维度，组合提示策略在两轮工作流中提供了最稳定的代码风格控制。

Abstract: Language models generate functionally correct code that tends toward excessive verbosity, with elaborate documentation and defensive patterns that diverge from human baselines. Two prompting mechanisms have emerged for stylistic control: instruction based prompts that articulate abstract directives, and example based prompts that provide concrete code demonstrations. The core problem is whether stylistic constraints persist when models enhance initial implementations with additional features while maintaining high functional accuracy. Here we show that instruction-based, example-based, and combined prompts produce distinct patterns of initial control and expansion discipline over one enhancement turn. We manipulated system prompts across four conditions in a paired two-turn protocol where models first generated solutions to an intermediate Python task, then revised their code under general improvement directives, holding the user task fixed (N = 160 paired programs). Combined prompts produced the strongest initial compression and greatest expansion discipline. Instructions showed large initial effects and moderate expansion discipline. Examples showed modest initial effects with no expansion discipline. These results show that initial prompt effectiveness and expansion discipline are separate aspects of prompt design, and that combined approaches provide the most stable stylistic control in this two-turn workflow.

</details>


### [2] [Exploring the Use of ChatGPT by Computer Science Students in Software Development: Applications, Ethical Considerations, and Insights for Engineering Education](https://arxiv.org/abs/2511.13996)
*Daihan Xu,Diana Martin*

Main category: cs.SE

TL;DR: 本研究通过质性访谈探讨英国某高校计算机专业学生在软件开发项目中如何策略性和伦理地使用ChatGPT，发现学生将其作为辅助工具以加深理解，但保留高阶决策，并普遍限制其贡献约30%；同时，学生关注隐私、技能退化等风险，呼吁教师制定明确使用规范。


<details>
  <summary>Details</summary>
Motivation: 尽管ChatGPT在计算机教育中的应用日益广泛，现有研究多依赖问卷调查，缺乏对学生使用策略与伦理意识的深入分析。本文旨在填补这一空白，探究学生如何在学术与职业背景下策略性且合乎伦理地使用ChatGPT。

Method: 采用半结构化访谈对一所英国高校的计算机科学学生进行质性研究，聚焦其在软件开发项目中使用ChatGPT的策略与伦理认知。

Result: 学生将学习模式从“独立思考—手动编码—迭代调试”转变为“AI辅助构思—交互式编程—协作优化”；多数人将ChatGPT用于对话式学习，保留创造性任务，限制其贡献约30%，并评估输出以防过度依赖；但仅少数深入分析AI生成代码；学生普遍反对未署名使用，关注隐私泄露与技能退化，并希望教师提供清晰指引。

Conclusion: 研究揭示了学生与AI工具之间不断演变的互动关系，强调需通过明确的教学指导促进ChatGPT在教育中的负责任和有效使用。

Abstract: ChatGPT has been increasingly used in computer science, offering efficient support across software development tasks. While it helps students navigate programming challenges, its use also raises concerns about academic integrity and overreliance. Despite growing interest in this topic, prior research has largely relied on surveys, emphasizing trends over in-depth analysis of students' strategies and ethical awareness. This study complements existing work through a qualitative investigation of how computer science students in one UK institution strategically and ethically engage with ChatGPT in software development projects. Drawing on semi-structured interviews, it explores two key questions: How do computer science students ethically and strategically report using ChatGPT in software development projects? How do students understand and perceive the ethical issues associated with using ChatGPT in academic and professional contexts? Findings reveal a shift in students' learning models, moving from traditional "independent thinking-manual coding-iterative debugging" to "AI-assisted ideation-interactive programming-collaborative optimization." Importantly, many use ChatGPT conversationally to deepen understanding, while consciously reserving creative and high-level decision-making tasks for themselves. Students tend to cap ChatGPT's contribution to roughly 30%, and evaluate its output to mitigate overreliance. However, only a minority thoroughly analyze AI-generated code, raising concerns about reduced critical engagement. Meanwhile, students reject uncredited use, highlight risks such as privacy breaches and skill degradation, and call for clear usage guidelines set by their teachers. This research offers novel insights into the evolving learner-AI dynamic and highlights the need for explicit guidance to support responsible and pedagogically sound use of such tools.

</details>


### [3] [LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering](https://arxiv.org/abs/2511.13998)
*Jielin Qiu,Zuxin Liu,Zhiwei Liu,Rithesh Murthy,Jianguo Zhang,Haolin Chen,Shiyu Wang,Ming Zhu,Liangwei Yang,Juntao Tan,Roshan Ram,Akshara Prabhakar,Tulika Awalgaonkar,Zixiang Chen,Zhepeng Cen,Cheng Qian,Shelby Heinecke,Weiran Yao,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.SE

TL;DR: 本文提出了 LoCoBench-Agent，一个面向大语言模型（LLM）智能体在长上下文软件工程任务中的综合性评估框架，通过多轮交互、工具使用和效率-理解权衡等维度，系统评测智能体在真实开发场景中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准（如 LoCoBench）仅支持单轮评估，无法捕捉真实编码智能体所需的多轮交互、工具使用和自适应推理能力，因此需要一个更贴近实际开发流程的评估框架。

Method: 将 LoCoBench 的 8,000 个场景扩展为支持智能体交互的环境，提供 8 种专用工具（如文件操作、搜索、代码分析），并在 10K 至 1M token 的上下文长度范围内，通过 9 项涵盖理解与效率的指标对 LLM 智能体进行系统评估。

Result: 实验发现：(1) 智能体在长上下文中表现出强鲁棒性；(2) 理解深度与执行效率存在负相关权衡；(3) 不同模型在对话效率和工具使用策略上差异显著。

Conclusion: LoCoBench-Agent 是首个面向软件工程的长上下文 LLM 智能体基准，为衡量智能体能力、识别性能差距和推动大规模自主软件开发提供了坚实基础。

Abstract: As large language models (LLMs) evolve into sophisticated autonomous agents capable of complex software development tasks, evaluating their real-world capabilities becomes critical. While existing benchmarks like LoCoBench~\cite{qiu2025locobench} assess long-context code understanding, they focus on single-turn evaluation and cannot capture the multi-turn interactive nature, tool usage patterns, and adaptive reasoning required by real-world coding agents. We introduce \textbf{LoCoBench-Agent}, a comprehensive evaluation framework specifically designed to assess LLM agents in realistic, long-context software engineering workflows. Our framework extends LoCoBench's 8,000 scenarios into interactive agent environments, enabling systematic evaluation of multi-turn conversations, tool usage efficiency, error recovery, and architectural consistency across extended development sessions. We also introduce an evaluation methodology with 9 metrics across comprehension and efficiency dimensions. Our framework provides agents with 8 specialized tools (file operations, search, code analysis) and evaluates them across context lengths ranging from 10K to 1M tokens, enabling precise assessment of long-context performance. Through systematic evaluation of state-of-the-art models, we reveal several key findings: (1) agents exhibit remarkable long-context robustness; (2) comprehension-efficiency trade-off exists with negative correlation, where thorough exploration increases comprehension but reduces efficiency; and (3) conversation efficiency varies dramatically across models, with strategic tool usage patterns differentiating high-performing agents. As the first long-context LLM agent benchmark for software engineering, LoCoBench-Agent establishes a rigorous foundation for measuring agent capabilities, identifying performance gaps, and advancing autonomous software development at scale.

</details>


### [4] [FlakyGuard: Automatically Fixing Flaky Tests at Industry Scale](https://arxiv.org/abs/2511.14002)
*Chengpeng Li,Farnaz Behrang,August Shi,Peng Liu*

Main category: cs.SE

TL;DR: FlakyGuard 是一种基于图结构选择性探索相关上下文的新方法，用于自动修复非确定性失败的“脆弱测试”（flaky tests），在工业环境中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的脆弱测试修复方法（如 FlakyDoctor）在工业场景中效果不佳，主要受限于“上下文问题”：要么提供的上下文太少（缺少关键生产代码），要么太多（包含大量无关信息），影响修复效果。

Method: FlakyGuard 将代码建模为图结构，并通过选择性图探索仅提取与修复最相关的上下文信息，从而有效缓解上下文问题。

Result: 在真实工业仓库的脆弱测试数据集上，FlakyGuard 成功修复了 47.6% 的可复现脆弱测试，其中 51.8% 的修复被开发者接受；其修复成功率比当前最优方法至少高出 22%；开发者调查表明 100% 认为其根因解释有用。

Conclusion: FlakyGuard 通过图结构和选择性上下文提取，有效提升了 LLM 在工业级脆弱测试修复任务中的性能和实用性，具有较高的开发者接受度和解释可信度。

Abstract: Flaky tests that non-deterministically pass or fail waste developer time and slow release cycles. While large language models (LLMs) show promise for automatically repairing flaky tests, existing approaches like FlakyDoctor fail in industrial settings due to the context problem: providing either too little context (missing critical production code) or too much context (overwhelming the LLM with irrelevant information). We present FlakyGuard, which addresses this problem by treating code as a graph structure and using selective graph exploration to find only the most relevant context. Evaluation on real-world flaky tests from industrial repositories shows that FlakyGuard repairs 47.6 % of reproducible flaky tests with 51.8 % of the fixes accepted by developers. Besides it outperforms state-of-the-art approaches by at least 22 % in repair success rate. Developer surveys confirm that 100 % find FlakyGuard's root cause explanations useful.

</details>


### [5] [Keeping Code-Aware LLMs Fresh: Full Refresh, In-Context Deltas, and Incremental Fine-Tuning](https://arxiv.org/abs/2511.14022)
*Pradeep Kumar Sharma,Ishaan Puri,Mantinder Jit Singh,Swapnil Shivaprasad,Hritvik Shrivastava*

Main category: cs.SE

TL;DR: 本文研究如何在代码库持续演进的背景下，保持自然语言到代码文件路径映射模型的时效性，同时避免遗忘旧知识。作者比较了三种策略：完全重训练（Full Refresh）、上下文学习（ICL）和增量微调（Inc-FT），并在多个主流代码库上评估其效果。


<details>
  <summary>Details</summary>
Motivation: 现代代码库不断演变（如文件重命名、删除、API漂移等），导致昨天训练好的模型在今天性能下降，即使用户查询未变。因此需要一种方法在不丢失对旧代码记忆的前提下，使模型保持对新代码的适应能力。

Method: 将模型时效性问题视为基础快照与当前HEAD之间的领域漂移，对比三类更新策略：(A) 完全重训练；(B) 推理时通过上下文学习注入近期变更（原始git diff或英文摘要）；(C) 基于变更数据集进行增量微调，并控制新旧数据混合比例以缓解灾难性遗忘。提出别名感知评估协议和遗忘探针用于评测。

Result: 在Flask、SQLAlchemy、Pandas和Poetry等代码库上的实验表明：Inc-FT结合新旧数据混合在整体表现上最优；当无法训练时，使用英文摘要的ICL能最快提升对新代码的响应能力；若追求最高新代码准确率，Full Refresh仍是上限。此外，基于git diff的Inc-FT在重命名/删除频繁的窗口中表现更好，而基于完整文件的Inc-FT在行为变更为主的窗口中更优。

Conclusion: 保持代码理解模型时效性需权衡新旧知识保留。增量微调配合新旧数据混合是实用且高效的方案，而上下文学习可作为无训练场景下的快速替代。评估应考虑文件重命名并避免奖励已删除路径。

Abstract: Modern codebases evolve continuously: files are renamed or deleted; public APIs drift; behavior shifts within otherwise familiar modules. A model trained yesterday to map a developer's natural-language question to the exact set of repository file paths that matter will degrade tomorrow, even if the questions themselves look unchanged. In this paper we study, at system scale and across several widely used repositories, how to keep such a model fresh without surrendering retention on earlier code. We frame freshness as a form of domain drift between a base snapshot and the current HEAD, and we compare three families of update strategies: (A) Full Refresh, retraining the entire model at the new snapshot; (B) In-Context Learning (ICL) that injects recent deltas (raw git diffs or concise English summaries) at inference; and (C) Incremental Fine-Tuning (Inc-FT) on delta-derived training sets, with carefully controlled NEW:OLD mixing to mitigate catastrophic forgetting. We contribute an alias-aware evaluation protocol that credits rename while never rewarding deleted paths, and a practical Forgetting Probe that quantifies residual emissions of obsolete paths. Across Flask, SQLAlchemy, Pandas, and Poetry, Inc-FT with old-aware mixes delivers the best overall balance on mixed sets, ICL with English delta summaries delivers the fastest new-code lift when training is not feasible, and Full Refresh remains the ceiling when maximum NEW accuracy matters. We also compare Git-diff Inc-FT to full-file Inc-FT, showing that diffs excel in rename/delete-heavy windows while full-file context wins in behavior-change-heavy windows.

</details>


### [6] [LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering](https://arxiv.org/abs/2511.14062)
*Shenglin Zhang,Ziang Chen,Zijing Que,Yilun Liu,Yongqian Sun,Sicheng Wei,Dan Pei,Hailin Li*

Main category: cs.SE

TL;DR: 本文提出了一种名为LogPurge的低成本、规则增强的日志净化框架，通过两阶段过滤算法自动从含异常的日志序列中筛选出正常样本用于训练异常检测模型，在多个数据集上显著优于现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前日志异常检测方法依赖于干净无异常的日志数据进行模型训练，但获取此类数据需昂贵且繁琐的人工标注，而现有的自动清洗方法未能充分结合日志的特性和语义信息。

Method: LogPurge采用两阶段过滤算法：第一阶段利用大语言模型（LLM）去除聚类的异常模式，并增强系统规则以提升LLM对系统日志的理解；第二阶段采用分治策略，将剩余污染区域分解为更小的子问题，并递归应用第一阶段流程进行净化。

Result: 在两个公开数据集和一个工业数据集上的实验表明，该方法平均可去除98.74%的异常，同时保留82.39%的正常样本；相比最新无监督日志样本选择算法，F1分数分别提升了35.7%、84.11%和149.72%。

Conclusion: LogPurge能高效自动地从污染日志中提取高质量正常样本，显著提升日志异常检测性能，具有良好的实用性和泛化能力。

Abstract: Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.

</details>


### [7] [A Practical Implementation of Customized Scrum-Based Agile Framework in Aerospace Software Development Under DO-178C Constraints](https://arxiv.org/abs/2511.14215)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: 本文提出并验证了一种专为DO-178C合规的航空安全关键软件定制的Scrum敏捷框架，通过角色、工件和事件的调整，在满足严格认证要求的同时显著提升了开发效率与质量。


<details>
  <summary>Details</summary>
Motivation: 航空航天系统日益复杂，亟需在保持敏捷性的同时满足严苛的安全性和适航认证要求；传统敏捷方法难以直接适用于DO-178C等安全关键标准，因此需要一种既能合规又能发挥敏捷优势的开发框架。

Method: 研究设计了一个经过实证检验的Scrum变体框架，引入多学科产品负责人模型、双重验收标准（合规性+功能性）、独立测试与文档团队以及专职认证联络人，并在两个可比项目中（一个采用该定制敏捷流程，另一个采用传统瀑布模型）进行对比评估。

Result: 采用定制敏捷框架的项目实现了每需求总工作量减少76%、缺陷发现速度提升75%、缺陷修复速度提升78%、缺陷密度降低超50%，同时完全满足DO-178C DAL A级合规要求。

Conclusion: 研究表明，通过有纪律的流程裁剪和与认证机构的主动协作，敏捷实践与航空安全关键系统的合规要求可以有效共存；未来可通过自动化、CI/CD及文档/验证/配置管理的自动化进一步提升效益。

Abstract: The increasing complexity of aerospace systems requires development processes that balance agility with stringent safety and certification demands. This study presents an empirically validated Scrum-based Agile framework tailored for DO-178C compliant, safety-critical aerospace software. The framework adapts core Scrum roles, artifacts, and events to meet certification, verification, and independence objectives. Key enhancements include a multi-disciplinary product ownership model, dual compliance-and-functionality acceptance criteria, independent testing and documentation teams, and dedicated certification liaisons. The approach was evaluated through two comparable aerospace projects-one using the customized Agile process and the other a traditional Waterfall model. Results showed significant improvements: a 76% reduction in Total Effort per Requirement, 75% faster Defect Detection, 78% faster Defect Resolution, and over 50% lower Defect Density, while maintaining full compliance with DO-178C Design Assurance Level A. These findings demonstrate that Agile practices and regulatory compliance can coexist effectively when supported by disciplined tailoring and proactive engagement with certification authorities. The study also notes challenges, including increased V&V effort due to recurring Sprint activities and refactoring inherent to iterative development. Nonetheless, it identifies substantial opportunities for further gains through workflow automation, CI/CD practices, and automated documentation, verification, and configuration management. Future research should expand validation of this framework across the aerospace domain and other safety-critical industries with similar certification requirements.

</details>


### [8] [KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation](https://arxiv.org/abs/2511.14224)
*Anji Li,Mingwei Liu,Zhenxi Chen,Zheng Pei,Zike Li,Dekun Dai,Yanlin Wang,Zibin Zheng*

Main category: cs.SE

TL;DR: KTester is a knowledge-enhanced framework that improves LLM-based unit test generation by integrating project-specific and testing-domain knowledge, achieving better correctness, coverage, readability, and maintainability than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based unit test generators often produce tests that lack correctness and maintainability in real-world software projects, due to insufficient contextual and domain-specific guidance.

Method: KTester extracts project structure and usage patterns via static analysis, separates test case design from test method generation guided by testing-domain knowledge, and uses multi-perspective prompting with structured templates to steer the LLM.

Result: KTester outperforms state-of-the-art baselines by 5.69% in execution pass rate and 8.83% in line coverage, while generating fewer tests in less time; human evaluators also rate its outputs higher in correctness, readability, and maintainability.

Conclusion: Integrating project-specific and testing-domain knowledge into LLM-based test generation significantly enhances the quality and practicality of automatically generated unit tests.

Abstract: Automated unit test generation using large language models (LLMs) holds great promise but often struggles with generating tests that are both correct and maintainable in real-world projects. This paper presents KTester, a novel framework that integrates project-specific knowledge and testing domain knowledge to enhance LLM-based test generation. Our approach first extracts project structure and usage knowledge through static analysis, which provides rich context for the model. It then employs a testing-domain-knowledge-guided separation of test case design and test method generation, combined with a multi-perspective prompting strategy that guides the LLM to consider diverse testing heuristics. The generated tests follow structured templates, improving clarity and maintainability. We evaluate KTester on multiple open-source projects, comparing it against state-of-the-art LLM-based baselines using automatic correctness and coverage metrics, as well as a human study assessing readability and maintainability. Results demonstrate that KTester significantly outperforms existing methods across six key metrics, improving execution pass rate by 5.69% and line coverage by 8.83% over the strongest baseline, while requiring less time and generating fewer test cases. Human evaluators also rate the tests produced by KTester significantly higher in terms of correctness, readability, and maintainability, confirming the practical advantages of our knowledge-driven framework.

</details>


### [9] [Watchdogs and Oracles: Runtime Verification Meets Large Language Models for Autonomous Systems](https://arxiv.org/abs/2511.14435)
*Angelo Ferrando*

Main category: cs.SE

TL;DR: 本文提出将运行时验证（RV）与大语言模型（LLMs）协同整合，以提升自主系统的可信性：RV为LLM提供安全保障，LLM则增强RV在规范获取、预测推理和不确定性处理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 在包含学习型组件和开放环境的自主系统中，确保其安全性与可信性极具挑战。形式化方法虽能提供强保证，但依赖完整模型和静态假设；而大语言模型虽擅长自然语言到形式化表达的转换及模式识别，却缺乏形式化保障且易出错。

Method: 提出一种RV与LLMs的共生集成框架，其中RV作为LLM驱动自主性的“护栏”，而LLM则辅助RV进行规范捕获、支持前瞻性推理并应对不确定性。

Result: 论文勾勒了该协同机制如何区别于现有综述与路线图，探讨了相关挑战、认证影响，并指明了面向可靠自主系统的未来研究方向。

Conclusion: RV与LLMs的相互增强为构建可信赖的自主系统提供了新路径，但仍需克服技术与认证层面的多重挑战。

Abstract: Assuring the safety and trustworthiness of autonomous systems is particularly difficult when learning-enabled components and open environments are involved. Formal methods provide strong guarantees but depend on complete models and static assumptions. Runtime verification (RV) complements them by monitoring executions at run time and, in its predictive variants, by anticipating potential violations. Large language models (LLMs), meanwhile, excel at translating natural language into formal artefacts and recognising patterns in data, yet they remain error-prone and lack formal guarantees. This vision paper argues for a symbiotic integration of RV and LLMs. RV can serve as a guardrail for LLM-driven autonomy, while LLMs can extend RV by assisting specification capture, supporting anticipatory reasoning, and helping to handle uncertainty. We outline how this mutual reinforcement differs from existing surveys and roadmaps, discuss challenges and certification implications, and identify future research directions towards dependable autonomy.

</details>


### [10] [LLM-Assisted Thematic Analysis: Opportunities, Limitations, and Recommendations](https://arxiv.org/abs/2511.14528)
*Tatiane Ornelas,Allysson Allex Araújo,Júlia Araújo,Marina Araújo,Bianca Trinkenreich,Marcos Kalinowski*

Main category: cs.SE

TL;DR: 本研究通过工作坊探讨软件工程领域资深研究者如何理解大语言模型（LLMs）在主题分析中的机遇与风险，发现LLMs可提升效率但不能替代人类解释，强调需保持人工监督与提示素养。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件工程定性研究中的应用日益增多，其对解释性分析过程（如主题分析）在严谨性、透明度和研究者主体性方面的方法论影响尚不明确，亟需深入探讨。

Method: 组织25位ISERN研究人员参与反思性工作坊，通过结构化讨论聚焦LLM辅助的开放式编码、主题生成与主题审查，并使用彩色画布记录参与者对机会、局限与建议的看法。

Result: 参与者认可LLM带来的效率与可扩展性优势，但也指出其存在偏见、上下文信息丢失、可复现性差及模型快速演进等风险，并强调提示素养和持续人工监督的重要性。

Conclusion: LLM应被视为支持而非替代解释性分析的工具；研究为软件工程社区负责任地将LLM融入定性研究提供了方法论反思与实践指导。

Abstract: [Context] Large Language Models (LLMs) are increasingly used to assist qualitative research in Software Engineering (SE), yet the methodological implications of this usage remain underexplored. Their integration into interpretive processes such as thematic analysis raises fundamental questions about rigor, transparency, and researcher agency. [Objective] This study investigates how experienced SE researchers conceptualize the opportunities, risks, and methodological implications of integrating LLMs into thematic analysis. [Method] A reflective workshop with 25 ISERN researchers guided participants through structured discussions of LLM-assisted open coding, theme generation, and theme reviewing, using color-coded canvases to document perceived opportunities, limitations, and recommendations. [Results] Participants recognized potential efficiency and scalability gains, but highlighted risks related to bias, contextual loss, reproducibility, and the rapid evolution of LLMs. They also emphasized the need for prompting literacy and continuous human oversight. [Conclusion] Findings portray LLMs as tools that can support, but not substitute, interpretive analysis. The study contributes to ongoing community reflections on how LLMs can responsibly enhance qualitative research in SE.

</details>


### [11] [FHIRconnect: Towards a seamless integration of openEHR and FHIR](https://arxiv.org/abs/2511.14618)
*Severin Kohler,Jordi Piera Jiménez,Michael Anywar,Lars Fuhrmann,Heather Leslie,Maximilian Meixner,Julian Saß,Florian Kärcher,Diego Boscá,Birger Haarbrandt,Michael Marschollek,Roland Eils*

Main category: cs.SE

TL;DR: 本文提出了FHIRconnect，一种用于openEHR与HL7 FHIR之间标准化双向数据转换的领域特定语言（DSL）及开源引擎，通过三层架构实现65%的映射复用，并覆盖多个临床领域的国际原型与FHIR配置文件。


<details>
  <summary>Details</summary>
Motivation: 由于openEHR与HL7 FHIR在数据建模方法上的根本差异以及缺乏标准化的转换机制，医疗系统间的互操作性面临挑战。

Method: 设计并实现了一种三层架构的领域特定语言（DSL）和开源转换引擎（openFHIR），结合国际原型基础与本地定制支持，实现openEHR与FHIR之间的双向映射。

Result: 成功将24个国际openEHR原型映射到7个临床领域中的15个FHIR配置文件，映射复用率达65%，并构建了一个开放的高影响力临床原型映射库。

Conclusion: FHIRconnect为社区驱动的openEHR-FHIR映射标准化提供了技术基础，减少了对定制ETL方案的依赖，推动了基于开放标准的医疗IT系统在语法与语义层面的互操作性。

Abstract: Healthcare interoperability between openEHR and HL7 FHIR remains challenging due to fundamental differences in their data modeling approaches and the absence of standardized transformation mechanisms. This paper presents FHIRconnect, a novel domain-specific language and open-source transformation engine that enables standardized, bidirectional data exchange between openEHR and FHIR. Our approach addresses critical interoperability gaps through a triple-layered architecture that achieves 65% mapping reuse across projects by leveraging international archetype-based foundations while supporting local customizations. Using this framework, FHIRconnect successfully mapped 24 international archetypes to 15 FHIR profiles across seven clinical domains. Key contributions include the first comprehensive DSL for openEHR-FHIR transformation with a formal specification, an open-source execution engine (openFHIR), and an accessible mapping library covering high-impact clinical archetypes. Together, these components establish the technical basis for community-driven mapping standardization, reducing reliance on custom ETL solutions and advancing syntactic and semantic interoperability in healthcare IT systems built on open standards.

</details>


### [12] [From Legacy Fortran to Portable Kokkos: An Autonomous Agentic AI Workflow](https://arxiv.org/abs/2509.12443)
*Sparsh Gupta,Kamalavasan Kamalakkannan,Maxim Moraru,Galen Shipman,Patrick Diehl*

Main category: cs.SE

TL;DR: 本文提出一种基于多智能体大语言模型（LLM）的自动化工作流，用于将遗留 Fortran 代码转换为性能可移植的 Kokkos C++ 代码，实现在异构 GPU 架构上的高效运行。实验表明，付费模型（如 GPT-5）能以较低成本生成优于原始 Fortran 的优化代码，而开源模型则常无法产出可用结果。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算向 GPU 异构架构演进，大量依赖传统 Fortran 编写的科学应用面临缺乏原生 GPU 支持的问题，亟需现代化改造以实现跨平台性能可移植性。然而，手动将 Fortran 转换为 Kokkos C++ 耗时且需要专业知识，因此探索利用大语言模型实现全自动转换具有重要意义。

Method: 设计了一个由多个 LLM 智能体协作的 AI 工作流，涵盖翻译、验证、编译、运行、测试、调试和优化等环节，将 Fortran 并行核函数自动转换为 Kokkos C++ 程序。

Result: 该流程成功现代化了多个基准测试核函数，在不同硬件上生成了性能可移植的 Kokkos 代码；使用 GPT-5 等付费模型仅花费几美元即可生成优于 Fortran 原始版本的优化代码，而 Llama4-Maverick 等开源模型通常无法生成可运行代码。

Conclusion: 本研究验证了基于智能体的大语言模型在 Fortran 到 Kokkos 自动转换中的可行性，为遗留科学应用的自主现代化提供了有效路径，并展示了 LLM 在科学与系统领域执行结构化、领域特定推理任务的潜力。

Abstract: Scientific applications continue to rely on legacy Fortran codebases originally developed for homogeneous, CPU-based systems. As High-Performance Computing (HPC) shifts toward heterogeneous GPU-accelerated architectures, many accelerators lack native Fortran bindings, creating an urgent need to modernize legacy codes for portability. Frameworks like Kokkos provide performance portability and a single-source C++ abstraction, but manual Fortran-to-Kokkos porting demands significant expertise and time. Large language models (LLMs) have shown promise in source-to-source code generation, yet their use in fully autonomous workflows for translating and optimizing parallel code remains largely unexplored, especially for performance portability across diverse hardware. This paper presents an agentic AI workflow where specialized LLM "agents" collaborate to translate, validate, compile, run, test, debug, and optimize Fortran kernels into portable Kokkos C++ programs. Results show the pipeline modernizes a range of benchmark kernels, producing performance-portable Kokkos codes across hardware partitions. Paid OpenAI models such as GPT-5 and o4-mini-high executed the workflow for only a few U.S. dollars, generating optimized codes that surpassed Fortran baselines, whereas open-source models like Llama4-Maverick often failed to yield functional codes. This work demonstrates the feasibility of agentic AI for Fortran-to-Kokkos transformation and offers a pathway for autonomously modernizing legacy scientific applications to run portably and efficiently on diverse supercomputers. It further highlights the potential of LLM-driven agentic systems to perform structured, domain-specific reasoning tasks in scientific and systems-oriented applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [NL-DPE: An Analog In-memory Non-Linear Dot Product Engine for Efficient CNN and LLM Inference](https://arxiv.org/abs/2511.13950)
*Lei Zhao,Luca Buonanno,Archit Gajjar,John Moon,Aishwarya Natarajan,Sergey Serebryakov,Ron M. Roth,Xia Sheng,Youtao Zhang,Paolo Faraboschi,Jim Ignowski,Giacomo Pedretti*

Main category: cs.AR

TL;DR: NL-DPE是一种新型的基于RRAM的存内计算引擎，通过引入模拟内容可寻址存储器（ACAM）和噪声感知微调技术，实现了对非线性函数与数据依赖乘法的高效模拟域计算，显著提升了能效与速度，同时避免了传统IMC架构中的ADC开销与器件非理想性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于RRAM的存内计算加速器存在三大局限：仅支持静态点积运算、需要高功耗ADC电路、以及因器件非理想性导致权重映射误差，这些问题限制了其在现代大语言模型中的可扩展性和精度。

Method: NL-DPE通过在交叉阵列中集成RRAM-based ACAM，将任意非线性函数和数据依赖矩阵乘法转化为决策树形式，在模拟域执行；同时采用无需片上校准的软件级噪声感知微调（NAF）方法来缓解器件噪声影响。

Result: 实验表明，NL-DPE相比GPU基线实现28倍能效提升和249倍加速，相比现有IMC加速器实现22倍能效提升和245倍加速，同时保持高精度。

Conclusion: NL-DPE有效克服了传统RRAM IMC在功能灵活性、能效和精度方面的关键瓶颈，为面向大语言模型的高效存内计算提供了可行路径。

Abstract: Resistive Random Access Memory (RRAM) based in-memory computing (IMC) accelerators offer significant performance and energy advantages for deep neural networks (DNNs), but face three major limitations: (1) they support only \textit{static} dot-product operations and cannot accelerate arbitrary non-linear functions or data-dependent multiplications essential to modern LLMs; (2) they demand large, power-hungry analog-to-digital converter (ADC) circuits; and (3) mapping model weights to device conductance introduces errors from cell nonidealities. These challenges hinder scalable and accurate IMC acceleration as models grow.
  We propose NL-DPE, a Non-Linear Dot Product Engine that overcomes these barriers. NL-DPE augments crosspoint arrays with RRAM-based Analog Content Addressable Memory (ACAM) to execute arbitrary non-linear functions and data-dependent matrix multiplications in the analog domain by transforming them into decision trees, fully eliminating ADCs. To address device noise, NL-DPE uses software-based Noise Aware Fine-tuning (NAF), requiring no in-device calibration. Experiments show that NL-DPE delivers 28X energy efficiency and 249X speedup over a GPU baseline, and 22X energy efficiency and 245X speedup over existing IMC accelerators, while maintaining high accuracy.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [TT-Edge: A Hardware-Software Co-Design for Energy-Efficient Tensor-Train Decomposition on Edge AI](https://arxiv.org/abs/2511.13738)
*Hyunseok Kwak,Kyeongwon Lee,Kyeongpil Min,Chaebin Jung,Woojoo Lee*

Main category: cs.DC

TL;DR: 本文提出了TT-Edge，一种面向边缘设备的软硬件协同设计框架，通过将张量列分解（TTD）中的关键计算任务卸载到专用硬件引擎，在保持高模型压缩率和低精度损失的同时，显著提升了TTD在资源受限设备上的能效与速度。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上进行分布式学习时，高效的设备端模型压缩至关重要。尽管张量列分解（TTD）能实现高压缩比且精度损失小，但其依赖的重复奇异值分解（SVD）和矩阵乘法操作在低功耗处理器上带来显著的延迟和能耗开销。

Method: TT-Edge将SVD分为双对角化和对角化两个阶段，并将计算密集型任务卸载至专门设计的TTD引擎；该引擎与现有GEMM加速器紧密集成，减少频繁的矩阵-向量数据传输。整体方案基于RISC-V边缘AI处理器实现，采用轻量级设计复用GEMM资源并共享浮点单元。

Result: 在ResNet-32模型的TTD压缩任务中，TT-Edge相比仅使用GEMM的基线方案实现了1.7倍的速度提升和40.2%的能耗降低，仅增加4%的总功耗和极小的硬件开销。FPGA原型和45nm工艺下的功耗分析验证了其有效性。

Conclusion: TT-Edge有效缓解了TTD在边缘设备上应用时的延迟与能耗瓶颈，展示了软硬件协同设计在高效模型压缩中的潜力。

Abstract: The growing demands of distributed learning on resource constrained edge devices underscore the importance of efficient on device model compression. Tensor Train Decomposition (TTD) offers high compression ratios with minimal accuracy loss, yet repeated singular value decompositions (SVDs) and matrix multiplications can impose significant latency and energy costs on low power processors. In this work, we present TT-Edge, a hardware software co designed framework aimed at overcoming these challenges. By splitting SVD into two phases--bidiagonalization and diagonalization--TT-Edge offloads the most compute intensive tasks to a specialized TTD Engine. This engine integrates tightly with an existing GEMM accelerator, thereby curtailing the frequent matrix vector transfers that often undermine system performance and energy efficiency. Implemented on a RISC-V-based edge AI processor, TT-Edge achieves a 1.7x speedup compared to a GEMM only baseline when compressing a ResNet 32 model via TTD, while reducing overall energy usage by 40.2 percent. These gains come with only a 4 percent increase in total power and minimal hardware overhead, enabled by a lightweight design that reuses GEMM resources and employs a shared floating point unit. Our experimental results on both FPGA prototypes and post-synthesis power analysis at 45 nm demonstrate that TT-Edge effectively addresses the latency and energy bottlenecks of TTD based compression in edge environments.

</details>


### [15] [Inside VOLT: Designing an Open-Source GPU Compiler](https://arxiv.org/abs/2511.13751)
*Shinnung Jeong,Chihyo Ahn,Huanzhi Pu,Jisheng Zhao,Hyesoon Kim,Blaise Tine*

Main category: cs.DC

TL;DR: 本文提出了Vortex优化轻量级工具链（VOLT），用于支持开源GPU架构上的SIMT代码生成与优化，通过分层设计实现多前端语言兼容和中间端核心优化复用，并展示了其在ISA扩展和主机运行时API方面的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 开源GPU研究虽取得进展，但缺乏高效、可复用的编译器框架来支持现有GPU程序在新型开源SIMT ISA上的执行与性能优化，而构建此类框架技术复杂且常被低估。

Method: 设计并实现了VOLT编译器工具链，采用分层架构，将关键的SIMT相关分析与优化集中于中间端，以支持多种前端语言和不断演化的开源GPU硬件。

Result: VOLT成功支持了SIMT代码在多个抽象层级上的生成与优化，并通过ISA扩展和主机运行时API两个案例验证了其良好的可扩展性和适应性。

Conclusion: VOLT为开源GPU生态提供了一个灵活、可扩展且高效的编译器基础设施，有助于降低开源GPU软硬件协同开发的门槛。

Abstract: Recent efforts in open-source GPU research are opening new avenues in a domain that has long been tightly coupled with a few commercial vendors. Emerging open GPU architectures define SIMT functionality through their own ISAs, but executing existing GPU programs and optimizing performance on these ISAs relies on a compiler framework that is technically complex and often undercounted in open hardware development costs.
  To address this challenge, the Vortex-Optimized Lightweight Toolchain (VOLT) has been proposed. This paper presents its design principles, overall structure, and the key compiler transformations required to support SIMT execution on Vortex. VOLT enables SIMT code generation and optimization across multiple levels of abstraction through a hierarchical design that accommodates diverse front-end languages and open GPU hardware. To ensure extensibility as GPU architectures evolve, VOLT centralizes fundamental SIMT-related analyses and optimizations in the middle-end, allowing them to be reused across front-ends and easily adapted to emerging open-GPU variants. Through two case studies on ISA extensions and host-runtime API, this paper also demonstrates how VOLT can support extensions

</details>


### [16] [Boosting performance: Gradient Clock Synchronisation with two-way measured links](https://arxiv.org/abs/2511.13727)
*Sophie Wenning*

Main category: cs.DC

TL;DR: 该硕士论文将GCS算法的现有形式化模型从单向测量范式扩展为更贴近实际实现的双向测量范式，在保留GCS核心行为的同时，显著放宽了先前的限制条件并大幅降低了估计误差。


<details>
  <summary>Details</summary>
Motivation: 先前关于GCS算法的研究基于单向测量范式，引入了诸多不切实际的假设（如单位链路长度），限制了其在实际部署中的适用性；本文旨在通过采用双向测量范式构建更贴近实现的形式化模型，以提升算法的实用性和性能。

Method: 将GCS算法的形式化模型从单向测量范式转换为双向测量范式，并在此基础上：1）取消单位链路长度要求；2）对先前工作中假设的频率源进行形式化建模；3）细粒度区分算法估计误差的不同组成部分；4）分析并降低不确定性对估计误差的影响。

Result: 1）实现了适用于非单位链路长度的现实部署模型；2）建立了频率源的形式化模型；3）将算法的总体估计误差降低了多个数量级；4）将每条链路上不确定性对估计误差的贡献从与链路延迟同阶降低至延迟的0.1%–10%，并给出了局部和全局偏斜的匹配上界。

Conclusion: 通过引入双向测量范式，本文显著提升了GCS算法模型的实用性与精度，在保持其核心机制的同时，大幅削弱了原有理论模型中的理想化假设，并为实际部署提供了坚实的理论基础。

Abstract: This master thesis extends the formal model of the GCS algorithm as presented by (Fan and Lynch 2004, 325), (Lenzen, Locher and Wattenhofer 2008, 510) and (Függer et al. 2023) to operate under implementation-near assumptions by replacing the one-way measurement paradigm assumed in prior work by the two-way measurement paradigm. With this change of paradigm, we remove many restrictions previously enforced to allow provable performance. Most notability, while maintaining the core behaviour of GCS, we: 1. Lift the requirement for unitary link lengths and thereby create a realistic model for flexible deployment of implementations of GCS in practice. 2. Provide a formal model of frequency sources assumed in prior work. 3. Perform a fine grained distinction between the different components of the algorithm's estimation error and globally reduce its impact by multiple orders of magnitude. 4. Significantly reduce the contribution of the uncertainty to the algorithm's estimation error to be in the range of 10\% to 0,1\% of the delay per link instead of being in the oder of the delay per link as in prior work and show matching upper bounds on the local and global skew of GCS.

</details>


### [17] [Gaia: Hybrid Hardware Acceleration for Serverless AI in the 3D Compute Continuum](https://arxiv.org/abs/2511.13728)
*Maximilian Reisecker,Cynthia Marcelino,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: Gaia 是一种面向无服务器 AI 工作负载的 GPU 即服务架构，通过在部署时识别执行模式并在运行时动态调整 CPU/GPU 后端，实现 SLO 感知且成本高效的硬件加速，在异构环境中将端到端延迟最多降低 95%。


<details>
  <summary>Details</summary>
Motivation: 当前无服务器平台在异构环境（如边-云-空三维连续体）中难以有效管理硬件加速：静态分配无法满足变化负载下的 SLO 要求，一次性动态选择又常导致次优或高成本配置。

Method: 提出 Gaia 架构，包含两个核心组件：(i) 轻量级执行模式识别器，在部署时分析函数代码并指定四种执行模式之一；(ii) 动态函数运行时，持续评估用户定义的 SLO，并在 CPU 与 GPU 后端之间动态升降级。

Result: 实验表明 Gaia 能无缝选择最适合工作负载的硬件加速方案，端到端延迟最多降低 95%。

Conclusion: Gaia 将硬件加速变为平台级关注点，实现了在异构环境中对无服务器 AI 工作负载的 SLO 感知、成本高效的加速支持。

Abstract: Serverless computing offers elastic scaling and pay-per-use execution, making it well-suited for AI workloads. As these workloads run in heterogeneous environments such as the Edge-Cloud-Space 3D Continuum, they often require intensive parallel computation, which GPUs can perform far more efficiently than CPUs. However, current platforms struggle to manage hardware acceleration effectively, as static user-device assignments fail to ensure SLO compliance under varying loads or placements, and one-time dynamic selections often lead to suboptimal or cost-inefficient configurations. To address these issues, we present Gaia, a GPU-as-a-service model and architecture that makes hardware acceleration a platform concern. Gaia combines (i) a lightweight Execution Mode Identifier that inspects function code at deploy time to emit one of four execution modes, and a Dynamic Function Runtime that continuously reevaluates user-defined SLOs to promote or demote between CPU- and GPU backends. Our evaluation shows that it seamlessly selects the best hardware acceleration for the workload, reducing end-to-end latency by up to 95%. These results indicate that Gaia enables SLO-aware, cost-efficient acceleration for serverless AI across heterogeneous environments.

</details>


### [18] [Semantic Multiplexing](https://arxiv.org/abs/2511.13779)
*Mohammad Abdi,Francesca Meneghello,Francesco Restuccia*

Main category: cs.DC

TL;DR: 本文提出语义复用（Semantic Multiplexing）新概念，通过在语义层将多个任务的压缩表示融合为单一语义表示，在不增加天线或带宽的前提下突破传统通信系统仅支持比特级并行传输的限制，显著提升边缘计算中可并发处理的任务数量，并在实验中验证了其在图像分类和情感分析任务中的高效性与准确性。


<details>
  <summary>Details</summary>
Motivation: 现有无线通信系统仅支持比特级并行传输，限制了可在无线边缘并发处理的任务数量，难以满足移动设备对多任务并行卸载的需求。

Method: 提出语义复用方法，将多个任务相关的压缩表示在语义层面融合为单一语义表示进行传输，从而在不违反香农容量限制的前提下扩展语义层的有效自由度。

Result: 实验表明，语义复用在4×4信道上将复用任务数从2增至8时，图像分类准确率下降不到4%；相比现有基线，延迟、能耗和通信负载分别最多降低8倍、25倍和54倍，同时保持相当的任务性能。

Conclusion: 语义复用是一种有效提升无线边缘多任务并发处理能力的新范式，在保证任务准确性的前提下显著优化了系统效率，并具备良好的可复现性。

Abstract: Mobile devices increasingly require the parallel execution of several computing tasks offloaded at the wireless edge. Existing communication systems only support parallel transmissions at the bit level, which fundamentally limits the number of tasks that can be concurrently processed. To address this bottleneck, this paper introduces the new concept of Semantic Multiplexing. Our approach shifts stream multiplexing from bits to tasks by merging multiple task-related compressed representations into a single semantic representation. As such, Semantic Multiplexing can multiplex more tasks than the number of physical channels without adding antennas or widening bandwidth by extending the effective degrees of freedom at the semantic layer, without contradicting Shannon capacity rules. We have prototyped Semantic Multiplexing on an experimental testbed with Jetson Orin Nano and millimeter-wave software-defined radios and tested its performance on image classification and sentiment analysis while comparing to several existing baselines in semantic communications. Our experiments demonstrate that Semantic Multiplexing allows jointly processing multiple tasks at the semantic level while maintaining sufficient task accuracy. For example, image classification accuracy drops by less than 4% when increasing from 2 to 8 the number of tasks multiplexed over a 4$\times$4 channel. Semantic Multiplexing reduces latency, energy consumption, and communication load respectively by up to 8$\times$, 25$\times$, and 54$\times$ compared to the baselines while keeping comparable performance. We pledge to publicly share the complete software codebase and the collected datasets for reproducibility.

</details>


### [19] [Guaranteed DGEMM Accuracy While Using Reduced Precision Tensor Cores Through Extensions of the Ozaki Scheme](https://arxiv.org/abs/2511.13778)
*Angelika Schwarz,Anton Anders,Cole Brower,Harun Bayraktar,John Gunnels,Kate Clark,RuQing G. Xu,Samuel Rodriguez,Sebastien Cayrols,Paweł Tabaszewski,Victor Podlozhnyuk*

Main category: cs.DC

TL;DR: 本文提出了一种名为自动动态精度（ADP）的全GPU驻留框架，利用低精度计算单元高效可靠地模拟双精度矩阵乘法，并通过指数跨度容量（ESC）等技术保证FP64级别的精度，在保持高保真度的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI发展，现代GPU越来越侧重于FP16、FP8乃至FP4等低精度格式，而传统FP64计算性能相对受限。因此，如何利用低精度硬件高效实现双精度精度成为重要研究方向。

Method: 作者提出了自动动态精度（ADP）框架，其核心是硬件无关的指数跨度容量（ESC）估计器，用于确定实现FP64精度所需的分解参数。ADP还集成了异常处理、运行时启发式策略和无缝回退机制，并改进了Ozaki分解方法，采用无符号整数切片方案以提高表示效率。

Result: 在BLAS分级测试中，ADP在挑战性输入下始终维持FP64精度，运行时开销低于10%；在55位尾数设置下，相较于原生FP64 GEMM，在NVIDIA Blackwell GB200和RTX Pro 6000 Blackwell Server Edition上分别实现最高2.3倍和13.2倍加速。

Conclusion: 低精度加速器可作为高性能、高保真科学计算任务的实用且生产就绪的基础。

Abstract: The rapid growth of artificial intelligence (AI) has made low-precision formats such as FP16, FP8, and, most recently, block-scaled FP4 the primary focus of modern GPUs, where Tensor Cores now deliver orders-of-magnitude higher throughput than traditional FP64 pipelines. This hardware shift has sparked a new line of algorithm research: using low-precision units to emulate double-precision accuracy through schemes such as Ozaki decompositions. We advance this direction with Automatic Dynamic Precision (ADP), a fully GPU-resident framework that makes emulated FP64 matrix multiplication both efficient and reliable. At its core is the Exponent Span Capacity (ESC), a hardware-agnostic estimator that conservatively determines the decomposition parameter (also known as slices) required to achieve FP64-level accuracy. Built on ESC, ADP integrates exception handling, run time heuristics, and seamless fallback to native FP64, ensuring correctness without host-device synchronization or user intervention. Additionally, we further improve Ozaki-style decompositions with an unsigned integer slicing scheme, which increases representational efficiency and reduces computational waste. Validated against recently proposed BLAS grading tests, ADP consistently preserves FP64 fidelity on challenging inputs while incurring less than 10% run time overhead. In a 55-bit mantissa setting, our approach achieves up to 2.3x and 13.2x speedups over native FP64 GEMM on NVIDIA Blackwell GB200 and the RTX Pro 6000 Blackwell Server Edition, respectively. Our results demonstrate that low-precision accelerators can serve as a practical, production-ready foundation for high-fidelity and high-performance scientific computing workloads.

</details>


### [20] [Do MPI Derived Datatypes Actually Help? A Single-Node Cross-Implementation Study on Shared-Memory Communication](https://arxiv.org/abs/2511.13804)
*Temitayo Adefemi*

Main category: cs.DC

TL;DR: 该论文对MPI派生数据类型（DDTs）在多个MPI实现中的性能进行了跨平台评估，发现其性能高度依赖于具体应用、通信语义和MPI库，无法保证可移植性，建议在目标环境中同时测试DDT和手动打包方案。


<details>
  <summary>Details</summary>
Motivation: MPI的派生数据类型虽承诺简化非连续数据的无拷贝通信，但其实际性能存在争议且缺乏跨MPI实现的系统性评估。

Method: 作者使用三个2D应用（Jacobi CFD求解器、康威生命游戏、基于格点的图像重建），分别以手动打包（BASIC）和DDT方式实现，并在四种主流MPI实现（MPICH、Open MPI、Intel MPI、MVAPICH2）上，针对多种通信语义（非阻塞点对点、邻域集合通信、MPI-4持久操作）进行强/弱扩展性测试，验证结果一致性并比较性能。

Result: 结果表明DDT性能表现不一：在某些组合下最快（如图像重建在Intel MPI和MPICH上），但在其他组合下最慢（如同一代码在Open MPI和MVAPICH2上）；CFD求解器中BASIC普遍优于DDT，而生命游戏中性能排序随MPI库变化；还观察到特定MPI栈的异常行为（如MPICH在DDT邻域和持久模式下的减速）。

Conclusion: 没有一种策略能在所有程序、通信语义和MPI实现中占优，DDT的性能可移植性无法保证，建议在目标MPI实现和通信模式下对DDT与手动打包方案都进行性能剖析。

Abstract: MPI's derived datatypes (DDTs) promise easier, copy-free communication of non-contiguous data, yet their practical performance remains debated and is often reported only for a single MPI stack. We present a cross-implementation assessment using three 2D applications: a Jacobi CFD solver, Conway's Game of Life, and a lattice-based image reconstruction. Each application is written in two ways: (i) a BASIC version with manual packing and unpacking of non-contiguous regions and (ii) a DDT version using MPI_Type_vector and MPI_Type_create_subarray with correct true extent via MPI_Type_create_resized. For API parity, we benchmark identical communication semantics: non-blocking point-to-point (Irecv/Isend + Waitall), neighborhood collectives (MPI_Neighbor_alltoallw), and MPI-4 persistent operations (*_init). We run strong and weak scaling on 1-4 ranks, validate bitwise-identical halos, and evaluate four widely used MPI implementations: MPICH, Open MPI, Intel MPI, and MVAPICH2 on a single ARCHER2 node. Results are mixed. DDTs can be fastest, for example for the image reconstruction code on Intel MPI and MPICH, but can also be among the slowest on other stacks, such as Open MPI and MVAPICH2 for the same code. For the CFD solver, BASIC variants generally outperform DDTs across semantics, whereas for Game of Life the ranking flips depending on the MPI library. We also observe stack-specific anomalies, for example MPICH slowdowns with DDT neighborhood and persistent modes. Overall, no strategy dominates across programs, semantics, and MPI stacks; performance portability for DDTs is not guaranteed. We therefore recommend profiling both DDT-based and manual-packing designs under the intended MPI implementation and communication mode. Our study is limited to a single node and does not analyze memory overhead; multi-node and GPU-aware paths are left for future work.

</details>


### [21] [ParallelKittens: Systematic and Practical Simplification of Multi-GPU AI Kernels](https://arxiv.org/abs/2511.13940)
*Stuart H. Sul,Simran Arora,Benjamin F. Spector,Christopher Ré*

Main category: cs.DC

TL;DR: ParallelKittens (PK) 是一个基于 ThunderKittens 的极简 CUDA 框架，通过八个核心原语和统一编程模板，系统化指导多 GPU 重叠通信与计算内核的设计，在 Hopper 和 Blackwell 架构上显著提升各类并行工作负载性能。


<details>
  <summary>Details</summary>
Motivation: 随着 AI 模型规模扩大，GPU 间通信已成为性能瓶颈，现有系统虽采用计算-通信重叠策略，但在异构工作负载和新型加速器上难以达到理论峰值性能。作者希望探索是否可通过少量通用、可复用的原则系统性地指导高效多 GPU 内核设计。

Method: 提出 ParallelKittens（PK）框架，基于对影响多 GPU 性能的关键因素（数据传输机制、资源调度和设计开销）的全面分析，提炼出八个多 GPU 内核设计原语，并提供统一编程模板，简化重叠内核开发。

Result: 在 Hopper 和 Blackwell 架构上验证：仅需不到 50 行设备代码，PK 在数据/张量并行任务中提速最高达 2.33 倍，序列并行任务达 4.08 倍，专家并行任务达 1.22 倍。

Conclusion: ParallelKittens 证明了通过少量通用设计原则可高效构建高性能多 GPU 重叠内核，显著简化开发并提升多种并行模式下的实际性能。

Abstract: Inter-GPU communication has become a major bottleneck for modern AI workloads as models scale and improvements in hardware compute throughput outpace improvements in interconnect bandwidth. Existing systems mitigate this through compute-communication overlap but often fail to meet theoretical peak performance across heterogeneous workloads and new accelerators. Instead of operator-specific techniques, we ask whether a small set of simple, reusable principles can systematically guide the design of optimal multi-GPU kernels. We present ParallelKittens (PK), a minimal CUDA framework that drastically simplifies the development of overlapped multi-GPU kernels. PK extends the ThunderKittens framework and embodies the principles of multi-GPU kernel design through eight core primitives and a unified programming template, derived from a comprehensive analysis of the factors that govern multi-GPU performance$\unicode{x2014}$data-transfer mechanisms, resource scheduling, and design overheads. We validate PK on both Hopper and Blackwell architectures. With fewer than 50 lines of device code, PK achieves up to $2.33 \times$ speedup for data- and tensor-parallel workloads, $4.08 \times$ for sequence-parallel workloads, and $1.22 \times$ for expert-parallel workloads.

</details>


### [22] [FailSafe: High-performance Resilient Serving](https://arxiv.org/abs/2511.14116)
*Ziyi Xu,Zhiqiang Xie,Swapnil Gandhi,Christos Kozyrakis*

Main category: cs.DC

TL;DR: FailSafe 是一种面向张量并行（TP）的容错大语言模型（LLM）推理系统，通过循环 KVCache 放置、混合注意力机制和细粒度负载感知路由，在 GPU 故障情况下仍能维持高性能与资源均衡，并显著降低恢复延迟。


<details>
  <summary>Details</summary>
Motivation: 张量并行虽能提升 LLM 推理效率，但其强耦合特性导致系统对 GPU 故障极为敏感：单点故障会中断执行、引发昂贵的 KVCache 重计算，并造成长期计算与内存失衡。

Method: FailSafe 提出三项关键技术：(1) 循环 KVCache 放置以实现内存均匀利用；(2) 混合注意力（结合张量并行与数据并行）消除拖慢节点；(3) 细粒度负载感知路由动态平衡请求。此外，采用主动 KVCache 备份与按需权重恢复机制避免重计算和冗余传输，并集成到轻量级服务引擎中。

Result: 在 8×H100 DGX 系统上使用真实故障轨迹和典型工作负载评估，FailSafe 相比标准容错方法最高实现 2 倍吞吐量提升和两个数量级更低的恢复延迟；即使在最多 3 个 GPU 故障下仍保持高吞吐与资源均衡。

Conclusion: FailSafe 能在动态且不可靠的硬件条件下实现鲁棒、高效的 LLM 推理服务，显著提升张量并行系统的容错能力与性能稳定性。

Abstract: Tensor parallelism (TP) enables large language models (LLMs) to scale inference efficiently across multiple GPUs, but its tight coupling makes systems fragile: a single GPU failure can halt execution, trigger costly KVCache recomputation, and introduce long-term compute and memory imbalance. We present FailSafe, a fault-tolerant TP serving system that sustains high performance under irregular GPU availability. FailSafe introduces three techniques to balance computation and memory across GPUs: (1) Cyclic KVCache Placement for uniform memory utilization, (2) Hybrid Attention combining tensor- and data-parallel attention to eliminate stragglers, and (3) Fine-Grained Load-Aware Routing to dynamically balance requests. It further employs proactive KVCache backup and on-demand weight recovery to avoid expensive recomputation and redundant data transfers. We implement these techniques in a lightweight serving engine compatible with existing LLM infrastructures. Evaluated on an 8xH100 DGX system with real-world fault traces and representative workloads, FailSafe achieves up to 2x higher throughput and two orders of magnitude lower recovery latency compared to standard fault handling approaches. Even with up to three GPU failures, FailSafe sustains high throughput and balanced utilization, demonstrating robust and efficient LLM serving under dynamic and unreliable hardware conditions.

</details>


### [23] [10Cache: Heterogeneous Resource-Aware Tensor Caching and Migration for LLM Training](https://arxiv.org/abs/2511.14124)
*Sabiha Afroz,Redwan Ibne Seraj Khan,Hadeel Albahar,Jingoo Han,Ali R. Butt*

Main category: cs.DC

TL;DR: 10Cache 是一种面向云环境的资源感知张量缓存与迁移系统，通过优化 GPU、CPU 和 NVMe 之间的内存协同，显著提升大语言模型训练效率。


<details>
  <summary>Details</summary>
Motivation: 云端训练大语言模型面临 GPU 内存容量有限和成本高昂的问题，现有内存卸载方法存在张量迁移延迟高和设备内存利用率低的缺陷，导致训练时间延长和云成本增加。

Method: 10Cache 通过分析张量执行顺序构建预取策略，在固定内存中按张量大小分布分配缓冲区，并重用缓冲区以减少分配开销，实现跨 GPU、CPU 和 NVMe 的智能内存协调。

Result: 在多种大语言模型工作负载下，10Cache 相比现有先进卸载方法，训练速度最高提升 2 倍，GPU 缓存命中率最高提升 86.6 倍，CPU 和 GPU 内存利用率分别最高提升 2.15 倍和 1.33 倍。

Conclusion: 10Cache 是一种实用且可扩展的解决方案，能有效优化云环境中大语言模型训练的吞吐量和资源利用效率。

Abstract: Training large language models (LLMs) in the cloud faces growing memory bottlenecks due to the limited capacity and high cost of GPUs. While GPU memory offloading to CPU and NVMe has made large-scale training more feasible, existing approaches suffer from high tensor migration latency and suboptimal device memory utilization, ultimately increasing training time and cloud costs. To address these challenges, we present 10Cache, a resource-aware tensor caching and migration system that accelerates LLM training by intelligently coordinating memory usage across GPU, CPU, and NVMe tiers. 10Cache profiles tensor execution order to construct prefetch policies, allocates memory buffers in pinned memory based on tensor size distributions, and reuses memory buffers to minimize allocation overhead.
  Designed for cloud-scale deployments, 10Cache improves memory efficiency and reduces reliance on high-end GPUs. Across diverse LLM workloads, it achieves up to 2x speedup in training time, improves GPU cache hit rate by up to 86.6x, and increases CPU/GPU memory utilization by up to 2.15x and 1.33x, respectively, compared to state-of-the-art offloading methods. These results demonstrate that 10Cache is a practical and scalable solution for optimizing LLM training throughput and resource efficiency in cloud environments.

</details>


### [24] [Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning](https://arxiv.org/abs/2511.14456)
*Fabian Stricker,David Bermbach,Christian Zirpins*

Main category: cs.DC

TL;DR: 本文研究了跨组织小规模参与的跨孤岛联邦学习中参与者失败对模型质量的影响，发现数据偏斜会导致评估过于乐观，且失败发生的时机显著影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 在跨孤岛联邦学习场景中，参与组织可能因通信问题或配置错误而失败，但目前缺乏对此类失败对模型质量影响的深入研究，尤其相较于跨设备联邦学习而言。

Method: 开展了一项广泛实验研究，分析参与者失败对模型质量的影响，重点关注失败发生的时间、数据分布特性以及对模型评估结果的影响。

Result: 研究发现，在高度数据偏斜情况下，模型评估结果过于乐观，掩盖了失败的真实影响；同时，失败发生的时机对最终模型质量有显著影响。

Conclusion: 该研究为构建鲁棒的跨孤岛联邦学习系统提供了重要见解，有助于研究人员和软件架构师更好地理解和应对参与者失败带来的挑战。

Abstract: Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.

</details>


### [25] [Hapax Locks : Value-Based Mutual Exclusion](https://arxiv.org/abs/2511.14608)
*Dave Dice,Alex Kogan*

Main category: cs.DC

TL;DR: Hapax Locks 是一种新型锁算法，具有常数时间的加锁/解锁路径、FIFO 排队顺序、空间高效、低缓存一致性开销，并且易于集成到现有系统中。


<details>
  <summary>Details</summary>
Motivation: 现有高性能锁通常对运行时环境有较多依赖或约束，难以在已有系统中集成；作者希望设计一种简单、高效且兼容性强的锁机制。

Method: 提出 Hapax Locks 算法，其特点包括常数时间的到达与解锁路径、FIFO 入队顺序、无指针在线程间转移或逃逸所有权。

Result: Hapax Locks 在延迟和可扩展性方面性能媲美当前最先进的锁，同时显著减少对运行时环境的依赖。

Conclusion: Hapax Locks 在保持高性能的同时具备良好的兼容性和实现简洁性，适合在现有系统中部署或改造使用。

Abstract: We present Hapax Locks, a novel locking algorithm that is simple, enjoys constant-time arrival and unlock paths, provides FIFO admission order, and which is also space efficient and generates relatively little coherence traffic under contention in the common case. Hapax Locks offer performance (both latency and scalability) that is comparable with the best state of the art locks, while at the same time Hapax Locks impose fewer constraints and dependencies on the ambient runtime environment, making them particularly easy to integrate or retrofit into existing systems or under existing application programming interfaces Of particular note, no pointers shift or escape ownership between threads in our algorithm.

</details>


### [26] [Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning](https://arxiv.org/abs/2511.14617)
*Ruoyu Qin,Weiran He,Weixiao Huang,Yangkun Zhang,Yikai Zhao,Bo Pang,Xinran Xu,Yingdi Shan,Yongwei Wu,Mingxing Zhang*

Main category: cs.DC

TL;DR: Seer 是一种新型在线上下文学习系统，通过利用共享相同提示的请求在输出长度和生成模式上的相似性，显著提升强化学习（RL）中 rollout 阶段的吞吐量并降低长尾延迟。


<details>
  <summary>Details</summary>
Motivation: 现有同步 RL 系统在 rollout 阶段存在严重性能瓶颈，包括长尾延迟高和资源利用率低的问题，主要源于工作负载不平衡。

Method: Seer 引入三项关键技术：分片 rollout 实现动态负载均衡、上下文感知调度，以及自适应分组推测解码。

Result: 在生产级 RL 工作负载上的评估表明，Seer 相比当前最先进的同步 RL 系统，端到端 rollout 吞吐量提升 74%–97%，长尾延迟降低 75%–93%。

Conclusion: Seer 显著加速了 RL 训练迭代过程，有效解决了 rollout 阶段的性能瓶颈问题。

Abstract: Reinforcement Learning (RL) has become critical for advancing modern Large Language Models (LLMs), yet existing synchronous RL systems face severe performance bottlenecks. The rollout phase, which dominates end-to-end iteration time, suffers from substantial long-tail latency and poor resource utilization due to inherent workload imbalance. We present Seer, a novel online context learning system that addresses these challenges by exploiting previously overlooked similarities in output lengths and generation patterns among requests sharing the same prompt. Seer introduces three key techniques: divided rollout for dynamic load balancing, context-aware scheduling, and adaptive grouped speculative decoding. Together, these mechanisms substantially reduce long-tail latency and improve resource efficiency during rollout. Evaluations on production-grade RL workloads demonstrate that Seer improves end-to-end rollout throughput by 74% to 97% and reduces long-tail latency by 75% to 93% compared to state-of-the-art synchronous RL systems, significantly accelerating RL training iterations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [27] [SQL-to-Text Generation with Weighted-AST Few-Shot Prompting](https://arxiv.org/abs/2511.13907)
*Sriom Chakrabarti,Chuangtao Ma,Arijit Khan,Sebastian Link*

Main category: cs.DB

TL;DR: 提出一种基于加权抽象语法树（Weighted-AST）检索与大语言模型提示相结合的方法，用于将SQL查询准确翻译为自然语言描述，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在SQL-to-Text任务中难以保持SQL查询的精确语义，尤其在存在多种正确表述时容易产生偏差。

Method: 结合结构化查询表示与大语言模型提示机制，利用带学习权重的抽象语法树（AST）计算相似度，检索语义相关的示例作为少样本提示，引导生成忠实且流畅的自然语言描述。

Result: 在Spider、SParC和CoSQL三个基准数据集上，该方法在执行准确率（EX）上最高提升17.24%，在精确匹配（EM）和人工评估的语义保真度方面也表现更优，同时保持良好的运行效率。

Conclusion: Weighted-AST提示是一种可扩展且高效的方法，能有效从结构化数据库查询中生成准确、语义一致的自然语言解释。

Abstract: SQL-to-Text generation aims at translating structured SQL queries into natural language descriptions, thereby facilitating comprehension of complex database operations for non-technical users. Although large language models (LLMs) have recently demonstrated promising results, current methods often fail to maintain the exact semantics of SQL queries, particularly when there are multiple possible correct phrasings. To address this problem, our work proposes Weighted-AST retrieval with prompting, an architecture that integrates structural query representations and LLM prompting. This method retrieves semantically relevant examples as few-shot prompts using a similarity metric based on an Abstract Syntax Tree (AST) with learned feature weights. Our structure-aware prompting technique ensures that generated descriptions are both fluent and faithful to the original query logic. Numerous experiments on three benchmark datasets - Spider, SParC, and CoSQL show that our method outperforms the current baselines by up to +17.24% in execution Accuracy (EX), performs superior in Exact Match (EM) and provides more consistent semantic fidelity when evaluated by humans, all while preserving competitive runtime performance. These results demonstrate that Weighted-AST prompting is a scalable and effective method for deriving natural language explanations from structured database queries.

</details>


### [28] [Chipmink: Efficient Delta Identification for Massive Object Graph](https://arxiv.org/abs/2511.14162)
*Supawit Chockchowwat,Sumay Thakurdesai,Zhaoheng Li,Matthew Krafczyk,Yongjoo Park*

Main category: cs.DB

TL;DR: 本文提出了一种名为Chipmink的基于图的对象存储系统，通过动态划分对象为“pods”来实现高效的部分持久化，显著减少了存储开销和持久化时间。


<details>
  <summary>Details</summary>
Motivation: 现有数据科学工具中的对象持久化机制（如Pickle、Dill）依赖完整快照，导致在执行和探索过程中重复存储未更改的对象，造成时间和存储上的低效；同时缺乏类似数据库系统的集中式缓冲管理器来追踪脏对象，且对象状态分布在内存、共享内存、GPU和远程机器等多种位置，使得脏对象识别极具挑战。

Method: 提出Chipmink系统，其将对象按引用结构和大小动态划分为称为“pods”的子组，作为持久化单元，从而隔离脏对象并支持高效的局部持久化。

Result: 实验表明，Chipmink具有良好的通用性，支持共享内存、GPU和远程对象的库，并在真实世界的notebook和脚本中相比最佳基线实现了最高36.5倍的存储节省和12.4倍的持久化加速。

Conclusion: Chipmink通过引入类似数据库缓冲管理器的机制，有效解决了数据科学环境中对象持久化的效率问题，为非线性、持续的数据探索提供了更高效的支持。

Abstract: Ranging from batch scripts to computational notebooks, modern data science tools rely on massive and evolving object graphs that represent structured data, models, plots, and more. Persisting these objects is critical, not only to enhance system robustness against unexpected failures but also to support continuous, non-linear data exploration via versioning. Existing object persistence mechanisms (e.g., Pickle, Dill) rely on complete snapshotting, often redundantly storing unchanged objects during execution and exploration, resulting in significant inefficiency in both time and storage. Unlike DBMSs, data science systems lack centralized buffer managers that track dirty objects. Worse, object states span various locations such as memory heaps, shared memory, GPUs, and remote machines, making dirty object identification fundamentally more challenging. In this work, we propose a graph-based object store, named Chipmink, that acts like the centralized buffer manager. Unlike static pages in DBMSs, persistence units in Chipmink are dynamically induced by partitioning objects into appropriate subgroups (called pods), minimizing expected persistence costs based on object sizes and reference structure. These pods effectively isolate dirty objects, enabling efficient partial persistence. Our experiments show that Chipmink is general, supporting libraries that rely on shared memory, GPUs, and remote objects. Moreover, Chipmink achieves up to 36.5x smaller storage sizes and 12.4x faster persistence than the best baselines in real-world notebooks and scripts.

</details>


### [29] [Scalable Enforcement of Fine Grained Access Control Policies in Relational Database Management Systems](https://arxiv.org/abs/2511.14629)
*Anadi Shakya,Primal Pappachan,David Maier,Roberto Yus,Sharad Mehrotra,Johann-Christoph Freytag*

Main category: cs.DB

TL;DR: Sieve 是一种用于关系型数据库的中间件，通过查询重写和缓存机制高效执行细粒度访问控制（FGAC）策略，在大规模策略集下显著提升查询性能和系统可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着智能技术普及和 GDPR、CPRA 等隐私法规出台，数据库系统需高效管理大量细粒度访问控制策略，但现有方法在策略数量激增时性能急剧下降。

Method: Sieve 采用查询重写技术，将策略转化为带保护表达式的查询以利用数据库索引，并结合具备高效替换与刷新机制的缓存策略来适应动态负载。

Result: 在两个 DBMS 上的实验表明，Sieve 在 200 至 1,200 条策略下可将策略评估性能提升 2 至 10 倍；缓存机制在动态负载下进一步提升查询性能 6%–22%，尤其在大缓存时效果更佳。

Conclusion: Sieve 能够支持智能环境中实时访问控制需求，为用户偏好与隐私策略提供高效、可扩展的管理方案。

Abstract: The proliferation of smart technologies and evolving privacy regulations such as the GDPR and CPRA has increased the need to manage fine-grained access control (FGAC) policies in database management systems (DBMSs). Existing approaches to enforcing FGAC policies do not scale to thousands of policies, leading to degraded query performance and reduced system effectiveness. We present Sieve, a middleware for relational DBMSs that combines query rewriting and caching to optimize FGAC policy enforcement. Sieve rewrites a query with guarded expressions that group and filter policies and can efficiently use indexes in the DBMS. It also integrates a caching mechanism with an effective replacement strategy and a refresh mechanism to adapt to dynamic workloads. Experiments on two DBMSs with real and synthetic datasets show that Sieve scales to large datasets and policy corpora, maintaining low query latency and system load and improving policy evaluation performance by between 2x and 10x on workloads with 200 to 1,200 policies. The caching extension further improves query performance by between 6 and 22 percent under dynamic workloads, especially with larger cache sizes. These results highlight Sieve's applicability for real-time access control in smart environments and its support for efficient, scalable management of user preferences and privacy policies.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [30] [Benchmarking OpenWiFiSync on ESP32: Towards Cost-Effective Wireless Time Synchronization](https://arxiv.org/abs/2511.14457)
*Michael Gundall,Jan Herbst,Robin Müller,Hans D. Schotten*

Main category: cs.NI

TL;DR: 该论文提出并验证了一种基于无线广播的低成本时间同步协议，可在ESP32等低功耗硬件上实现±30微秒的同步精度。


<details>
  <summary>Details</summary>
Motivation: 工业4.0应用需要移动设备间高精度无线时间同步，而传统有线同步协议在无线环境中性能不足。

Method: 采用参考广播基础设施同步协议（RBIS），利用无线通信的广播特性，在ESP32模块和商用Wi-Fi接入点构成的测试平台上实现，并开源发布。

Result: 在低成本、低功耗硬件上实现了±30微秒的时间同步精度。

Conclusion: 所提方法具有非侵入性、标准兼容性及高性价比，适用于多种工业4.0应用场景。

Abstract: Wireless time synchronization of mobile devices is a key enabler for numerous Industry 4.0 applications, such as coordinated and synchronized tasks or the generation of high-precision timestamps for machine learning or artificial intelligence algorithms. Traditional wireline clock synchronization protocols, however, cannot achieve the performance in wireless environments without significant modifications. To address this challenge, we make use of the Reference Broadcast Infrastructure Synchronization protocol, which leverages the broadcast nature of wireless communications and remains both non-invasive and standard-compliant. We implement and validate this protocol on a low-cost testbed using ESP32 modules and a commercial Wi-Fi access point. To support further research and development, we release our implementation as open-source software under the GNU General Public License Version 3 license via the OpenWifiSync project on GitHub.
  Our results demonstrate that synchronization accuracies within +/-30 microseconds are achievable using energy-efficient and affordable hardware, making this approach suitable for a wide range of use cases.

</details>


### [31] [Evaluating the Impact of Packet Scheduling and Congestion Control Algorithms on MPTCP Performance over Heterogeneous Networks](https://arxiv.org/abs/2511.14550)
*Dimitrios Dimopoulos,Apostolis K. Salkintzis,Dimitris Tsolkas,Nikos Passas,Lazaros Merakos*

Main category: cs.NI

TL;DR: 本文综述了多路径TCP（MPTCP）协议，分析了现有的分组调度与拥塞控制算法，并通过大量实验评估了不同算法组合在路径异构性条件下的性能表现。


<details>
  <summary>Details</summary>
Motivation: MPTCP在路径特性相似时表现良好，但在路径异构性增加时性能受限，因此需要系统评估不同调度与拥塞控制算法对性能的影响。

Method: 对MPTCP协议机制进行概述，梳理现有分组调度和拥塞控制算法，并在多种路径异构条件下开展广泛的实验评估。

Result: 实验揭示了不同算法组合在异构路径环境下的性能差异，为MPTCP优化提供了实证依据。

Conclusion: MPTCP的性能高度依赖于所采用的调度与拥塞控制算法组合，在路径异构场景下需谨慎选择或设计适配算法以提升整体性能。

Abstract: Modern mobile and stationary devices are equipped with multiple network interfaces aiming to provide wireless and wireline connectivity either in a local LAN or the Internet. Multipath TCP (MPTCP) protocol has been developed on top of legacy TCP to allow the simultaneous use of multiple network paths in the communication route between two end-systems. Although the combination of multiple paths is beneficial in case of links with similar network characteristics, MPTCP performance is challenged as heterogeneity among the used paths increases. This work provides an overview of the MPTCP protocol operation, analyzes the state-of-art packet scheduling and congestion control algorithms available in literature, and examines the impact of the various algorithm combinations on MPTCP performance, by conducting an extensive experimental evaluation under diverse path-heterogeneity conditions.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [32] [Variational multiscale enrichment method for dynamic response of hyperelastic materials at finite deformation](https://arxiv.org/abs/2511.13723)
*Abhishek Arora,Caglar Oskay*

Main category: cs.CE

TL;DR: 本文将变分多尺度增强（VME）方法扩展至超弹性材料大变形下的动态响应模拟，能够处理波传播中的尺度不可分问题，并考虑材料与几何非线性对波形演化的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模拟具有强非线性和短波长特征的超弹性材料动态响应时存在局限，亟需一种能同时捕捉宏观与微观动力学行为的多尺度计算框架。

Method: 采用位移场的加性分解，推导出包含微惯性效应的粗细尺度控制方程；通过算子分裂法迭代求解半离散方程，粗尺度显式积分，细尺度可选显式或隐式时间积分方案。

Result: 数值实验表明，所提多尺度耗散格式能有效抑制虚假振荡，并揭示了材料/几何非线性及异质微结构刚度差异对波的色散、衰减和陡化等特性的影响。

Conclusion: 该多尺度计算框架为研究具有复杂微结构的超弹性材料在动态载荷下的响应提供了有效工具，尤其适用于短波长和强非线性情形。

Abstract: In this manuscript, we extend the variational multiscale enrichment (VME) method to model the dynamic response of hyperelastic materials undergoing large deformations. This approach enables the simulation of wave propagation under scale-inseparable conditions, including short-wavelength regimes, while accounting for material and geometric nonlinearities that lead to wave steepening or flattening. By employing an additive decomposition of the displacement field, we derive multiscale governing equations for the coarse- and fine-scale problems, which naturally incorporate micro-inertial effects. The framework allows the discretization of each unit cell with a patch of coarse-scale elements, which is essential to accurately capture wave propagation in short-wavelength regimes. An operator-split procedure is used to iteratively solve the semi-discrete equations at both scales until convergence is achieved. The coarse-scale problem is integrated explicitly, while the fine-scale problem is solved using either explicit or implicit time integration schemes, including both dissipative and non-dissipative methods. Numerical examples demonstrate that multiscale dissipative schemes effectively suppress spurious oscillations. The multiscale framework was applied to investigate how material and geometric nonlinearities, along with elastic stiffness contrast in heterogeneous microstructures, influence key wave characteristics such as dispersion, attenuation, and steepening. This multiscale computational framework provides a foundation for studying the dynamic response of architected materials.

</details>


### [33] [PGD-TO: A Scalable Alternative to MMA Using Projected Gradient Descent for Multi-Constraint Topology Optimization](https://arxiv.org/abs/2511.13905)
*Amin Heyrani Nobari,Faez Ahmed*

Main category: cs.CE

TL;DR: 本文提出了PGD-TO框架，通过将投影步骤重构为正则化凸二次问题，避免了传统投影梯度下降法在拓扑优化中处理多约束和非线性问题时的活跃集检测难题。该方法结合半光滑牛顿求解器、二分搜索投影、谱步长自适应与非线性共轭梯度方向，在多种线性和非线性、单约束与多约束基准问题上实现了与MMA和OC相当的收敛性和柔度性能，同时显著降低了每轮迭代的计算时间（一般问题快10–43倍，独立约束问题快115–312倍），为大规模拓扑优化提供了高效、稳健且可扩展的新方案。


<details>
  <summary>Details</summary>
Motivation: 传统投影梯度下降（PGD）方法在处理拓扑优化中的非线性和多约束问题时，因活跃集检测复杂而表现不佳，尤其在约束不可行时缺乏良好定义。因此，亟需一种无需活跃集搜索、能稳定处理各类约束并保持高效计算的优化框架。

Method: PGD-TO将投影步骤转化为正则化凸二次问题，消除对活跃集搜索的依赖；对一般多约束情形采用半光滑牛顿求解器，对单约束或独立约束使用二分搜索投影；并引入谱步长自适应和非线性共轭梯度方向以提升稳定性和效率。

Result: 在四类代表性拓扑优化基准问题（包括线性/非线性、单/多约束）上，PGD-TO实现了与MMA和OC方法相当的收敛性和最终柔度，同时每轮迭代计算时间大幅减少：一般问题快10–43倍，独立约束问题快115–312倍。

Conclusion: PGD-TO是一种快速、稳健且可扩展的拓扑优化方法，有效克服了传统PGD在多约束和非线性场景下的局限性，为实际大规模复杂设计问题提供了可行替代方案。

Abstract: Projected Gradient Descent (PGD) methods offer a simple and scalable approach to topology optimization (TO), yet they often struggle with nonlinear and multi-constraint problems due to the complexity of active-set detection. This paper introduces PGD-TO, a framework that reformulates the projection step into a regularized convex quadratic problem, eliminating the need for active-set search and ensuring well-posedness even when constraints are infeasible. The framework employs a semismooth Newton solver for general multi-constraint cases and a binary search projection for single or independent constraints, achieving fast and reliable convergence. It further integrates spectral step-size adaptation and nonlinear conjugate-gradient directions for improved stability and efficiency. We evaluate PGD-TO on four benchmark families representing the breadth of TO problems: (i) minimum compliance with a linear volume constraint, (ii) minimum volume under a nonlinear compliance constraint, (iii) multi-material minimum compliance with four independent volume constraints, and (iv) minimum compliance with coupled volume and center-of-mass constraints. Across these single- and multi-constraint, linear and nonlinear cases, PGD-TO achieves convergence and final compliance comparable to the Method of Moving Asymptotes (MMA) and Optimality Criteria (OC), while reducing per-iteration computation time by 10-43x on general problems and 115-312x when constraints are independent. Overall, PGD-TO establishes a fast, robust, and scalable alternative to MMA, advancing topology optimization toward practical large-scale, multi-constraint, and nonlinear design problems. Public code available at: https://github.com/ahnobari/pyFANTOM

</details>


### [34] [MoMoE: A Mixture of Expert Agent Model for Financial Sentiment Analysis](https://arxiv.org/abs/2511.13983)
*Peng Shu,Junhao Chen,Zhengliang Liu,Hanqi Jiang,Yi Pan,Khanh Nhu Nguyen,Zihao Wu,Huaqin Zhao,Yiwei Li,Enze Shi,ShaoChen Xu*

Main category: cs.CE

TL;DR: 本文提出了一种名为“专家混合的混合”（MoMoE）的新方法，将专家混合（MoE）架构与多智能体协作框架相结合，在LLaMA 3.1 8B模型中为每层智能体引入MoE模块，显著提升了语言理解和生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模型规模扩展和任务专业化之间存在权衡，作者旨在通过融合专家混合机制与多智能体协作，实现更高效的任务分解与性能提升。

Method: 在LLaMA 3.1 8B架构基础上，为分层协作结构中的每个智能体在其最后一层注意力模块中引入MoE层，构建由多个专业化专家智能体组成的集成系统，并通过迭代优化输出。

Result: 在多个语言理解与生成基准测试中取得显著性能提升，验证了神经层面与智能体层面专家路由结合的有效性。

Conclusion: MoMoE通过在模型架构和智能体协作两个层面引入专家混合机制，实现了更强的语言处理能力，展示了多粒度专业化路径的协同优势。

Abstract: We present a novel approach called Mixture of Mixture of Expert (MoMoE) that combines the strengths of Mixture-of-Experts (MoE) architectures with collaborative multi-agent frameworks. By modifying the LLaMA 3.1 8B architecture to incorporate MoE layers in each agent of a layered collaborative structure, we create an ensemble of specialized expert agents that iteratively refine their outputs. Each agent leverages an MoE layer in its final attention block, enabling efficient task decomposition while maintaining computational feasibility. This hybrid approach creates specialized pathways through both the model architecture and the agent collaboration layers. Experimental results demonstrate significant improvements across multiple language understanding and generation benchmarks, highlighting the synergistic benefits of combining expert routing at both the neural and agent levels.

</details>
