<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Cache Mechanism for Agent RAG Systems](https://arxiv.org/abs/2511.02919)
*Shuhang Lin,Zhencan Peng,Lingyao Li,Xiao Lin,Xi Zhu,Yongfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了ARC（Agent RAG Cache Mechanism），一种无需标注的动态缓存机制，通过结合历史查询分布和嵌入空间几何结构，为每个智能体维护高相关性的小型语料库，在仅保留原始语料0.015%的情况下实现高达79.8%的回答覆盖率，并降低80%的检索延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于RAG的LLM智能体在缓存管理方面缺乏对每个智能体需求动态适配的紧凑、高相关性语料库构建与维护机制，这一问题尚未得到充分研究。

Method: 提出ARC框架，利用历史查询分布模式与缓存项在嵌入空间中的内在几何结构，自动构建并动态更新每个智能体专属的小型高价值语料缓存，无需人工标注。

Result: 在三个检索数据集上的实验表明，ARC仅需原始语料0.015%的存储空间，即可达到最高79.8%的有答案率，并将平均检索延迟降低80%。

Conclusion: ARC显著提升了RAG驱动的LLM智能体在效率与效果方面的表现，验证了动态、轻量级缓存机制在智能体系统中的关键作用。

Abstract: Recent advances in Large Language Model (LLM)-based agents have been
propelled by Retrieval-Augmented Generation (RAG), which grants the models
access to vast external knowledge bases. Despite RAG's success in improving
agent performance, agent-level cache management, particularly constructing,
maintaining, and updating a compact, relevant corpus dynamically tailored to
each agent's need, remains underexplored. Therefore, we introduce ARC (Agent
RAG Cache Mechanism), a novel, annotation-free caching framework that
dynamically manages small, high-value corpora for each agent. By synthesizing
historical query distribution patterns with the intrinsic geometry of cached
items in the embedding space, ARC automatically maintains a high-relevance
cache. With comprehensive experiments on three retrieval datasets, our
experimental results demonstrate that ARC reduces storage requirements to
0.015% of the original corpus while offering up to 79.8% has-answer rate and
reducing average retrieval latency by 80%. Our results demonstrate that ARC can
drastically enhance efficiency and effectiveness in RAG-powered LLM agents.

</details>


### [2] [LEGO-Eval: Towards Fine-Grained Evaluation on Synthesizing 3D Embodied Environments with Tool Augmentation](https://arxiv.org/abs/2511.03001)
*Gyeom Hwangbo,Hyungjoo Chae,Minseok Kang,Hyeonjong Ju,Soohyun Oh,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 本文提出了LEGO-Eval评估框架和LEGO-Bench基准，以解决当前大语言模型生成3D场景时缺乏真实空间布局与物体属性的问题，并验证细粒度指令与生成场景之间的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法因指令过于粗略，导致生成结果缺乏现实环境中的空间布局和物体属性，影响具身智能体的学习效果；同时当前评估方法（如CLIPScore、VLM）难以准确衡量生成场景与细粒度指令之间的一致性。

Method: 提出LEGO-Eval评估框架，利用多样化工具显式地对场景组件进行接地（grounding），从而更准确地评估场景与指令的一致性；同时构建LEGO-Bench基准，包含描述复杂布局和真实环境属性的细粒度指令。

Result: 实验表明，LEGO-Eval在评估场景-指令一致性方面比VLM-as-a-judge高出0.41的F1分数；在LEGO-Bench上的评测显示，现有生成方法在完全符合细粒度指令方面的成功率最高仅为10%。

Conclusion: 当前3D场景生成方法在遵循细粒度指令方面存在显著不足，而LEGO-Eval能更可靠地评估生成质量，为未来改进提供有效工具和基准。

Abstract: Despite recent progress in using Large Language Models (LLMs) for
automatically generating 3D scenes, generated scenes often lack realistic
spatial layouts and object attributes found in real-world environments. As this
problem stems from insufficiently detailed, coarse-grained instructions,
advancing 3D scene synthesis guided by more detailed, fine-grained instructions
that reflect real-world environments becomes crucial. Without such realistic
scenes, training embodied agents in unrealistic environments can lead them to
learn priors that diverge significantly from real-world physics and semantics,
degrading their performance when deployed. Thus, verifying the alignment
between the fine-grained instruction and the generated scene is essential for
effective learning. However, current evaluation methods, such as CLIPScore and
vision-language models (VLMs), often fail to reliably assess such alignment.
This shortcoming arises primarily from their shallow understanding of 3D
scenes, which often leads to improperly grounded scene components. To address
this, we introduce LEGO-Eval, an evaluation framework equipped with diverse
tools designed to explicitly ground scene components, enabling more accurate
alignment assessments. We also present LEGO-Bench, a benchmark of detailed
instructions that specify complex layouts and attributes of real-world
environments. Experiments demonstrate that LEGO-Eval outperforms VLM-as-a-judge
by 0.41 F1 score in assessing scene-instruction alignment. Benchmarking with
LEGO-Bench reveals significant limitations in current generation methods.
Across all evaluated approaches, success rates reached at most 10% in
generating scenes that fully align with fine-grained instructions.

</details>


### [3] [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034)
*Yan Cathy Hua,Paul Denny,Jörg Wicker,Katerina Taškova*

Main category: cs.CL

TL;DR: 本文针对低资源领域（如教育和医疗）的方面级情感分析（ABSA）问题，提出了更灵活的评估方法FTS-OBP，探索了小型生成式语言模型在极少量数据下的有效训练策略，并发布了首个教育评论ABSA公开资源集。


<details>
  <summary>Details</summary>
Motivation: 当前ABSA研究主要集中于商业领域，而在教育、医疗等高需求但低资源领域缺乏有效方法；同时传统评估方式过于严格，且现有模型依赖大量资源进行知识注入，难以适用于资源受限场景。

Method: 提出新型评估方法FTS-OBP以容忍边界变化；对小于7B参数的小型解码器语言模型进行系统性研究，包括无数据（上下文学习、权重融合）与轻量数据微调方法，并设计多任务微调策略；发布教育评论ABSA数据资源。

Result: 所提多任务微调策略使1.5–3.8B参数模型在仅200–1,000条样本下超越闭源大模型并接近基准性能；FTS-OBP评估方法与传统指标高度相关且提供细粒度诊断；首次构建并开源教育领域ABSA资源。

Conclusion: 本研究推动了ABSA在低资源领域的应用，验证了小模型在有限数据下的潜力，并为未来研究提供了新评估标准与数据基础。

Abstract: Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining
approach that identifies and classifies opinions associated with specific
entities (aspects) or their categories within a sentence. Despite its rapid
growth and broad potential, ABSA research and resources remain concentrated in
commercial domains, leaving analytical needs unmet in high-demand yet
low-resource areas such as education and healthcare. Domain adaptation
challenges and most existing methods' reliance on resource-intensive
in-training knowledge injection further hinder progress in these areas.
Moreover, traditional evaluation methods based on exact matches are overly
rigid for ABSA tasks, penalising any boundary variations which may misrepresent
the performance of generative models. This work addresses these gaps through
three contributions: 1) We propose a novel evaluation method, Flexible Text
Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates
realistic extraction boundary variations while maintaining strong correlation
with traditional metrics and offering fine-grained diagnostics. 2) We present
the first ABSA study of small decoder-only generative language models (SLMs;
<7B parameters), examining resource lower bounds via a case study in education
review ABSA. We systematically explore data-free (in-context learning and
weight merging) and data-light fine-tuning methods, and propose a multitask
fine-tuning strategy that significantly enhances SLM performance, enabling
1.5-3.8 B models to surpass proprietary large models and approach benchmark
results with only 200-1,000 examples on a single GPU. 3) We release the first
public set of education review ABSA resources to support future research in
low-resource domains.

</details>


### [4] [ROBoto2: An Interactive System and Dataset for LLM-assisted Clinical Trial Risk of Bias Assessment](https://arxiv.org/abs/2511.03048)
*Anthony Hevia,Sanjana Chintalapati,Veronica Ka Wai Lai,Thanh Tam Nguyen,Wai-Tat Wong,Terry Klassen,Lucy Lu Wang*

Main category: cs.CL

TL;DR: ROBOTO2 是一个开源的网页平台，利用大语言模型（LLM）辅助进行临床试验偏倚风险（ROB2）评估，结合PDF解析、检索增强提示和人工反馈，提升评估效率，并发布了包含521项儿科临床试验的数据集用于基准测试。


<details>
  <summary>Details</summary>
Motivation: 传统ROB2偏倚风险评估过程耗时费力，亟需自动化工具提高效率并保持评估质量。

Method: 开发了ROBOTO2平台，整合PDF解析、检索增强的LLM提示和人工校正机制；构建并公开了一个包含521份儿科临床试验报告及其ROB2标注的数据集；在此基础上对4个LLM在ROB2任务上的表现进行了基准测试。

Result: 成功构建并发布了包含8954个信号问题和1202条证据段落的儿科临床试验ROB2标注数据集；通过该数据集评估了多个LLM在ROB2任务中的性能，揭示了当前模型的能力与挑战。

Conclusion: ROBOTO2显著提升了ROB2评估的效率与可及性，所发布的数据集为系统综述中的偏倚风险自动化评估提供了重要基准，推动了该领域的研究与应用。

Abstract: We present ROBOTO2, an open-source, web-based platform for large language
model (LLM)-assisted risk of bias (ROB) assessment of clinical trials. ROBOTO2
streamlines the traditionally labor-intensive ROB v2 (ROB2) annotation process
via an interactive interface that combines PDF parsing, retrieval-augmented LLM
prompting, and human-in-the-loop review. Users can upload clinical trial
reports, receive preliminary answers and supporting evidence for ROB2 signaling
questions, and provide real-time feedback or corrections to system suggestions.
ROBOTO2 is publicly available at https://roboto2.vercel.app/, with code and
data released to foster reproducibility and adoption. We construct and release
a dataset of 521 pediatric clinical trial reports (8954 signaling questions
with 1202 evidence passages), annotated using both manually and LLM-assisted
methods, serving as a benchmark and enabling future research. Using this
dataset, we benchmark ROB2 performance for 4 LLMs and provide an analysis into
current model capabilities and ongoing challenges in automating this critical
aspect of systematic review.

</details>


### [5] [PolyNorm: Few-Shot LLM-Based Text Normalization for Text-to-Speech](https://arxiv.org/abs/2511.03080)
*Michel Wong,Ali Alshehri,Sophia Kao,Haotian He*

Main category: cs.CL

TL;DR: 本文提出PolyNorm，一种基于提示的大语言模型文本归一化方法，并构建了跨语言自动数据构建与评估流程，在八种语言上均优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 传统文本归一化系统虽准确但工程复杂、难以扩展且在低资源语言中覆盖困难，亟需减少人工规则依赖并提升多语言适用性。

Method: 采用基于提示（prompt-based）的大语言模型进行文本归一化，并设计语言无关的自动数据构建与评估流水线。

Result: 在八种语言上的实验表明，相比当前生产级系统，PolyNorm显著降低了词错误率（WER）。

Conclusion: PolyNorm有效减少了对人工规则的依赖，提升了多语言文本归一化的可扩展性和性能，并通过发布PolyNorm-Benchmark数据集促进后续研究。

Abstract: Text Normalization (TN) is a key preprocessing step in Text-to-Speech (TTS)
systems, converting written forms into their canonical spoken equivalents.
Traditional TN systems can exhibit high accuracy, but involve substantial
engineering effort, are difficult to scale, and pose challenges to language
coverage, particularly in low-resource settings. We propose PolyNorm, a
prompt-based approach to TN using Large Language Models (LLMs), aiming to
reduce the reliance on manually crafted rules and enable broader linguistic
applicability with minimal human intervention. Additionally, we present a
language-agnostic pipeline for automatic data curation and evaluation, designed
to facilitate scalable experimentation across diverse languages. Experiments
across eight languages show consistent reductions in the word error rate (WER)
compared to a production-grade-based system. To support further research, we
release PolyNorm-Benchmark, a multilingual data set covering a diverse range of
text normalization phenomena.

</details>


### [6] [A Computational Approach to Analyzing Disrupted Language in Schizophrenia: Integrating Surprisal and Coherence Measures](https://arxiv.org/abs/2511.03089)
*Gowtham Premananth,Carol Espy-Wilson*

Main category: cs.CL

TL;DR: 该研究利用计算语言学方法，通过惊讶度（surprisal）和语义连贯性（semantic coherence）两个指标，分析精神分裂症患者与健康对照组在自发语言产出上的差异，并探讨这些语言异常如何随症状严重程度变化。


<details>
  <summary>Details</summary>
Motivation: 精神分裂症常伴随语言紊乱，表现为言语混乱和话语连贯性受损，这些语言异常可能反映潜在的认知障碍，并可作为症状严重程度和诊断的客观指标。因此，有必要通过计算语言学手段量化这些语言特征。

Method: 采用计算语言模型，计算参与者语言产出中的惊讶度和语义连贯性，比较精神分裂症患者与健康对照组之间的差异，并分析这些指标与症状严重程度的关系。

Result: 研究发现精神分裂症患者的语言在惊讶度和语义连贯性方面与健康对照组存在显著差异，且这些语言指标随症状严重程度变化而变化。

Conclusion: 惊讶度和语义连贯性可作为量化精神分裂症语言紊乱的有效计算指标，有助于客观评估症状严重程度并辅助诊断。

Abstract: Language disruptions are one of the well-known effects of schizophrenia
symptoms. They are often manifested as disorganized speech and impaired
discourse coherence. These abnormalities in spontaneous language production
reflect underlying cognitive disturbances and have the potential to serve as
objective markers for symptom severity and diagnosis of schizophrenia. This
study focuses on how these language disruptions can be characterized in terms
of two computational linguistic measures: surprisal and semantic coherence. By
computing surprisal and semantic coherence of language using computational
models, this study investigates how they differ between subjects with
schizophrenia and healthy controls. Furthermore, this study provides further
insight into how language disruptions in terms of these linguistic measures
change with varying degrees of schizophrenia symptom severity.

</details>


### [7] [CARMA: Comprehensive Automatically-annotated Reddit Mental Health Dataset for Arabic](https://arxiv.org/abs/2511.03102)
*Saad Mankarious,Ayah Zirikly*

Main category: cs.CL

TL;DR: 本文提出了CARMA，这是首个大规模自动标注的阿拉伯语Reddit帖子数据集，涵盖六种心理健康状况及对照组，通过多种模型实验验证了其在阿拉伯语心理健康检测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于文化污名和资源匮乏，阿拉伯语人群的心理健康问题早期识别面临巨大挑战，且现有研究主要集中在英语，阿拉伯语相关数据集稀缺。

Method: 构建并自动标注大规模阿拉伯语Reddit帖子数据集CARMA，涵盖六种心理疾病；进行词汇与语义层面的定性与定量分析，并使用从浅层分类器到大语言模型等多种模型进行分类实验。

Result: CARMA在规模和多样性上超越现有资源；分类实验结果表明该数据集能有效支持阿拉伯语心理健康状况的检测任务。

Conclusion: CARMA为阿拉伯语等代表性不足语言的心理健康检测研究提供了重要基础，展示了在该领域推进自动检测的可行性与前景。

Abstract: Mental health disorders affect millions worldwide, yet early detection
remains a major challenge, particularly for Arabic-speaking populations where
resources are limited and mental health discourse is often discouraged due to
cultural stigma. While substantial research has focused on English-language
mental health detection, Arabic remains significantly underexplored, partly due
to the scarcity of annotated datasets. We present CARMA, the first
automatically annotated large-scale dataset of Arabic Reddit posts. The dataset
encompasses six mental health conditions, such as Anxiety, Autism, and
Depression, and a control group. CARMA surpasses existing resources in both
scale and diversity. We conduct qualitative and quantitative analyses of
lexical and semantic differences between users, providing insights into the
linguistic markers of specific mental health conditions. To demonstrate the
dataset's potential for further mental health analysis, we perform
classification experiments using a range of models, from shallow classifiers to
large language models. Our results highlight the promise of advancing mental
health detection in underrepresented languages such as Arabic.

</details>


### [8] [Control Barrier Function for Aligning Large Language Models](https://arxiv.org/abs/2511.03121)
*Yuya Miyaoka,Masaki Inoue*

Main category: cs.CL

TL;DR: 本文提出了一种基于控制屏障函数（CBF）的附加型安全过滤框架，用于在不微调基线大语言模型的前提下实现对齐，确保生成用户期望的正面文本。


<details>
  <summary>Details</summary>
Motivation: 为了解决大语言模型生成内容可能不符合用户期望的问题，作者希望在不修改或微调原始模型的情况下，通过外部机制实现对生成文本的安全对齐。

Method: 该方法利用控制屏障函数（CBF）构建一个安全过滤器，作用于基线大语言模型预测的token上，从而干预生成过程；该过滤器为附加型，可直接结合任意评估模型进行设计。

Result: 该框架在开源语言模型上成功实现了正面文本的生成，验证了其无需微调即可有效对齐模型输出的能力。

Conclusion: 基于CBF的安全过滤框架提供了一种高效、灵活且无需微调的对齐策略，适用于多种对齐目标，并具备良好的可扩展性。

Abstract: This paper proposes a control-based framework for aligning large language
models (LLMs) by leveraging a control barrier function (CBF) to ensure
user-desirable text generation. The presented framework applies the CBF safety
filter to the predicted token generated from the baseline LLM, to intervene in
the generated text. The safety filter includes two significant advantages: this
safety filter is an add-on type, allowing it to be used for alignment purposes
without fine-tuning the baseline LLM, and if there is an evaluation model
regarding the desired alignment, it can be directly applied to the filter
design. The overall text-generation system is implemented with open-source
language models, aiming to generate positive text.

</details>


### [9] [MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity](https://arxiv.org/abs/2511.03146)
*Kaiyuan Zhang,Chenghao Yang,Zhoufutu Wen,Sihang Yuan,Qiuyue Wang,Chaoyi Huang,Guosheng Zhu,He Wang,Huawenyu Lu,Jianing Wen,Jianpeng Jiao,Lishu Luo,Longxiang Liu,Sijin Wu,Xiaolei Zhu,Xuanliang Zhang,Ge Zhang,Yi Lin,Guang Shi,Chaoyou Fu,Wenhao Huang*

Main category: cs.CL

TL;DR: 本文提出了MME-CC，一个专注于视觉认知能力的多模态评估基准，涵盖空间、几何和知识推理三类任务，对16个主流多模态大模型进行了系统评测，发现当前模型在空间与几何推理方面表现较弱，并揭示了常见错误模式及思维链的三阶段结构。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（MLLMs）的多模态评测基准要么过度侧重文本推理，要么未能系统衡量以视觉为中心的认知行为，导致模型认知能力评估不足。

Method: 构建MME-CC基准，将11个代表性推理任务划分为空间、几何和知识三类，并对16个主流MLLMs进行细粒度评估；同时分析错误模式和Chain-of-Thought推理过程。

Result: 闭源模型整体领先（如Gemini-2.5-Pro得分42.66），但空间与几何推理普遍薄弱（≤30%）；常见错误包括方向判断错误、跨视角身份一致性差、难以遵循反事实指令；思维链主要依赖视觉信息提取，遵循“提取→推理→验证”三阶段流程。

Conclusion: 应将MLLMs的认知能力置于评估与模型设计的核心位置，MME-CC为此提供了系统性工具和实证基础。

Abstract: As reasoning models scale rapidly, the essential role of multimodality in
human cognition has come into sharp relief, driving a growing need to probe
vision-centric cognitive behaviors. Yet, existing multimodal benchmarks either
overemphasize textual reasoning or fall short of systematically capturing
vision-centric cognitive behaviors, leaving the cognitive capacity of MLLMs
insufficiently assessed. To address this limitation, we introduce MME-CC
(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-grounded
benchmark that organizes 11 representative reasoning tasks into three
fundamental categories of visual information: spatial, geometric, and
knowledge-based reasoning, and provides fine-grained analyses of MLLMs'
cognitive capacity across these dimensions. Based on MME-CC, we conduct
extensive experiments over 16 representative MLLMs. Our study reveals that
closed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.
30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak
(less than or equal to 30%). We further identify common error patterns,
including orientation mistakes, fragile cross-view identity persistence, and
poor adherence to counterfactual instructions, and observe that
Chain-of-Thought typically follows a three-stage process (extract -> reason ->
verify) with heavy reliance on visual extraction. We hope this work catalyzes a
shift toward treating the cognitive capacity of MLLMs as central to both
evaluation and model design.

</details>


### [10] [Who Sees the Risk? Stakeholder Conflicts and Explanatory Policies in LLM-based Risk Assessment](https://arxiv.org/abs/2511.03152)
*Srishti Yadav,Jasmina Gajcin,Erik Miehling,Elizabeth Daly*

Main category: cs.CL

TL;DR: 本文提出一个基于大语言模型（LLM）的框架，用于从不同利益相关者的视角评估AI系统风险，结合Risk Atlas Nexus和GloVE解释方法生成可解释的风险政策，并通过交互式可视化揭示利益相关者之间的风险认知冲突。


<details>
  <summary>Details</summary>
Motivation: AI系统的负责任部署需要理解不同利益相关者对风险的感知差异，而现有方法缺乏对这些主观视角的整合与解释。

Method: 利用大语言模型作为“裁判”，结合Risk Atlas Nexus和GloVE解释方法，构建一个能生成利益相关者特定、可解释风险政策的评估框架，并开发交互式可视化工具展示冲突成因。

Result: 在医疗AI、自动驾驶和欺诈检测三个真实场景中验证了该方法，结果表明不同利益相关者的视角显著影响风险感知与冲突模式。

Conclusion: 强调在LLM驱动的风险评估中纳入利益相关者视角的重要性，以提升透明度、可解释性，并推动以人为本的AI治理。

Abstract: Understanding how different stakeholders perceive risks in AI systems is
essential for their responsible deployment. This paper presents a framework for
stakeholder-grounded risk assessment by using LLMs, acting as judges to predict
and explain risks. Using the Risk Atlas Nexus and GloVE explanation method, our
framework generates stakeholder-specific, interpretable policies that shows how
different stakeholders agree or disagree about the same risks. We demonstrate
our method using three real-world AI use cases of medical AI, autonomous
vehicles, and fraud detection domain. We further propose an interactive
visualization that reveals how and why conflicts emerge across stakeholder
perspectives, enhancing transparency in conflict reasoning. Our results show
that stakeholder perspectives significantly influence risk perception and
conflict patterns. Our work emphasizes the importance of these
stakeholder-aware explanations needed to make LLM-based evaluations more
transparent, interpretable, and aligned with human-centered AI governance
goals.

</details>


### [11] [Measuring Aleatoric and Epistemic Uncertainty in LLMs: Empirical Evaluation on ID and OOD QA Tasks](https://arxiv.org/abs/2511.03166)
*Kevin Wang,Subre Abdoul Moktar,Jia Li,Kangshuo Li,Feng Chen*

Main category: cs.CL

TL;DR: 本文对12种不确定性估计（UE）方法在大语言模型（LLM）问答任务中的表现进行了全面实证研究，评估其在分布内（ID）和分布外（OOD）数据上的鲁棒性与有效性。结果表明，基于信息的方法在ID场景中表现优异，而基于密度的方法和P(True)指标在OOD场景中更优，语义一致性方法则在各类设置中表现稳定。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型输出的可信度至关重要，而不确定性估计（UE）是实现这一目标的关键手段。现有UE方法在不同场景下的有效性尚不明确，因此需要系统评估其在ID和OOD数据上的表现。

Method: 作者在问答任务中采用12种不同的UE方法，结合四种生成质量指标（包括来自LLM评判者的LLMScore），分别在ID和OOD数据集上评估这些方法对模型输出的不确定性估计能力。

Result: 研究发现：基于信息的方法（利用token和序列概率）在ID设置中表现突出；密度方法和P(True)指标在OOD情境下更有效；语义一致性方法在不同数据集和评估指标下均表现稳健。

Conclusion: 不同类型的UE方法各有优势，应根据具体应用场景（如ID或OOD）选择合适的方法。该研究为实际应用中如何选择和部署UE策略提供了实证依据。

Abstract: Large Language Models (LLMs) have become increasingly pervasive, finding
applications across many industries and disciplines. Ensuring the
trustworthiness of LLM outputs is paramount, where Uncertainty Estimation (UE)
plays a key role. In this work, a comprehensive empirical study is conducted to
examine the robustness and effectiveness of diverse UE measures regarding
aleatoric and epistemic uncertainty in LLMs. It involves twelve different UE
methods and four generation quality metrics including LLMScore from LLM
criticizers to evaluate the uncertainty of LLM-generated answers in
Question-Answering (QA) tasks on both in-distribution (ID) and
out-of-distribution (OOD) datasets. Our analysis reveals that information-based
methods, which leverage token and sequence probabilities, perform exceptionally
well in ID settings due to their alignment with the model's understanding of
the data. Conversely, density-based methods and the P(True) metric exhibit
superior performance in OOD contexts, highlighting their effectiveness in
capturing the model's epistemic uncertainty. Semantic consistency methods,
which assess variability in generated answers, show reliable performance across
different datasets and generation metrics. These methods generally perform well
but may not be optimal for every situation.

</details>


### [12] [BengaliMoralBench: A Benchmark for Auditing Moral Reasoning in Large Language Models within Bengali Language and Culture](https://arxiv.org/abs/2511.03180)
*Shahriyar Zaman Ridoy,Azmine Toushik Wasi,Koushik Ahamed Tonmoy*

Main category: cs.CL

TL;DR: 本文提出了首个面向孟加拉语的大规模伦理基准 BengaliMoralBench，涵盖五个道德领域和50个文化相关子主题，并通过原生使用者共识标注，评估了主流多语言大模型在文化契合度、常识推理和道德公平性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有伦理基准主要以英语为中心，受西方框架主导，忽视了南亚地区（尤其是拥有2.85亿使用者的孟加拉语）的文化细微差异，难以支持本地化AI系统的负责任部署。

Method: 构建包含五个道德领域（日常活动、习惯、育儿、家庭关系、宗教活动）的 BengaliMoralBench 基准，由母语者基于美德伦理、常识伦理和正义伦理三重视角进行标注，并对 Llama、Gemma、Qwen 和 DeepSeek 等多语言大模型进行零样本系统评估。

Result: 模型在该基准上的准确率差异显著（50–91%），定性分析显示其在文化理解、常识推理和道德公平性方面普遍存在不足。

Conclusion: BengaliMoralBench 为低资源多语言环境（如孟加拉国）中伦理对齐的 AI 系统提供了评估基础，有助于推动文化适配且负责任的本地化人工智能发展。

Abstract: As multilingual Large Language Models (LLMs) gain traction across South Asia,
their alignment with local ethical norms, particularly for Bengali, which is
spoken by over 285 million people and ranked 6th globally, remains
underexplored. Existing ethics benchmarks are largely English-centric and
shaped by Western frameworks, overlooking cultural nuances critical for
real-world deployment. To address this, we introduce BengaliMoralBench, the
first large-scale ethics benchmark for the Bengali language and socio-cultural
contexts. It covers five moral domains, Daily Activities, Habits, Parenting,
Family Relationships, and Religious Activities, subdivided into 50 culturally
relevant subtopics. Each scenario is annotated via native-speaker consensus
using three ethical lenses: Virtue, Commonsense, and Justice ethics. We conduct
systematic zero-shot evaluation of prominent multilingual LLMs, including
Llama, Gemma, Qwen, and DeepSeek, using a unified prompting protocol and
standard metrics. Performance varies widely (50-91% accuracy), with qualitative
analysis revealing consistent weaknesses in cultural grounding, commonsense
reasoning, and moral fairness. BengaliMoralBench provides a foundation for
responsible localization, enabling culturally aligned evaluation and supporting
the deployment of ethically robust AI in diverse, low-resource multilingual
settings such as Bangladesh.

</details>


### [13] [LGM: Enhancing Large Language Models with Conceptual Meta-Relations and Iterative Retrieval](https://arxiv.org/abs/2511.03214)
*Wenchang Lei,Ping Zou,Yue Wang,Feng Sun,Lei Zhao*

Main category: cs.CL

TL;DR: 本文提出语言图模型（LGM），通过从自然语言中提取继承、别名和组成等元关系，并结合反思机制验证这些关系，利用概念迭代检索算法动态增强大语言模型对概念的理解，从而在不依赖扩展上下文窗口的情况下处理任意长度文本，并在标准基准上优于现有RAG方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽具备强大的语义理解能力，但在用户指令包含模糊或概念错位的术语时表现不佳，因此需要一种能提升概念清晰度的方法。

Method: 提出语言图模型（LGM），从自然语言中提取元关系（继承、别名、组成），并通过反思机制验证这些关系；再利用概念迭代检索算法将相关关系与描述动态提供给大语言模型。

Result: 在标准基准测试中，LGM持续优于现有的检索增强生成（RAG）基线方法。

Conclusion: LGM通过引入元关系和动态检索机制，有效提升了大语言模型对模糊或错位概念的理解能力，并克服了传统RAG方法对上下文长度的限制。

Abstract: Large language models (LLMs) exhibit strong semantic understanding, yet
struggle when user instructions involve ambiguous or conceptually misaligned
terms. We propose the Language Graph Model (LGM) to enhance conceptual clarity
by extracting meta-relations-inheritance, alias, and composition-from natural
language. The model further employs a reflection mechanism to validate these
meta-relations. Leveraging a Concept Iterative Retrieval Algorithm, these
relations and related descriptions are dynamically supplied to the LLM,
improving its ability to interpret concepts and generate accurate responses.
Unlike conventional Retrieval-Augmented Generation (RAG) approaches that rely
on extended context windows, our method enables large language models to
process texts of any length without the need for truncation. Experiments on
standard benchmarks demonstrate that the LGM consistently outperforms existing
RAG baselines.

</details>


### [14] [Beyond Ranked Lists: The SARAL Framework for Cross-Lingual Document Set Retrieval](https://arxiv.org/abs/2511.03228)
*Shantanu Agarwal,Joel Barry,Elizabeth Boschee,Scott Miller*

Main category: cs.CL

TL;DR: ISI的SARAL系统在IARPA的MATERIAL项目中提出了一种新颖的跨语言信息检索方法，重点在于检索与查询相关的文档集合而非仅排序列表，并在Phase-3评估中于六项条件中的五项表现优于其他团队，涵盖波斯语、哈萨克语和格鲁吉亚语三种语言。


<details>
  <summary>Details</summary>
Motivation: 提升跨语言信息检索（CLIR）性能，特别是在多语言环境下有效检索与查询相关的文档集合。

Method: 开发一种适用于检索查询相关文档集的新方法，强调集合而非传统排序列表，应用于MATERIAL项目中的SARAL系统。

Result: 在MATERIAL第三阶段评估中，SARAL在六种评估条件中的五种上超越其他团队，覆盖波斯语、哈萨克语和格鲁吉亚语。

Conclusion: 所提出的方法在多语言跨语言信息检索任务中表现出色，验证了其在实际应用中的有效性与先进性。

Abstract: Machine Translation for English Retrieval of Information in Any Language
(MATERIAL) is an IARPA initiative targeted to advance the state of
cross-lingual information retrieval (CLIR). This report provides a detailed
description of Information Sciences Institute's (ISI's) Summarization and
domain-Adaptive Retrieval Across Language's (SARAL's) effort for MATERIAL.
Specifically, we outline our team's novel approach to handle CLIR with emphasis
in developing an approach amenable to retrieve a query-relevant document
\textit{set}, and not just a ranked document-list. In MATERIAL's Phase-3
evaluations, SARAL exceeded the performance of other teams in five out of six
evaluation conditions spanning three different languages (Farsi, Kazakh, and
Georgian).

</details>


### [15] [IndicSuperTokenizer: An Optimized Tokenizer for Indic Multilingual LLMs](https://arxiv.org/abs/2511.03237)
*Souvik Rana,Arul Menezes,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 本文提出了IndicSuperTokenizer，一种结合子词与多词切分及语言特定预切分的新型分词器，在印度多语言大模型中显著提升了分词效率和推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有子词方法（如BPE）在多语言场景下效果尚未充分探索，尤其面对印度语言多样性和形态丰富性时，缺乏高效且语言对齐的分词方案。

Method: 提出IndicSuperTokenizer，融合子词与多词切分策略，并引入语言特定的预切分机制；通过调整训练数据规模、词表大小、合并策略和预切分方式进行了系统消融实验。

Result: 在英语、22种印度语言和代码数据上评估，平均fertility score比LLaMA4提升39.5%，比当前最优Sutra提升18%；推理吞吐量相比LLaMA4提高44%，同时在英语和印度语言基准上保持相当性能。

Conclusion: IndicSuperTokenizer通过更符合语言特性的分词设计，在多语言大模型中实现了更优的分词效率与推理性能，验证了其设计选择的鲁棒性。

Abstract: Tokenizers play a crucial role in determining the performance, training
efficiency, and the inference cost of Large Language Models (LLMs). Designing
effective tokenizers for multilingual LLMs is particularly challenging due to
diverse scripts and rich morphological variation. While subword methods such as
Byte Pair Encoding (BPE) are widely adopted, their effectiveness in
multilingual settings remains underexplored. We present IndicSuperTokenizer, a
tokenizer for Indic multilingual LLMs, that combines both subword and
multi-word tokenization, along with language-specific pre-tokenization, leading
to more linguistically aligned tokens and achieving a new state-of-the-art in
fertility score. Evaluated across English, 22 Indian languages and code data,
our tokenizer improves the average fertility score by 39.5% over LLaMA4 and by
18% over Sutra (the current best). This translates to 44% improvement in
inference throughput over LLaMA4 while maintaining comparable performance on
English and Indic benchmarks. We also present detailed ablations across
tokenizer training data size, vocabulary size, merging techniques, and
pre-tokenization strategies, demonstrating the robustness of our design
choices.

</details>


### [16] [Comparing the Performance of LLMs in RAG-based Question-Answering: A Case Study in Computer Science Literature](https://arxiv.org/abs/2511.03261)
*Ranul Dayarathne,Uvini Ranaweera,Upeksha Ganegoda*

Main category: cs.CL

TL;DR: 该研究比较了四种开源大语言模型（Mistral-7b-instruct、LLaMa2-7b-chat、Falcon-7b-instruct、Orca-mini-v3-7b）与GPT-3.5在结合检索增强生成（RAG）技术下对计算机科学文献问答任务的表现。结果表明，GPT-3.5整体表现最佳，而Mistral-7b-instruct在开源模型中表现最优；Orca-mini-v3-7b响应延迟最低，LLaMa2-7b-chat延迟最高。


<details>
  <summary>Details</summary>
Motivation: 随着RAG技术的发展，其与大语言模型（LLMs）结合用于减少生成幻觉并提升问答性能的应用日益广泛。因此，有必要系统评估不同LLMs在RAG支持下的问答能力，特别是在专业领域如计算机科学中的表现。

Method: 研究选取四个开源LLMs和GPT-3.5，在计算机科学文献数据集上结合RAG进行问答任务。采用准确率和精确率评估二元问题，对长答案则通过人类专家排名、Google Gemini模型排名及余弦相似度进行评估，并记录各模型的平均响应延迟。

Result: GPT-3.5在RAG支持下在二元和长答案问题上均表现最佳；开源模型中，Mistral-7b-instruct综合表现最优；Orca-mini-v3-7b具有最低平均延迟，而LLaMa2-7b-chat延迟最高。

Conclusion: 研究表明，开源大语言模型在适当基础设施支持下，可在RAG增强问答任务中与GPT-3.5等闭源先进模型相媲美，尤其Mistral-7b-instruct展现出强大潜力。

Abstract: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to
enhance the capabilities of Generative AI models by reducing hallucination.
Thus, the increasing prominence of RAG alongside Large Language Models (LLMs)
has sparked interest in comparing the performance of different LLMs in
question-answering (QA) in diverse domains. This study compares the performance
of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat,
Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA
tasks within the computer science literature leveraging RAG support. Evaluation
metrics employed in the study include accuracy and precision for binary
questions and ranking by a human expert, ranking by Google's AI model Gemini,
alongside cosine similarity for long-answer questions. GPT-3.5, when paired
with RAG, effectively answers binary and long-answer questions, reaffirming its
status as an advanced LLM. Regarding open-source LLMs, Mistral AI's
Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary
and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b
reports the shortest average latency in generating responses, whereas
LLaMa2-7b-chat by Meta reports the highest average latency. This research
underscores the fact that open-source LLMs, too, can go hand in hand with
proprietary models like GPT-3.5 with better infrastructure.

</details>


### [17] [SCALE: Upscaled Continual Learning of Large Language Models](https://arxiv.org/abs/2511.03270)
*Jin-woo Lee,Junhwa Choi,Bongkyu Hwang,Jinho Choo,Bogun Kim,JeongSeon Yi,Joonseok Lee,DongYoung Jung,Jaeseon Park,Kyoungwon Park,Suk-hoon Jung*

Main category: cs.CL

TL;DR: 该论文提出SCALE架构，通过在冻结预训练参数的前提下对线性模块进行轻量级宽度扩展，在持续预训练中实现知识获取与遗忘抑制的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的持续预训练进展更多依赖于结构扩展而非单纯参数量增加，因此需要一种既能保留原始模型功能又能有效学习新知识的方法。

Method: 提出SCALE架构，基于“持久保留”（冻结并初始化保留原始行为）和“协同适应”（选择性训练扩展组件以最小干扰获取新知识）两大原则，并具体实现为SCALE-Preserve、SCALE-Adapt和带路由机制的SCALE-Route三种变体。

Result: 在合成传记基准和韩语语料上的持续预训练实验表明，SCALE显著减轻了灾难性遗忘，同时在新任务上取得有竞争力的性能，实现了稳定性与可塑性的良好权衡。

Conclusion: SCALE通过结构化宽度扩展有效解决了持续预训练中的稳定性-可塑性困境，为大模型高效增量学习提供了新思路。

Abstract: We revisit continual pre-training for large language models and argue that
progress now depends more on scaling the right structure than on scaling
parameters alone. We introduce SCALE, a width upscaling architecture that
inserts lightweight expansion into linear modules while freezing all
pre-trained parameters. This preserves the residual and attention topologies
and increases capacity without perturbing the base model's original
functionality. SCALE is guided by two principles: Persistent Preservation,
which maintains the base model's behavior via preservation-oriented
initialization and freezing of the pre-trained weights, and Collaborative
Adaptation, which selectively trains a subset of expansion components to
acquire new knowledge with minimal interference. We instantiate these ideas as
SCALE-Preserve (preservation-first), SCALE-Adapt (adaptation-first), and
SCALE-Route, an optional routing extension that performs token-level routing
between preservation and adaptation heads. On a controlled synthetic biography
benchmark, SCALE mitigates the severe forgetting observed with depth expansion
while still acquiring new knowledge. In continual pre-training on a Korean
corpus, SCALE variants achieve less forgetting on English evaluations and
competitive gains on Korean benchmarks, with these variants offering the best
overall stability-plasticity trade-off. Accompanying analysis clarifies when
preservation provably holds and why the interplay between preservation and
adaptation stabilizes optimization compared to standard continual learning
setups.

</details>


### [18] [Benchmarking the Thinking Mode of Multimodal Large Language Models in Clinical Tasks](https://arxiv.org/abs/2511.03328)
*Jindong Hong,Tianjie Chen,Lingjie Luo,Chuanyang Zheng,Ting Xu,Haibao Yu,Jianing Qiu,Qianzhong Chen,Suning Huang,Yan Xu,Yong Gui,Yijun He,Jiankai Sun*

Main category: cs.CL

TL;DR: 本文评估了两种主流多模态大语言模型（Seed1.5-VL 和 Gemini-2.5-Flash）在“思考模式”下的临床任务表现，发现该模式对多数医疗视觉任务的性能提升有限，尤其在开放式医学问答和图像解读等复杂任务上表现仍不理想。


<details>
  <summary>Details</summary>
Motivation: 随着具备“思考模式”的双状态多模态大语言模型（MLLMs）迅速发展与应用，有必要系统评估其增强推理能力在临床任务中的实际效果与可靠性。

Method: 在VQA-RAD和ROCOv2数据集上，对Seed1.5-VL和Gemini-2.5-Flash两个模型在四种视觉医学任务中分别启用和禁用“思考模式”进行性能对比评估。

Result: 启用“思考模式”在大多数任务中仅带来微弱性能提升；在开放式医学视觉问答和医学图像解释等复杂任务上，模型表现仍不理想。

Conclusion: 当前“思考模式”对医学任务帮助有限，未来需引入领域特定的医学数据及更先进的医学知识融合方法以提升模型在临床场景中的能力。

Abstract: A recent advancement in Multimodal Large Language Models (MLLMs) research is
the emergence of "reasoning MLLMs" that offer explicit control over their
internal thinking processes (normally referred as the "thinking mode")
alongside the standard "non-thinking mode". This capability allows these models
to engage in a step-by-step process of internal deliberation before generating
a final response. With the rapid transition to and adoption of these
"dual-state" MLLMs, this work rigorously evaluated how the enhanced reasoning
processes of these MLLMs impact model performance and reliability in clinical
tasks. This paper evaluates the active "thinking mode" capabilities of two
leading MLLMs, Seed1.5-VL and Gemini-2.5-Flash, for medical applications. We
assessed their performance on four visual medical tasks using VQA-RAD and
ROCOv2 datasets. Our findings reveal that the improvement from activating the
thinking mode remains marginal compared to the standard non-thinking mode for
the majority of the tasks. Their performance on complex medical tasks such as
open-ended VQA and medical image interpretation remains suboptimal,
highlighting the need for domain-specific medical data and more advanced
methods for medical knowledge integration.

</details>


### [19] [Generative Artificial Intelligence in Bioinformatics: A Systematic Review of Models, Applications, and Methodological Advances](https://arxiv.org/abs/2511.03354)
*Riasad Alvi,Sayeem Been Zaman,Wasimul Karim,Arefin Ittesafun Abian,Mohaimenul Azam Khan Raiaan,Saddam Mukta,Md Rafi Ur Rashid,Md Rafiqul Islam,Yakub Sebastian,Sami Azam*

Main category: cs.CL

TL;DR: 本文系统综述了生成式人工智能（GenAI）在生物信息学中的应用，围绕六个研究问题评估其方法创新、预测性能与领域适配性，并总结了当前优势、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 为系统识别和评估生成式人工智能在基因组学、蛋白质组学、转录组学、结构生物学和药物发现等生物信息学子领域的快速发展，明确其方法论进展、预测能力提升和专业化策略的有效性。

Method: 基于系统综述与元分析的报告规范，提出六个研究问题（RQs），对GenAI在生物信息学中的应用进行分类评估，涵盖模型架构、数据集、性能指标及局限性等方面。

Result: RQ1显示GenAI在序列分析、分子设计和整合建模中优于传统方法；RQ2表明专用模型架构因针对性预训练而表现更佳；RQ3强调其在分子分析与数据整合中提升准确性；RQ4验证其在结构建模、功能预测和合成数据生成上的进步；RQ5指出可扩展性不足和数据偏差是主要限制；RQ6确认UniProtKB、ProteinNet12、CELLxGENE、GTEx、PubMedQA和OMIM等数据集对模型训练至关重要。

Conclusion: 生成式人工智能在生物信息学中展现出显著潜力，尤其在专用模型设计与多源数据整合方面，但需解决数据偏差、可扩展性及生物学合理性等问题以推动稳健发展。

Abstract: Generative artificial intelligence (GenAI) has become a transformative
approach in bioinformatics that often enables advancements in genomics,
proteomics, transcriptomics, structural biology, and drug discovery. To
systematically identify and evaluate these growing developments, this review
proposed six research questions (RQs), according to the preferred reporting
items for systematic reviews and meta-analysis methods. The objective is to
evaluate impactful GenAI strategies in methodological advancement, predictive
performance, and specialization, and to identify promising approaches for
advanced modeling, data-intensive discovery, and integrative biological
analysis. RQ1 highlights diverse applications across multiple bioinformatics
subfields (sequence analysis, molecular design, and integrative data modeling),
which demonstrate superior performance over traditional methods through pattern
recognition and output generation. RQ2 reveals that adapted specialized model
architectures outperformed general-purpose models, an advantage attributed to
targeted pretraining and context-aware strategies. RQ3 identifies significant
benefits in the bioinformatics domains, focusing on molecular analysis and data
integration, which improves accuracy and reduces errors in complex analysis.
RQ4 indicates improvements in structural modeling, functional prediction, and
synthetic data generation, validated by established benchmarks. RQ5 suggests
the main constraints, such as the lack of scalability and biases in data that
impact generalizability, and proposes future directions focused on robust
evaluation and biologically grounded modeling. RQ6 examines that molecular
datasets (such as UniProtKB and ProteinNet12), cellular datasets (such as
CELLxGENE and GTEx) and textual resources (such as PubMedQA and OMIM) broadly
support the training and generalization of GenAI models.

</details>


### [20] [Silenced Biases: The Dark Side LLMs Learned to Refuse](https://arxiv.org/abs/2511.03369)
*Rom Himelstein,Amit LeVi,Brit Youngmann,Yaniv Nemcovsky,Avi Mendelson*

Main category: cs.CL

TL;DR: 该论文提出“沉默偏见”概念，指出当前安全对齐的大语言模型在公平性评估中存在表面合规但内在仍含偏见的问题，并引入“沉默偏见基准（SBB）”通过激活操控揭示这些隐藏偏见。


<details>
  <summary>Details</summary>
Motivation: 现有公平性评估方法多依赖问答形式，将模型拒绝回答误判为公平表现，忽视了模型潜在空间中被安全对齐掩盖的不公平偏好。

Method: 提出沉默偏见基准（SBB），利用激活操控（activation steering）降低模型在问答中的拒绝率，从而暴露其潜在偏见；该方法支持灵活扩展至新人群和主题。

Result: 在多个大语言模型上的实验表明，模型的直接回应与其内在公平性存在显著差异，揭示了安全对齐可能掩盖真实偏见的问题。

Conclusion: SBB提供了一个更深入、可扩展的公平性评估框架，有助于推动超越对齐训练表层效果的公平模型与工具的发展。

Abstract: Safety-aligned large language models (LLMs) are becoming increasingly
widespread, especially in sensitive applications where fairness is essential
and biased outputs can cause significant harm. However, evaluating the fairness
of models is a complex challenge, and approaches that do so typically utilize
standard question-answer (QA) styled schemes. Such methods often overlook
deeper issues by interpreting the model's refusal responses as positive
fairness measurements, which creates a false sense of fairness. In this work,
we introduce the concept of silenced biases, which are unfair preferences
encoded within models' latent space and are effectively concealed by
safety-alignment. Previous approaches that considered similar indirect biases
often relied on prompt manipulation or handcrafted implicit queries, which
present limited scalability and risk contaminating the evaluation process with
additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to
uncover these biases by employing activation steering to reduce model refusals
during QA. SBB supports easy expansion to new demographic groups and subjects,
presenting a fairness evaluation framework that encourages the future
development of fair models and tools beyond the masking effects of alignment
training. We demonstrate our approach over multiple LLMs, where our findings
expose an alarming distinction between models' direct responses and their
underlying fairness issues.

</details>


### [21] [LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced Logical Reasoning](https://arxiv.org/abs/2511.03372)
*Shenghao Li*

Main category: cs.CL

TL;DR: 提出LFC-DA方法，通过符号逻辑控制的数据增强流程，在保持逻辑严谨性的同时提升逻辑推理数据的多样性，并显著提高预训练模型在逻辑推理任务上的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有复杂逻辑数据增强方法要么依赖昂贵的人工标注，要么直接使用大语言模型生成缺乏可解释性且逻辑同质的样本。

Method: 将自然语言逻辑文本映射为命题表达式，构建紧凑规则库，并通过有界状态空间搜索系统地发现有效公式，再将其转回自然语言问题。

Result: 在ReClor和LogiQA数据集上实验表明，该方法显著提升了预训练模型的逻辑推理准确率。

Conclusion: LFC-DA是一种有效的、结合符号逻辑与大语言模型的逻辑数据增强方法，兼具多样性与逻辑严谨性。

Abstract: For complex logical data augmentation, heavy reliance on human annotation is
costly, whereas direct generation with large language models yields
uninterpretable and logically homogeneous examples. To address this, we present
LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to
propositional expressions, a compact rule library is compiled, and a bounded
state-space search systematically discovers valid formulas that are then
verbalized back into natural-language questions, ensuring both diversity and
logical rigor under propositional logic. Experiments on ReClor and LogiQA show
significant improvements in the logical-reasoning accuracy of pretrained
models, confirming the effectiveness of LFC-DA for LLM-guided logical data
augmentation.

</details>


### [22] [Segmentation Beyond Defaults: Asymmetrical Byte Pair Encoding for Optimal Machine Translation Performance](https://arxiv.org/abs/2511.03383)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: 本文发现，在机器翻译中使用非对称BPE（源语言和目标语言采用不同数量的合并操作）比传统的对称BPE能显著提升性能，尤其在低资源场景下效果更佳。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译研究通常对源语言和目标语言采用相同数量的BPE合并操作（对称BPE），但这种统一策略未必适用于所有语言对和数据规模。作者旨在探索更优的BPE分词策略以提升翻译性能。

Method: 作者系统地评估了不同数据量和语言对下对称与非对称BPE对机器翻译系统性能的影响，测试了多种源/目标语言合并操作数（NMO）组合，并在多个低资源语言对上进行了验证。

Result: 在英语-印地语等低资源语言对（50K–500K句对）上，非对称BPE相比对称BPE带来显著提升（CHRF++平均提高0.7–5.32）。在6个额外语言对的12个系统中，有10个取得统计显著改进。最优配置通常为源语言高NMO（4K–32K）、目标语言低NMO（0.5K–2K）。

Conclusion: 非对称BPE是一种更优的分词策略，尤其适用于低资源机器翻译场景，建议根据语言对和数据规模动态调整源/目标语言的BPE合并操作数。

Abstract: Existing Machine Translation (MT) research often suggests a single, fixed set
of hyperparameters for word segmentation models, symmetric Byte Pair Encoding
(BPE), which applies the same number of merge operations (NMO) to train
tokenizers for both source and target languages. However, we demonstrate that
this uniform approach doesn't guarantee optimal MT performance across different
language pairs and data sizes. This work investigates BPE segmentation recipes
across various data volumes and language pairs to evaluate MT system
performance. We find that utilizing asymmetric BPE, where the source and target
languages have different NMOs, significantly improves results over the
symmetric approach, especially in low-resource settings (50K, 100K, and 500K
sentence pairs). Specifically, asymmetric BPE yield statistically significant
($p<0.05$) average gains of 5.32, 4.46, and 0.7 CHRF++ on English-Hindi in
low-resource setups. We validated this trend across six additional language
pairs (English and Telugu, Shona, Norwegian, Kyrgyz, Hausa, and Inuktitut),
observing statistically significant improvement in 10 out of 12 systems
compared to symmetric BPE. Our findings indicate a high NMO for the source (4K
to 32K) and a low NMO for the target (0.5K to 2K) provides optimal results,
particularly benefiting low-resource MT.

</details>


### [23] [Overcoming the Generalization Limits of SLM Finetuning for Shape-Based Extraction of Datatype and Object Properties](https://arxiv.org/abs/2511.03407)
*Célian Ringwald,Fabien Gandon,Catherine Faron,Franck Michel,Hanna Abi Akl*

Main category: cs.CL

TL;DR: 本文研究小型语言模型（SLMs）在关系抽取中处理数据类型属性和对象属性的能力，发现罕见属性的长尾分布是主要瓶颈，并通过实验验证构建满足属性出现次数阈值的训练集是最有效的策略。


<details>
  <summary>Details</summary>
Motivation: 现有小型语言模型在关系抽取任务中主要关注常见数据类型属性，缺乏对完整RDF图（包括对象属性）的有效支持，且罕见属性因长尾分布难以被充分学习。

Method: 作者评估了分层采样、加权损失、数据集扩展和基于模板的合成数据增强等多种策略，重点比较不同方法在不平衡目标属性上的表现。

Result: 实验表明，最有效的方法是构建一个训练集，其中每个属性的出现次数超过预设阈值，从而在各类属性上实现均衡性能。

Conclusion: 该研究为训练具有形状感知能力的小型语言模型提供了实用指导，并指出语义关系抽取领域未来的研究方向；相关数据集、结果与代码已公开以促进可复现性。

Abstract: Small language models (SLMs) have shown promises for relation extraction (RE)
when extracting RDF triples guided by SHACL shapes focused on common datatype
properties. This paper investigates how SLMs handle both datatype and object
properties for a complete RDF graph extraction. We show that the key bottleneck
is related to long-tail distribution of rare properties. To solve this issue,
we evaluate several strategies: stratified sampling, weighted loss, dataset
scaling, and template-based synthetic data augmentation. We show that the best
strategy to perform equally well over unbalanced target properties is to build
a training set where the number of occurrences of each property exceeds a given
threshold. To enable reproducibility, we publicly released our datasets,
experimental results and code. Our findings offer practical guidance for
training shape-aware SLMs and highlight promising directions for future work in
semantic RE.

</details>


### [24] [Efficient Reasoning via Thought-Training and Thought-Free Inference](https://arxiv.org/abs/2511.03408)
*Canhui Wu,Qiong Cao,Chao Xue,Wei Xi,Xiaodong He*

Main category: cs.CL

TL;DR: 本文提出3TF框架，通过“短到长”的训练策略，使模型在推理时无需显式生成思维链（thought-free），却能隐式执行高质量推理，从而在保持输出简洁的同时提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于思维链（CoT）的方法主要依赖显式推理并在推理阶段生成冗长的中间步骤，虽有压缩方法提升效率，但仍无法摆脱对显式推理的依赖。作者希望探索一种能在推理时输出简洁、但内部仍具备高质量推理能力的新范式。

Method: 提出3TF框架：首先训练一个可在推理与非推理模式间切换的混合模型；然后在带有CoT标注的数据上进一步训练，使其内化结构化推理能力；最终在推理阶段使用非推理模式，强制模型输出简洁结果，不显式生成中间推理步骤。

Result: 在多个推理基准测试中，采用3TF训练的模型在thought-free推理模式下显著优于基线方法，表明模型能够在不显式生成推理链的情况下隐式完成高质量推理。

Conclusion: 3TF证明了高质量推理能力可以被隐式学习和执行，无需依赖显式的逐步推理输出，为高效且高性能的语言模型推理提供了新思路。

Abstract: Recent advances in large language models (LLMs) have leveraged explicit
Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most
existing methods primarily compress verbose reasoning outputs. These
Long-to-Short transformations aim to improve efficiency, but still rely on
explicit reasoning during inference. In this work, we introduce \textbf{3TF}
(\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree
inference), a framework for efficient reasoning that takes a Short-to-Long
perspective. We first train a hybrid model that can operate in both reasoning
and non-reasoning modes, and then further train it on CoT-annotated data to
internalize structured reasoning, while enforcing concise, thought-free outputs
at inference time using the no-reasoning mode. Unlike compression-based
approaches, 3TF improves the reasoning quality of non-reasoning outputs,
enabling models to perform rich internal reasoning implicitly while keeping
external outputs short. Empirically, 3TF-trained models obtain large
improvements on reasoning benchmarks under thought-free inference,
demonstrating that high quality reasoning can be learned and executed
implicitly without explicit step-by-step generation.

</details>


### [25] [Knowledge-Augmented Question Error Correction for Chinese Question Answer System with QuestionRAG](https://arxiv.org/abs/2511.03410)
*Longpeng Qiu,Ting Li,Shuai Mao,Nan Yang,Xiaohui Yan*

Main category: cs.CL

TL;DR: QuestionRAG 是一个结合外部知识增强与强化学习对齐的框架，用于提升大语言模型在问题纠错任务中的表现，有效缓解误读用户意图和过度改写问题的缺陷。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在问答系统中常因输入错误而给出错误回答，且在纠正问题时容易误解用户意图或过度修改原问题结构。

Method: QuestionRAG 通过引入外部知识（如搜索结果、相关实体）来增强对错误问题的理解，并采用强化学习对齐模型目标，使其专注于精准纠错而非简单改写。

Result: 实验表明，知识增强对理解错误问题至关重要，而基于强化学习的对齐方法显著优于传统的监督微调，在指令遵循和泛化能力上均有提升。

Conclusion: 结合知识增强与强化学习对齐策略，QuestionRAG 充分释放了大语言模型在问题纠错任务中的潜力。

Abstract: Input errors in question-answering (QA) systems often lead to incorrect
responses. Large language models (LLMs) struggle with this task, frequently
failing to interpret user intent (misinterpretation) or unnecessarily altering
the original question's structure (over-correction). We propose QuestionRAG, a
framework that tackles these problems. To address misinterpretation, it
enriches the input with external knowledge (e.g., search results, related
entities). To prevent over-correction, it uses reinforcement learning (RL) to
align the model's objective with precise correction, not just paraphrasing. Our
results demonstrate that knowledge augmentation is critical for understanding
faulty questions. Furthermore, RL-based alignment proves significantly more
effective than traditional supervised fine-tuning (SFT), boosting the model's
ability to follow instructions and generalize. By integrating these two
strategies, QuestionRAG unlocks the full potential of LLMs for the question
correction task.

</details>


### [26] [CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the Biomedical Field](https://arxiv.org/abs/2511.03441)
*Doria Bonzi,Alexandre Guiggi,Frédéric Béchet,Carlos Ramisch,Benoit Favre*

Main category: cs.CL

TL;DR: 本文提出了CareMedEval，一个用于评估大语言模型（LLMs）在生物医学文献批判性评价与推理能力方面的新数据集，包含37篇科学论文中的534个问题。实验表明，即使是最先进的通用和生物医学专用LLM，在该任务上表现仍有限，尤其在研究局限性和统计分析问题上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在生物医学领域批判性推理任务中的可靠性不足，缺乏专门用于评估此类能力的高质量基准数据集。

Method: 构建CareMedEval数据集，源自法国医学生真实考试题目，涵盖37篇科学论文中的534个问题；在不同上下文条件下对主流通用和生物医学专用LLM进行基准测试，评估其批判性阅读与推理能力。

Result: 现有模型在CareMedEval上的精确匹配率未超过0.5；生成中间推理步骤可显著提升性能，但在研究局限性和统计分析类问题上仍表现较差。

Conclusion: CareMedEval为评估LLM在基于科学文献的批判性推理能力方面提供了一个具有挑战性的基准，揭示了当前模型的局限性，并为未来开发自动化批判性评价工具指明方向。

Abstract: Critical appraisal of scientific literature is an essential skill in the
biomedical field. While large language models (LLMs) can offer promising
support in this task, their reliability remains limited, particularly for
critical reasoning in specialized domains. We introduce CareMedEval, an
original dataset designed to evaluate LLMs on biomedical critical appraisal and
reasoning tasks. Derived from authentic exams taken by French medical students,
the dataset contains 534 questions based on 37 scientific articles. Unlike
existing benchmarks, CareMedEval explicitly evaluates critical reading and
reasoning grounded in scientific papers. Benchmarking state-of-the-art
generalist and biomedical-specialized LLMs under various context conditions
reveals the difficulty of the task: open and commercial models fail to exceed
an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens
considerably improves the results. Yet, models remain challenged especially on
questions about study limitations and statistical analysis. CareMedEval
provides a challenging benchmark for grounded reasoning, exposing current LLM
limitations and paving the way for future development of automated support for
critical appraisal.

</details>


### [27] [Kastor: Fine-tuned Small Language Models for Shape-based Active Relation Extraction](https://arxiv.org/abs/2511.03466)
*Ringwald Celian,Gandon Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: Kastor 是一个用于提升小语言模型在特定领域知识库补全与精炼任务中性能的框架，通过改进基于 SHACL 形状的 RDF 模式抽取方法，优化属性组合选择并采用迭代学习策略提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于 SHACL 形状的 RDF 模式抽取方法在知识库补全任务中存在泛化能力有限的问题，难以有效处理复杂属性组合和噪声数据，因此需要一种更高效、鲁棒的框架来提升小语言模型在专业领域中的关系抽取效果。

Method: Kastor 框架将传统单个 SHACL 形状验证任务扩展为评估形状导出的所有可能属性组合，并为每个训练样本选择最优组合；同时引入迭代学习机制，逐步优化含噪声的知识库。

Result: 该方法显著提升了模型的泛化能力和关系抽取性能，能够从有限文本和 RDF 数据中训练出高效模型，并发现新的相关事实。

Conclusion: Kastor 通过重构 SHACL 验证任务和引入迭代学习，有效增强了小语言模型在专业领域知识库构建中的实用性与鲁棒性。

Abstract: RDF pattern-based extraction is a compelling approach for fine-tuning small
language models (SLMs) by focusing a relation extraction task on a specified
SHACL shape. This technique enables the development of efficient models trained
on limited text and RDF data. In this article, we introduce Kastor, a framework
that advances this approach to meet the demands for completing and refining
knowledge bases in specialized domains. Kastor reformulates the traditional
validation task, shifting from single SHACL shape validation to evaluating all
possible combinations of properties derived from the shape. By selecting the
optimal combination for each training example, the framework significantly
enhances model generalization and performance. Additionally, Kastor employs an
iterative learning process to refine noisy knowledge bases, enabling the
creation of robust models capable of uncovering new, relevant facts

</details>


### [28] [BanglaSTEM: A Parallel Corpus for Technical Domain Bangla-English Translation](https://arxiv.org/abs/2511.03498)
*Kazi Reyazul Hasan,Mubasshira Musarrat,A. B. M. Alim Al Islam,Muhammad Abdullah Adnan*

Main category: cs.CL

TL;DR: 本文提出BanglaSTEM数据集和基于T5的翻译模型，以提升孟加拉语技术问题翻译成英语的准确性，从而改善大型语言模型在孟加拉语STEM任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在处理英语技术问题时表现良好，但在处理孟加拉语同类问题时效果不佳；直接翻译存在专业术语误译问题，导致答案错误。

Method: 构建包含5,000对高质量孟加拉语-英语STEM句子的BanglaSTEM数据集，通过人工筛选保留正确技术术语的翻译，并在此基础上训练T5翻译模型。

Result: 所提翻译模型在代码生成和数学问题求解任务中显著提升了技术内容的翻译准确性，使孟加拉语使用者能更有效地利用以英语为主的语言模型。

Conclusion: BanglaSTEM数据集和训练所得翻译模型有效缓解了技术术语误译问题，提升了孟加拉语用户使用英语大模型解决STEM问题的能力，相关资源已公开发布。

Abstract: Large language models work well for technical problem solving in English but
perform poorly when the same questions are asked in Bangla. A simple solution
would be to translate Bangla questions into English first and then use these
models. However, existing Bangla-English translation systems struggle with
technical terms. They often mistranslate specialized vocabulary, which changes
the meaning of the problem and leads to wrong answers. We present BanglaSTEM, a
dataset of 5,000 carefully selected Bangla-English sentence pairs from STEM
fields including computer science, mathematics, physics, chemistry, and
biology. We generated over 12,000 translations using language models and then
used human evaluators to select the highest quality pairs that preserve
technical terminology correctly. We train a T5-based translation model on
BanglaSTEM and test it on two tasks: generating code and solving math problems.
Our results show significant improvements in translation accuracy for technical
content, making it easier for Bangla speakers to use English-focused language
models effectively. Both the BanglaSTEM dataset and the trained translation
model are publicly released at https://huggingface.co/reyazul/BanglaSTEM-T5.

</details>


### [29] [HaluMem: Evaluating Hallucinations in Memory Systems of Agents](https://arxiv.org/abs/2511.03506)
*Ding Chen,Simin Niu,Kehang Li,Peng Liu,Xiangping Zheng,Bo Tang,Xinchi Li,Feiyu Xiong,Zhiyu Li*

Main category: cs.CL

TL;DR: 本文提出了首个面向记忆系统的操作级幻觉评估基准HaluMem，通过记忆提取、更新和问答三个任务，揭示大模型在不同记忆操作阶段的幻觉行为，并构建了包含长上下文和多轮交互的大规模数据集。


<details>
  <summary>Details</summary>
Motivation: 现有对记忆幻觉的评估主要依赖端到端问答，难以定位幻觉在记忆系统中的具体产生阶段，因此需要一个能细粒度评估各操作阶段幻觉行为的新基准。

Method: 作者构建了HaluMem基准，定义三个评估任务（记忆提取、记忆更新、记忆问答），并发布两个用户中心的多轮人机交互数据集HaluMem-Medium和HaluMem-Long，包含约15k记忆点和3.5k多类型问题，上下文长度超1M tokens。

Result: 实证研究表明，现有记忆系统在记忆提取和更新阶段容易产生并累积幻觉，这些错误会传播至问答阶段，影响整体可靠性。

Conclusion: 未来研究应聚焦于开发可解释且受约束的记忆操作机制，以系统性抑制幻觉并提升记忆系统的可靠性。

Abstract: Memory systems are key components that enable AI systems such as LLMs and AI
agents to achieve long-term learning and sustained interaction. However, during
memory storage and retrieval, these systems frequently exhibit memory
hallucinations, including fabrication, errors, conflicts, and omissions.
Existing evaluations of memory hallucinations are primarily end-to-end question
answering, which makes it difficult to localize the operational stage within
the memory system where hallucinations arise. To address this, we introduce the
Hallucination in Memory Benchmark (HaluMem), the first operation level
hallucination evaluation benchmark tailored to memory systems. HaluMem defines
three evaluation tasks (memory extraction, memory updating, and memory question
answering) to comprehensively reveal hallucination behaviors across different
operational stages of interaction. To support evaluation, we construct
user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and
HaluMem-Long. Both include about 15k memory points and 3.5k multi-type
questions. The average dialogue length per user reaches 1.5k and 2.6k turns,
with context lengths exceeding 1M tokens, enabling evaluation of hallucinations
across different context scales and task complexities. Empirical studies based
on HaluMem show that existing memory systems tend to generate and accumulate
hallucinations during the extraction and updating stages, which subsequently
propagate errors to the question answering stage. Future research should focus
on developing interpretable and constrained memory operation mechanisms that
systematically suppress hallucinations and improve memory reliability.

</details>


### [30] [One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction Following with a Benchmark Evolving Framework](https://arxiv.org/abs/2511.03508)
*Qi Jia,Kaiwei Zhang,Xiujie Song,Ye Shen,Xiangyang Zhu,Guangtao Zhai*

Main category: cs.CL

TL;DR: 本文提出了一种可扩展的多轮指令跟随评估框架，通过解耦语言表层形式与用户意图模拟，构建了包含九类约束的动态评测基准EvolIF。实验表明GPT-5在多轮对话中表现最优，平均维持18.54轮对话，鲁棒性达70.31%。


<details>
  <summary>Details</summary>
Motivation: 现有评测基准通常限制对话轮数，易饱和且无法真实反映用户交互体验，难以有效评估大语言模型在多主题、多轮对话中的指令跟随能力。

Method: 提出一个三层机制框架（追踪约束、指令和主题），将语言表层形式与用户意图模拟解耦，支持动态构建具有状态变化和回溯能力的对话评测，并在模拟用户耐心耗尽时终止对话；同时定义了一套衡量交互过程质量的指标，并基于此构建了包含九类约束的EvolIF基准。

Result: GPT-5在EvolIF基准上表现最佳，平均对话轮次达18.54轮，鲁棒性为70.31%，显著优于Gemini-2.5-Pro（高出11.41%），其他模型表现明显落后。

Conclusion: 所提出的评估框架能更真实地模拟用户与大语言模型的多轮交互，有效衡量模型的指令跟随能力；GPT-5在此类任务中展现出当前最强的性能。

Abstract: Understanding how well large language models can follow users' instructions
throughout a dialogue spanning multiple topics is of great importance for
data-intensive conversational applications. Existing benchmarks are often
limited to a fixed number of turns, making them susceptible to saturation and
failing to account for the user's interactive experience. In this work, we
propose an extensible framework for assessing multi-turn instruction-following
ability. At its core, our framework decouples linguistic surface forms from
user intent simulation through a three-layer mechanism that tracks constraints,
instructions, and topics. This framework mimics User-LLM interaction by
enabling the dynamic construction of benchmarks with state changes and
tracebacks, terminating a conversation only when the model exhausts a simulated
user's patience. We define a suite of metrics capturing the quality of the
interaction process. Using this framework, we construct EvolIF, an evolving
instruction-following benchmark incorporating nine distinct constraint types.
Our results indicate that GPT-5 exhibits superior instruction-following
performance. It sustains an average of 18.54 conversational turns and
demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant
margin of 11.41%, while other models lag far behind. All of the data and code
will be made publicly available online.

</details>


### [31] [MultiZebraLogic: A Multilingual Logical Reasoning Benchmark](https://arxiv.org/abs/2511.03553)
*Sofie Helene Bruun,Dan Saattrup Smart*

Main category: cs.CL

TL;DR: 该论文提出了MultiZebraLogic数据集，包含九种日耳曼语言中不同主题、尺寸和干扰项的斑马谜题，用于评估大语言模型在多语言环境下的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以全面衡量大语言模型（LLMs）在多语言环境下的逻辑推理能力，因此需要构建高质量、多语言、难度适中的逻辑推理数据集以支持跨模型比较。

Method: 作者通过生成包含14种线索类型和8种干扰项（无用线索）的斑马谜题，在多种语言、主题和谜题尺寸（2x3 和 4x5）下构建数据集，并测试了GPT-4o mini与o3-mini模型在不同设置下的表现。

Result: 研究发现2x3和4x5尺寸的谜题分别对非推理模型GPT-4o mini和推理模型o3-mini具有足够挑战性；加入5个干扰项会使o3-mini在4x5谜题上的准确率下降15±7%；语言（英语/丹麦语）和主题（通用/本地化）对o3-mini表现无显著影响；线索类型与难度无明显相关性。最终发布了每种语言各128+1024个谜题的MultiZebraLogic数据集及可扩展的生成代码。

Conclusion: MultiZebraLogic是一个适用于多语言逻辑推理评估的高质量、可扩展数据集，有助于更全面地评测大语言模型的推理能力。

Abstract: Measuring the full abilities of large language models (LLMs) requires
benchmarks representing multiple tasks. We aim to create large, high-quality
datasets for comparison of logical reasoning skills across several languages
and of suitable difficulty for LLMs of various reasoning ability. We explore
multiple ways of increasing difficulty. We generate zebra puzzles in multiple
languages, themes, sizes and including 14 different clue types and 8 red
herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are
sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a
reasoning model), respectively. Including 5 red herrings decreases o3-mini
puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5
puzzles are not significantly affected by use of English vs. Danish or the
common houses theme vs. the country-specific smoerrebroed theme. We find no
correlation between difficulty and the selected clue types. Datasets of
128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic
languages for sizes 2x3 and 4x5. We publish code for puzzle generation,
designed for adaptablity into more languages and themes.

</details>


### [32] [AILA--First Experiments with Localist Language Models](https://arxiv.org/abs/2511.03559)
*Joachim Diederich*

Main category: cs.CL

TL;DR: 本文首次实证展示了在Transformer语言模型中实现可控局部性（controllable locality）的方法，通过一个可调的局部性参数λ，在完全局部编码（localist）与分布式表示之间动态插值，无需重新训练模型。实验表明，局部性配置显著降低注意力熵并提升指针保真度，且中间值（如λ=0.6）在可解释性与性能之间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型仅依赖分布式表示，缺乏对表示局部性的控制，难以兼顾可解释性与性能。作者旨在构建一种新型架构，使模型能连续调节局部性程度，以满足对透明性和能力均有要求的应用场景（如受监管领域）的需求。

Method: 提出一种带有可调局部性拨盘参数λ的Transformer架构，在两层Transformer上于WikiText语料库进行实验，系统地将λ从1.0（完全局部）调整至0.0（完全分布），评估不同设置下的注意力熵、指针保真度、困惑度和准确率。

Result: λ=1.0时注意力熵为5.36比特（λ=0.0时为7.18比特），指针保真度更高；λ=0.6时测试困惑度达4.65，准确率达84.7%，在可解释性与性能间取得最优权衡。

Conclusion: 局部性语言模型提供了一个实用框架，通过信息论设计原则和显式惩罚阈值，实现对可解释性-性能谱系的精确数学控制，适用于需要透明性与高性能并存的场景。

Abstract: This paper presents the first empirical demonstration of controllable
locality in transformer language models, a novel architectural framework that
enables continuous control over the degree of representation localization
through a tunable locality dial parameter. Unlike traditional language models
that rely exclusively on distributed representations, our approach allows
dynamic interpolation between highly interpretable localist encodings and
efficient distributed representations without requiring model retraining. We
conducted experiments on the WikiText corpus using a two-layer transformer
architecture, systematically varying the locality parameter {\lambda} across
the full spectrum from 1.0 (fully localist) to 0.0 (fully distributed). Our
results demonstrate that localist configurations achieve dramatically lower
attention entropy, with {\lambda} = 1.0 yielding 5.36 bits compared to 7.18
bits at {\lambda} = 0.0, while maintaining substantially higher pointer
fidelity scores reflecting stronger alignment with rule-specified targets.
Prediction experiments reveal that intermediate locality values optimize the
tradeoff between interpretability and performance, with {\lambda} = 0.6
achieving test perplexity of 4.65 and accuracy of 84.7%. These findings
establish that localist language models provide a practical framework for
applications in regulated domains requiring both transparency and capability,
offering precise mathematical control over the interpretability-performance
spectrum through explicit penalty thresholds and information-theoretic design
principles.

</details>


### [33] [ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for Enhanced Legal Regulation](https://arxiv.org/abs/2511.03563)
*One Octadion,Bondan Sapta Prakoso,Nanang Yudi Setiawan,Novanto Yudistira*

Main category: cs.CL

TL;DR: 本文通过领域微调与检索增强生成（RAG）相结合，提升大语言模型在法律政策制定中的辅助能力。


<details>
  <summary>Details</summary>
Motivation: 为帮助政策制定者更有效地理解、分析和起草法律法规，需提升大语言模型在法律领域的专业能力。

Method: 构建面向法律领域的监督数据集对大语言模型进行微调，并结合检索增强生成（RAG）方法引入外部最新法律知识。

Result: 该方法显著提升了模型在法律研究和法规制定中的有效性，能主动协助政策制定者解读和起草法规。

Conclusion: 结合微调与RAG的大语言模型可成为法律政策制定中高效且实用的辅助工具。

Abstract: In this study, we explore the fine-tuning of Large Language Models (LLMs) to
better support policymakers in their crucial work of understanding, analyzing,
and crafting legal regulations. To equip the model with a deep understanding of
legal texts, we curated a supervised dataset tailored to the specific needs of
the legal domain. Additionally, we integrated the Retrieval-Augmented
Generation (RAG) method, enabling the LLM to access and incorporate up-to-date
legal knowledge from external sources. This combination of fine-tuning and
RAG-based augmentation results in a tool that not only processes legal
information but actively assists policymakers in interpreting regulations and
drafting new ones that align with current needs. The results demonstrate that
this approach can significantly enhance the effectiveness of legal research and
regulation development, offering a valuable resource in the ever-evolving field
of law.

</details>


### [34] [Step-Audio-EditX Technical Report](https://arxiv.org/abs/2511.03601)
*Chao Yan,Boyong Wu,Peng Yang,Pengfei Tan,Guoqiang Hu,Yuxin Zhang,Xiangyu,Zhang,Fei Tian,Xuerui Yang,Xiangyu Zhang,Daxin Jiang,Gang Yu*

Main category: cs.CL

TL;DR: Step-Audio-EditX 是一个开源的基于大语言模型的音频编辑系统，支持情感、说话风格和副语言特征的高表现力迭代编辑，并具备强大的零样本文本到语音（TTS）能力。


<details>
  <summary>Details</summary>
Motivation: 现有音频编辑方法通常依赖嵌入先验或辅助模块，难以实现高表现力与细粒度控制的统一。作者旨在通过仅使用大间隔合成数据的方法，摆脱对表示级解耦的依赖，实现更灵活高效的音频编辑。

Method: 提出 Step-Audio-EditX 模型，利用大间隔合成数据进行训练，不依赖嵌入先验或辅助模块，从而支持迭代式控制与多维度语音表达。

Result: 在情感编辑及其他细粒度控制任务上，Step-Audio-EditX 的性能优于 MiniMax-2.6-hd 和 Doubao-Seed-TTS-2.0。

Conclusion: Step-Audio-EditX 通过大间隔学习策略，在无需传统表示解耦的前提下，实现了高质量、高可控性的音频生成与编辑，为语音合成与编辑提供了新范式。

Abstract: We present Step-Audio-EditX, the first open-source LLM-based audio model
excelling at expressive and iterative audio editing encompassing emotion,
speaking style, and paralinguistics alongside robust zero-shot text-to-speech
(TTS) capabilities.Our core innovation lies in leveraging only large-margin
synthetic data, which circumvents the need for embedding-based priors or
auxiliary modules. This large-margin learning approach enables both iterative
control and high expressivity across voices, and represents a fundamental pivot
from the conventional focus on representation-level disentanglement. Evaluation
results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and
Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

</details>


### [35] [A systematic review of relation extraction task since the emergence of Transformers](https://arxiv.org/abs/2511.03610)
*Ringwald Celian,Gandon,Fabien,Faron Catherine,Michel Franck,Abi Akl Hanna*

Main category: cs.CL

TL;DR: 本文系统综述了自Transformer模型出现以来的关系抽取（RE）研究，分析了2019至2024年间发表的34篇综述、64个数据集和104个模型，总结了方法进展、基准资源及语义网技术的融合，并指出了当前趋势、局限与开放挑战。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer模型的兴起，关系抽取领域快速发展，亟需系统性梳理现有研究成果，以厘清技术演进脉络、识别关键挑战并为未来研究提供参考。

Method: 采用自动化框架收集并标注相关文献，对2019–2024年间发表的34篇综述、64个数据集和104个模型进行多维度综合分析。

Result: 揭示了关系抽取在方法论、基准资源和语义网技术整合方面的最新进展，明确了当前研究的主要趋势、存在的局限性以及尚未解决的挑战。

Conclusion: 该综述为研究人员和实践者提供了全面的参考，有助于深入理解关系抽取的发展历程，并为未来研究方向提供指导。

Abstract: This article presents a systematic review of relation extraction (RE)
research since the advent of Transformer-based models. Using an automated
framework to collect and annotate publications, we analyze 34 surveys, 64
datasets, and 104 models published between 2019 and 2024. The review highlights
methodological advances, benchmark resources, and the integration of semantic
web technologies. By consolidating results across multiple dimensions, the
study identifies current trends, limitations, and open challenges, offering
researchers and practitioners a comprehensive reference for understanding the
evolution and future directions of RE.

</details>


### [36] [Towards Transparent Stance Detection: A Zero-Shot Approach Using Implicit and Explicit Interpretability](https://arxiv.org/abs/2511.03635)
*Apoorva Upadhyaya,Wolfgang Nejdl,Marco Fisichella*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的可解释零样本立场检测框架IRIS，通过结合隐式和显式理由，在无需真实理由标签的情况下提升模型泛化能力与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有零样本立场检测方法存在泛化能力差、文本与目标之间缺乏一致性，或过度依赖显式推理、解释粗糙且未显式建模推理过程等问题，难以解释模型预测。

Method: IRIS将立场检测视为信息检索排序任务，利用文本中的隐式理由序列和基于语言学特征的显式理由（如交际特征）进行可解释建模，无需真实理由标签即可引导模型做出正确预测。

Result: 在VAST、EZ-STANCE、P-Stance和RFD等多个基准数据集上，即使仅使用10%~50%的训练数据，IRIS仍表现出优异的泛化性能。

Conclusion: IRIS通过融合隐式与显式理由，在提升零样本立场检测性能的同时实现了内在可解释性，有效解决了现有方法的局限性。

Abstract: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (LLMs) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help decode the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.

</details>


### [37] [ChiMDQA: Towards Comprehensive Chinese Document QA with Fine-grained Evaluation](https://arxiv.org/abs/2511.03656)
*Jing Gao,Shutiao Luo,Yumeng Liu,Yuanming Li,Hongji Zeng*

Main category: cs.CL

TL;DR: 本文提出了一个面向中文多文档问答的高质量数据集ChiMDQA，覆盖六个领域和十类问题，适用于多种NLP任务。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理技术的发展，对高质量中文文档问答数据集的需求日益增长，现有资源难以满足多领域实际业务场景的需要。

Method: 通过严格的文档筛选和系统性的问题设计方法，构建包含六个领域长文档、6,068个高质量问答对并细分为十类的ChiMDQA数据集。

Result: 构建了一个兼具多样性与高质量的中文多文档问答数据集，适用于文档理解、知识抽取和智能问答等任务，并提供了完整的构建流程与评估体系。

Conclusion: ChiMDQA为中文问答研究和应用提供了坚实的数据基础，有助于推动相关NLP任务的发展。

Abstract: With the rapid advancement of natural language processing (NLP) technologies,
the demand for high-quality Chinese document question-answering datasets is
steadily growing. To address this issue, we present the Chinese Multi-Document
Question Answering Dataset(ChiMDQA), specifically designed for downstream
business scenarios across prevalent domains including academic, education,
finance, law, medical treatment, and news. ChiMDQA encompasses long-form
documents from six distinct fields, consisting of 6,068 rigorously curated,
high-quality question-answer (QA) pairs further classified into ten
fine-grained categories. Through meticulous document screening and a systematic
question-design methodology, the dataset guarantees both diversity and high
quality, rendering it applicable to various NLP tasks such as document
comprehension, knowledge extraction, and intelligent QA systems. Additionally,
this paper offers a comprehensive overview of the dataset's design objectives,
construction methodologies, and fine-grained evaluation system, supplying a
substantial foundation for future research and practical applications in
Chinese QA. The code and data are available at:
https://anonymous.4open.science/r/Foxit-CHiMDQA/.

</details>


### [38] [Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask](https://arxiv.org/abs/2511.03718)
*Nan Li,Albert Gatt,Massimo Poesio*

Main category: cs.CL

TL;DR: 本文提出一种视角主义标注方案，用于分析HCRC MapTask语料库中的指代表达，分别捕捉说话者与听话者的理解状态，揭示表面共识下潜在的指代错位问题。


<details>
  <summary>Details</summary>
Motivation: 协作对话中参与者需逐步建立共同基础，但在不对称情境下，双方可能误以为达成一致，实则指向不同实体。现有方法难以追踪理解如何产生、分歧及修复。

Method: 设计一种视角主义标注框架，对HCRC MapTask语料库中的13,000个指代表达进行标注，分别记录说话人和听话人的接地解释，并通过受约束的大语言模型（LLM）流水线实现可靠标注。

Result: 研究发现，在统一词汇变体后，完全误解较为罕见，但多重性差异会系统性引发理解分歧，表明表面接地可能掩盖指代错位。

Conclusion: 该框架不仅提供了研究接地误解的新资源与分析视角，也为评估（视觉）大语言模型在协作对话中建模视角依赖接地能力提供了基础。

Abstract: Collaborative dialogue relies on participants incrementally establishing
common ground, yet in asymmetric settings they may believe they agree while
referring to different entities. We introduce a perspectivist annotation scheme
for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures
speaker and addressee grounded interpretations for each reference expression,
enabling us to trace how understanding emerges, diverges, and repairs over
time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k
annotated reference expressions with reliability estimates and analyze the
resulting understanding states. The results show that full misunderstandings
are rare once lexical variants are unified, but multiplicity discrepancies
systematically induce divergences, revealing how apparent grounding can mask
referential misalignment. Our framework provides both a resource and an
analytic lens for studying grounded misunderstanding and for evaluating
(V)LLMs' capacity to model perspective-dependent grounding in collaborative
dialogue.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [39] [An Event-Driven Spiking Compute-In-Memory Macro based on SOT-MRAM](https://arxiv.org/abs/2511.03203)
*Deyang Yu,Chenchen Liu,Chuanjie Zhang,Xiao Fang,Weisheng Zhao*

Main category: cs.AR

TL;DR: 本文提出了一种基于SOT-MRAM的事件驱动型存内计算宏架构，通过轻量级脉冲编码电路和混合串并联单元结构，在28nm工艺下实现了高达243.6 TOPS/W的能效，显著优于现有设计。


<details>
  <summary>Details</summary>
Motivation: 现有MRAM存内计算设计依赖复杂的模拟电路进行运算，导致能耗较高，限制了其能效表现。

Method: 采用自旋轨道矩MRAM（SOT-MRAM）交叉阵列，结合混合串并联存储单元结构支持矩阵-向量乘法，并利用轻量级电路将信号信息以脉冲形式编码，实现事件驱动的脉冲处理机制，避免使用高功耗模拟电路。

Result: 在28nm工艺下实现的SOT-MRAM CIM宏达到了243.6 TOPS/W的峰值能效，显著优于现有方案。

Conclusion: 所提出的SOT-MRAM CIM架构通过事件驱动与脉冲编码有效降低了能耗，为高能效存内计算提供了可行路径。

Abstract: The application of Magnetic Random-Access Memory (MRAM) in
computing-in-memory (CIM) has gained significant attention. However, existing
designs often suffer from high energy consumption due to their reliance on
complex analog circuits for computation. In this work, we present a Spin-Orbit-
Torque MRAM(SOT-MRAM)-based CIM macro that employs an event-driven spiking
processing for high energy efficiency. The SOT-MRAM crossbar adopts a hybrid
series-parallel cell structure to efficiently support matrix-vector
multiplication (MVM). Signal information is (en) decoded as spikes using
lightweight circuits, eliminating the need for conventional area- and
powerintensive analog circuits. The SOT-MRAM macro is designed and evaluated in
28nm technology, and experimental results show that it achieves a peak energy
efficiency of 243.6 TOPS/W, significantly outperforming existing designs.

</details>


### [40] [Design and Optimization of Mixed-Kernel Mixed-Signal SVMs for Flexible Electronics](https://arxiv.org/abs/2511.03427)
*Florentia Afentaki,Maha Shatta,Konstantinos Balaskas,Georgios Panagopoulos,Georgios Zervakis,Mehdi B. Tahoori*

Main category: cs.AR

TL;DR: 本文提出首个面向柔性电子（FE）的混合核与混合信号支持向量机（SVM）设计，通过联合优化训练与映射策略，在保持高准确率的同时显著降低面积和功耗。


<details>
  <summary>Details</summary>
Motivation: 柔性电子受限于大特征尺寸，难以实现高集成度的机器学习电路；现有SVM设计在硬件成本与精度之间存在权衡，线性核精度低，RBF核面积与功耗高。

Method: 提出一种混合核（线性+RBF）与混合信号（数字+模拟）的SVM架构，并引入协同优化方法，将二元分类器智能分配至合适的核类型与信号域，以在减少昂贵RBF使用的同时最大化准确率。

Result: 相比现有单一线性核SVM，准确率提升7.7%；相比全数字RBF实现，平均面积和功耗分别降低108倍和17倍。

Conclusion: 该混合设计有效平衡了柔性电子中SVM实现的成本与精度矛盾，为近传感器智能应用提供了高效可行的解决方案。

Abstract: Flexible Electronics (FE) have emerged as a promising alternative to
silicon-based technologies, offering on-demand low-cost fabrication,
conformality, and sustainability. However, their large feature sizes severely
limit integration density, imposing strict area and power constraints, thus
prohibiting the realization of Machine Learning (ML) circuits, which can
significantly enhance the capabilities of relevant near-sensor applications.
Support Vector Machines (SVMs) offer high accuracy in such applications at
relatively low computational complexity, satisfying FE technologies'
constraints. Existing SVM designs rely solely on linear or Radial Basis
Function (RBF) kernels, forcing a trade-off between hardware costs and
accuracy. Linear kernels, implemented digitally, minimize overhead but
sacrifice performance, while the more accurate RBF kernels are prohibitively
large in digital, and their analog realization contains inherent functional
approximation. In this work, we propose the first mixed-kernel and mixed-signal
SVM design in FE, which unifies the advantages of both implementations and
balances the cost/accuracy trade-off. To that end, we introduce a
co-optimization approach that trains our mixed-kernel SVMs and maps binary SVM
classifiers to the appropriate kernel (linear/RBF) and domain (digital/analog),
aiming to maximize accuracy whilst reducing the number of costly RBF
classifiers. Our designs deliver 7.7% higher accuracy than state-of-the-art
single-kernel linear SVMs, and reduce area and power by 108x and 17x on average
compared to digital RBF implementations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [41] [Harvesting energy consumption on European HPC systems: Sharing Experience from the CEEC project](https://arxiv.org/abs/2511.03029)
*Kajol Kulkarni,Samuel Kemmler,Anna Schwarz,Gulcin Gedik,Yanxiang Chen,Dimitrios Papageorgiou,Ioannis Kavroulakis,Roman Iakymchuk*

Main category: cs.DC

TL;DR: 本文总结了EuroHPC JU卓越中心在欧洲主要HPC系统上测量、分析和优化CFD应用能耗的经验，强调加速器和混合精度技术在降低能耗方面的优势，并呼吁加强HPC系统的能耗测量以推动可持续的百亿亿次计算。


<details>
  <summary>Details</summary>
Motivation: 现代高性能计算（HPC）系统因计算需求激增和架构复杂性提升而面临严峻的能效挑战，亟需系统性方法来评估和优化能耗。

Method: 综述能耗测量的关键方法与工具，定义结果报告指标，并通过典型CFD应用（waLBerla、FLEXI/GALÆXI、Neko和NekRS）在多种欧洲HPC系统（如LUMI、MareNostrum5、MeluXina和JUWELS Booster）的CPU和GPU分区上进行案例研究，评估“能耗-求解时间”和“时间-求解时间”指标。

Result: 实验结果表明，使用加速器和混合精度技术可在保持计算精度的同时显著降低能耗。

Conclusion: 应推动在HPC系统中普及能耗测量，以提升社区意识、加强教育并采取行动，实现更可持续的百亿亿次计算。

Abstract: Energy efficiency has emerged as a central challenge for modern
high-performance computing (HPC) systems, where escalating computational
demands and architectural complexity have led to significant energy footprints.
This paper presents the collective experience of the EuroHPC JU Center of
Excellence in Exascale CFD (CEEC) in measuring, analyzing, and optimizing
energy consumption across major European HPC systems. We briefly review key
methodologies and tools for energy measurement as well as define metrics for
reporting results. Through case studies using representative CFD applications
(waLBerla, FLEXI/GAL{\AE}XI, Neko, and NekRS), we evaluate energy-to-solution
and time-to-solution metrics on diverse architectures, including CPU- and
GPU-based partitions of LUMI, MareNostrum5, MeluXina, and JUWELS Booster. Our
results highlight the advantages of accelerators and mixed-precision techniques
for reducing energy consumption while maintaining computational accuracy.
Finally, we advocate the need to facilitate energy measurements on HPC systems
in order to raise awareness, teach the community, and take actions toward more
sustainable exascale computing.

</details>


### [42] [UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM](https://arxiv.org/abs/2511.03293)
*Hai Huang,Xuhong Qiang,Weisheng Zhao,Chenchen Liu*

Main category: cs.DC

TL;DR: 本文提出UMDAM，一种面向NPU-PIM协同执行的统一内存亲和性数据布局与DRAM地址映射方案，显著提升边缘设备上大语言模型推理效率。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大语言模型时，解码阶段内存密集，限制性能；尽管存内计算（PIM）有潜力，但NPU-PIM协同执行面临数据布局不匹配、带宽损失和冗余存储等问题。

Method: 提出UMDAM方案，采用列优先的分块数据布局和可配置的DRAM地址映射策略，在不增加额外内存开销或带宽损失的前提下，兼顾NPU计算兼容性与PIM效率。

Result: 在OPT模型上的评估表明，UMDAM最多可将首token生成时间（TTFT）缩短3.0倍，末token生成时间（TTLT）缩短2.18倍。

Conclusion: UMDAM显著提升了边缘设备上大语言模型端到端推理效率，为NPU-PIM协同架构提供了高效内存管理方案。

Abstract: Large Language Models (LLMs) are increasingly deployed on edge devices with
Neural Processing Units (NPUs), yet the decode phase remains memory-intensive,
limiting performance. Processing-in-Memory (PIM) offers a promising solution,
but co-executing NPU-PIM systems face challenges such as data layout
mismatches, bandwidth loss, and redundant storage. To address these issues, we
propose UMDAM, a unified memory-affinity data layout and DRAM address mapping
scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,
tile-based layout and a configurable DRAM mapping strategy to ensure
compatibility with NPU computation while maximizing PIM efficiency -- without
introducing extra memory overhead or bandwidth loss. Comprehensive evaluations
on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up
to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving
end-to-end LLM inference efficiency on edge devices.

</details>


### [43] [Investigating the Impact of Isolation on Synchronized Benchmarks](https://arxiv.org/abs/2511.03533)
*Nils Japke,Furat Hamdan,Diana Baumann,David Bermbach*

Main category: cs.DC

TL;DR: 本文研究了在云环境中使用Duet基准测试时，不同隔离机制对性能干扰的缓解效果，发现基于cgroups和CPU绑定的进程隔离能有效减少误报，而Docker容器反而更易受干扰。


<details>
  <summary>Details</summary>
Motivation: 云环境中的基准测试常因多租户资源竞争导致性能波动，Duet基准测试虽通过并行运行两个工作负载减轻外部干扰，但其内部同步工作负载之间仍存在资源争用问题，需引入额外隔离机制。

Method: 评估三种隔离策略：cgroups与CPU绑定、Docker容器和Firecracker微虚拟机，并与无隔离基线进行对比；通过在Duet设置中引入噪声生成器模拟资源抢占，测量各策略下的性能表现。

Result: 所有隔离策略在噪声影响下均表现出不同的延迟分布；除Docker容器外，其他隔离方式（尤其是cgroups与CPU绑定）普遍降低了误报率；Docker容器尽管底层也使用cgroups和CPU绑定，却更易受噪声干扰。

Conclusion: 推荐在同步工作负载的Duet基准测试中采用进程级隔离（如cgroups与CPU绑定），但应避免使用Docker容器，因其隔离效果不佳且更易受性能干扰。

Abstract: Benchmarking in cloud environments suffers from performance variability from
multi-tenant resource contention. Duet benchmarking mitigates this by running
two workload versions concurrently on the same VM, exposing them to identical
external interference. However, intra-VM contention between synchronized
workloads necessitates additional isolation mechanisms.
  This work evaluates three such strategies: cgroups and CPU pinning, Docker
containers, and Firecracker MicroVMs. We compare all strategies with an
unisolated baseline experiment, by running benchmarks with a duet setup
alongside a noise generator. This noise generator "steals" compute resources to
degrade performance measurements.
  All experiments showed different latency distributions while under the
effects of noise generation, but results show that process isolation generally
lowered false positives, except for our experiments with Docker containers.
Even though Docker containers rely internally on cgroups and CPU pinning, they
were more susceptible to performance degradation due to noise influence.
Therefore, we recommend to use process isolation for synchronized workloads,
with the exception of Docker containers.

</details>


### [44] [Stone Duality Proofs for Colorless Distributed Computability Theorems](https://arxiv.org/abs/2511.03609)
*Cameron Calk,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文通过引入谱空间对基于轮次的全信息对抗模型中的执行进行拓扑编码，提出了一种新的分布式可计算性定理，利用Stone对偶性刻画了在紧致对抗者下无色任务的可解性，并揭示了着色与无色模型在计算能力上的等价性具有拓扑根源。


<details>
  <summary>Details</summary>
Motivation: 传统方法使用抽象单纯复形来表示分布式协议执行后的全局状态，但这种方法在处理某些消息对抗模型时存在局限。作者旨在通过采用谱空间和Alexandrov拓扑提供一种更统一、更具表达力的拓扑框架，以更好地理解和刻画无色任务在复杂对抗模型下的可解性。

Method: 作者将有限执行后的全局状态视为面偏序集上的Alexandrov拓扑所诱导的谱空间，而非传统的抽象单纯复形。针对给定的对抗者ℳ及其输入集合ℐ，构造了一个在谱空间范畴中的射影极限对象Π^∞_ℳ(ℐ)，并借助Stone对偶性建立了一个新的分布式可计算性定理：无色任务(ℐ,𝒪,Δ)在紧致对抗者ℳ下可解当且仅当存在一个与Δ兼容的谱映射f: Π^∞_ℳ(ℐ) → 𝒪。

Result: 该方法成功导出了多个已知的无色可计算性定理，并意外发现着色模型与无色模型在任务求解能力上是等价的。更重要的是，本文首次从拓扑角度解释了这种等价性，而此前仅通过算法归约得知。

Conclusion: 通过将谱空间和Stone对偶性引入分布式计算的拓扑分析，本文为无色任务在紧致对抗者下的可解性提供了统一且深刻的刻画，不仅推广了已有结果，还揭示了着色与无色模型计算等价性的拓扑本质，推动了拓扑方法在分布式计算中的统一。

Abstract: We introduce a new topological encoding by spectral spaces of executions of
  round-based full-information adversaries, a model of distributed computations
that is functorially presented and that
  contains many message adversaries. We give a characterization of the
solvability of colorless tasks against compact adversaries.
  Message adversaries are distributed
  models that are known to be very expressive despite being
  round-based and crash-free. Colorless tasks are
  an important class of distributed tasks. For a colorless task, the
  specification does not depend upon the multiplicity of input or
  output values, like the ubiquitous agreement tasks.
  Therefore, our result is a significant
  step toward unifying topological methods in distributed computing.
  The main insight is to consider global states obtained after finite
executions of a distributed protocol
  not as abstract
  simplicial complexes as previously done, but as spectral
  spaces, considering the Alexandrov topology on the faces poset. Given
  an adversary $\mathcal M$ with a set of inputs $\mathcal I$,
  we define a limit object $\Pi^\infty_\mathcal M(\mathcal I)$
  by projective limit in the category of spectral spaces. We derive a new
general distributed computability
  theorem using Stone duality: there exists an algorithm solving a colorless
task $(\mathcal I,\mathcal O,\Delta)$
  against the compact adversary $\mathcal M$ if and only if there exists a
spectral
  map $f:\Pi^\infty_\mathcal M(\mathcal I)\longrightarrow\mathcal O$ compatible
with $\Delta$.
  From this general characterization are derived many known colorless
computability
  theorems.
  Quite surprisingly, colored and uncolored models have the same
  computability power (they solve the same tasks). Our new proofs give
  topological reasons for this equivalence, previously known through
  algorithmic reductions.

</details>


### [45] [A General Input-Dependent Colorless Computability Theorem and Applications to Core-Dependent Adversaries](https://arxiv.org/abs/2511.03662)
*Yannis Coutouly,Emmanuel Godard*

Main category: cs.DC

TL;DR: 本文将颜色无关可计算性定理推广到输入依赖的敌手模型，证明了IIS_n中核心弹性敌手与仅在初始时刻崩溃的核心弹性敌手具有相同的计算能力，并给出了条件依赖、核心依赖敌手下k-集协商任务可解的充要条件。


<details>
  <summary>Details</summary>
Motivation: 已有工作研究了在迭代即时快照（IIS）模型下颜色无关任务的可解性，以及针对特定输入子集的条件敌手模型。然而，尚未有研究将这些结果统一并扩展至输入依赖的敌手模型，也缺乏对条件-核心依赖敌手下k-集协商任务的完整刻画。

Method: 利用分布式计算中的拓扑框架和几何构造方法，将现有颜色无关可计算性定理推广至输入依赖敌手情形；通过分析载体映射Δ的结构性质简化证明；比较不同崩溃模式下核心弹性敌手的计算能力。

Result: 1) 推广了CG-24的结果至输入依赖敌手；2) 证明IIS_n中两类核心弹性敌手计算能力等价；3) 给出条件-核心依赖敌手下k-集协商可解的充要条件；4) 区分任务表示中的四种设定并揭示载体映射的结构特性。

Conclusion: 通过拓扑与几何方法，本文系统地刻画了输入依赖及条件-核心依赖敌手模型下的任务可解性，特别是k-集协商问题，并揭示了载体映射的结构性质可在不改变计算能力的前提下简化证明过程。

Abstract: Distributed computing tasks can be presented with a triple $(\I,\Ou,\Delta)$.
The solvability of a colorless task on the Iterated Immediate Snapshot model
(IIS) has been characterized by the Colorless Computability Theorem
\cite[Th.4.3.1]{HKRbook}. A recent paper~\cite{CG-24} generalizes this theorem
for any message adversaries $\ma \subseteq IIS$ by geometric methods. In 2001,
Most\'efaoui, Rajsbaum, Raynal, and Roy \cite{condbased} introduced
\emph{condition-based adversaries}. This setting considers a particular
adversary that will be applied only to a subset of input configurations. In
this setting, they studied the $k$-set agreement task with condition-based
$t$-resilient adversaries and obtained a sufficient condition on the conditions
that make $k$-Set Agreement solvable. In this paper we have three
contributions:
  -We generalize the characterization of~\cite{CG-24} to \emph{input-dependent}
adversaries, which means that the adversaries can change depending on the input
configuration.
  - We show that core-resilient adversaries of $IIS_n$ have the same
computability power as the core-resilient adversaries of $IIS_n$ where crashes
only happen at the start.
  - Using the two previous contributions, we provide a necessary and sufficient
characterization of the condition-based, core-dependent adversaries that can
solve $k$-Set Agreement. We also distinguish four settings that may appear when
presenting a distributed task as $(\I,\Ou,\Delta)$. Finally, in a later
section, we present structural properties on the carrier map $\Delta$. Such
properties allow simpler proof, without changing the computability power of the
task. Most of the proofs in this article leverage the topological framework
used in distributed computing by using simple geometric constructions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [46] [PublicAgent: Multi-Agent Design Principles From an LLM-Based Open Data Analysis Framework](https://arxiv.org/abs/2511.03023)
*Sina Montazeri,Yunhe Feng,Kewei Sha*

Main category: cs.AI

TL;DR: PublicAgent 是一个面向公共数据开放分析的多智能体框架，通过将复杂分析流程分解为意图澄清、数据发现、分析和报告四个专用智能体，解决了大语言模型在端到端分析中的注意力稀释、推理干扰与错误传播问题，并总结出五项多智能体系统设计原则。


<details>
  <summary>Details</summary>
Motivation: 开放数据仓库虽具决策支持潜力，但非专家用户因缺乏数据发现、模式映射和统计分析能力而难以使用；现有大语言模型在端到端分析流程中存在注意力分散、专用推理冲突和错误累积等根本性局限。

Method: 提出 PublicAgent 多智能体架构，将分析任务分解为四个专用智能体（意图澄清、数据发现、分析、报告），并在每个阶段引入验证机制；通过在五个模型和50个查询上的实验，提炼出多智能体系统的设计原则。

Result: 实验表明：1）专用智能体即使在最强模型上也显著优于单体模型（胜率达97.5%）；2）智能体可分为通用型（发现、分析）和条件型（报告、意图），前者效果稳定；3）移除不同智能体会引发不同类型失败；4）架构优势在不同任务复杂度下保持稳定；5）不同模型间智能体效能差异显著（42–96%），需模型感知设计。

Conclusion: 专用智能体架构能有效提升大语言模型在复杂数据分析任务中的性能与鲁棒性，所提出的五项设计原则为构建可扩展、可靠且用户友好的自然语言驱动公共数据分析系统提供了理论指导与实践路径。

Abstract: Open data repositories hold potential for evidence-based decision-making, yet
are inaccessible to non-experts lacking expertise in dataset discovery, schema
mapping, and statistical analysis. Large language models show promise for
individual tasks, but end-to-end analytical workflows expose fundamental
limitations: attention dilutes across growing contexts, specialized reasoning
patterns interfere, and errors propagate undetected. We present PublicAgent, a
multi-agent framework that addresses these limitations through decomposition
into specialized agents for intent clarification, dataset discovery, analysis,
and reporting. This architecture maintains focused attention within agent
contexts and enables validation at each stage. Evaluation across five models
and 50 queries derives five design principles for multi-agent LLM systems.
First, specialization provides value independent of model strength--even the
strongest model shows 97.5% agent win rates, with benefits orthogonal to model
scale. Second, agents divide into universal (discovery, analysis) and
conditional (report, intent) categories. Universal agents show consistent
effectiveness (std dev 12.4%) while conditional agents vary by model (std dev
20.5%). Third, agents mitigate distinct failure modes--removing discovery or
analysis causes catastrophic failures (243-280 instances), while removing
report or intent causes quality degradation. Fourth, architectural benefits
persist across task complexity with stable win rates (86-92% analysis, 84-94%
discovery), indicating workflow management value rather than reasoning
enhancement. Fifth, wide variance in agent effectiveness across models (42-96%
for analysis) requires model-aware architecture design. These principles guide
when and why specialization is necessary for complex analytical workflows while
enabling broader access to public data through natural language interfaces.

</details>


### [47] [No-Human in the Loop: Agentic Evaluation at Scale for Recommendation](https://arxiv.org/abs/2511.03051)
*Tao Zhang,Kehui Yao,Luyi Ma,Jiao Chen,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: ScalingEval is a large-scale benchmark that evaluates 36 LLMs as judges using a consensus-driven, multi-agent framework without human annotation, revealing performance tradeoffs and category-specific reliability.


<details>
  <summary>Details</summary>
Motivation: To build scalable and trustworthy evaluation pipelines by systematically assessing the effectiveness of LLMs as judges across diverse product categories.

Method: A consensus-driven multi-agent framework aggregates pattern audits and issue codes into ground-truth labels via scalable majority voting, enabling reproducible comparison of LLM evaluators without human annotation.

Result: Key findings: (i) Claude 3.5 Sonnet has highest decision confidence; (ii) Gemini 1.5 Pro best overall performance; (iii) GPT-4o offers best latency-accuracy-cost tradeoff; (iv) GPT-OSS 20B leads among open-source models. Strong consensus in structured domains (Electronics, Sports), but disagreement in lifestyle categories (Clothing, Food).

Conclusion: ScalingEval provides a reproducible benchmark and evaluation protocol for using LLMs as judges, offering actionable insights on model selection, scaling strategies, and reliability across domains.

Abstract: Evaluating large language models (LLMs) as judges is increasingly critical
for building scalable and trustworthy evaluation pipelines. We present
ScalingEval, a large-scale benchmarking study that systematically compares 36
LLMs, including GPT, Gemini, Claude, and Llama, across multiple product
categories using a consensus-driven evaluation protocol. Our multi-agent
framework aggregates pattern audits and issue codes into ground-truth labels
via scalable majority voting, enabling reproducible comparison of LLM
evaluators without human annotation. Applied to large-scale complementary-item
recommendation, the benchmark reports four key findings: (i) Anthropic Claude
3.5 Sonnet achieves the highest decision confidence; (ii) Gemini 1.5 Pro offers
the best overall performance across categories; (iii) GPT-4o provides the most
favorable latency-accuracy-cost tradeoff; and (iv) GPT-OSS 20B leads among
open-source models. Category-level analysis shows strong consensus in
structured domains (Electronics, Sports) but persistent disagreement in
lifestyle categories (Clothing, Food). These results establish ScalingEval as a
reproducible benchmark and evaluation protocol for LLMs as judges, with
actionable guidance on scaling, reliability, and model family tradeoffs.

</details>


### [48] [Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge](https://arxiv.org/abs/2511.03070)
*Drago Plecko,Patrik Okanovic,Torsten Hoefler,Elias Bareinboim*

Main category: cs.AI

TL;DR: 本文构建了首个用于评估大语言模型（LLMs）是否掌握现实世界概率分布知识的基准测试，结果表明LLMs在该方面表现不佳，缺乏对观测分布的理解，进而限制了其干预和反事实推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽被广泛认为能近似现实世界分布，但高维统计中的“维度诅咒”对其通用分布学习能力提出质疑。作者旨在系统评估LLMs是否真正内化了现实世界的概率分布知识。

Method: 开发首个直接测试LLMs对现实世界经验分布（涵盖经济、健康、教育和社会行为等领域）掌握程度的基准，并结合Pearl因果层级（PCH）框架进行分析。

Result: LLMs整体表现较差，未能自然内化现实世界的统计分布；在PCH的第一层（观测分布）上缺乏知识，暗示其在第二层（干预）和第三层（反事实）的能力也受限。

Conclusion: 大语言模型并未有效学习现实世界的概率分布，其因果推理能力因缺乏基础观测知识而受到根本性限制。

Abstract: Artificial intelligence (AI) systems hold great promise for advancing various
scientific disciplines, and are increasingly used in real-world applications.
Despite their remarkable progress, further capabilities are expected in order
to achieve more general types of intelligence. A critical distinction in this
context is between factual knowledge, which can be evaluated against true or
false answers (e.g., "what is the capital of England?"), and probabilistic
knowledge, reflecting probabilistic properties of the real world (e.g., "what
is the sex of a computer science graduate in the US?"). In this paper, our goal
is to build a benchmark for understanding the capabilities of LLMs in terms of
knowledge of probability distributions describing the real world. Given that
LLMs are trained on vast amounts of text, it may be plausible that they
internalize aspects of these distributions. Indeed, LLMs are touted as powerful
universal approximators of real-world distributions. At the same time,
classical results in statistics, known as curse of dimensionality, highlight
fundamental challenges in learning distributions in high dimensions,
challenging the notion of universal distributional learning. In this work, we
develop the first benchmark to directly test this hypothesis, evaluating
whether LLMs have access to empirical distributions describing real-world
populations across domains such as economics, health, education, and social
behavior. Our results demonstrate that LLMs perform poorly overall, and do not
seem to internalize real-world statistics naturally. When interpreted in the
context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that
language models do not contain knowledge on observational distributions (Layer
1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional
(Layer 2) and counterfactual (Layer 3) knowledge of these models is also
limited.

</details>


### [49] [SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators](https://arxiv.org/abs/2511.03092)
*Jonathan Li,Nasim Farahini,Evgenii Iuliugin,Magnus Vesterlund,Christian Haggstrom,Guangtao Wang,Shubhangi Upasani,Ayush Sachdeva,Rui Li,Faline Fu,Chen Wu,Ayesha Siddiqua,John Long,Tuowen Zhao,Matheen Musaddiq,Hakan Zeffer,Yun Du,Mingran Wang,Qinghua Li,Bo Li,Urmish Thakker,Raghu Prabhakar*

Main category: cs.AI

TL;DR: 本文提出了SnapStream，一种适用于工业部署的KV缓存压缩方法，在支持静态计算图和连续批处理的推理系统中实现了稀疏KV注意力机制，在显著降低片上内存使用的同时保持了模型精度。


<details>
  <summary>Details</summary>
Motivation: 当前工业级LLM推理框架（如vLLM、SGLang）因采用静态图和连续批处理，难以集成现有KV缓存压缩技术；同时，这些技术在现代指令遵循与推理模型上的精度影响尚不明确，阻碍了其实际部署。

Method: 作者评估了KV缓存压缩对Llama-3.1-8B-Instruct和DeepSeek-R1等模型的精度影响，并开发了SnapStream方法，使其兼容静态图与连续批处理架构，可在SambaNova SN40L加速器上大规模部署。

Result: 在DeepSeek-671B的16路张量并行部署中，SnapStream在128k上下文长度下实现最高1832 token/s的吞吐量，片上内存使用减少4倍，在LongBench-v2、AIME24和LiveCodeBench上仅引入极小精度损失。

Conclusion: SnapStream是首个在支持静态图和连续批处理的生产推理系统中成功部署的稀疏KV注意力技术，兼顾效率与精度，为大上下文LLM推理提供了实用解决方案。

Abstract: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large KV caches. Techniques such as StreamingLLM and SnapKV
demonstrate how to control KV cache size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables $4\times$ improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of sparse KV attention techniques deployed in a production inference system
with static graphs and continuous batching.

</details>


### [50] [Large language models require a new form of oversight: capability-based monitoring](https://arxiv.org/abs/2511.03106)
*Katherine C. Kellogg,Bingyang Ye,Yifan Hu,Guergana K. Savova,Byron Wallace,Danielle S. Bitterman*

Main category: cs.AI

TL;DR: 本文提出了一种面向大语言模型（LLMs）在医疗领域应用的新监控范式——基于能力的监控（capability-based monitoring），以替代传统基于任务的监控方法，从而更有效地识别系统性弱点、长尾错误和新兴行为。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习的监控方法依赖于任务性能退化假设，而大语言模型作为通用模型，并非针对特定人群或任务训练，因此原有方法不适用；需要一种更契合LLM开发与使用实际的可扩展监控框架。

Method: 提出“基于能力的监控”框架，围绕LLM共有的核心能力（如摘要、推理、翻译、安全防护等）组织监控活动，而非单独评估每个下游任务。

Result: 该方法能够实现跨任务检测系统性缺陷、罕见错误和突发行为，提升对LLM在医疗场景中表现的全面监控能力。

Conclusion: 基于能力的监控为大语言模型及未来通用人工智能在医疗领域的安全、适应性和协作性监控提供了可扩展的基础。

Abstract: The rapid adoption of large language models (LLMs) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
LLMs, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because LLMs were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist LLM monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that LLMs are
generalist systems whose overlapping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of LLMs and future generalist
artificial intelligence models in healthcare.

</details>


### [51] [miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward](https://arxiv.org/abs/2511.03108)
*Azim Ospanov,Farzan Farnia,Roozbeh Yousefzadeh*

Main category: cs.AI

TL;DR: 本文对 miniF2F 基准中的形式化与非形式化陈述进行了深入分析，发现两者间存在大量不一致问题，导致端到端数学证明流水线性能显著下降；作者修正这些问题后发布了 miniF2F-v2，在新基准上端到端准确率从 40% 提升至 70%，表明高质量基准对评估和诊断自动形式化与定理证明模型至关重要。


<details>
  <summary>Details</summary>
Motivation: 评估 AI 系统在数学奥林匹克竞赛中端到端解题能力时，发现当前自动形式化与定理证明模型的组合性能远低于各自单独的最佳表现，怀疑问题源于 miniF2F 基准中形式化与非形式化陈述之间的不一致。

Method: 系统分析 miniF2F 中形式化与非形式化陈述的差异，修正其中的错误、不一致和简化问题，构建新版基准 miniF2F-v2，并在该基准上重新评估端到端定理证明流水线的性能。

Result: 在原始 miniF2F 上，端到端流水线最佳准确率为 36%（后文称 40%）；在修正后的 miniF2F-v2 上提升至 70%。分析表明超过一半的问题存在形式与非形式陈述不一致。

Conclusion: 高质量、严格对齐的形式化与非形式化基准对于准确评估自动形式化和定理证明系统的进展至关重要，miniF2F-v2 为此提供了更可靠的测试平台。

Abstract: We perform a thorough analysis of the formal and informal statements in the
miniF2F benchmark from the perspective of an AI system that is tasked to
participate in a math Olympiad consisting of the problems in miniF2F. In such
setting, the model has to read and comprehend the problems in natural language,
formalize them in Lean language, then proceed with proving the problems, and it
will get credit for each problem if the formal proof corresponds to the
original informal statement presented to the model. Our evaluation results
reveal that the best accuracy of such pipeline can be about 36% using the SoTA
models in the literature, considerably lower than the individual SoTA
accuracies, 97% and 69% reported in the autoformalization and theorem proving
literature. Analyzing the failure modes, we trace back a considerable portion
of this drop to discrepancies between the formal and informal statements for
more than half of the problems in miniF2F. We proceed with correcting all the
errors, discrepancies and simplifications in formal and informal statements,
and present the miniF2F-v2 with fully verified formal and informal statements
and proofs. Evaluating the full theorem proving pipeline on miniF2F-v2 leads to
the best accuracy of 70%, a significant improvement from the 40% on the
original miniF2F, yet indicating considerable misalignment between the
autoformalization models and theorem provers. Our deep analysis suggests that a
higher quality benchmark can help the community better evaluate progress in the
field of formal reasoning and also better diagnose the failure and success
modes of autoformalization and theorem proving models. Our dataset is available
at https://github.com/roozbeh-yz/miniF2F_v2.

</details>


### [52] [Using Multi-modal Large Language Model to Boost Fireworks Algorithm's Ability in Settling Challenging Optimization Tasks](https://arxiv.org/abs/2511.03137)
*Shipeng Cen,Ying Tan*

Main category: cs.AI

TL;DR: 本文提出一种结合多模态大语言模型（MLLM）辅助设计烟花算法（FWA）的新框架，通过引入“关键部分”（Critical Part, CP）概念，提升FWA在高维复杂优化问题（如TSP和EDA）中的性能，并在多个实例上达到或超越当前最优（SOTA）结果。


<details>
  <summary>Details</summary>
Motivation: 传统零阶或一阶优化方法在处理非凸、高维、黑盒等复杂优化问题时效率低、梯度信息不准确、优化信息利用不足；而大语言模型的快速发展为优化算法设计提供了新思路。

Method: 以烟花算法（FWA）为基础，引入多模态大语言模型（MLLM），提出“关键部分”（CP）概念，利用MLLM的多模态特性增强优化过程中的信息利用，从而改进FWA在复杂高维任务中的表现。

Result: 在旅行商问题（TSP）和电子设计自动化问题（EDA）两个任务上的实验表明，该框架生成的FWA在多个问题实例上达到或超过了当前最优水平。

Conclusion: 结合多模态大语言模型可有效提升传统优化算法（如FWA）在复杂高维优化问题中的性能，为优化算法设计提供了一种有前景的新范式。

Abstract: As optimization problems grow increasingly complex and diverse, advancements
in optimization techniques and paradigm innovations hold significant
importance. The challenges posed by optimization problems are primarily
manifested in their non-convexity, high-dimensionality, black-box nature, and
other unfavorable characteristics. Traditional zero-order or first-order
methods, which are often characterized by low efficiency, inaccurate gradient
information, and insufficient utilization of optimization information, are
ill-equipped to address these challenges effectively. In recent years, the
rapid development of large language models (LLM) has led to substantial
improvements in their language understanding and code generation capabilities.
Consequently, the design of optimization algorithms leveraging large language
models has garnered increasing attention from researchers. In this study, we
choose the fireworks algorithm(FWA) as the basic optimizer and propose a novel
approach to assist the design of the FWA by incorporating multi-modal large
language model(MLLM). To put it simply, we propose the concept of Critical
Part(CP), which extends FWA to complex high-dimensional tasks, and further
utilizes the information in the optimization process with the help of the
multi-modal characteristics of large language models. We focus on two specific
tasks: the \textit{traveling salesman problem }(TSP) and \textit{electronic
design automation problem} (EDA). The experimental results show that FWAs
generated under our new framework have achieved or surpassed SOTA results on
many problem instances.

</details>


### [53] [A Proprietary Model-Based Safety Response Framework for AI Agents](https://arxiv.org/abs/2511.03138)
*Qi Li,Jianjun Xu,Pingtao Wei,Jiu Li,Peiqiang Zhao,Jiwei Shi,Xuan Zhang,Yanhui Yang,Xiaodong Hui,Peng Xu,Wenqin Shao*

Main category: cs.AI

TL;DR: 本文提出了一种面向大语言模型（LLMs）的新型安全响应框架，在输入和输出两个层面系统性提升模型安全性。输入端采用基于监督微调的安全分类模型，通过四类细粒度标签实现高风险识别；输出端结合检索增强生成（RAG）与专用解释模型，确保回复可追溯且无幻觉。实验表明该框架在公开与私有安全评测中均显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在关键领域的广泛应用，其安全问题日益突出，严重制约了可信部署。现有方法在风险覆盖、场景适应性和输出可靠性方面存在不足，亟需一种系统性的安全防护机制。

Method: 在输入层，构建基于监督微调的安全分类模型，采用“安全、不安全、条件安全、需关注”四类细粒度标签对用户查询进行精准风险识别与差异化处理；在输出层，集成检索增强生成（RAG）与专门微调的解释模型，确保所有响应均基于实时可信知识库，杜绝信息虚构并支持结果溯源。

Result: 在公开安全评估基准上，所提模型的安全得分显著优于基线模型TinyR1-Safety-8B；在自建高风险测试集上，框架各组件实现了100%的安全得分，风险召回率达99.3%，验证了其在复杂风险场景下的卓越防护能力。

Conclusion: 该研究为构建高安全性、高可信度的大语言模型应用提供了一条有效的工程化路径，兼具高风险覆盖率、强业务适应性与输出可追溯性。

Abstract: With the widespread application of Large Language Models (LLMs), their
associated security issues have become increasingly prominent, severely
constraining their trustworthy deployment in critical domains. This paper
proposes a novel safety response framework designed to systematically safeguard
LLMs at both the input and output levels. At the input level, the framework
employs a supervised fine-tuning-based safety classification model. Through a
fine-grained four-tier taxonomy (Safe, Unsafe, Conditionally Safe, Focused
Attention), it performs precise risk identification and differentiated handling
of user queries, significantly enhancing risk coverage and business scenario
adaptability, and achieving a risk recall rate of 99.3%. At the output level,
the framework integrates Retrieval-Augmented Generation (RAG) with a
specifically fine-tuned interpretation model, ensuring all responses are
grounded in a real-time, trustworthy knowledge base. This approach eliminates
information fabrication and enables result traceability. Experimental results
demonstrate that our proposed safety control model achieves a significantly
higher safety score on public safety evaluation benchmarks compared to the
baseline model, TinyR1-Safety-8B. Furthermore, on our proprietary high-risk
test set, the framework's components attained a perfect 100% safety score,
validating their exceptional protective capabilities in complex risk scenarios.
This research provides an effective engineering pathway for building
high-security, high-trust LLM applications.

</details>


### [54] [Uncovering Bugs in Formal Explainers: A Case Study with PyXAI](https://arxiv.org/abs/2511.03169)
*Xuanxiang Huang,Yacine Izza,Alexey Ignatiev,Joao Marques-Silva*

Main category: cs.AI

TL;DR: 本文提出了一种验证形式化可解释人工智能（XAI）工具的新方法，并通过该方法发现现有工具PyXAI在多数数据集上生成了错误的解释，凸显了验证形式化解释器实现的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管形式化可解释AI具有理论上的严谨性优势，但其实际实现缺乏有效验证；本文旨在填补这一空白。

Method: 开发了一种用于验证形式化解释器的新方法论，并将其应用于公开的形式化解释器PyXAI进行评估。

Result: 实验表明，PyXAI在大多数测试数据集上生成了不正确的解释。

Conclusion: 所提出的验证方法对确保形式化解释器的正确性至关重要，未来的形式化XAI工具应采用类似验证流程。

Abstract: Formal explainable artificial intelligence (XAI) offers unique theoretical
guarantees of rigor when compared to other non-formal methods of
explainability. However, little attention has been given to the validation of
practical implementations of formal explainers. This paper develops a novel
methodology for validating formal explainers and reports on the assessment of
the publicly available formal explainer PyXAI. The paper documents the
existence of incorrect explanations computed by PyXAI on most of the datasets
analyzed in the experiments, thereby confirming the importance of the proposed
novel methodology for the validation of formal explainers.

</details>


### [55] [Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework](https://arxiv.org/abs/2511.03179)
*Varun Kumar,George Em Karniadakis*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体AI框架的工程设计方法，通过知识图谱与协作反馈机制，实现了对NACA四位数翼型的高效气动优化。


<details>
  <summary>Details</summary>
Motivation: 传统工程设计过程依赖多领域专家协作，资源消耗大且效率低下，亟需一种更高效、一致且高质量的设计方法。

Method: 构建包含图本体智能体、设计工程师智能体和系统工程师智能体的多智能体框架；利用大语言模型从文献中构建领域知识图谱，结合人机协同设定技术需求，并通过迭代评审与优化生成满足性能指标（如升阻比）的设计方案。

Result: 该框架成功应用于NACA四位数翼型的气动优化，展示了其在提升设计效率、一致性和质量方面的潜力。

Conclusion: 融合结构化知识表示的协作式AI智能体可显著增强工程设计流程的效能与可靠性。

Abstract: The engineering design process often demands expertise from multiple domains,
leading to complex collaborations and iterative refinements. Traditional
methods can be resource-intensive and prone to inefficiencies. To address this,
we formalize the engineering design process through a multi-agent AI framework
that integrates structured design and review loops. The framework introduces
specialized knowledge-driven agents that collaborate to generate and refine
design candidates. As an exemplar, we demonstrate its application to the
aerodynamic optimization of 4-digit NACA airfoils. The framework consists of
three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems
Engineer. The Graph Ontologist employs a Large Language Model (LLM) to
construct two domain-specific knowledge graphs from airfoil design literature.
The Systems Engineer, informed by a human manager, formulates technical
requirements that guide design generation and evaluation. The Design Engineer
leverages the design knowledge graph and computational tools to propose
candidate airfoils meeting these requirements. The Systems Engineer reviews and
provides feedback both qualitative and quantitative using its own knowledge
graph, forming an iterative feedback loop until a design is validated by the
manager. The final design is then optimized to maximize performance metrics
such as the lift-to-drag ratio. Overall, this work demonstrates how
collaborative AI agents equipped with structured knowledge representations can
enhance efficiency, consistency, and quality in the engineering design process.

</details>


### [56] [Adobe Summit Concierge Evaluation with Human in the Loop](https://arxiv.org/abs/2511.03186)
*Yiru Chen,Sally Fang,Sai Sree Harsha,Dan Luo,Vaishnavi Muppala,Fei Wu,Shun Jiang,Kun Qian,Yunyao Li*

Main category: cs.AI

TL;DR: 本文介绍了为Adobe Summit开发的领域专用AI助手Summit Concierge，通过人机协作流程应对数据稀疏、质量保障和快速部署等现实挑战，验证了敏捷、反馈驱动的开发方法在冷启动场景下构建可靠AI助手的有效性。


<details>
  <summary>Details</summary>
Motivation: 在企业环境中，生成式AI助手虽有提升生产力和用户体验的潜力，但在实际部署中面临数据稀疏、质量保障和快速上线等挑战，亟需有效的开发与部署策略。

Method: 采用“人在环路”（human-in-the-loop）的开发流程，结合提示工程、检索增强（retrieval grounding）和轻量级人工验证，构建并部署领域专用AI助手。

Result: 成功部署了Summit Concierge系统，能够有效处理各类活动相关查询，并在真实场景中展现出可扩展性与可靠性。

Conclusion: 敏捷且以反馈驱动的开发方法能够在冷启动等资源受限条件下，高效构建可靠、可扩展的企业级AI助手。

Abstract: Generative AI assistants offer significant potential to enhance productivity,
streamline information access, and improve user experience in enterprise
contexts. In this work, we present Summit Concierge, a domain-specific AI
assistant developed for Adobe Summit. The assistant handles a wide range of
event-related queries and operates under real-world constraints such as data
sparsity, quality assurance, and rapid deployment. To address these challenges,
we adopt a human-in-the-loop development workflow that combines prompt
engineering, retrieval grounding, and lightweight human validation. We describe
the system architecture, development process, and real-world deployment
outcomes. Our experience shows that agile, feedback-driven development enables
scalable and reliable AI assistants, even in cold-start scenarios.

</details>


### [57] [From Five Dimensions to Many: Large Language Models as Precise and Interpretable Psychological Profilers](https://arxiv.org/abs/2511.03235)
*Yi-Fei Liu,Yi-Long Lu,Di He,Hang Zhang*

Main category: cs.AI

TL;DR: 本文研究发现大语言模型（LLMs）能仅凭个体的大五人格得分，准确模拟其在其他九种心理量表上的反应，展现出对人类心理结构的高度捕捉能力（R² > 0.89）。LLMs通过两阶段推理过程：先将原始人格数据压缩为自然语言摘要，再基于摘要生成目标量表反应。这些摘要不仅非冗余，还包含高阶交互信息，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型是否能够仅凭少量定量输入（如大五人格得分）建模人类心理特质之间的相关结构，并理解其背后的推理机制。

Method: 研究者使用816名参与者的大五人格量表得分作为输入，提示多种LLMs角色扮演这些个体在另外九个心理量表上的回答；通过比较LLM生成数据与真实人类数据的跨量表相关模式评估准确性，并分析其推理轨迹以揭示内部机制。

Result: LLMs在零样本设置下表现出色（R² > 0.89），显著优于基于语义相似性的预测，接近专用机器学习模型的性能；其推理过程包含两个阶段：信息选择与压缩生成人格摘要，再基于摘要推理生成目标量表反应；生成的摘要包含协同信息，可提升预测一致性。

Conclusion: 大语言模型能够通过抽象与推理，从极简数据中精准预测个体心理特质，不仅为心理模拟提供强大工具，也揭示了其涌现的高阶推理能力。

Abstract: Psychological constructs within individuals are widely believed to be
interconnected. We investigated whether and how Large Language Models (LLMs)
can model the correlational structure of human psychological traits from
minimal quantitative inputs. We prompted various LLMs with Big Five Personality
Scale responses from 816 human individuals to role-play their responses on nine
other psychological scales. LLMs demonstrated remarkable accuracy in capturing
human psychological structure, with the inter-scale correlation patterns from
LLM-generated responses strongly aligning with those from human data $(R^2 >
0.89)$. This zero-shot performance substantially exceeded predictions based on
semantic similarity and approached the accuracy of machine learning algorithms
trained directly on the dataset. Analysis of reasoning traces revealed that
LLMs use a systematic two-stage process: First, they transform raw Big Five
responses into natural language personality summaries through information
selection and compression, analogous to generating sufficient statistics.
Second, they generate target scale responses based on reasoning from these
summaries. For information selection, LLMs identify the same key personality
factors as trained algorithms, though they fail to differentiate item
importance within factors. The resulting compressed summaries are not merely
redundant representations but capture synergistic information--adding them to
original scores enhances prediction alignment, suggesting they encode emergent,
second-order patterns of trait interplay. Our findings demonstrate that LLMs
can precisely predict individual participants' psychological traits from
minimal data through a process of abstraction and reasoning, offering both a
powerful tool for psychological simulation and valuable insights into their
emergent reasoning capabilities.

</details>


### [58] [Towards Scalable Web Accessibility Audit with MLLMs as Copilots](https://arxiv.org/abs/2511.03471)
*Ming Gu,Ziwei Wang,Sicen Lai,Zirui Gao,Sheng Zhou,Jiajun Bu*

Main category: cs.AI

TL;DR: 本文提出了一个名为AAA的人机协作网页无障碍审计框架，通过图采样方法GRASP和多模态大语言模型助手MaC，实现了可扩展的端到端无障碍审计，并发布了四个新数据集用于评估。


<details>
  <summary>Details</summary>
Motivation: 当前网页无障碍审计依赖人力密集且难以扩展的方法（如WCAG-EM），导致大多数网站用户界面仍不符合无障碍标准，阻碍了数字空间中的社会公平与包容性。

Method: 提出AAA框架，包含两项核心技术：1）GRASP——基于图的多模态采样方法，利用视觉、文本和关系线索的嵌入实现代表性页面覆盖；2）MaC——基于多模态大语言模型的协作者，通过跨模态推理辅助审计员完成高负担任务。

Result: 实验表明所提方法有效，且经微调的小规模语言模型也能胜任专家级任务；同时贡献了四个用于审计流程各阶段评测的新数据集。

Conclusion: AAA框架通过人机协同显著提升了网页无障碍审计的可扩展性与实用性，为推动真实世界中的数字包容提供了可行路径。

Abstract: Ensuring web accessibility is crucial for advancing social welfare, justice,
and equality in digital spaces, yet the vast majority of website user
interfaces remain non-compliant, due in part to the resource-intensive and
unscalable nature of current auditing practices. While WCAG-EM offers a
structured methodology for site-wise conformance evaluation, it involves great
human efforts and lacks practical support for execution at scale. In this work,
we present an auditing framework, AAA, which operationalizes WCAG-EM through a
human-AI partnership model. AAA is anchored by two key innovations: GRASP, a
graph-based multimodal sampling method that ensures representative page
coverage via learned embeddings of visual, textual, and relational cues; and
MaC, a multimodal large language model-based copilot that supports auditors
through cross-modal reasoning and intelligent assistance in high-effort tasks.
Together, these components enable scalable, end-to-end web accessibility
auditing, empowering human auditors with AI-enhanced assistance for real-world
impact. We further contribute four novel datasets designed for benchmarking
core stages of the audit pipeline. Extensive experiments demonstrate the
effectiveness of our methods, providing insights that small-scale language
models can serve as capable experts when fine-tuned.

</details>


### [59] [Explaining Decisions in ML Models: a Parameterized Complexity Analysis (Part I)](https://arxiv.org/abs/2511.03545)
*Sebastian Ordyniak,Giacomo Paesani,Mateusz Rychlicki,Stefan Szeider*

Main category: cs.AI

TL;DR: 本文系统研究了多种透明机器学习模型中解释问题的参数化复杂性，涵盖局部与全局的溯因和对比解释类型。


<details>
  <summary>Details</summary>
Motivation: 现有可解释人工智能（XAI）领域缺乏对透明模型解释问题计算复杂性的系统理解，本文旨在填补这一理论空白。

Method: 通过参数化复杂性分析，研究决策树、决策集、决策列表、布尔电路及其集成模型中的两类解释问题（溯因与对比）。

Result: 揭示了不同透明ML模型在生成局部和全局解释时的计算复杂性特征，为XAI提供了理论基础。

Conclusion: 该研究深化了对透明AI模型解释机制复杂性的理解，有助于推动XAI领域的进一步发展，并支持AI系统透明性与问责制的实现。

Abstract: This paper presents a comprehensive theoretical investigation into the
parameterized complexity of explanation problems in various machine learning
(ML) models. Contrary to the prevalent black-box perception, our study focuses
on models with transparent internal mechanisms. We address two principal types
of explanation problems: abductive and contrastive, both in their local and
global variants. Our analysis encompasses diverse ML models, including Decision
Trees, Decision Sets, Decision Lists, Boolean Circuits, and ensembles thereof,
each offering unique explanatory challenges. This research fills a significant
gap in explainable AI (XAI) by providing a foundational understanding of the
complexities of generating explanations for these models. This work provides
insights vital for further research in the domain of XAI, contributing to the
broader discourse on the necessity of transparency and accountability in AI
systems.

</details>


### [60] [Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via Self-Play and Reinforcement Learning](https://arxiv.org/abs/2511.03724)
*Richard Dewey,Janos Botyanszki,Ciamac C. Moallemi,Andrew T. Zheng*

Main category: cs.AI

TL;DR: 本文提出了名为Solly的AI智能体，首次在多人参与度高的简化版“骗子扑克”（Liar's Poker）中达到人类顶尖水平，其采用无模型的深度强化学习算法进行自博弈训练，在胜率和盈利方面均优于人类专家和大语言模型，并展现出难以被利用的随机化策略。


<details>
  <summary>Details</summary>
Motivation: 现有AI在类似扑克的游戏中虽取得进展，但多聚焦于两人对局，缺乏对高互动性多人博弈场景的有效建模。为探索更复杂的多人不完全信息博弈，作者选择简化版“骗子扑克”作为新测试平台。

Method: 使用无模型的Actor-Critic深度强化学习算法，通过自博弈方式训练AI智能体Solly。

Result: Solly在单挑和多人“骗子扑克”中胜率超过50%，盈利表现优异，显著优于包括具备推理能力的大语言模型在内的基线方法；其策略具有创新性、随机性强且难以被人类高手利用。

Conclusion: Solly成功展示了深度强化学习在高互动性多人不完全信息博弈中的潜力，为未来复杂博弈AI研究提供了新方向。

Abstract: AI researchers have long focused on poker-like games as a testbed for
environments characterized by multi-player dynamics, imperfect information, and
reasoning under uncertainty. While recent breakthroughs have matched elite
human play at no-limit Texas hold'em, the multi-player dynamics are subdued:
most hands converge quickly with only two players engaged through multiple
rounds of bidding. In this paper, we present Solly, the first AI agent to
achieve elite human play in reduced-format Liar's Poker, a game characterized
by extensive multi-player engagement. We trained Solly using self-play with a
model-free, actor-critic, deep reinforcement learning algorithm. Solly played
at an elite human level as measured by win rate (won over 50% of hands) and
equity (money won) in heads-up and multi-player Liar's Poker. Solly also
outperformed large language models (LLMs), including those with reasoning
abilities, on the same metrics. Solly developed novel bidding strategies,
randomized play effectively, and was not easily exploitable by world-class
human players.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [61] [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)
*Mohsen Ahmadzadeh,Kaichang Chen,Georges Gielen*

Main category: cs.LG

TL;DR: 本文提出了一种名为AnaFlow的新型多智能体AI框架，利用基于大语言模型（LLM）的智能体协作实现高效、可解释的模拟电路尺寸优化，显著减少仿真次数并提供人类可理解的设计推理过程。


<details>
  <summary>Details</summary>
Motivation: 模拟/混合信号电路设计目前仍高度依赖手工操作，周期长且易出错；现有基于AI的自动化方法受限于大量耗时仿真和结果缺乏可解释性，阻碍了其广泛应用。

Method: 采用多智能体工作流，由多个基于LLM的智能体协同解析电路拓扑、理解设计目标，并通过自适应仿真策略迭代优化电路参数，同时记录优化历史以避免重复错误、加速收敛。

Result: 在两个不同复杂度的电路上验证了AnaFlow框架，能够全自动完成尺寸优化任务，相比纯贝叶斯优化和强化学习方法更高效，并具备良好的可解释性。

Conclusion: 该框架为模拟电路设计空间探索提供了强大工具，开创了AI智能体作为透明设计助手的新范式，推动了可解释、高样本效率的模拟EDA发展。

Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the
physical world. Their design, however, remains a largely handcrafted process,
resulting in long and error-prone design cycles. While the recent rise of
AI-based reinforcement learning and generative AI has created new techniques to
automate this task, the need for many time-consuming simulations is a critical
bottleneck hindering the overall efficiency. Furthermore, the lack of
explainability of the resulting design solutions hampers widespread adoption of
the tools. To address these issues, a novel agentic AI framework for
sample-efficient and explainable analog circuit sizing is presented. It employs
a multi-agent workflow where specialized Large Language Model (LLM)-based
agents collaborate to interpret the circuit topology, to understand the design
goals, and to iteratively refine the circuit's design parameters towards the
target goals with human-interpretable reasoning. The adaptive simulation
strategy creates an intelligent control that yields a high sample efficiency.
The AnaFlow framework is demonstrated for two circuits of varying complexity
and is able to complete the sizing task fully automatically, differently from
pure Bayesian optimization and reinforcement learning approaches. The system
learns from its optimization history to avoid past mistakes and to accelerate
convergence. The inherent explainability makes this a powerful tool for analog
design space exploration and a new paradigm in analog EDA, where AI agents
serve as transparent design assistants.

</details>


### [62] [FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels](https://arxiv.org/abs/2511.02872)
*Jiedong Jiang,Wanyi He,Yuefeng Wang,Guoxiong Gao,Yongle Hu,Jingting Wang,Nailing Guan,Peihao Wu,Chunbo Dai,Liang Xiao,Bin Dong*

Main category: cs.LG

TL;DR: 本文提出了FATE（形式代数定理评估）基准，包含FATE-H和FATE-X两个部分，涵盖从本科到博士资格考试以上难度的抽象与交换代数问题。实验表明当前最先进的大语言模型在该基准上表现极差（FATE-H仅3%，FATE-X为0%），揭示了其在自然语言推理与形式化能力之间的显著差距，并系统分析了形式化过程中的常见错误。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在IMO等竞赛数学基准上表现优异，但这些竞赛无法反映现代数学研究的深度、广度与抽象性。为填补这一空白，作者构建了一个更贴近高等数学研究的形式化代数推理新基准。

Method: 作者设计并发布了FATE基准，包括FATE-H和FATE-X两个子集，各含100道形式化代数问题；通过两阶段评估（自然语言推理与形式化）测试当前最先进的大语言模型证明器，并对错误类型进行系统分类，同时比较专用证明器与通用模型的表现差异。

Result: 最佳模型在FATE-H上的准确率仅为3%（pass@64），在FATE-X上为0%；模型的自然语言推理能力显著优于其形式化能力；专用证明器在自然语言阶段的反思能力不如通用模型，导致整体表现更差。

Conclusion: FATE是一个具有挑战性的新基准，能够有效衡量大语言模型在迈向研究级形式化数学推理道路上的能力，并为未来研究提供关键评估节点。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in formal theorem proving, particularly on contest-based
mathematical benchmarks like the IMO. However, these contests do not reflect
the depth, breadth, and abstraction of modern mathematical research. To bridge
this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new
benchmark series in formal algebra designed to chart a course toward advanced
mathematical reasoning. We present two new components, FATE-H and FATE-X, each
with 100 problems in abstract and commutative algebra. The FATE series spans a
difficulty spectrum from undergraduate exercises to problems exceeding PhD
qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both
PhD-level exam difficulty and the coverage of the Mathlib library. Our
evaluations of state-of-the-art LLM provers on this new benchmark reveal a
stark performance gap compared to contest math: the best model achieves only 3%
(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals
that models' natural-language reasoning is notably more accurate than their
ability to formalize this reasoning. We systematically classify the common
errors that arise during this formalization process. Furthermore, a comparative
study shows that a specialized prover can exhibit less effective reflection
than general-purpose models, reducing its accuracy at the natural-language
stage. We believe FATE provides a robust and challenging benchmark that
establishes essential checkpoints on the path toward research-level formal
mathematical reasoning.

</details>


### [63] [Test-time Adaptation of Tiny Recursive Models](https://arxiv.org/abs/2511.02886)
*Ronan Killian McGovern*

Main category: cs.LG

TL;DR: 本文提出了一种在计算资源受限条件下高效微调预训练微型递归模型（TRM）的方法，在ARC Prize竞赛中实现了6.67%的得分。


<details>
  <summary>Details</summary>
Motivation: 在ARC Prize竞赛中，现有领先的开源方法TRM虽然性能较好，但所需计算资源远超竞赛限制，因此需要一种在合规算力下仍能有效提升性能的微调策略。

Method: 首先在一个包含1,280个公开ARC任务的数据集上对7M参数的微型递归模型进行预训练，随后在竞赛期间对该完整模型进行全参数微调（而非仅LoRA或任务嵌入微调），仅用12,500个梯度步完成。

Result: 预训练模型在公开评估集上达到约10%的得分；经竞赛阶段微调后，在半私有评估任务上取得6.67%的得分，且满足竞赛计算资源限制。

Conclusion: 通过在公开任务上充分预训练并结合全参数微调，可在竞赛允许的计算预算内有效提升TRM模型在ARC任务上的表现。

Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source
approach - known as TRM, or Tiny Recursive Models - involved training a 7M
parameter recursive neural network on augmented variants of ARC tasks. That
approach scored approximately 7.8% on the public ARC AGI II evaluation set, but
required a level of compute far in excess of what is allowed during the
competition. This paper shows that, by starting from a tiny recursive model
that has been pre-trained on public ARC tasks, one can efficiently fine-tune on
competition tasks within the allowed compute limits. Specifically, a model was
pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on
4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model
was then post-trained in just 12,500 gradient steps during the competition to
reach a score of 6.67% on semi-private evaluation tasks. Notably, such
post-training performance is achieved by full-fine tuning of the tiny model,
not LoRA fine-tuning or fine-tuning of task embeddings alone.

</details>


### [64] [Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets](https://arxiv.org/abs/2511.02887)
*Chaitanya Rele,Aditya Rathod,Kaustubh Natu,Saurabh Kulkarni,Ajay Koli,Swapnali Makdey*

Main category: cs.LG

TL;DR: 本文提出一种基于人工智能的潜在渔区（PFZ）预测框架，利用海表温度和叶绿素浓度等海洋参数，旨在帮助北印度洋渔民更高效、可持续地定位渔场。


<details>
  <summary>Details</summary>
Motivation: 北印度洋沿海渔民在寻找高产渔场时面临不确定性，影响生计；现有方法在准确性和区域适用性方面存在不足。

Method: 采用人工智能辅助框架，结合海表温度和叶绿素浓度等海洋学参数，进行潜在渔区预测。

Result: 初步结果表明，该框架可减少渔民搜寻时间、降低燃油消耗，并提升资源利用效率。

Conclusion: 该AI辅助框架有助于提高潜在渔区识别的准确性，为北印度洋地区实现可持续渔业提供技术支持。

Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,
represents a vital source of livelihood for coastal communities, yet fishermen
often face uncertainty in locating productive fishing grounds. To address this
challenge, we present an AI-assisted framework for predicting Potential Fishing
Zones (PFZs) using oceanographic parameters such as sea surface temperature and
chlorophyll concentration. The approach is designed to enhance the accuracy of
PFZ identification and provide region-specific insights for sustainable fishing
practices. Preliminary results indicate that the framework can support
fishermen by reducing search time, lowering fuel consumption, and promoting
efficient resource utilization.

</details>


### [65] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 本文提出一种基于大语言模型（LLM）的新框架，用于可穿戴物联网中人体活动识别（HAR）系统的数据投毒检测与清洗，采用零样本、单样本和少样本学习，并结合角色扮演提示和逐步推理策略，在无需大量标注数据的情况下实现实时、自适应的安全防护。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴传感设备在物联网（如医疗、智能家居和工业场景）中的广泛应用，人体活动识别（HAR）系统面临数据投毒攻击的威胁，而传统防御方法依赖大量标注数据，难以适应动态环境，因此亟需更灵活、高效的防御机制。

Method: 利用大语言模型（LLM），结合角色扮演提示（让LLM扮演专家评估传感器异常）和逐步推理（引导LLM识别原始传感器数据中的投毒迹象并生成合理干净数据），在零样本、单样本和少样本学习范式下实现投毒检测与数据清洗。

Result: 实验全面评估了该框架在检测准确率、清洗质量、延迟和通信开销等方面的性能，验证了LLM在提升可穿戴物联网系统安全性和可靠性方面的实用性和有效性。

Conclusion: 所提出的基于LLM的框架显著减少了对大规模标注数据的依赖，能够在动态IoT环境中提供实时、鲁棒且自适应的HAR系统安全防护，为可穿戴设备的安全应用提供了新思路。

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [66] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: 本文利用开源大语言模型Llama 3.1-405B对引用基因组数据集的文献进行零样本数据使用场景分类，提出了一种新的评估框架，在未预定义类别的情况下取得了0.674的F1分数。


<details>
  <summary>Details</summary>
Motivation: 在已知论文引用特定数据集的基础上，进一步理解其使用方式和原因具有重要意义；传统方法依赖人工标注或构建训练数据成本高昂，而近年来的大语言模型为此提供了可扩展的自动化解决方案。

Method: 应用开源大语言模型Llama 3.1-405B，在零样本设置下为已知引用特定基因组数据集的论文生成结构化的数据使用标签，并提出一种新颖的评估框架来衡量方法效果。

Result: 该模型在无预定义类别的零样本数据引用分类任务中达到了0.674的F1分数，显示出良好潜力，但也受到数据可用性、提示过拟合、计算资源及评估成本等限制。

Conclusion: 大语言模型在自动识别科学文献中数据使用场景方面具有可行性，但仍需克服数据、提示设计、基础设施和评估成本等方面的挑战。

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [67] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: 本文提出了ROGUE-TS算法，结合概率裁剪机制，在ROGUE bandit框架下兼顾个性化推荐与群体效应估计，在保持低遗憾的同时提升微随机试验（MRT）中的统计功效。


<details>
  <summary>Details</summary>
Motivation: 现有算法在处理奖励随时间变化（如习惯化与恢复）的非平稳性问题时，过度强调利用而探索不足，难以准确估计群体层面的干预效果，这在微随机试验（MRT）中尤为关键。

Method: 提出适用于ROGUE框架的Thompson Sampling算法ROGUE-TS，并引入概率裁剪机制，在遗憾与最小探索概率之间进行量化权衡。

Result: 在两个MRT数据集（促进身体活动和双相情感障碍治疗）上的实验表明，该方法比现有方法遗憾更低，且通过裁剪机制保持高统计功效，未显著增加遗憾。

Conclusion: 该框架为MRT研究者提供了在个性化推荐与统计有效性之间取得平衡的实用指导，有助于可靠检测考虑个体行为动态的干预效果。

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [68] [Inference-Time Personalized Alignment with a Few User Preference Queries](https://arxiv.org/abs/2511.02966)
*Victor-Alexandru Pădurean,Parameswaran Kamalaruban,Nachiket Kotalwar,Alkis Gotovos,Adish Singla*

Main category: cs.LG

TL;DR: 本文提出了一种名为UserAlign的推理时个性化对齐方法，通过少量成对响应比较查询快速识别用户偏好的最佳生成结果。


<details>
  <summary>Details</summary>
Motivation: 现有个性化对齐方法要么需要大量用户偏好查询，要么要求偏好以文本形式明确指定，限制了实际应用。因此，作者希望设计一种更高效、更少依赖显式输入的个性化对齐方法。

Method: UserAlign基于逻辑斯蒂老虎机（logistic bandits）中的best-arm identification理论框架，在模型生成的固定响应池中，通过少量成对比较反馈快速选出最符合用户偏好的响应，并假设用户反馈是一致且无噪声的。

Result: 在多个涉及个性化文本和图像生成的任务中，UserAlign展现出良好的个性化对齐效果。

Conclusion: UserAlign是一种高效的推理时个性化对齐方法，仅需少量成对比较即可有效捕捉用户偏好，适用于多种生成任务。

Abstract: We study the problem of aligning a generative model's response with a user's
preferences. Recent works have proposed several different formulations for
personalized alignment; however, they either require a large amount of user
preference queries or require that the preference be explicitly specified as a
text input. In this paper, we propose a novel inference-time personalized
alignment method, UserAlign, that elicits the user's preferences with a few
queries as pairwise response comparisons. In particular, UserAlign builds on
the theoretical framework of best-arm identification in logistic bandits and
selects a personalized response from a fixed pool of the model's generated
responses. The key idea is to consider the user's feedback consistent and
noise-free, and incorporate it into the theoretical framework to identify the
best response quickly. Experimental results across several tasks, involving
personalized text and image generation, showcase the effectiveness of UserAlign
in achieving personalized alignment.

</details>


### [69] [Adaptive-Sensorless Monitoring of Shipping Containers](https://arxiv.org/abs/2511.03022)
*Lingqing Shen,Chi Heem Wong,Misaki Mito,Arnab Chakrabarti*

Main category: cs.LG

TL;DR: 本文提出了一种名为“残差校正方法”的通用框架，用于在获取实时遥测数据后修正无传感器模型中的系统性偏差，从而提升对集装箱内部温湿度预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 无传感器监测虽有潜力替代传统传感器监测，但其未利用遥测信息且存在系统误差，导致预测结果与实际数据偏差较大，影响用户判断。

Method: 引入残差校正方法，构建“自适应无传感器”模型，在观测到实时遥测数据后对原始无传感器模型的系统偏差进行校正。

Result: 在包含348万条数据点的最大集装箱传感器数据集上评估，自适应无传感器模型在温度和相对湿度预测上均优于基线无传感器模型，MAE和RMSE显著降低。

Conclusion: 自适应无传感器模型能更准确地监测货物状态、提前发现风险，并减少对全球航运中全程连接的依赖。

Abstract: Monitoring the internal temperature and humidity of shipping containers is
essential to preventing quality degradation during cargo transportation.
Sensorless monitoring -- machine learning models that predict the internal
conditions of the containers using exogenous factors -- shows promise as an
alternative to monitoring using sensors. However, it does not incorporate
telemetry information and correct for systematic errors, causing the
predictions to differ significantly from the live data and confusing the users.
In this paper, we introduce the residual correction method, a general framework
for correcting for systematic biases in sensorless models after observing live
telemetry data. We call this class of models ``adaptive-sensorless''
monitoring. We train and evaluate adaptive-sensorless models on the 3.48
million data points -- the largest dataset of container sensor readings ever
used in academic research -- and show that they produce consistent improvements
over the baseline sensorless models. When evaluated on the holdout set of the
simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$
2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$
7.09% for relative humidity (vs 7.99% by sensorless) and average root
mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs
3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs
10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo
monitoring, early risk detection, and less dependence on full connectivity in
global shipping.

</details>


### [70] [Discrete Bayesian Sample Inference for Graph Generation](https://arxiv.org/abs/2511.03015)
*Ole Petersen,Marcel Kollovieh,Marten Lienen,Stephan Günnemann*

Main category: cs.LG

TL;DR: GraphBSI 是一种基于贝叶斯样本推理（BSI）的一次生成图模型，通过在连续分布参数空间中迭代优化对图的信念，有效处理离散结构，并在分子和合成图生成任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型难以处理图数据的离散性和无序性，促使研究者探索如离散扩散和流匹配等新方法；本文旨在提出一种更高效、理论基础扎实的一次性图生成模型。

Method: 提出 GraphBSI 模型，基于贝叶斯样本推理（BSI），在连续分布参数空间中迭代更新图的后验信念；将 BSI 表述为随机微分方程（SDE），并推导出一类保持边缘分布的噪声控制 SDE，利用得分函数近似实现生成。

Result: 在 Moses 和 GuacaMol 等标准基准上，GraphBSI 在分子和合成图生成任务中优于现有的一次性图生成模型，取得当前最优性能。

Conclusion: GraphBSI 通过贝叶斯推理框架有效解决了图生成中的离散结构建模难题，兼具理论严谨性和实证优越性，为图生成任务提供了新思路。

Abstract: Generating graph-structured data is crucial in applications such as molecular
generation, knowledge graphs, and network analysis. However, their discrete,
unordered nature makes them difficult for traditional generative models,
leading to the rise of discrete diffusion and flow matching models. In this
work, we introduce GraphBSI, a novel one-shot graph generative model based on
Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI
iteratively refines a belief over graphs in the continuous space of
distribution parameters, naturally handling discrete structures. Further, we
state BSI as a stochastic differential equation (SDE) and derive a
noise-controlled family of SDEs that preserves the marginal distributions via
an approximation of the score function. Our theoretical analysis further
reveals the connection to Bayesian Flow Networks and Diffusion models. Finally,
in our empirical evaluation, we demonstrate state-of-the-art performance on
molecular and synthetic graph generation, outperforming existing one-shot graph
generative models on the standard benchmarks Moses and GuacaMol.

</details>


### [71] [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)
*Gaia Grosso,Sai Sumedh R. Hindupur,Thomas Fel,Samuel Bright-Thonney,Philip Harris,Demba Ba*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏、局部性和竞争性原则的自组织局部核方法SparKer，用于在缺乏先验信息的情况下高效、可解释地检测高维数据中的异常。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在处理现代AI生成的复杂数据表示时，因统计特性控制不足而难以发现微弱或罕见的异常信号，亟需一种在极少先验信息下仍有效的检测框架。

Method: 提出结构化设计准则（稀疏性、局部性、竞争性），构建自组织局部核；具体实现为SparKer——一个在半监督Neyman-Pearson框架下训练的稀疏高斯核集成，用于局部建模样本与无异常参考之间的似然比。

Result: SparKer在科学发现、开放世界新奇检测、入侵检测和生成模型验证等多个高维真实任务中表现优异，仅用少量核即可在数千维空间中识别出具有统计显著性的异常区域。

Conclusion: 所提方法兼具可解释性、效率与可扩展性，有效弥合了现有异常检测方法在复杂表示空间中对弱/稀有异常信号检测能力的不足。

Abstract: Modern artificial intelligence has revolutionized our ability to extract rich
and versatile data representations across scientific disciplines. Yet, the
statistical properties of these representations remain poorly controlled,
causing misspecified anomaly detection (AD) methods to falter. Weak or rare
signals can remain hidden within the apparent regularity of normal data,
creating a gap in our ability to detect and interpret anomalies. We examine
this gap and identify a set of structural desiderata for detection methods
operating under minimal prior information: sparsity, to enforce parsimony;
locality, to preserve geometric sensitivity; and competition, to promote
efficient allocation of model capacity. These principles define a class of
self-organizing local kernels that adaptively partition the representation
space around regions of statistical imbalance. As an instantiation of these
principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained
within a semi-supervised Neyman--Pearson framework to locally model the
likelihood ratio between a sample that may contain anomalies and a nominal,
anomaly-free reference. We provide theoretical insights into the mechanisms
that drive detection and self-organization in the proposed model, and
demonstrate the effectiveness of this approach on realistic high-dimensional
problems of scientific discovery, open-world novelty detection, intrusion
detection, and generative-model validation. Our applications span both the
natural- and computer-science domains. We demonstrate that ensembles containing
only a handful of kernels can identify statistically significant anomalous
locations within representation spaces of thousands of dimensions, underscoring
both the interpretability, efficiency and scalability of the proposed approach.

</details>


### [72] [Leveraging Discrete Function Decomposability for Scientific Design](https://arxiv.org/abs/2511.03032)
*James C. Bowden,Sergey Levine,Jennifer Listgarten*

Main category: cs.LG

TL;DR: 本文提出了一种名为DADO（Decomposition-Aware Distributional Optimization）的新算法，用于在离散设计空间中更高效地进行分布优化，尤其适用于具有可分解结构的科学设计问题（如蛋白质设计）。该方法利用变量间的分解结构（通过连接树表示），采用软因子化的搜索分布并结合图消息传递机制，协调各因子间的优化过程。


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的科学与工程中，常需根据用户指定属性在计算机中设计离散对象（如蛋白质、电路、材料）。现有分布优化算法难以利用许多科学预测模型中固有的可分解结构（即目标函数可在设计变量上因子化），导致优化效率低下。

Method: 提出DADO算法，通过构建一个软因子化的“搜索分布”（即学习得到的生成模型），并利用基于连接树的消息传递机制，在具有可分解结构的设计空间中高效协调各因子的联合优化。

Result: DADO能够有效利用设计变量间的可分解性结构，相比现有分布优化方法，在理论上和实验上展现出更高的优化效率。

Conclusion: DADO为具有可分解结构的离散设计问题提供了一种更高效的分布优化框架，有望提升AI在科学发现与工程设计中的实用性。

Abstract: In the era of AI-driven science and engineering, we often want to design
discrete objects in silico according to user-specified properties. For example,
we may wish to design a protein to bind its target, arrange components within a
circuit to minimize latency, or find materials with certain properties. Given a
property predictive model, in silico design typically involves training a
generative model over the design space (e.g., protein sequence space) to
concentrate on designs with the desired properties. Distributional optimization
-- which can be formalized as an estimation of distribution algorithm or as
reinforcement learning policy optimization -- finds the generative model that
maximizes an objective function in expectation. Optimizing a distribution over
discrete-valued designs is in general challenging because of the combinatorial
nature of the design space. However, many property predictors in scientific
applications are decomposable in the sense that they can be factorized over
design variables in a way that could in principle enable more effective
optimization. For example, amino acids at a catalytic site of a protein may
only loosely interact with amino acids of the rest of the protein to achieve
maximal catalytic activity. Current distributional optimization algorithms are
unable to make use of such decomposability structure. Herein, we propose and
demonstrate use of a new distributional optimization algorithm,
Decomposition-Aware Distributional Optimization (DADO), that can leverage any
decomposability defined by a junction tree on the design variables, to make
optimization more efficient. At its core, DADO employs a soft-factorized
"search distribution" -- a learned generative model -- for efficient navigation
of the search space, invoking graph message-passing to coordinate optimization
across linked factors.

</details>


### [73] [Scaling Multi-Agent Environment Co-Design with Diffusion Models](https://arxiv.org/abs/2511.03100)
*Hao Xiang Li,Michael Amir,Amanda Prorok*

Main category: cs.LG

TL;DR: 本文提出了Diffusion Co-Design（DiCoDe）框架，通过Projected Universal Guidance（PUG）和critic蒸馏机制，实现了在高维环境设计空间中高效、可扩展的智能体-环境协同设计，在多个多智能体基准任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前智能体-环境协同设计方法难以扩展，面对高维环境设计空间时性能下降，并且在联合优化过程中因目标动态变化而样本效率低下。

Method: 提出DiCoDe框架，包含两项核心创新：1）Projected Universal Guidance（PUG），用于在满足硬约束条件下探索奖励最大化的环境分布；2）critic蒸馏机制，利用强化学习critic的知识共享，使引导扩散模型能随智能体策略演化而持续更新。

Result: 在仓库自动化、多智能体路径规划和风电场优化等任务上验证了方法的有效性，例如在仓库场景中以减少66%的仿真样本获得39%更高的奖励，显著超越现有技术。

Conclusion: DiCoDe为智能体-环境协同设计设立了新标准，是迈向实际应用的重要一步。

Abstract: The agent-environment co-design paradigm jointly optimises agent policies and
environment configurations in search of improved system performance. With
application domains ranging from warehouse logistics to windfarm management,
co-design promises to fundamentally change how we deploy multi-agent systems.
However, current co-design methods struggle to scale. They collapse under
high-dimensional environment design spaces and suffer from sample inefficiency
when addressing moving targets inherent to joint optimisation. We address these
challenges by developing Diffusion Co-Design (DiCoDe), a scalable and
sample-efficient co-design framework pushing co-design towards practically
relevant settings. DiCoDe incorporates two core innovations. First, we
introduce Projected Universal Guidance (PUG), a sampling technique that enables
DiCoDe to explore a distribution of reward-maximising environments while
satisfying hard constraints such as spatial separation between obstacles.
Second, we devise a critic distillation mechanism to share knowledge from the
reinforcement learning critic, ensuring that the guided diffusion model adapts
to evolving agent policies using a dense and up-to-date learning signal.
Together, these improvements lead to superior environment-policy pairs when
validated on challenging multi-agent environment co-design benchmarks including
warehouse automation, multi-agent pathfinding and wind farm optimisation. Our
method consistently exceeds the state-of-the-art, achieving, for example, 39%
higher rewards in the warehouse setting with 66% fewer simulation samples. This
sets a new standard in agent-environment co-design, and is a stepping stone
towards reaping the rewards of co-design in real world domains.

</details>


### [74] [Data-Efficient Realized Volatility Forecasting with Vision Transformers](https://arxiv.org/abs/2511.03046)
*Emi Soroka,Artem Arzyn*

Main category: cs.LG

TL;DR: 本文探索将视觉Transformer（ViT）用于期权数据，通过单日隐含波动率曲面预测未来30天的实际波动率，初步结果表明ViT能有效捕捉曲面中的非线性特征和季节性模式。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformer模型在金融时间序列预测中已展现潜力，但其在期权数据上的应用仍鲜有研究；作者旨在探索适用于期权数据的Transformer模型。

Method: 使用通常用于图像识别的Vision Transformer（ViT）架构，以包含日期信息的单日隐含波动率曲面作为输入，训练模型预测资产未来30天的实际波动率。

Result: ViT能够从隐含波动率曲面中学习到季节性模式和非线性特征，显示出在期权数据建模中的潜力。

Conclusion: 将ViT应用于期权数据是一个有前景的研究方向，为开发更复杂的金融机器学习模型提供了新思路。

Abstract: Recent work in financial machine learning has shown the virtue of complexity:
the phenomenon by which deep learning methods capable of learning highly
nonlinear relationships outperform simpler approaches in financial forecasting.
While transformer architectures like Informer have shown promise for financial
time series forecasting, the application of transformer models for options data
remains largely unexplored. We conduct preliminary studies towards the
development of a transformer model for options data by training the Vision
Transformer (ViT) architecture, typically used in modern image recognition and
classification systems, to predict the realized volatility of an asset over the
next 30 days from its implied volatility surface (augmented with date
information) for a single day. We show that the ViT can learn seasonal patterns
and nonlinear features from the IV surface, suggesting a promising direction
for model development.

</details>


### [75] [Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions](https://arxiv.org/abs/2511.03047)
*Emi Soroka,Tanmay Chopra,Krish Desai,Sanjay Lall*

Main category: cs.LG

TL;DR: 本文提出了首个用于目标驱动交互的无监督评估指标，利用未标注交互数据的统计特性及微调后的大型语言模型（LLM）来适应分布变化，无需依赖人工标注的理想响应即可评估用户目标识别、目标完成度和LLM不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法在企业级LLM应用中面临挑战：数据复杂且缺乏标签、人工标注难以规模化、定制指标无法检测未知错误、LLM评判结果不可靠。

Method: 提出一套无监督评估指标，结合未标注交互数据的统计特性和微调LLM，以适应分布偏移，并在不依赖人工理想响应的前提下，实现对用户目标标注、目标完成度和模型不确定性的量化。

Result: 该方法在开放域和任务特定的交互数据上进行了验证，展示了其有效性。

Conclusion: 所提出的无监督指标为评估目标驱动的LLM交互提供了一种可行且可扩展的新途径。

Abstract: Large language models (LLMs) have seen increasing popularity in enterprise
applications where AI agents and humans engage in objective-driven
interactions. However, these systems are difficult to evaluate: data may be
complex and unlabeled; human annotation is often impractical at scale; custom
metrics can monitor for specific errors, but not previously-undetected ones;
and LLM judges can produce unreliable results. We introduce the first set of
unsupervised metrics for objective-driven interactions, leveraging statistical
properties of unlabeled interaction data and using fine-tuned LLMs to adapt to
distributional shifts. We develop metrics for labeling user goals, measuring
goal completion, and quantifying LLM uncertainty without grounding evaluations
in human-generated ideal responses. Our approach is validated on open-domain
and task-specific interaction data.

</details>


### [76] [FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation](https://arxiv.org/abs/2511.03113)
*Jiameng Chen,Yida Xiong,Kun Li,Hongzhi Zhang,Xiantao Cai,Wenbin Hu,Jia Wu*

Main category: cs.LG

TL;DR: FP-AbDiff 是一种基于 Fokker-Planck 方程物理约束的新型抗体生成模型，在 CDR-H3 和六 CDR 共设计任务中显著优于现有方法，实现了更高几何精度和氨基酸恢复率。


<details>
  <summary>Details</summary>
Motivation: 现有抗体生成模型存在动力学不一致（导致结构不可信）和泛化能力差（因数据稀缺与结构偏差）两大核心问题。

Method: 提出 FP-AbDiff 模型，首次在整条生成轨迹上施加 Fokker-Planck 方程（FPE）物理约束；通过在 CDR 几何混合流形（R³ × SO(3)）上最小化新颖的 FPE 残差损失，使局部去噪得分构成全局一致的概率流，并结合 SE(3)-等变扩散框架与深度生物先验。

Result: 在 RAbD 基准测试中，FP-AbDiff 在从头 CDR-H3 设计中达到 0.99 Å 的平均 RMSD（较 AbX 提升 25%），接触氨基酸恢复率达 39.91%；在六 CDR 共设计中，全链 RMSD 降低约 15%，CDR-H3 环的全链氨基酸恢复率达 45.67%。

Conclusion: 通过将生成动力学与物理定律对齐，FP-AbDiff 提升了模型的鲁棒性与泛化能力，为物理可信且功能可行的抗体设计提供了原则性方法。

Abstract: Computational antibody design holds immense promise for therapeutic
discovery, yet existing generative models are fundamentally limited by two core
challenges: (i) a lack of dynamical consistency, which yields physically
implausible structures, and (ii) poor generalization due to data scarcity and
structural bias. We introduce FP-AbDiff, the first antibody generator to
enforce Fokker-Planck Equation (FPE) physics along the entire generative
trajectory. Our method minimizes a novel FPE residual loss over the mixed
manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising
scores to assemble into a globally coherent probability flow. This
physics-informed regularizer is synergistically integrated with deep biological
priors within a state-of-the-art SE(3)-equivariant diffusion framework.
Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a
new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean
Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25%
improvement over the previous state-of-the-art model, AbX, and the highest
reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored
in the more challenging six-CDR co-design task, where our model delivers
consistently superior geometric precision, cutting the average full-chain Root
Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain
Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By
aligning generative dynamics with physical laws, FP-AbDiff enhances robustness
and generalizability, establishing a principled approach for physically
faithful and functionally viable antibody design.

</details>


### [77] [The Curved Spacetime of Transformer Architectures](https://arxiv.org/abs/2511.03060)
*Riccardo Di Sipio,Jairo Diaz-Rodriguez,Luis Serrano*

Main category: cs.LG

TL;DR: 本文提出了一种将Transformer语言模型与广义相对论类比的几何框架，认为注意力机制在嵌入空间中诱导出曲率，并通过实验验证了词元表征在层间演化路径上的弯曲现象。


<details>
  <summary>Details</summary>
Motivation: 为了深入理解Transformer模型内部工作机制，作者受广义相对论启发，试图从几何角度解释注意力机制如何塑造嵌入空间中的表征演化路径。

Method: 将查询和键视为在表示空间中诱导有效度量，注意力作为离散联络实现值向量的平行移动；通过堆叠层构建离散时间切片，并将反向传播类比为最小作用量原理。设计三项实验：(i) 可视化段落级别的曲率景观，(ii) 通过模拟排除维度和随机性对路径弯曲的影响，(iii) 类比爱丁顿日食实验，通过上下文编辑探测嵌入轨迹的偏折。

Result: 实验结果表明：嵌入轨迹存在显著弯曲，局部转向角随词元和层数变化；尖锐/平坦角度的异常分布及长度-弦长比无法由高维或随机因素解释；在受控上下文编辑下，轨迹偏折具有语义一致性，证实了注意力诱导的曲率效应。

Conclusion: Transformer中的注意力机制确实在嵌入空间中引入了几何曲率，词元表征的演化路径并非直线而是受曲率影响而弯曲，支持了所提出的类比框架。

Abstract: We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein's eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.

</details>


### [78] [An Augmentation Overlap Theory of Contrastive Learning](https://arxiv.org/abs/2511.03114)
*Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 本文提出了基于数据增强重叠的新理论，用于解释对比学习的机制，并据此设计了一种无需额外模块即可有效评估表征质量的无监督指标。


<details>
  <summary>Details</summary>
Motivation: 当前自监督对比学习虽在多种任务中表现优异，但其内在机制尚不明确，尤其在条件独立性假设过于理想化的情况下，亟需更贴近实际的理论解释。

Method: 作者首先在广泛采用的条件独立性假设下推导出最紧致的性能界限，随后放松该假设，提出更具实用性的“增强重叠”假设，并在此基础上推导出下游性能的渐近闭式界；同时基于该理论构建了一个无监督的表征评估指标。

Result: 所提出的增强重叠理论揭示了强数据增强下同类样本支持集重叠增加的现象，解释了仅对正样本对齐即可实现类内聚类的原因；新设计的无监督评估指标与下游性能高度一致。

Conclusion: 对比学习的有效性可归因于数据增强带来的类内样本支持重叠，基于此提出的评估指标能有效反映表征质量，为理解与优化对比学习提供了新视角。

Abstract: Recently, self-supervised contrastive learning has achieved great success on
various tasks. However, its underlying working mechanism is yet unclear. In
this paper, we first provide the tightest bounds based on the widely adopted
assumption of conditional independence. Further, we relax the conditional
independence assumption to a more practical assumption of augmentation overlap
and derive the asymptotically closed bounds for the downstream performance. Our
proposed augmentation overlap theory hinges on the insight that the support of
different intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Moreover, from the newly derived augmentation overlap perspective, we
develop an unsupervised metric for the representation evaluation of contrastive
learning, which aligns well with the downstream performance almost without
relying on additional modules. Code is available at
https://github.com/PKU-ML/GARC.

</details>


### [79] [Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach](https://arxiv.org/abs/2511.03074)
*Fatemeh Ghaffari,Siddarth Sitaraman,Xutong Liu,Xuchuang Wang,Mohammad Hajiesmaili*

Main category: cs.LG

TL;DR: 本文提出了一种名为MSUCB的鲁棒在线学习排序算法，通过引入一种新颖的“均值-中位数”估计器，在无干扰时保持最优对数后悔界，在存在点击欺诈等干扰时仅增加与总干扰量相关的加性后悔项，并在真实数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线学习排序系统易受点击欺诈等恶意干扰影响，导致学习过程被误导并损害用户体验，因此需要设计对干扰具有鲁棒性的算法。

Method: 提出MSUCB算法，采用一种新的均值-中位数估计器：在无干扰时表现如标准均值，在有干扰时通过中位数步骤过滤异常和被污染样本，并在每轮更新估计以加速收敛。

Result: 在无干扰下实现最优对数后悔界；在有干扰下后悔仅增加一个与总干扰量相关的加性项；在真实数据集上相比两种前沿方法分别实现了97.35%和91.60%的后悔改善。

Conclusion: MSUCB在保持无干扰性能的同时，对点击欺诈等干扰具有强鲁棒性，显著优于现有方法，验证了所提估计器在带干扰老虎机问题中的有效性。

Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list
of items from a large pool and improves future rankings based on user clicks.
This setting is commonly modeled as cascading bandits, where the objective is
to maximize the likelihood that the user clicks on at least one of the
presented items across as many timesteps as possible. However, such systems are
vulnerable to click fraud and other manipulations (i.e., corruption), where
bots or paid click farms inject corrupted feedback that misleads the learning
process and degrades user experience. In this paper, we propose MSUCB, a robust
algorithm that incorporates a novel mean-of-medians estimator, which to our
knowledge is applied to bandits with corruption setting for the first time.
This estimator behaves like a standard mean in the absence of corruption, so no
cost is paid for robustness. Under corruption, the median step filters out
outliers and corrupted samples, keeping the estimate close to its true value.
Updating this estimate at every round further accelerates empirical convergence
in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence
of corruption and degrades gracefully under corruptions, with regret increasing
only by an additive term tied to the total corruption. Comprehensive and
extensive experiments on real-world datasets further demonstrate that our
approach consistently outperforms prior methods while maintaining strong
robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret
improvement over two state-of-the-art methods.

</details>


### [80] [Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction](https://arxiv.org/abs/2511.03149)
*Atif Hassan,Tarun Kumar,Ashish Mishra,Sergey Serebryakov,Satish Kumar Mopur,Phanidhar Koganti,Murthy Chelankuri,Ramanagopal Vogety,Suparna Bhattacharya,Martin Foltin*

Main category: cs.LG

TL;DR: 本文提出Forecast2Anomaly（F2A）框架，通过联合预测-异常损失和检索增强生成模块，使预训练时间序列基础模型（TSFM）具备零样本异常预测能力，在16个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有异常预测方法难以泛化到不断演变的异常模式，而预训练时间序列基础模型（TSFM）虽具备强大的零样本预测能力，但尚未被有效用于异常预测任务。

Method: F2A包含两个核心创新：一是设计联合预测-异常损失函数，对TSFM进行微调以在异常点准确预测；二是引入检索增强生成（RAG）模块，在推理时动态检索历史相关时段并据此调整预测，以适应分布变化。

Result: 在16个多样化数据集和多种TSFM主干上的实验表明，F2A在零样本异常预测任务中始终优于当前最先进的方法。

Conclusion: F2A成功将TSFM的零样本预测能力拓展至异常预测领域，提供了一种可扩展、无需更新模型即可适应动态异常的实用解决方案。

Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from
different real-world, dynamic, and complex systems is vital for preempting
critical failures, leading to a substantial minimization in operational costs
and human labor. Yet, existing methods are limited to specific systems while
failing to generalize to evolving anomaly patterns over time. In contrast,
pretrained Time Series Foundation Models (TSFMs) have recently demonstrated
strong generalization and zero-shot forecasting capabilities. However, their
potential remains untapped for anomaly prediction, a task fundamentally
different from forecasting normal behavior. Thus, we present Forecast2Anomaly
(F2A), a novel framework that empowers TSFMs with anomaly prediction abilities
through two key innovations. First, we propose a joint forecast-anomaly loss
that fine-tunes TSFMs to accurately forecast future signals even at anomalous
time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module
that retrieves historically relevant horizons and conditions predictions on
them. This component dynamically adapts to distributional shifts at inference
time, enabling F2A to track evolving anomalies without requiring model updates.
By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap
between robust TSFM zero-shot forecasting and zero-shot anomaly prediction.
Extensive experiments across 16 diverse datasets and multiple TSFM backbones
show that F2A consistently outperforms state-of-the-art methods, offering a
scalable, zero-shot anomaly prediction solution for real-world applications.

</details>


### [81] [Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality](https://arxiv.org/abs/2511.03190)
*Mingtao Zhang,Guoli Yang,Zhanxing Zhu,Mengzhu Wang,Xiaoying Bai*

Main category: cs.LG

TL;DR: 本文提出了一种基于熵的线性注意力机制，通过理论分析表明注意力机制的有效性可能源于权重分布的平衡性而非softmax的非线性，并在多个时空数据集上验证了该方法在保持或提升预测性能的同时显著降低了计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制因二次计算复杂度难以扩展到长序列，限制了其在时间序列建模等任务中的应用。

Method: 基于熵作为概率单纯形上的严格凹函数这一性质，提出一种线性复杂度的近似算法，通过保持概率排序一致性和熵值相近来构建线性注意力机制。

Result: 在四个时空数据集上的实验表明，所提方法在预测性能上具有竞争力甚至更优，同时显著减少了内存占用和计算时间。

Conclusion: 注意力机制在时空序列建模中的有效性可能主要来自于获得适度且均衡的权重分布，而非softmax的非线性；所提出的线性注意力机制在效率和性能之间取得了良好平衡。

Abstract: Attention mechanisms have been extensively employed in various applications,
including time series modeling, owing to their capacity to capture intricate
dependencies; however, their utility is often constrained by quadratic
computational complexity, which impedes scalability for long sequences. In this
work, we propose a novel linear attention mechanism designed to overcome these
limitations. Our approach is grounded in a theoretical demonstration that
entropy, as a strictly concave function on the probability simplex, implies
that distributions with aligned probability rankings and similar entropy values
exhibit structural resemblance. Building on this insight, we develop an
efficient approximation algorithm that computes the entropy of
dot-product-derived distributions with only linear complexity, enabling the
implementation of a linear attention mechanism based on entropy equality.
Through rigorous analysis, we reveal that the effectiveness of attention in
spatio-temporal time series modeling may not primarily stem from the
non-linearity of softmax but rather from the attainment of a moderate and
well-balanced weight distribution. Extensive experiments on four
spatio-temporal datasets validate our method, demonstrating competitive or
superior forecasting performance while achieving substantial reductions in both
memory usage and computational time.

</details>


### [82] [An Efficient Classification Model for Cyber Text](https://arxiv.org/abs/2511.03107)
*Md Sakhawat Hossen,Md. Zashid Iqbal Borshon,A. S. M. Badrudduza*

Main category: cs.LG

TL;DR: 本文提出了一种改进的TF-IDF算法（CTF-IDF）和更快的IRLBA降维方法，用于传统文本分析流程，以降低计算资源消耗和碳足迹，在仅轻微牺牲准确率的情况下显著提升效率和速度。


<details>
  <summary>Details</summary>
Motivation: 深度学习在文本分析中的广泛应用带来了巨大的计算资源消耗和碳足迹问题，促使作者探索更高效、低能耗的替代方案。

Method: 修改原始TF-IDF算法为CTF-IDF，并结合快速IRLBA算法进行降维，将其集成到经典机器学习文本分析流程中。

Result: 实验表明，所提方法在经典机器学习模型中显著降低了时间复杂度，同时提升了模型准确率，且相比深度学习方法大幅减少了计算开销和碳排放。

Conclusion: 将CTF-IDF与快速IRLBA引入传统文本分析流程，可在保持较高准确率的同时实现更高效、低碳的文本处理，为替代高能耗深度学习方法提供了可行路径。

Abstract: The uprising of deep learning methodology and practice in recent years has
brought about a severe consequence of increasing carbon footprint due to the
insatiable demand for computational resources and power. The field of text
analytics also experienced a massive transformation in this trend of
monopolizing methodology. In this paper, the original TF-IDF algorithm has been
modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has
been proposed for data preprocessing. This paper primarily discusses the
effectiveness of classical machine learning techniques in text analytics with
CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The
introduction of both of these techniques in the conventional text analytics
pipeline ensures a more efficient, faster, and less computationally intensive
application when compared with deep learning methodology regarding carbon
footprint, with minor compromise in accuracy. The experimental results also
exhibit a manifold of reduction in time complexity and improvement of model
accuracy for the classical machine learning methods discussed further in this
paper.

</details>


### [83] [From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation](https://arxiv.org/abs/2511.03128)
*Najrin Sultana,Md Rafi Ur Rashid,Kang Gu,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 本文提出了两种新型对抗攻击框架 StaDec 和 DyDec，利用大语言模型（LLM）自身理解能力自动生成语义保持、自然且具有强迁移性的对抗样本，用于系统评估 LLM 在敏感任务中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在将大语言模型应用于敏感任务时，需系统评估其对对抗性输入的鲁棒性，而现有方法往往依赖外部启发式规则，缺乏自适应性和泛化能力。

Method: 提出 Static Deceptor (StaDec) 和 Dynamic Deceptor (DyDec) 两种攻击框架，通过 LLM 驱动的自动化流程生成语义相似但能有效欺骗目标模型的对抗样本，无需外部启发式规则。

Result: 所提出的攻击方法能生成自然且细微的对抗样本，在多个未知模型上展现出强迁移性，并能随 LLM 的发展而演进。

Conclusion: 该工作为 LLM 的鲁棒性自评估提供了系统性方法，有助于提升其在敏感任务中的安全性与可靠性。

Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a
simple task prompt, eliminating the need for training or fine-tuning. However,
when applying these models to sensitive tasks, it is crucial to thoroughly
assess their robustness against adversarial inputs. In this work, we introduce
Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack
frameworks designed to systematically generate dynamic and adaptive adversarial
examples by leveraging the understanding of the LLMs. We produce subtle and
natural-looking adversarial inputs that preserve semantic similarity to the
original text while effectively deceiving the target LLM. By utilizing an
automated, LLM-driven pipeline, we eliminate the dependence on external
heuristics. Our attacks evolve with the advancements in LLMs and demonstrate
strong transferability across models unknown to the attacker. Overall, this
work provides a systematic approach for the self-assessment of an LLM's
robustness. We release our code and data at
https://github.com/Shukti042/AdversarialExample.

</details>


### [84] [Test Time Adaptation Using Adaptive Quantile Recalibration](https://arxiv.org/abs/2511.03148)
*Paria Mehrbod,Pedro Vianna,Geraldin Nanfack,Guy Wolf,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出了一种名为自适应分位数重校准（AQR）的测试时域自适应方法，通过通道级分位数对齐调整预激活分布，在无需模型重训练的情况下提升模型在分布偏移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统域自适应方法通常依赖目标域先验知识或需要模型重训练，难以适用于动态或资源受限场景；现有测试时自适应方法在捕捉复杂激活分布和泛化到不同归一化层方面存在局限。

Method: AQR在测试时通过通道级分位数对齐来调整预激活分布，利用训练阶段计算的源域统计信息，并引入鲁棒尾部校准策略以应对不同批量大小下的尾部分布估计问题，适用于BatchNorm、GroupNorm和LayerNorm等多种架构。

Result: 在CIFAR-10-C、CIFAR-100-C和ImageNet-C等多个数据集和架构上，AQR显著优于现有测试时自适应方法，展现出更强的鲁棒性和泛化能力。

Conclusion: AQR是一种高效、通用且无需重训练的测试时自适应方法，适用于真实世界中动态且不可预测的数据分布场景。

Abstract: Domain adaptation is a key strategy for enhancing the generalizability of
deep learning models in real-world scenarios, where test distributions often
diverge significantly from the training domain. However, conventional
approaches typically rely on prior knowledge of the target domain or require
model retraining, limiting their practicality in dynamic or
resource-constrained environments. Recent test-time adaptation methods based on
batch normalization statistic updates allow for unsupervised adaptation, but
they often fail to capture complex activation distributions and are constrained
to specific normalization layers. We propose Adaptive Quantile Recalibration
(AQR), a test-time adaptation technique that modifies pre-activation
distributions by aligning quantiles on a channel-wise basis. AQR captures the
full shape of activation distributions and generalizes across architectures
employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of
estimating distribution tails under varying batch sizes, AQR incorporates a
robust tail calibration strategy that improves stability and precision. Our
method leverages source-domain statistics computed at training time, enabling
unsupervised adaptation without retraining models. Experiments on CIFAR-10-C,
CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR
achieves robust adaptation across diverse settings, outperforming existing
test-time adaptation baselines. These results highlight AQR's potential for
deployment in real-world scenarios with dynamic and unpredictable data
distributions.

</details>


### [85] [Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods](https://arxiv.org/abs/2511.03304)
*Felix Störck,Fabian Hinder,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种适用于连续受保护属性的公平性方法，通过将迭代零空间投影推广到核方法，显著扩展了其适用范围，并在支持向量回归中展现出优于或媲美现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前公平性研究主要集中在离散的目标和受保护属性上，而针对连续属性（尤其是在回归任务中）的研究较少；现有方法如迭代零空间投影仅限于线性模型或非线性编码器生成的嵌入，限制了其应用范围。

Method: 将迭代零空间投影方法推广至核方法，提出一种与模型和公平性评分无关的、适用于连续受保护属性的核嵌入公平性处理方法。

Result: 在多个数据集上，结合支持向量回归（SVR）使用所提方法，其性能优于或至少与当前其他先进方法相当。

Conclusion: 该方法有效拓展了连续公平性问题的解决手段，为处理具有连续受保护属性的回归任务提供了一种通用且高效的解决方案。

Abstract: With the on-going integration of machine learning systems into the everyday
social life of millions the notion of fairness becomes an ever increasing
priority in their development. Fairness notions commonly rely on protected
attributes to assess potential biases. Here, the majority of literature focuses
on discrete setups regarding both target and protected attributes. The
literature on continuous attributes especially in conjunction with regression
-- we refer to this as \emph{continuous fairness} -- is scarce. A common
strategy is iterative null-space projection which as of now has only been
explored for linear models or embeddings such as obtained by a non-linear
encoder. We improve on this by generalizing to kernel methods, significantly
extending the scope. This yields a model and fairness-score agnostic method for
kernel embeddings applicable to continuous protected attributes. We demonstrate
that our novel approach in conjunction with Support Vector Regression (SVR)
provides competitive or improved performance across multiple datasets in
comparisons to other contemporary methods.

</details>


### [86] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: 本文提出了一种名为周期性技能发现（PSD）的无监督强化学习框架，通过将状态映射到圆形潜在空间来自然地编码周期性，从而在复杂机器人任务中学习具有多样周期的技能，并在下游任务（如跨栏）中展现出优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法通常忽略所学技能的周期性，而许多机器人任务（尤其是运动类任务）需要在不同时间尺度上执行周期性行为，因此有必要开发能够发现多样化周期性技能的方法。

Method: 提出Periodic Skill Discovery（PSD）框架，训练一个编码器将状态映射到圆形潜在空间，以自然地在潜在表示中编码周期性，并通过捕捉时间距离来学习多样周期的技能。

Result: PSD能够在复杂机器人任务中有效学习多样周期的技能，即使仅使用像素观测；所学技能在下游任务（如跨栏）中表现优异，并且与现有技能发现方法结合后可进一步提升行为多样性。

Conclusion: PSD通过引入圆形潜在空间成功实现了无监督的周期性技能发现，显著增强了智能体在周期性任务中的能力与行为多样性。

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [87] [Cross-Modal Alignment via Variational Copula Modelling](https://arxiv.org/abs/2511.03196)
*Feng Wu,Tsai Hor Chan,Fuying Wang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于Copula的多模态学习框架，通过建模多模态数据间的复杂联合分布，有效对齐并融合不同模态的表示，在MIMIC数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中存在多种数据模态（如电子健康记录、医学图像和临床文本），现有方法在融合这些模态时过于简化其交互结构，难以捕捉高阶依赖关系，因此需要更强大的多模态联合分布建模方法。

Method: 提出一种Copula驱动的多模态学习框架，假设每个模态服从高斯混合分布，并在其联合分布上引入Copula模型，以高效对齐各模态的边缘分布并建模复杂交互。

Result: 在公开MIMIC数据集上的大量实验表明，该方法在多模态表示学习和缺失模态生成任务上优于现有方法。

Conclusion: Copula能够有效建模多模态之间的复杂联合分布，所提出的框架在理论和实验上均验证了其在多模态学习中的优势。

Abstract: Various data modalities are common in real-world applications (e.g.,
electronic health records, medical images and clinical notes in healthcare). It
is essential to develop multimodal learning methods to aggregate various
information from multiple modalities. The main challenge is how to
appropriately align and fuse the representations of different modalities into a
joint distribution. Existing methods mainly rely on concatenation or the
Kronecker product, oversimplifying the interaction structure between modalities
and indicating a need to model more complex interactions. Additionally, the
joint distribution of latent representations with higher-order interactions is
underexplored. Copula is a powerful statistical structure for modelling the
interactions among variables, as it naturally bridges the joint distribution
and marginal distributions of multiple variables. We propose a novel
copula-driven multimodal learning framework, which focuses on learning the
joint distribution of various modalities to capture the complex interactions
among them. The key idea is to interpret the copula model as a tool to align
the marginal distributions of the modalities efficiently. By assuming a
Gaussian mixture distribution for each modality and a copula model on the joint
distribution, our model can generate accurate representations for missing
modalities. Extensive experiments on public MIMIC datasets demonstrate the
superior performance of our model over other competitors. The code is available
at https://github.com/HKU-MedAI/CMCM.

</details>


### [88] [A Probabilistic U-Net Approach to Downscaling Climate Simulations](https://arxiv.org/abs/2511.03197)
*Maryam Alipourhajiagha,Pierre-Louis Lemaire,Youssef Diouane,Julie Carreau*

Main category: cs.LG

TL;DR: 本文将概率U-Net用于气候模型输出的统计降尺度任务，通过结合确定性U-Net主干和变分潜在空间来捕捉不确定性，并比较了四种训练目标在降水与温度降尺度中的表现。


<details>
  <summary>Details</summary>
Motivation: 气候模型因计算成本高而输出粗分辨率数据，但许多气候变化影响研究需要更高空间分辨率，因此需借助统计降尺度方法弥合这一差距。

Method: 采用概率U-Net架构，以确定性U-Net为主干并引入变分潜在空间以建模随机不确定性；评估了afCRPS和WMSE-MS-SSIM两种损失函数（后者含三种设置）在16倍降尺度任务中的效果。

Result: 实验表明，WMSE-MS-SSIM在特定设置下对极端值表现良好，而afCRPS更能有效捕捉跨尺度的空间变异性。

Conclusion: 概率U-Net是一种有效的统计降尺度方法，不同训练目标适用于不同需求：afCRPS适合保留空间结构，WMSE-MS-SSIM适合预测极端事件。

Abstract: Climate models are limited by heavy computational costs, often producing
outputs at coarse spatial resolutions, while many climate change impact studies
require finer scales. Statistical downscaling bridges this gap, and we adapt
the probabilistic U-Net for this task, combining a deterministic U-Net backbone
with a variational latent space to capture aleatoric uncertainty. We evaluate
four training objectives, afCRPS and WMSE-MS-SSIM with three settings for
downscaling precipitation and temperature from $16\times$ coarser resolution.
Our main finding is that WMSE-MS-SSIM performs well for extremes under certain
settings, whereas afCRPS better captures spatial variability across scales.

</details>


### [89] [A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams](https://arxiv.org/abs/2511.03239)
*Philipp Reis,Philipp Rigoll,Christian Steinhauser,Jacob Langner,Eric Sax*

Main category: cs.LG

TL;DR: 本文提出了一种名为FCDC的闭环数据收集范式，通过在线概率模型和反馈机制动态调控样本保留，在提升数据集多样性的同时显著减少冗余和存储开销。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统受限于数据质量和多样性，而传统开环式数据收集方式容易积累冗余样本，导致存储浪费、标注成本高和泛化能力有限。

Method: 将数据收集建模为闭环控制问题，利用在线概率模型持续估计已收集数据分布，并基于似然和马氏距离等反馈信号自适应调节样本保留策略。

Result: 在真实数据流实验中，FCDC使数据集平衡性提升25.9%，同时减少39.8%的数据存储量。

Conclusion: 数据收集可被主动控制，从被动流程转变为以反馈驱动、自我调节的核心环节，推动数据为中心的AI发展。

Abstract: Modern AI systems are increasingly constrained not by model capacity but by
the quality and diversity of their data. Despite growing emphasis on
data-centric AI, most datasets are still gathered in an open-loop manner which
accumulates redundant samples without feedback from the current coverage. This
results in inefficient storage, costly labeling, and limited generalization. To
address this, this paper introduces \ac{FCDC}, a paradigm that formulates data
collection as a closed-loop control problem. \ac{FCDC} continuously
approximates the state of the collected data distribution using an online
probabilistic model and adaptively regulates sample retention using based on
feedback signals such as likelihood and Mahalanobis distance. Through this
feedback mechanism, the system dynamically balances exploration and
exploitation, maintains dataset diversity, and prevents redundancy from
accumulating over time. Besides showcasing the controllability of \ac{FCDC} on
a synthetic dataset, experiments on a real data stream show that \ac{FCDC}
produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data
storage by $\SI{39.8}{\percent}$. These results demonstrate that data
collection itself can be actively controlled, transforming collection from a
passive pipeline stage into a self-regulating, feedback-driven process at the
core of data-centric AI.

</details>


### [90] [A unified physics-informed generative operator framework for general inverse problems](https://arxiv.org/abs/2511.03241)
*Gang Bao,Yaohua Zang*

Main category: cs.LG

TL;DR: IGNO is a novel generative neural operator framework that solves challenging inverse PDE problems with sparse, noisy, or high-dimensional data without requiring labeled training pairs, achieving accurate and stable results across diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for inverse PDE problems struggle with sparse/noisy measurements, high-dimensional or discontinuous coefficients, and often require large labeled datasets or are restricted to specific measurement types, limiting their real-world applicability.

Method: IGNO encodes high-dimensional coefficient fields into a low-dimensional latent space and uses neural operator decoders to reconstruct both coefficients and PDE solutions. It trains solely via physics-based PDE residuals and performs inversion through gradient-based optimization in latent space, accelerated by a normalizing flow model.

Result: IGNO achieves accurate, stable, and scalable inversion on challenging tasks—including discontinuous coefficient recovery and the EIT problem—even under severe noise, outperforming state-of-the-art methods and generalizing well to out-of-distribution targets.

Conclusion: IGNO provides a unified, powerful, and practical framework for solving a wide range of inverse problems in computational science without reliance on labeled data.

Abstract: Solving inverse problems governed by partial differential equations (PDEs) is
central to science and engineering, yet remains challenging when measurements
are sparse, noisy, or when the underlying coefficients are high-dimensional or
discontinuous. Existing deep learning approaches either require extensive
labeled datasets or are limited to specific measurement types, often leading to
failure in such regimes and restricting their practical applicability. Here, a
novel generative neural operator framework, IGNO, is introduced to overcome
these limitations. IGNO unifies the solution of inverse problems from both
point measurements and operator-valued data without labeled training pairs.
This framework encodes high-dimensional, potentially discontinuous coefficient
fields into a low-dimensional latent space, which drives neural operator
decoders to reconstruct both coefficients and PDE solutions. Training relies
purely on physics constraints through PDE residuals, while inversion proceeds
via efficient gradient-based optimization in latent space, accelerated by an a
priori normalizing flow model. Across a diverse set of challenging inverse
problems, including recovery of discontinuous coefficients from solution-based
measurements and the EIT problem with operator-based measurements, IGNO
consistently achieves accurate, stable, and scalable inversion even under
severe noise. It consistently outperforms the state-of-the-art method under
varying noise levels and demonstrates strong generalization to
out-of-distribution targets. These results establish IGNO as a unified and
powerful framework for tackling challenging inverse problems across
computational science domains.

</details>


### [91] [Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances](https://arxiv.org/abs/2511.03565)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 本文综述了模仿学习领域的最新进展，提出了一种新的分类方法，系统分析了当前的研究趋势、方法创新、实际应用，并讨论了代表性工作的优缺点及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 模仿学习近年来因深度学习的发展而迅速扩展，但面临泛化能力、协变量偏移和示范质量等长期挑战，亟需系统性梳理与新视角的分类框架以反映当前研究现状。

Method: 作者通过文献综述方式，对近期模仿学习的研究成果进行归纳，并提出一种区别于现有体系的新分类法，同时对代表性方法的优势、局限性和评估实践进行批判性分析。

Result: 该综述明确了当前模仿学习的主要趋势和方法论创新，揭示了评估标准的不足，并识别出若干关键挑战和未来研究方向。

Conclusion: 模仿学习虽取得显著进展，但仍存在诸多开放问题；所提出的新型分类有助于更清晰地理解该领域结构，并为后续研究提供指导。

Abstract: Imitation learning (IL) enables agents to acquire skills by observing and
replicating the behavior of one or multiple experts. In recent years, advances
in deep learning have significantly expanded the capabilities and scalability
of imitation learning across a range of domains, where expert data can range
from full state-action trajectories to partial observations or unlabeled
sequences. Alongside this growth, novel approaches have emerged, with new
methodologies being developed to address longstanding challenges such as
generalization, covariate shift, and demonstration quality. In this survey, we
review the latest advances in imitation learning research, highlighting recent
trends, methodological innovations, and practical applications. We propose a
novel taxonomy that is distinct from existing categorizations to better reflect
the current state of the IL research stratum and its trends. Throughout the
survey, we critically examine the strengths, limitations, and evaluation
practices of representative works, and we outline key challenges and open
directions for future research.

</details>


### [92] [Decoupled Entropy Minimization](https://arxiv.org/abs/2511.03256)
*Jing Ma,Hanlin Li,Xiang Xiang*

Main category: cs.LG

TL;DR: 该论文提出了一种自适应解耦熵最小化方法（AdaDEM），通过将传统熵最小化分解为具有相反作用的两个部分，并引入归一化奖励与边缘熵校准器，有效克服了传统方法中的奖励崩溃和易类偏差问题，在多种不完全监督学习任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统熵最小化（EM）虽在减少类别重叠、弥合域间差距和限制不确定性方面有效，但其耦合形式存在奖励崩溃和易类偏差等局限，限制了其潜力。

Method: 作者将经典EM解耦为聚类聚合驱动因子（CADF）和梯度缓解校准器（GMC），并提出AdaDEM方法：对CADF带来的奖励进行归一化，并用边缘熵校准器（MEC）替代GMC。

Result: AdaDEM在多种不完全监督学习任务中优于经典EM的上界变体DEM*，在噪声和动态环境中展现出更优性能。

Conclusion: 通过解耦与自适应校准，AdaDEM有效克服了传统熵最小化的内在缺陷，显著提升了模型在复杂环境下的学习效果。

Abstract: Entropy Minimization (EM) is beneficial to reducing class overlap, bridging
domain gap, and restricting uncertainty for various tasks in machine learning,
yet its potential is limited. To study the internal mechanism of EM, we
reformulate and decouple the classical EM into two parts with opposite effects:
cluster aggregation driving factor (CADF) rewards dominant classes and prompts
a peaked output distribution, while gradient mitigation calibrator (GMC)
penalizes high-confidence classes based on predicted probabilities.
Furthermore, we reveal the limitations of classical EM caused by its coupled
formulation: 1) reward collapse impedes the contribution of high-certainty
samples in the learning process, and 2) easy-class bias induces misalignment
between output distribution and label distribution. To address these issues, we
propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the
reward brought from CADF and employs a marginal entropy calibrator (MEC) to
replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM,
and achieves superior performance across various imperfectly supervised
learning tasks in noisy and dynamic environments.

</details>


### [93] [Diffusion Language Models are Super Data Learners](https://arxiv.org/abs/2511.03276)
*Jinjie Ni,Qian Liu,Longxu Dou,Chao Du,Zili Wang,Hang Yan,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 在严格控制的预训练条件下，当独特数据有限时，扩散语言模型（DLM）通过更多轮次训练持续优于自回归（AR）模型；该优势源于任意顺序建模、迭代双向去噪带来的高密度计算以及内置的蒙特卡洛增强机制。


<details>
  <summary>Details</summary>
Motivation: 探索在数据受限和计算资源匹配条件下，扩散语言模型相较于自回归模型是否具有性能优势及其成因。

Method: 在严格对齐的预训练设置下，比较扩散语言模型与自回归模型在不同数据量、模型规模和架构下的表现，并分析其性能差异的来源。

Result: 1.7B参数的DLM在10B唯一Python token上以约1.5T-token计算预算超越同等设置下的AR模型；1B参数DLM仅用1B token即可在HellaSwag和MMLU分别达到>56%和>33%准确率；验证集交叉熵上升并不意味着下游任务性能下降。

Conclusion: 扩散语言模型在数据受限场景下具备显著优势，其性能提升源于建模方式、计算密度和数据增强机制的综合作用，且验证损失与下游性能之间在此情境下并非强相关。

Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag
and > 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.

</details>


### [94] [Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning](https://arxiv.org/abs/2511.03279)
*Ning Lyu,Yuxi Wang,Ziyu Cheng,Qingyuan Zhang,Feng Chen*

Main category: cs.LG

TL;DR: 本文提出一种基于深度强化学习的自适应API限流策略，通过结合DQN与A3C算法，在Kubernetes环境中显著提升系统吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统限流算法（如令牌桶和滑动窗口）难以应对动态流量和系统负载变化，无法在保障服务稳定性的同时优化性能。

Method: 将限流决策建模为马尔可夫决策过程，设计融合Deep Q-Network（DQN）与Asynchronous Advantage Actor-Critic（A3C）的混合架构，通过持续监控微服务状态并与环境交互来学习最优限流策略。

Result: 在高负载场景下，相比传统固定阈值策略，该方法在Kubernetes集群中实现吞吐量提升23.7%、P99延迟降低31.4%；在90天生产部署中，服务降级事件减少82%，人工干预减少68%。

Conclusion: 所提出的基于深度强化学习的自适应限流方法在真实生产环境中验证了其有效性和实用性，能够显著提升系统稳定性与服务质量。

Abstract: As cloud computing and microservice architectures become increasingly
prevalent, API rate limiting has emerged as a critical mechanism for ensuring
system stability and service quality. Traditional rate limiting algorithms,
such as token bucket and sliding window, while widely adopted, struggle to
adapt to dynamic traffic patterns and varying system loads. This paper proposes
an adaptive rate limiting strategy based on deep reinforcement learning that
dynamically balances system throughput and service latency. We design a hybrid
architecture combining Deep Q-Network (DQN) and Asynchronous Advantage
Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a
Markov Decision Process. The system continuously monitors microservice states
and learns optimal rate limiting policies through environmental interaction.
Extensive experiments conducted in a Kubernetes cluster environment demonstrate
that our approach achieves 23.7% throughput improvement and 31.4% P99 latency
reduction compared to traditional fixed-threshold strategies under high-load
scenarios. Results from a 90-day production deployment handling 500 million
daily requests validate the practical effectiveness of the proposed method,
with 82% reduction in service degradation incidents and 68% decrease in manual
interventions.

</details>


### [95] [A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems](https://arxiv.org/abs/2511.03280)
*Rob Romijnders,Gabriele Cesa,Christos Louizos,Kumar Pratik,Arash Behboodi*

Main category: cs.LG

TL;DR: 本文提出了一种基于概率模型的多参考对齐（MRA）新算法，通过将相对姿态作为干扰变量进行边缘化，消除了全局对称性，从而实现更直接的求解和更快的收敛。该方法具有去中心化特性，避免了传统集中式方法的立方级计算复杂度，并在实验中展现出更低的重构误差。


<details>
  <summary>Details</summary>
Motivation: 多参考对齐（MRA）问题广泛存在于分子成像、计算机视觉和无线通信等领域，现有方法常受全局对称性和高计算复杂度限制，亟需更高效且鲁棒的解决方案。

Method: 采用概率建模方法，将相对姿态作为干扰变量进行边缘化，利用循环一致性实现去中心化计算，从而绕过集中式方法的高计算开销。

Result: 所提出的两种算法在多种实验设置下均实现了更低的信号重构误差，并显著降低了计算复杂度。

Conclusion: 通过边缘化相对姿态并引入去中心化策略，新算法有效解决了MRA中的对称性与计算效率问题，在性能和可扩展性方面优于现有方法。

Abstract: From molecular imaging to wireless communications, the ability to align and
reconstruct signals from multiple misaligned observations is crucial for system
performance. We study the problem of multi-reference alignment (MRA), which
arises in many real-world problems, such as cryo-EM, computer vision, and, in
particular, wireless communication systems. Using a probabilistic approach to
model MRA, we find a new algorithm that uses relative poses as nuisance
variables to marginalize out -- thereby removing the global symmetries of the
problem and allowing for more direct solutions and improved convergence. The
decentralization of this approach enables significant computational savings by
avoiding the cubic scaling of centralized methods through cycle consistency.
Both proposed algorithms achieve lower reconstruction error across experimental
settings.

</details>


### [96] [Structured Matrix Scaling for Multi-Class Calibration](https://arxiv.org/abs/2511.03685)
*Eugène Berta,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: cs.LG

TL;DR: 本文提出基于逻辑回归的参数化重校准方法，并通过结构正则化、鲁棒预处理和高效优化，在多类校准中有效平衡偏差-方差权衡，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 标准温度缩放等校准方法表达能力有限，而更复杂的模型在校准数据有限时易过拟合，因此需要在多类分类中探索更具表达力且稳健的校准方法。

Method: 采用基于逻辑回归的参数化重校准函数，结合结构正则化、鲁棒预处理和高效优化策略来管理偏差-方差权衡。

Result: 所提方法在多类校准任务中显著优于现有的基于逻辑回归的校准技术，并提供了高效易用的开源实现。

Conclusion: 结构化的正则化与优化策略能有效提升复杂校准模型的性能，使其成为温度缩放等传统方法的有力替代方案。

Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers
provide faithful probability estimates. We argue that parametric recalibration
functions based on logistic regression can be motivated from a simple
theoretical setting for both binary and multiclass classification. This insight
motivates the use of more expressive calibration methods beyond standard
temperature scaling. For multi-class calibration however, a key challenge lies
in the increasing number of parameters introduced by more complex models, often
coupled with limited calibration data, which can lead to overfitting. Through
extensive experiments, we demonstrate that the resulting bias-variance tradeoff
can be effectively managed by structured regularization, robust preprocessing
and efficient optimization. The resulting methods lead to substantial gains
over existing logistic-based calibration techniques. We provide efficient and
easy-to-use open-source implementations of our methods, making them an
attractive alternative to common temperature, vector, and matrix scaling
implementations.

</details>


### [97] [SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration](https://arxiv.org/abs/2511.03344)
*Elif Arslan,Jacobus G. M. van der Linden,Serge Hoogendoorn,Marco Rinaldi,Emir Demirović*

Main category: cs.LG

TL;DR: 本文提出了SORTD框架，用于高效枚举稀疏决策树的Rashomon集（即性能相近但结构不同的树集合），显著提升计算效率，并支持灵活的后评估目标。


<details>
  <summary>Details</summary>
Motivation: 现有方法在寻找最优稀疏决策树时面临NP-hard难题，难以高效枚举Rashomon集；而Rashomon集对于提升模型可解释性、变量重要性分析及满足用户偏好（如公平性）具有重要意义。

Method: 提出SORTD框架，按目标函数值顺序枚举Rashomon集中的树，具备anytime特性，并适用于任意可分离且全序的目标函数，同时支持使用其他可分离（部分有序）目标进行后评估。

Result: 实验表明，SORTD相比现有技术最多可将运行时间减少两个数量级，并能高效计算多种目标下的Rashomon集。

Conclusion: SORTD显著提升了Rashomon集探索的可扩展性与实用性，使其更适用于现实世界的高风险应用场景。

Abstract: Sparse decision tree learning provides accurate and interpretable predictive
models that are ideal for high-stakes applications by finding the single most
accurate tree within a (soft) size limit. Rather than relying on a single
"best" tree, Rashomon sets-trees with similar performance but varying
structures-can be used to enhance variable importance analysis, enrich
explanations, and enable users to choose simpler trees or those that satisfy
stakeholder preferences (e.g., fairness) without hard-coding such criteria into
the objective function. However, because finding the optimal tree is NP-hard,
enumerating the Rashomon set is inherently challenging. Therefore, we introduce
SORTD, a novel framework that improves scalability and enumerates trees in the
Rashomon set in order of the objective value, thus offering anytime behavior.
Our experiments show that SORTD reduces runtime by up to two orders of
magnitude compared with the state of the art. Moreover, SORTD can compute
Rashomon sets for any separable and totally ordered objective and supports
post-evaluating the set using other separable (and partially ordered)
objectives. Together, these advances make exploring Rashomon sets more
practical in real-world applications.

</details>


### [98] [A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications](https://arxiv.org/abs/2511.03363)
*Xiaocai Zhang,Hur Lim,Ke Wang,Zhe Xiao,Jing Wang,Kelvin Lee,Xiuju Fu,Zheng Qin*

Main category: cs.LG

TL;DR: 本文提出了一种无需数据的模块化多标签意图识别流程DMTC，通过提示工程生成合成查询、Sentence-T5编码语义嵌入，并结合新颖的在线焦点对比损失训练轻量分类器，在海事交通场景中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统意图识别系统依赖大量标注语料，难以实现细粒度的多标签区分，且数据收集成本高。为解决这些问题，作者旨在构建一个无需真实数据、高效准确的多标签意图识别方案，适用于智能体AI在交通领域的应用。

Method: 所提DMTC流程包含三步：1）利用提示工程引导大语言模型在不同交通场景下生成多样化的合成查询；2）使用Sentence-T5模型对文本查询进行编码以获得紧凑语义嵌入；3）采用新颖的在线焦点对比（OFC）损失训练轻量级分类器，强调难样本并最大化类间可分性。

Result: 在海事交通场景的实验中，DMTC取得5.35%的Hamming损失和95.92%的AUC，优于当前最先进的多标签分类器和端到端LLM基线。Sentence-T5相比其他编码器至少提升3.29%的子集准确率，OFC损失相较标准对比目标额外提升0.98%。

Conclusion: 该系统能无缝将用户查询路由至任务特定模块（如预计到达时间、交通风险评估等），为无需人工标注的全自动、意图感知型智能体奠定基础。

Abstract: In this study, a modular, data-free pipeline for multi-label intention
recognition is proposed for agentic AI applications in transportation. Unlike
traditional intent recognition systems that depend on large, annotated corpora
and often struggle with fine-grained, multi-label discrimination, our approach
eliminates the need for costly data collection while enhancing the accuracy of
multi-label intention understanding. Specifically, the overall pipeline, named
DMTC, consists of three steps: 1) using prompt engineering to guide large
language models (LLMs) to generate diverse synthetic queries in different
transport scenarios; 2) encoding each textual query with a Sentence-T5 model to
obtain compact semantic embeddings; 3) training a lightweight classifier using
a novel online focal-contrastive (OFC) loss that emphasizes hard samples and
maximizes inter-class separability. The applicability of the proposed pipeline
is demonstrated in an agentic AI application in the maritime transportation
context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%
and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers
and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that
Sentence-T5 embeddings improve subset accuracy by at least 3.29% over
alternative encoders, and integrating the OFC loss yields an additional 0.98%
gain compared to standard contrastive objectives. In conclusion, our system
seamlessly routes user queries to task-specific modules (e.g., ETA information,
traffic risk evaluation, and other typical scenarios in the transportation
domain), laying the groundwork for fully autonomous, intention-aware agents
without costly manual labelling.

</details>


### [99] [TripleWin: Fixed-Point Equilibrium Pricing for Data-Model Coupled Markets](https://arxiv.org/abs/2511.03368)
*Hongrun Ren,Yun Xiong,Lei You,Yingying Wang,Haixu Xiong,Yangyong Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种统一的数据-模型耦合市场机制，通过供需双向映射和Shapley值分配，在数据卖家、模型生产者和模型买家之间实现对称定价，并证明其均衡价格具有存在性、唯一性和全局收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型经济中的数据与模型定价大多分离处理，或依赖偏向某一方的中介机制，缺乏同时兼顾数据提供方、模型生产方和模型购买方的对称、统一市场机制。

Method: 构建一个统一的数据-模型耦合市场，通过供给侧映射将数据支付转化为买家可见的模型报价，通过需求侧映射利用基于Shapley值的分配将买家价格反馈至数据集，形成包含双向供需传播与买卖双方内部耦合的闭环系统，并证明该系统的联合算子为标准干扰函数（SIF）。

Result: 实验证明该机制能高效收敛，并在公平性方面优于以中介为中心和单边定价的基线方法。

Conclusion: 所提出的耦合市场机制能够有效协调数据与模型交易，实现多方共赢，并具备良好的理论保证和实际性能。

Abstract: The rise of the machine learning (ML) model economy has intertwined markets
for training datasets and pre-trained models. However, most pricing approaches
still separate data and model transactions or rely on broker-centric pipelines
that favor one side. Recent studies of data markets with externalities capture
buyer interactions but do not yield a simultaneous and symmetric mechanism
across data sellers, model producers, and model buyers. We propose a unified
data-model coupled market that treats dataset and model trading as a single
system. A supply-side mapping transforms dataset payments into buyer-visible
model quotations, while a demand-side mapping propagates buyer prices back to
datasets through Shapley-based allocation. Together, they form a closed loop
that links four interactions: supply-demand propagation in both directions and
mutual coupling among buyers and among sellers. We prove that the joint
operator is a standard interference function (SIF), guaranteeing existence,
uniqueness, and global convergence of equilibrium prices. Experiments
demonstrate efficient convergence and improved fairness compared with
broker-centric and one-sided baselines. The code is available on
https://github.com/HongrunRen1109/Triple-Win-Pricing.

</details>


### [100] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 本文提出了一种利用已知群对称性的核方法强化学习框架，通过引入不变核构建对称性感知的乐观最小二乘值迭代算法，在理论和实验上均验证了其相较于标准核方法在样本效率和性能上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的强化学习问题常具有内在对称性，有效利用这些对称性有助于提升学习效率。然而现有核方法强化学习未充分考虑结构先验信息，限制了样本效率。

Method: 提出一种对称性感知的乐观最小二乘值迭代（LSVI）算法，利用不变再生核希尔伯特空间（RKHS）中的不变核来编码奖励函数与状态转移动力学中的对称不变性，并推导了相应的信息增益与覆盖数界。

Result: 在定制的Frozen Lake环境和2D布局设计问题上的实验表明，所提对称性感知RL算法显著优于标准核方法；理论分析也量化了对称性带来的样本效率增益。

Conclusion: 将结构先验（如群对称性）融入核方法强化学习可有效提升样本效率，为设计更高效的RL算法提供了新思路。

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [101] [RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse](https://arxiv.org/abs/2511.03475)
*Yinsicheng Jiang,Yeqi Huang,Liang Cheng,Cheng Deng,Xuan Sun,Luo Mai*

Main category: cs.LG

TL;DR: RAGBoost 是一种高效的检索增强生成（RAG）系统，通过准确保留上下文重用，在不牺牲推理准确性的前提下显著提升缓存复用率和预填充性能。


<details>
  <summary>Details</summary>
Motivation: 现有 RAG 系统在处理现代应用中更长、更复杂的输入时，预填充性能下降；而现有缓存技术难以兼顾高缓存复用率与推理准确性。

Method: RAGBoost 通过高效上下文索引、排序和去重，检测并发会话与多轮交互中的重叠检索项，并结合轻量级上下文提示以维持推理保真度。

Result: RAGBoost 在多种 RAG 和智能体 AI 工作负载上，相比当前最先进的方法，将 LLM 推理引擎的预填充性能提升 1.5–3 倍，同时保持甚至提升推理准确性。

Conclusion: RAGBoost 成功实现了高缓存复用与高推理准确性的统一，为 RAG 系统提供了一种高效且实用的优化方案。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with retrieved context but often suffers from downgraded prefill performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low cache reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high cache reuse without sacrificing accuracy through
accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing LLM inference engines and improves their prefill performance by 1.5-3X
over state-of-the-art methods, while preserving or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.

</details>


### [102] [NAP: Attention-Based Late Fusion for Automatic Sleep Staging](https://arxiv.org/abs/2511.03488)
*Alvise Dei Rossi,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Luigi Fiorillo,Francesca Faraci*

Main category: cs.LG

TL;DR: 本文提出NAP（Neural Aggregator of Predictions），一种基于三轴注意力机制的模型，用于聚合多通道、多模态的睡眠多导图信号预测结果，在无需微调的情况下实现跨数据集的零样本泛化，并在自动睡眠分期任务中达到领先性能。


<details>
  <summary>Details</summary>
Motivation: 现有处理多导睡眠图信号的模型通常依赖固定模态或通道子集，未能充分利用其固有的多模态特性；同时，不同数据集和临床站点之间在模态组成、通道可用性和采集协议上存在高度异质性。

Method: 引入NAP模型，利用时间、空间和预测器层级的三轴注意力机制，动态聚合多个预训练单通道模型的输出；NAP在训练时可适应不同输入维度，并保持底层单通道模型冻结。

Result: NAP在多个数据集上实现了优于单个预测器和简单集成方法的性能，展现出卓越的零样本泛化能力。

Conclusion: NAP有效解决了多导睡眠图信号异构性带来的挑战，不仅在自动睡眠分期任务中表现优异，还可推广至其他多模态生理信号分析场景。

Abstract: Polysomnography signals are highly heterogeneous, varying in modality
composition (e.g., EEG, EOG, ECG), channel availability (e.g., frontal,
occipital EEG), and acquisition protocols across datasets and clinical sites.
Most existing models that process polysomnography data rely on a fixed subset
of modalities or channels and therefore neglect to fully exploit its inherently
multimodal nature. We address this limitation by introducing NAP (Neural
Aggregator of Predictions), an attention-based model which learns to combine
multiple prediction streams using a tri-axial attention mechanism that captures
temporal, spatial, and predictor-level dependencies. NAP is trained to adapt to
different input dimensions. By aggregating outputs from frozen, pretrained
single-channel models, NAP consistently outperforms individual predictors and
simple ensembles, achieving state-of-the-art zero-shot generalization across
multiple datasets. While demonstrated in the context of automated sleep staging
from polysomnography, the proposed approach could be extended to other
multimodal physiological applications.

</details>


### [103] [Why Less is More (Sometimes): A Theory of Data Curation](https://arxiv.org/abs/2511.03492)
*Elvis Dohmatob,Mohammad Pezeshki,Reyhane Askari-Hemmat*

Main category: cs.LG

TL;DR: 本文提出一个理论框架，解释在何种条件下使用更少但经过精心筛选的数据反而能提升模型泛化性能，并推导出数据质量与规模之间的精确相变曲线。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器学习中的一个核心悖论：何时使用更少的数据反而更好？传统“越多越好”的缩放定律正受到如LIMO和s1等方法的挑战，这些方法通过小而高质量的数据集取得了更优性能。

Method: 研究由不完美预言机根据样本难度和正确性进行数据筛选的策略，推导在标签无关和标签感知两种筛选规则下的测试误差缩放律，并给出数据规模与质量之间的相变条件。

Result: 理论分析表明，在特定条件下，小规模精选数据集可优于完整数据集；在ImageNet上的实验验证了该理论预测，并展示了数据筛选如何提升准确率甚至缓解模型崩溃。

Conclusion: 该框架不仅解释了“少即是多”现象的成因，还为近期大语言模型在数学推理中出现的矛盾筛选策略提供了原理性解释。

Abstract: This paper introduces a theoretical framework to resolve a central paradox in
modern machine learning: When is it better to use less data? This question has
become critical as classical scaling laws suggesting ``more is more'' (Sun et
al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et
al., 2025; Muenighoff et al., 2025), which achieve superior performance with
small, aggressively curated datasets. Here, we study data curation strategies
where an imperfect oracle selects the training examples according to their
difficulty and correctness. Our results provide exact scaling law curves for
test error under both label-agnostic and label-aware curation rules, revealing
when and why keeping only a subset of data can improve generalization. In
contrast to classical scaling laws, we show that under certain conditions,
small curated datasets can outperform full datasets, and we provide analytical
conditions for this by deriving precise phase transition curves tied to data
size and quality. We validate these theoretical claims with empirical results
on ImageNet, confirming our predictions about when curation improves accuracy
and can even mitigate model collapse. Furthermore, our framework provides a
principled explanation for the contradictory curation strategies recently
observed in LLM mathematical reasoning.

</details>


### [104] [Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments](https://arxiv.org/abs/2511.03527)
*Bryan L. M. de Oliveira,Felipe V. Frujeri,Marcos P. C. M. Queiroz,Luana G. B. Martins,Telma W. de L. Soares,Luckeciano C. Melo*

Main category: cs.LG

TL;DR: 本文系统研究了无评论家的Group Relative Policy Optimization（GRPO）在经典单任务强化学习环境中的表现，发现其仅在短视界任务中有效，而长视界任务仍需依赖学习得到的评论家；此外，GRPO在高折扣因子下表现更佳（HalfCheetah除外），且小规模分组优于大规模分组。


<details>
  <summary>Details</summary>
Motivation: 探讨在策略梯度方法中是否必须使用学习得到的基线（如评论家），并系统评估GRPO这一无需评论家、通过轨迹间群体相对比较估计优势函数的方法在经典控制任务中的有效性与局限性。

Method: 在涵盖离散与连续控制的经典单任务强化学习环境中，对GRPO进行受控消融实验，分别隔离基线类型、折扣因子和群体采样策略的影响，系统比较其与PPO的性能差异。

Result: (1) 除CartPole等短视界任务外，所有无评论家基线均劣于PPO；(2) GRPO通常受益于高折扣因子（γ=0.99），但在HalfCheetah中适度折扣（γ=0.9）更优；(3) 小群体规模表现优于大群体，表明基于批次混合无关轨迹的分组策略存在局限。

Conclusion: 无评论家方法在经典控制任务中存在明显局限，仅在特定条件下（如短视界、适当折扣因子和小群体）可作为学习价值函数的可行替代方案。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable
alternative to Proximal Policy Optimization (PPO) by eliminating the learned
critic and instead estimating advantages through group-relative comparisons of
trajectories. This simplification raises fundamental questions about the
necessity of learned baselines in policy-gradient methods. We present the first
systematic study of GRPO in classical single-task reinforcement learning
environments, spanning discrete and continuous control tasks. Through
controlled ablations isolating baselines, discounting, and group sampling, we
reveal three key findings: (1) learned critics remain essential for
long-horizon tasks: all critic-free baselines underperform PPO except in
short-horizon environments like CartPole where episodic returns can be
effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except
in HalfCheetah, where lack of early termination favors moderate discounting
(gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting
limitations in batch-based grouping strategies that mix unrelated episodes.
These results reveal both the limitations of critic-free methods in classical
control and the specific conditions where they remain viable alternatives to
learned value functions.

</details>


### [105] [Byzantine-Robust Federated Learning with Learnable Aggregation Weights](https://arxiv.org/abs/2511.03529)
*Javad Parsa,Amir Hossein Daghestani,André M. H. Teixeira,Mikael Johansson*

Main category: cs.LG

TL;DR: 本文提出一种新颖的拜占庭鲁棒联邦学习方法，通过将聚合权重作为可学习参数与全局模型联合优化，并设计具有强收敛保证的交替最小化算法，在异构数据和高比例恶意客户端场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，恶意（拜占庭）客户端的存在严重威胁模型的鲁棒性，尤其在客户端数据高度异构的情况下，传统方法难以有效应对。

Method: 提出一种新的拜占庭鲁棒联邦学习优化问题，将聚合权重视为可学习参数，与全局模型参数联合优化，并采用具有强收敛性保证的交替最小化算法求解。

Result: 在多种数据集和攻击场景下的实验表明，该方法在高度异构数据和大量恶意客户端设置中始终优于当前最先进的拜占庭鲁棒联邦学习方法。

Conclusion: 所提方法通过自适应加权机制显著提升了联邦学习在拜占庭攻击和数据异构环境下的鲁棒性和性能。

Abstract: Federated Learning (FL) enables clients to collaboratively train a global
model without sharing their private data. However, the presence of malicious
(Byzantine) clients poses significant challenges to the robustness of FL,
particularly when data distributions across clients are heterogeneous. In this
paper, we propose a novel Byzantine-robust FL optimization problem that
incorporates adaptive weighting into the aggregation process. Unlike
conventional approaches, our formulation treats aggregation weights as
learnable parameters, jointly optimizing them alongside the global model
parameters. To solve this optimization problem, we develop an alternating
minimization algorithm with strong convergence guarantees under adversarial
attack. We analyze the Byzantine resilience of the proposed objective. We
evaluate the performance of our algorithm against state-of-the-art
Byzantine-robust FL approaches across various datasets and attack scenarios.
Experimental results demonstrate that our method consistently outperforms
existing approaches, particularly in settings with highly heterogeneous data
and a large proportion of malicious clients.

</details>


### [106] [Flat Minima and Generalization: Insights from Stochastic Convex Optimization](https://arxiv.org/abs/2511.03548)
*Matan Schliserman,Shira Vansover-Hager,Tomer Koren*

Main category: cs.LG

TL;DR: 本文研究了在随机凸优化中平坦极小值与泛化性能之间的关系，发现即使使用旨在寻找平坦解的锐度感知算法（如SA-GD和SAM），其解仍可能具有较差的泛化能力（总体风险为Ω(1)），并进一步通过算法稳定性技术给出了这两种算法的总体风险上界。


<details>
  <summary>Details</summary>
Motivation: 近年来有观点认为学习算法之所以在实践中表现良好，是因为它们收敛到与更好泛化性能相关的平坦极小值。然而，这种关联在理论上的有效性尚未完全明确，尤其在经典且被广泛研究的随机凸优化设定下是否成立仍需验证。

Method: 作者在非负、β-光滑目标函数的随机凸优化框架下，分析了经验风险的平坦极小值与总体风险之间的关系，并对两种锐度感知算法——SA-GD和SAM——进行了理论分析，包括其收敛行为及泛化性能。此外，利用算法稳定性技术推导了这两种算法的总体风险上界。

Result: 1. 在该凸优化设定下，平坦的经验极小值可能导致Ω(1)的总体风险，而尖锐极小值反而可能实现最优泛化；  
2. SA-GD虽能快速收敛到平坦极小值，但其解的总体风险仍可能高达Ω(1)；  
3. SAM虽能最小化经验损失，却可能收敛到尖锐极小值，同样导致Ω(1)的总体风险；  
4. 通过算法稳定性方法，得到了SA-GD和SAM的总体风险上界。

Conclusion: 平坦极小值并不总能保证良好的泛化性能，即使是在凸优化的经典设定下，锐度感知算法也可能产生泛化能力差的解。因此，仅依赖“平坦性”作为泛化解释存在局限性，需结合其他理论工具（如算法稳定性）来更全面地理解泛化行为。

Abstract: Understanding the generalization behavior of learning algorithms is a central
goal of learning theory. A recently emerging explanation is that learning
algorithms are successful in practice because they converge to flat minima,
which have been consistently associated with improved generalization
performance. In this work, we study the link between flat minima and
generalization in the canonical setting of stochastic convex optimization with
a non-negative, $\beta$-smooth objective. Our first finding is that, even in
this fundamental and well-studied setting, flat empirical minima may incur
trivial $\Omega(1)$ population risk while sharp minima generalizes optimally.
Then, we show that this poor generalization behavior extends to two natural
''sharpness-aware'' algorithms originally proposed by Foret et al. (2021),
designed to bias optimization toward flat solutions: Sharpness-Aware Gradient
Descent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which
performs gradient steps on the maximal loss in a predefined neighborhood, we
prove that while it successfully converges to a flat minimum at a fast rate,
the population risk of the solution can still be as large as $\Omega(1)$,
indicating that even flat minima found algorithmically using a sharpness-aware
gradient method might generalize poorly. For SAM, a computationally efficient
approximation of SA-GD based on normalized ascent steps, we show that although
it minimizes the empirical loss, it may converge to a sharp minimum and also
incur population risk $\Omega(1)$. Finally, we establish population risk upper
bounds for both SA-GD and SAM using algorithmic stability techniques.

</details>


### [107] [TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval](https://arxiv.org/abs/2511.03570)
*Günther Schindler,Maximilian Schambach,Michael Medek,Sam Thelin*

Main category: cs.LG

TL;DR: TabGemma 是一种适用于混合类型表格数据的上下文学习方法，通过科学记数法标准化数值、继续预训练 Gemma 3 模型，并结合 n-gram 检索机制，在语义丰富的分类任务中达到新 SOTA，但在大数据量回归任务中仍落后于传统方法。


<details>
  <summary>Details</summary>
Motivation: 将预训练大语言模型（LLMs）应用于表格预测面临两大挑战：数值分词不稳定和上下文长度有限。作者旨在设计一种通用、高效的方案来克服这些问题，使 LLM 能有效处理包含文本、数值和类别字段的表格数据。

Method: 提出 TabGemma 方法：1）使用带符号的科学记数法对数值进行规范化；2）在大规模真实世界数据集上以目标填充为目标对 12B 参数的 Gemma 3 模型继续预训练；3）推理时采用基于 n-gram 的紧凑检索策略，选取信息丰富的示例以适配 128k-token 上下文窗口。

Result: 在语义丰富的分类基准上，TabGemma 在低数据和高数据场景下均取得新的最优性能，且随着上下文行数增加而持续提升；在回归任务中，小样本下表现具竞争力，但随数据量增大落后于传统方法。

Conclusion: LLM 在配备专门的数值处理和上下文检索机制后，可成为高度语义化表格任务的有效上下文学习器，但仍需在数值建模和长上下文扩展方面进一步突破。

Abstract: We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.

</details>


### [108] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文使用 Lean 4 定理证明器基于 Mathlib 库，形式化验证了使用马尔可夫样本的 Q 学习和线性时序差分（TD）学习算法的几乎必然收敛性，并构建了一个可扩展的统一框架。


<details>
  <summary>Details</summary>
Motivation: Q 学习和线性 TD 是强化学习领域最早且最具影响力的算法，其收敛性研究不仅是早期 RL 发展的重要课题，如今仍受到广泛关注。形式化验证这些算法的收敛性有助于建立严格可靠的理论基础。

Method: 基于 Robbins-Siegmund 定理，在 Lean 4 中利用 Mathlib 库构建统一框架，对 Q 学习和线性 TD 学习在马尔可夫样本下的几乎必然收敛性进行形式化证明。

Result: 成功形式化验证了 Q 学习与线性 TD 学习的几乎必然收敛性，并提供了可扩展至收敛速率及其他收敛模式的通用框架。

Conclusion: 该工作为全面形式化强化学习中的收敛性结果迈出了重要一步，增强了相关理论的严谨性和可靠性。

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [109] [nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN](https://arxiv.org/abs/2511.03634)
*Alexander Pfefferle,Johannes Hog,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: 本文提出了 nanoTabPFN，一种简化且轻量级的 TabPFN v2 架构实现，显著降低了计算资源需求，使表格基础模型更易于教学和研究使用。


<details>
  <summary>Details</summary>
Motivation: 现有开源表格基础模型（如 TabPFN）代码复杂、缺乏文档且难以理解，不利于初学者和研究人员快速上手与实验。

Method: 作者实现了一个简化的 TabPFN v2 架构 nanoTabPFN，并采用预生成训练数据进行训练，在单 GPU 上大幅缩短预训练时间。

Result: 在小规模数据设定下，nanoTabPFN 仅用一分钟预训练即可达到传统机器学习基线水平，比原版 TabPFN v2 快 160,000 倍。

Conclusion: nanoTabPFN 极大降低了表格基础模型的使用门槛，使其适用于教育和快速实验场景，提升了可访问性和可复现性。

Abstract: Tabular foundation models such as TabPFN have revolutionized predictive
machine learning for tabular data. At the same time, the driving factors of
this revolution are hard to understand. Existing open-source tabular foundation
models are implemented in complicated pipelines boasting over 10,000 lines of
code, lack architecture documentation or code quality. In short, the
implementations are hard to understand, not beginner-friendly, and complicated
to adapt for new experiments. We introduce nanoTabPFN, a simplified and
lightweight implementation of the TabPFN v2 architecture and a corresponding
training loop that uses pre-generated training data. nanoTabPFN makes tabular
foundation models more accessible to students and researchers alike. For
example, restricted to a small data setting it achieves a performance
comparable to traditional machine learning baselines within one minute of
pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This
eliminated requirement of large computational resources makes pre-training
tabular foundation models accessible for educational purposes. Our code is
available at https://github.com/automl/nanoTabPFN.

</details>


### [110] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: BAQ is a new offline-to-online reinforcement learning framework that uses implicit behavior modeling to stabilize and accelerate policy adaptation during online fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Offline RL policies often fail in dynamic environments due to distributional shift and unreliable value estimates on unseen data; thus, a reliable transition method from offline to online settings is needed.

Method: BAQ introduces a dual-objective loss that adaptively balances alignment with offline behavior under high uncertainty and gradual relaxation of this constraint as confident online experience accumulates.

Result: BAQ outperforms existing methods on standard benchmarks, showing faster recovery, better robustness, and higher performance during online adaptation.

Conclusion: Implicit behavior adaptation via BAQ provides a principled and practical approach for reliable real-world deployment of RL policies trained offline.

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


### [111] [Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2511.03710)
*Guanning Zeng,Zhaoyi Zhou,Daman Arora,Andrea Zanette*

Main category: cs.LG

TL;DR: 本文提出在强化学习中使用收缩估计量（shrinkage estimator）来改进策略梯度方法中的基线估计，从而降低梯度估计方差、提升训练稳定性，且无需额外超参数或计算开销。


<details>
  <summary>Details</summary>
Motivation: 在基于可验证奖励的强化学习（RLVR）中，为稳定训练常对轨迹奖励进行中心化处理，通常采用每个提示（prompt）的经验均值作为基线。然而，在生成样本较少的情况下，这种经验均值估计不准确，影响训练效果。

Method: 受Stein悖论启发，作者提出一种收缩估计方法，将每个提示的均值与跨提示的总体均值相结合，以更准确地估计每个提示的奖励均值，并将其作为策略梯度中的新基线。

Result: 理论分析表明该收缩基线能严格降低策略梯度估计的方差；实验结果显示其在多种算法中均优于传统经验均值基线，带来更低方差的梯度更新和更稳定的训练过程。

Conclusion: 所提出的收缩基线是一种即插即用的改进方案，能在不增加计算负担的前提下有效提升RLVR训练的稳定性和性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for post-training large reasoning models (LRMs) using
policy-gradient methods such as GRPO. To stabilize training, these methods
typically center trajectory rewards by subtracting the empirical mean for each
prompt. Statistically, this centering acts as a control variate (or baseline),
reducing the variance of the policy-gradient estimator.
  Typically, the mean reward is estimated using per-prompt empirical averages
for each prompt in a batch. Drawing inspiration from Stein's paradox, we
propose using shrinkage estimators that combine per-prompt and across-prompt
means to improve the overall per-prompt mean estimation accuracy --
particularly in the low-generation regime typical of RLVR. Theoretically, we
construct a shrinkage-based baseline that provably yields lower-variance
policy-gradient estimators across algorithms. Our proposed baseline serves as a
drop-in replacement for existing per-prompt mean baselines, requiring no
additional hyper-parameters or computation. Empirically, shrinkage baselines
consistently outperform standard empirical-mean baselines, leading to
lower-variance gradient updates and improved training stability.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [112] [PerfDojo: Automated ML Library Generation for Heterogeneous Architectures](https://arxiv.org/abs/2511.03586)
*Andrei Ivanov,Siyuan Shen,Gioele Gottardo,Marcin Chrapek,Afif Boudaoud,Timo Schneider,Luca Benini,Torsten Hoefler*

Main category: cs.PF

TL;DR: PerfLLM 是一种结合大语言模型（LLM）与强化学习（RL）的新型自动优化方法，通过名为 PerfDojo 的环境，利用人类可读且语义有效的代码表示，在无需硬件先验知识的情况下实现跨 CPU 和 GPU 架构的高性能优化。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型日益复杂，加上硬件架构（如 CPU、GPU、加速器）的多样性，使得性能优化极具挑战。现有自动优化方法依赖复杂的硬件特定启发式规则和不可解释的中间表示，难以实现性能可移植性。

Method: 提出 PerfLLM 方法，结合大语言模型与强化学习；核心是 PerfDojo 环境，将优化问题建模为 RL 游戏，采用数学启发、人类可读且语义有效的代码表示，确保变换过程中的语义正确性。

Result: 在多种 CPU（x86、Arm、RISC-V）和 GPU 架构上实现了显著的性能提升，验证了该方法的有效性和通用性。

Conclusion: PerfLLM 通过结合 LLM 与 RL，在无需硬件先验知识的前提下，实现了高效、可解释且跨平台的自动性能优化，为未来编译器和优化工具提供了新范式。

Abstract: The increasing complexity of machine learning models and the proliferation of
diverse hardware architectures (CPUs, GPUs, accelerators) make achieving
optimal performance a significant challenge. Heterogeneity in instruction sets,
specialized kernel requirements for different data types and model features
(e.g., sparsity, quantization), and architecture-specific optimizations
complicate performance tuning. Manual optimization is resource-intensive, while
existing automatic approaches often rely on complex hardware-specific
heuristics and uninterpretable intermediate representations, hindering
performance portability. We introduce PerfLLM, a novel automatic optimization
methodology leveraging Large Language Models (LLMs) and Reinforcement Learning
(RL). Central to this is PerfDojo, an environment framing optimization as an RL
game using a human-readable, mathematically-inspired code representation that
guarantees semantic validity through transformations. This allows effective
optimization without prior hardware knowledge, facilitating both human analysis
and RL agent training. We demonstrate PerfLLM's ability to achieve significant
performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.

</details>
