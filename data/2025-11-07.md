<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 56]
- [cs.AI](#cs.AI) [Total: 24]
- [cs.DC](#cs.DC) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 本文提出了一种基于大五人格特质的新型方法，通过从Transformer层中提取隐藏状态激活并识别与特定人格特质相关的低秩子空间，实现对大语言模型生成行为的精准控制。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽表现出隐含的人格特征，但如何可靠地控制或对齐这些特征以满足特定需求仍是一个开放性挑战。现有研究缺乏有效机制在生成过程中操控模型行为，且人格心理构念与其在模型中的表示之间的关系尚未充分探索。

Method: 作者构建了一个新流程：利用大五人格特质（开放性、尽责性、外向性、宜人性和神经质）作为理论框架，从Transformer不同层中提取隐藏状态激活；应用低秩子空间发现方法，识别出适用于不同模型架构的、与特质相关的最优层；并通过动态选择层的灵活引导框架，将人格对齐方向操作化，以精确调控输出中的人格表达。

Result: 研究发现人格特质占据一个低秩共享子空间，通过对该潜在结构进行精细扰动，可在不影响语言流畅性、输出多样性和模型通用能力的前提下，有效实现对模型行为的引导。

Conclusion: 该工作揭示了心理理论与大语言模型内部表示之间的联系，并提供了一种实用且有效的机制，用于将人格特质转化为可操作的模型对齐工具，弥合了心理学理论与实际模型控制之间的鸿沟。

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: 本文提出了TextualVerifier，一种用于TextGrad的自验证框架，通过大语言模型的思维链推理和多数投票机制，在无需数值梯度的情况下提升文本优化中推理的有效性。


<details>
  <summary>Details</summary>
Motivation: TextGrad缺乏确保文本决策中推理有效性的自验证机制，限制了其在复合AI系统中的可靠性。

Method: TextualVerifier采用四阶段工作流：思维链分解、变体生成、多数投票和共识聚合，并与TextGrad在损失函数和优化结果验证阶段非侵入式集成。

Result: 在PRM800K上单独评估时，推理步骤有效性提升29%；与TextGrad集成后，在多个基准测试中显著提升性能（如GPQA提升8.08个百分点），且仅引入平均5.9次LLM调用的开销。

Conclusion: TextualVerifier是首个为TextGrad设计的基于LLM的自验证框架，无需数值梯度即可增强文本优化的可靠性，为文本级验证开辟了新方向。

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 本文提出了扩展版的希腊方言数据集GRDD+，涵盖10种方言、共计637万词，并首次在如此多样性和规模的数据上对多个大语言模型进行微调实验，评估方言数据对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有希腊方言数据集（GRDD）覆盖有限，缺乏对多种重要方言变体的充分表示；作者旨在构建一个更全面、更具代表性的希腊方言语料库，以支持方言语言学研究和提升大语言模型在方言任务上的表现。

Method: 在原有GRDD基础上扩充克里特、塞浦路斯、本都和北希腊方言数据，并新增六种方言变体，构建包含6,374,939词、10种方言的GRDD+数据集；随后对Llama-3-8B、Llama-3.1-8B和Krikri-8B三种模型架构进行微调，并与Claude-3.7-Sonnet、Gemini-2.5和ChatGPT-5等前沿模型进行性能比较。

Result: 成功构建了目前规模最大、方言种类最丰富的希腊方言数据集GRDD+；微调实验表明高质量方言数据能有效提升大语言模型在方言相关任务上的表现。

Conclusion: GRDD+为希腊方言研究和方言感知语言模型开发提供了宝贵资源，证明了高质量方言数据在提升模型语言多样性处理能力方面的重要价值。

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [4] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: 本文介绍了PLLuM（波兰大型语言模型），这是专为波兰语设计的最大开源基础模型系列，由波兰多家研究机构联合开发，包含1400亿词元的预训练语料库、77k指令数据集和100k偏好优化数据集，并集成负责任AI框架，旨在推动开放研究和增强波兰本土AI技术主权。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型主要聚焦英语，对其他语言支持有限；因此需要开发高质量、透明且文化相关的非英语语言模型，以满足本地化需求并减少对英语中心商业模型的依赖。

Method: 构建1400亿词元的波兰语文本语料库用于预训练，创建77k自定义指令数据集和100k偏好优化数据集；采用混合模块进行输出校正与安全过滤，并实施严格的数据治理框架；开发基础模型和指令微调模型，并在公共行政下游任务中验证其效用。

Result: 成功开发并公开发布了PLLuM系列模型，在公共行政任务中展现出实用价值，同时通过负责任AI框架确保模型安全性与合规性。

Conclusion: PLLuM填补了波兰语高质量开源大模型的空白，有助于促进开放研究并加强波兰在人工智能领域的技术主权。

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [5] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 提出了一种名为STARS的解码时算法，通过分段采样、评分与拒绝/接受机制，在提升大语言模型对齐效果的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法如微调计算成本高且效果有限，而推理时方法（如Best-of-N）虽有效但计算不可行，亟需一种高效且高质量的替代方案。

Method: STARS（Segment-level Token Alignment with Rejection Sampling）：在生成过程中迭代地对固定长度的短token片段进行采样、打分，并决定是否接受或拒绝，从而早期修正生成路径。

Result: 在六个大语言模型上，STARS在胜率指标上比监督微调（SFT）最高提升14.9个百分点，比直接偏好优化（DPO）最高提升4.3个百分点，同时与强Best-of-N基线具有竞争力。

Conclusion: STARS证明了细粒度、奖励引导的采样是一种通用、鲁棒且高效的LLM对齐方法，可作为传统微调和全序列排序方法的有效替代。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [6] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文提出一种将多标签文本分类任务重构为一系列二元（是/否）判断的方法，结合前缀缓存机制和大语言模型到小语言模型的蒸馏，在不损失准确率的前提下显著提升短文本推理效率。


<details>
  <summary>Details</summary>
Motivation: 传统多标签分类方法在使用大语言模型时存在效率低下问题，尤其在处理大量标签维度时。作者旨在通过任务重构与优化推理机制，实现高效且准确的多标签分类。

Method: 将多标签分类任务拆解为多个独立的二元决策问题，每个目标维度单独查询；引入前缀缓存机制加速推理，并利用大模型（DeepSeek-V3）生成多标注数据，通过蒸馏微调小模型（如HerBERT-Large、Gemma3-1B等）。

Result: 微调后的小模型在情感与情绪等24个维度上显著优于零样本基线，尤其在训练中见过的维度上表现突出，同时推理效率大幅提升。

Conclusion: 将多标签分类分解为二元查询，结合蒸馏与缓存感知推理，是一种通用、可扩展且高效的LLM分类框架，适用于包括情感分析在内的多种领域。

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [7] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 该论文研究了三种低资源语言（阿法尔语、阿姆哈拉语和提格雷尼亚语）机器翻译数据集的质量，发现尽管训练数据以政治和宗教文本为主，但基准数据集则集中于新闻、健康和体育领域；同时数据集中存在严重的性别偏向，尤其对女性有有害和刻板的描绘，表明数据量并不等于数据质量。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言日益被纳入自然语言处理研究的背景下，当前研究过于强调大规模数据集的收集而忽视数据质量，可能导致语言技术性能不佳并传播社会偏见，因此有必要评估这些数据集的质量，特别是性别表征问题。

Method: 作者分析了阿法尔语、阿姆哈拉语和提格雷尼亚语的机器翻译数据集，考察其领域分布、性别代表性（包括人名、动词语法性别及刻板描绘）以及是否存在对女性的有害内容。

Result: 研究发现训练数据主要来自政治和宗教领域，而基准数据集则聚焦新闻、健康和体育；数据集中存在显著的男性偏向，并包含对女性的有害与毒性内容，且数据量最大的语言中此类问题更为突出。

Conclusion: 数据数量不能保证质量，低资源语言数据集中的性别偏见和有害内容亟需关注；作者呼吁对低资源语言数据集进行更深入的研究，并在早期阶段采取措施减轻有害内容的影响。

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [8] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: GRAD是一种无需重新训练的轻量级解码方法，通过构建稀疏词元转移图融合语料库中的统计证据，有效降低大语言模型的幻觉率并提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 现有缓解大语言模型幻觉的方法依赖外部知识源，但提示引导方式脆弱且领域敏感，符号知识整合则带来高昂的检索与格式化成本。

Method: 提出Graph-Retrieved Adaptive Decoding（GRAD），在解码阶段通过单次前向传播从少量检索语料中累积下一词元logits，构建稀疏词元转移图，并将图检索logits与模型logits自适应融合以偏好高证据连续性。

Result: 在三个模型和多种问答基准上，GRAD相比贪婪解码最高提升9.7%内在准确率、降低8.6%幻觉率、提高6.9%正确性，并在真实性-信息量乘积得分上优于所有对比方法。

Conclusion: GRAD证明了语料层级的词元转移统计证据可有效引导生成更真实、可验证的输出，为对比解码和知识图谱增强提供了轻量级即插即用的替代方案。

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [9] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 在迭代指代表达任务中，视觉-语言模型在缺乏相关上下文时表现远逊于人类，但在提供相关上下文后性能显著提升，但仍难以应对抽象指称的少样本任务。


<details>
  <summary>Details</summary>
Motivation: 探究人类与视觉-语言模型在多轮语言交互中进行上下文敏感的语用推理能力，特别是在迭代指代表达游戏中的表现差异。

Method: 通过设计不同上下文条件（数量、顺序和相关性）下的迭代指代表达任务，对比人类与视觉-语言模型的表现。

Result: 无相关上下文时，模型表现仅略高于随机水平且明显弱于人类；但当提供相关上下文后，模型表现随轮次显著提升。

Conclusion: 尽管上下文能显著提升模型在迭代指代表达任务中的表现，但处理抽象指称的少样本任务仍是当前模型的一大挑战。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [10] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 本文基于2013–2023年约26亿条美国地理定位推文，利用微调的大语言模型构建了“人类繁荣地理指数”（HFGI），涵盖48项与哈佛全球繁荣研究框架一致的指标，提供县级和州级月度/年度数据，以高时空分辨率衡量人类繁荣相关话语。


<details>
  <summary>Details</summary>
Motivation: 现有衡量人类繁荣的方法在空间和时间分辨率上不足，难以全面反映社会福祉。因此需要一种能捕捉多维度繁荣（如幸福、健康、目标、美德、人际关系和财务稳定）并具备高分辨率的新指标。

Method: 利用微调的大语言模型分析约26亿条带有地理位置的美国推文（2013–2023年），对48个繁荣相关指标进行分类，包括哈佛全球繁荣研究框架中的维度以及对移民态度和腐败感知，并生成县级和州级的月度/年度指标。

Result: 构建出高时空分辨率的人类繁荣地理指数（HFGI），经验证能准确反映底层构念，并与已有指标呈现预期相关性，可用于跨学科研究福祉、不平等和社会变迁。

Conclusion: 该研究通过社交媒体话语提供了过去十年美国人类繁荣动态的新视角，为多学科研究提供了前所未有的高分辨率数据资源。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [11] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 该论文提出通过向量翻译在大语言模型之间建立语义桥梁，实现跨模型的潜在语义通信，从而提升多智能体协作效率。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统中，大语言模型通过纯文本传递消息，丢失了大部分潜在语义，限制了信息传输效率并增加了计算开销。

Method: 构建一个双编码器翻译器，在Llama-2-7B与Mistral-7B-Instruct之间学习表示空间的映射，通过向量注入（30%混合强度）实现语义传递。

Result: 翻译器在两个模型间达到平均余弦对齐度0.538；双向评估显示2.01:1的传输不对称性，表明通用模型比指令微调模型具有更强的表示可迁移性。

Conclusion: 保守的向量注入方式在保持计算稳定性的同时，验证了跨模型潜在语义通信的可行性，为协作式AI系统提供了新路径。

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [12] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 本文提出将溯因推理整合到检索增强生成（RAG）系统中，通过检测证据不足、生成缺失前提并验证其一致性与合理性，从而提升答案准确性和推理可信度。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在检索到的证据不完整时表现不佳，导致推理过程存在漏洞；溯因推理能生成合理的缺失前提以弥补这些漏洞，因此作者希望将其引入RAG以增强系统鲁棒性与可解释性。

Method: 该框架首先检测证据是否充分，若不足则生成候选缺失前提，并通过一致性和合理性检查对这些前提进行验证，最终用于增强LLM的推理过程。

Result: 在溯因推理和多跳问答基准上的实验表明，该方法显著提升了答案准确率和推理忠实度。

Conclusion: 将溯因推理融入RAG系统是一种有效策略，有助于提升其在知识密集型任务中的鲁棒性和可解释性。

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [13] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 该论文提出“专家对齐”作为评估大语言模型（LLM）解释质量的新标准，并构建了涵盖七个知识密集型领域的基准T-FIX，联合领域专家开发新指标以衡量LLM解释与专家判断的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM解释的评估主要关注合理性或内部忠实性，无法反映解释内容是否符合领域专家的直觉和推理逻辑，尤其在医疗、天文、心理等高专业知识场景中存在明显不足。

Method: 作者形式化定义“专家对齐”标准，构建跨七个知识密集型领域的T-FIX基准，并与领域专家合作设计新指标来评估LLM生成解释与专家判断的对齐程度。

Result: 提出了T-FIX基准和基于专家判断的新评估指标，为衡量LLM在专业领域中解释质量提供了更贴合实际需求的方法。

Conclusion: 要有效评估知识密集型场景下LLM的解释能力，必须引入专家对齐视角，仅依赖传统评估标准不足以反映解释的专业性和可信度。

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [14] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 本文提出了一种名为PoK（Plan of Knowledge）的新框架，通过将复杂的时间问题分解为子目标，并结合对比式时间知识检索机制，显著提升了大语言模型在时序知识图谱问答任务中的推理准确性和事实一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理时序知识图谱问答（TKGQA）时难以充分理解时间约束的复杂语义，而大语言模型虽具备强大语义理解能力，却存在时间推理能力弱、易产生幻觉和知识缺失的问题。

Method: 提出PoK框架，包含两个核心组件：一是“知识规划”模块，将复杂时间问题分解为预定义工具的子目标序列以引导推理；二是构建带对比检索机制的时序知识库（TKS），从时序知识图谱中选择性地检索语义与时间对齐的事实。

Result: 在四个TKGQA基准数据集上的实验表明，PoK显著提升了大语言模型的检索精度和推理准确性，性能最多超越当前最优方法56.0%。

Conclusion: 通过结构化规划与时序知识检索相结合，PoK有效增强了大语言模型在时间敏感问答任务中的可解释性和事实一致性，为TKGQA提供了新的高效解决方案。

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [15] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 该论文比较了人类与大语言模型（LLMs）在情感词汇联想上的行为，发现两者存在中等程度的重叠，但LLMs倾向于放大刺激词的情感负荷，且其联想更具可预测性、创造性较低。


<details>
  <summary>Details</summary>
Motivation: 人类词汇联想能揭示心理词汇结构，但受个体经验、情绪和认知风格影响，具有不可预测性和创造性；研究旨在探究大语言模型是否以类似方式生成联想，特别是在情感词汇方面。

Method: 对人类参与者与大语言模型针对情感负载词汇所产生的联想进行对比分析。

Result: 人类与LLMs的联想存在中等重叠；LLMs的联想更倾向于放大原始词的情感负荷，且更具可预测性、创造性较弱。

Conclusion: 大语言模型在词汇联想任务中虽能部分模拟人类行为，但在情感处理和创造性方面仍与人类存在显著差异。

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [16] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 本文提出一种基于大规模放射科报告训练的Transformer去标识化模型，在多个数据集上显著优于现有学术和商业系统，F1分数最高达0.996，并引入新PHI类别AGE，验证了合成PHI的有效性与隐私保护能力。


<details>
  <summary>Details</summary>
Motivation: 提升自动化去除放射科报告中受保护健康信息（PHI）的能力，解决现有方法在跨机构泛化性和准确性方面的不足，并与主流商业云服务系统进行性能对比。

Method: 基于现有最先进的Transformer去标识化流程，利用斯坦福大学两个大型标注放射科语料库（涵盖胸片、胸部CT、腹部/盆腔CT和脑部MR报告）进行微调，并在架构中新增AGE类别的PHI；在斯坦福和宾夕法尼亚大学测试集上评估token级PHI检测性能，同时采用“hide-in-plain-sight”方法生成合成PHI以评估稳定性，并与多个商业系统进行对比。

Result: 模型在Penn和Stanford测试集上的总体F1分数分别为0.973和0.996，优于或持平先前最优模型；在50个独立去标识化的Penn数据集中，合成PHI检测F1稳定在0.959；在合成Penn报告上，模型F1为0.960，显著优于商业系统（0.632–0.754）。

Conclusion: 基于多样化放射科数据训练的Transformer去标识化模型在PHI检测任务中超越现有学术与商业系统，为安全临床文本处理设立了新基准。

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [17] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 本文研究了在极限情况下的语言识别问题，其中学习者在每个时间步输出一个包含k个猜测的列表，目标是最终某个猜测始终正确。作者给出了可被k-列表识别的语言集合的精确刻画，并证明这类集合可分解为k个可单独识别的子集。此外，还分析了统计设定下的识别速率，表明若集合可k-列表识别，则能以指数速率实现，否则无法以趋于零的速率识别。


<details>
  <summary>Details</summary>
Motivation: 受语言生成问题近期积极成果的启发，作者重新审视经典的语言识别问题，探索当学习者每次可输出k个猜测时，能否克服Gold所揭示的经典不可识别性限制。

Method: 基于Angluin对单猜测语言识别的经典刻画，作者提出了其递归版本，并据此给出k-列表识别的精确条件；同时利用该刻画分析i.i.d.输入流下的统计识别速率。

Result: 1）给出了语言集合可被k-列表识别的充要条件：该集合可分解为k个各自可单猜测识别的子集；2）证明若集合可k-列表识别，则可在统计设定下以指数速率识别，且这是最优的；否则无法以任何趋于零的速率识别。

Conclusion: 通过引入k-列表识别模型，作者不仅推广了经典语言识别理论，还建立了与集合结构和识别速率之间的深刻联系，为计算学习理论提供了新的视角。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [18] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 本文发现批处理提示不仅能降低大语言模型的推理成本，还能作为推理过程中的正则化手段，提升准确率并显著减少推理所需的token数量。


<details>
  <summary>Details</summary>
Motivation: 探索批处理提示在大语言模型中除降低推理成本外的潜在优势，特别是在多步推理过程中对模型行为的影响。

Method: 在13个多样化基准上进行系统性实验，分析批处理对大型推理模型（LRMs）推理行为和性能的影响。

Result: 批处理提升了推理准确率，减少了3至5倍的推理token使用，并抑制了模型的过度思考与犹豫表达，同时展现出同批次内示例间的模式泛化能力。

Conclusion: 批处理不仅是一种吞吐量优化技术，更是一种有效的推理时正则化方法，可提升大语言模型推理的效率与可靠性。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [19] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: 本文提出RIDE框架，利用项目反应理论（IRT）和强化学习生成更具挑战性且结构良好的数学问题变体，以更严格地评估大语言模型的数学推理能力。实验表明，经RIDE扰动后的问题使26个先进模型平均性能下降21.73%，揭示了当前模型在数学推理方面的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的对抗扰动方法常生成病态问题，难以系统评估问题难度及推动评测基准演进，因此需要一种能生成良构且更具挑战性问题的新方法来真实衡量大语言模型的数学推理能力。

Method: 提出RIDE框架，结合项目反应理论（IRT）与强化学习：利用35个大语言模型模拟学生作答，构建难度排序器；该排序器作为奖励信号，指导问题重写模型在不同难度层级上改写原始数学问题。

Result: 将RIDE应用于竞赛级数学基准后，生成的扰动问题使26个先进大语言模型的平均性能下降21.73%，有效暴露了模型在数学推理上的鲁棒性不足。

Conclusion: RIDE能有效生成高质量、高难度的数学问题变体，为评估大语言模型的真实数学推理能力提供了可靠工具，并揭示了当前模型在此任务上的局限性。

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [20] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: CantoASR 是一个结合声学特征与大音频语言模型（LALM）推理的协作式粤语自动语音识别框架，通过强制对齐、LoRA 微调的 Whisper 和指令微调的 Qwen-Audio，在低资源条件下显著降低字符错误率。


<details>
  <summary>Details</summary>
Motivation: 粤语由于标注数据稀缺、六声调系统、变调规则和口音差异，导致现有 ASR 模型（如 Whisper）在该语言上表现不佳，亟需融合声学线索与上下文推理能力以提升识别准确率。

Method: 提出 CantoASR 框架，整合强制对齐提取声学特征、LoRA 微调的 Whisper 提升声调区分能力，并利用指令微调的 Qwen-Audio 进行韵律感知的错误校正。

Result: 在自发粤语数据集上，CantoASR 相较 Whisper-Large-V3 实现了显著的字符错误率（CER）下降。

Conclusion: 将声学线索与大音频语言模型的推理能力相结合，是提升低资源声调语言和方言 ASR 性能的有效且可扩展策略。

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [21] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 本文探索了三种多智能体大语言模型（LLM）流水线用于Text-to-SQL任务，系统评估了从小到大的开源模型性能。实验表明，多智能体讨论机制可显著提升小模型表现，而Planner-Coder流水线效果最佳。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在自然语言生成SQL时面临数据库模式庞大和推理复杂的问题，且以往研究多依赖复杂而不实用的旗舰模型流水线，忽视了小型高效模型的潜力。

Method: 提出并评估三种多智能体LLM流水线：(1) 多智能体讨论机制，通过迭代批评与优化生成最终SQL；(2) Planner-Coder机制，由规划模型生成步骤计划，再由编码模型生成SQL；(3) Coder-Aggregator机制，多个编码器独立生成SQL，再由推理智能体选择最优结果。

Result: 在Bird-Bench Mini-Dev数据集上，多智能体讨论机制使Qwen2.5-7b-Instruct模型的执行准确率最多提升10.6%；Planner-Coder流水线表现最佳，使用DeepSeek-R1-32B和QwQ-32B作为规划器，将Gemma 3 27B IT的准确率从52.4%提升至56.4%。

Conclusion: 多智能体协作机制能有效提升Text-to-SQL任务中各类规模LLM的性能，尤其Planner-Coder架构展现出最强的增益效果，为高效利用中小型模型提供了可行路径。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [22] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: 该论文提出LAAC（LLM作为沟通者）新范式，将大语言模型用作智能沟通中介，通过结构化对话捕捉发送者意图并促进真实知识交换，同时系统评估其在信息捕获保真度、可复现性和查询响应完整性三个维度上的可信度要求。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成内容导致沟通失真：发送方用LLM将简单想法膨胀为冗长内容，接收方又用LLM压缩回摘要，双方均未接触真实内容。为打破这一循环，作者提出LAAC范式以实现更真实的跨场景沟通。

Method: 通过构建LAAC多智能体架构，在多种使用场景下开展受控实验，系统评估其在信息捕获保真度（不同沟通类型中意图提取的准确性）、可复现性（多次交互中结构化知识的一致性）和查询响应完整性（避免幻觉、来源混淆或捏造）三个维度的表现。

Result: 初步实验揭示了LAAC在高风险沟通场景部署前必须解决的可度量的信任差距，尤其在上述三个信任维度上存在不足。

Conclusion: LAAC作为一种新型沟通范式具有潜力，但其作为可信沟通中介的可靠性仍需在信息保真、一致性和响应真实性方面进一步提升，方可应用于高风险场景。

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [23] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 本文提出一种计算图灵测试框架，用于评估大语言模型（LLM）生成文本在多大程度上接近人类语言，并系统比较了九个开源LLM在五种校准策略下的表现。研究发现，即使经过校准，LLM输出仍明显区别于人类文本，尤其在情感表达方面；指令微调模型表现不如基础模型，增大模型规模也未提升拟人性；同时，拟人性与语义保真度之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 当前社会科学中广泛使用大语言模型模拟人类行为，但其生成文本是否真实反映人类语言这一基本假设缺乏可靠验证。现有方法依赖人类判断区分AI与人类文本，但该方法粗糙且不可靠，导致缺乏有效工具评估和校准LLM生成内容的真实性。

Method: 作者提出一个计算图灵测试框架，结合聚合指标（如基于BERT的可检测性和语义相似性）与可解释的语言特征（如文体标记和主题模式），评估LLM在特定数据集中对人类语言的逼近程度；并系统比较九个开源LLM在五种校准策略（包括微调、文体提示和上下文检索等）下在X、Bluesky和Reddit用户互动数据上的表现。

Result: 研究发现：（1）即使经过校准，LLM生成文本在情感语气和情绪表达上仍明显区别于人类文本；（2）指令微调模型表现不如其基础版本；（3）增大模型规模并未提升拟人性；（4）优化拟人性常以牺牲语义保真度为代价，反之亦然。

Conclusion: 本研究提供了一个可扩展的LLM生成文本验证与校准框架，同时警示当前LLM在模拟人类交流方面仍存在显著局限，尤其是在情感表达和真实性方面。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [24] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 本文评估了当前大语言模型（LLM）是否能通过波兰国家上诉委员会的官方资格考试，发现尽管模型在知识测试中表现尚可，但在撰写判决书等实践任务中未达及格线，且“LLM作为评委”的自动评分与官方评审存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在法律专业领域（特别是波兰公共采购法）中的实际应用潜力，检验其是否具备替代人类考官或法官的能力，并评估“LLM作为考生”和“LLM作为评委”两种范式的有效性。

Method: 构建混合信息检索与抽取管道，测试多个LLM（如GPT-4.1、Claude 4 Sonnet、Bielik-11B-v2.6）在闭卷和不同检索增强生成（RAG）设置下的表现；考试包括公共采购法多项选择题和书面判决写作；同时采用其他LLM对答案进行自动评分，并与官方评审结果对比。

Result: 模型在知识测试中得分尚可，但无一通过书面判决部分；LLM自动评分结果与官方评审存在较大分歧；模型普遍存在幻觉、引用法律条文错误、逻辑论证薄弱等问题。

Conclusion: 尽管技术进步迅速，当前LLM尚不能替代人类法官或独立考官参与波兰公共采购裁决，需法律专家与技术团队紧密协作以克服现有局限。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [25] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: 本文提出了一种名为 REMIND 的新方法，用于更敏感地评估机器学习模型是否真正“遗忘”了特定训练数据。该方法通过分析模型在输入微小扰动下的损失变化，识别残留的记忆影响，相比现有仅基于单点输入的评估方法更为有效和鲁棒。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘评估方法通常只在单个输入层面判断是否遗忘，可能忽略语义相似样本中残留的影响，从而导致隐私泄露风险。因此，需要一种更敏感、能捕捉细微残留影响的评估方法。

Method: REMIND 通过考察模型在目标输入及其邻近变体上的损失曲面形态（如平坦度与陡峭度）来判断数据是否被有效遗忘。该方法只需查询访问权限，无需内部模型参数。

Result: 实验表明，REMIND 能有效区分已遗忘、保留或无关数据，其损失景观特征明显不同；该方法在多种模型、数据集和改写输入下均表现稳健，优于现有评估方法。

Conclusion: REMIND 提供了一个更敏感、可解释且实用的机器遗忘评估框架，为语言模型的遗忘效果验证提供了新视角，并有助于提升模型的隐私保障与可信度。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [26] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 本文通过检索增强生成（RAG）和测试时计算，量化了大语言模型在预训练过程中未充分利用的数据价值，发现结合检索可显著提升模型在多个基准上的准确率，表明当前预训练方法仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型虽依赖大规模预训练语料，但缺乏对预训练过程提取知识效率的系统理解；作者旨在评估现有预训练方法是否充分挖掘了已有数据中的信息价值。

Method: 采用检索增强生成（RAG）框架，在测试阶段从标准且多为开源的数据集中检索相关信息，并结合额外计算解析检索内容，以衡量预训练后仍可从中获取多少性能提升。

Result: 在MMLU、Math-500和SimpleQA等任务上，引入检索显著提升了准确率；其中MMLU上检索相当于约5倍预训练计算量的增益，进一步利用测试时计算可使LLaMA 3.1 8B模型提升10个百分点。

Conclusion: 当前的预训练方法未能充分利用现有数据集中的信息，通过检索增强和测试时计算仍有显著性能提升空间。

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [27] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaooui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 本文提出一种基于图的轻量级方法，用于为概率主题模型生成的主题分配可解释的标签，在保持计算效率的同时，在BERTScore和余弦相似度指标上优于传统基准方法，并与ChatGPT-3.5表现相当。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模方法虽能自动发现文本中的主题，但生成的主题通常以词分布形式呈现，缺乏清晰的语义解释；同时，许多现有方法计算开销大。因此，作者希望在降低计算成本的同时提升主题的可解释性。

Method: 提出一种基于图的方法：首先对主题词进行语义扩展，构建包含主题词及其相关词的图结构，然后通过分析图中词语之间的关系，自动生成能准确表达主题含义的标签。

Result: 在两个数据集上的实验表明，该方法在BERTScore和余弦相似度方面优于传统基准方法，且性能与ChatGPT-3.5相当，同时保持了较高的计算效率。

Conclusion: 所提出的图方法在保证计算效率的前提下有效提升了主题标签的可解释性，未来可在自动化和语义增强方面进一步探索。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [28] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 本文提出SSPO算法，通过引入句子级重要性比率和基于句子熵的动态裁剪机制，在提升大语言模型推理能力的同时，解决了GRPO训练不稳定和GSPO采样数据利用率低的问题，并在多个数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法如GRPO和GSPO分别存在训练不稳定和采样数据利用率低的问题：GRPO基于token级重要性比率易受异常值影响导致训练崩溃，而GSPO使用响应级比率则因整体裁剪导致有效响应被误弃。

Method: 提出SSPO（Sentence-level Sequence Policy Optimization），采用句子级重要性比率平衡GRPO与GSPO的优缺点，并结合句子熵动态调整PPO-CLIP的裁剪边界，鼓励高熵token探索、缩小低熵token裁剪范围。

Result: SSPO在五个数据集上平均得分达46.57，优于GRPO（43.01）和GSPO（44.42），并在其中三个数据集上达到当前最优性能。

Conclusion: SSPO通过句子级重要性比率和熵感知裁剪机制，有效提升了训练稳定性与数据利用效率，在RLVR框架下展现出优越的推理能力提升效果。

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [29] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 本文提出一种用于机器翻译微调的数据选择方法，通过学习模型与预训练参考模型之间的协同作用，定义可学习性分数并采用批次选择策略，在多个语言对上显著提升了数据效率、计算效率和翻译性能。


<details>
  <summary>Details</summary>
Motivation: 提升机器翻译模型性能的关键在于高质量且有效的数据选择。现有方法在数据利用效率和相关性方面存在不足，因此需要一种更高效、更具针对性的数据选择策略来优化微调过程。

Method: 该方法结合学习模型与预训练参考模型，通过定义“可学习性分数”评估每个数据点的训练价值，并采用考虑数据点间相互依赖性的批次选择策略，从而在微调过程中仅保留最相关且最有影响力的数据。

Result: 在英语-波斯语等多个语言对上的实验表明，相比独立同分布（iid）基线，该方法在数据效率上最高提升5倍；使用缓存嵌入时计算效率提升24倍，并在减少训练数据量的同时提高了泛化能力和翻译质量。

Conclusion: 所提出的数据选择方法能显著提升机器翻译微调阶段的数据与计算效率，并增强模型泛化能力，优于随机选择等传统方法。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [30] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 本文研究了大语言模型（LLMs）在时间推理方面的能力，通过使用一本1940年的挪威语常识问答书，要求模型以1940年的视角回答问题，并分别用英语和挪威语提问。结果发现英语提示效果优于挪威语，且更大规模的模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否具备时间推理能力，即能否根据特定历史时期的知识背景正确回答问题，并比较不同语言提示和模型规模对性能的影响。

Method: 使用1940年出版的挪威语常识问答书作为数据集，以英语和挪威语两种语言向多个大语言模型（包括DeepSeek-R1、Gemma3、Qwen3、Llama3.1及专为挪威语优化的最大模型）提问，要求模型模拟1940年的知识状态作答；答案评估采用“LLM-as-judge”方法，并由母语者抽样验证。

Result: 英语提示下的模型表现始终优于挪威语提示，这一结果出乎意料；同时，模型规模越大，答题准确率越高。

Conclusion: 大语言模型在时间推理任务中表现出一定能力，但其性能受提示语言和模型规模显著影响，英语提示和更大模型更有利于提升准确性。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [31] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 本文提出了PTTSD框架，一种基于概率文本时间序列的抑郁检测方法，通过建模临床访谈中的不确定性并预测PHQ-8评分，在仅使用文本的系统中达到最先进性能，并提供校准良好的预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁严重程度预测模型通常缺乏不确定性估计和时间动态建模，难以支持临床决策；因此需要一个既能准确预测又能提供可解释不确定性的时序建模方法。

Method: PTTSD结合双向LSTM、自注意力机制和残差连接，采用高斯或Student-t输出头并通过负对数似然训练，包含seq2seq和seq2one两种变体，用于从逐句临床访谈中预测PHQ-8分数并建模时间上的不确定性。

Result: 在E-DAIC和DAIC-WOZ数据集上，PTTSD在纯文本系统中取得最优结果（如E-DAIC上MAE=3.85，DAIC上MAE=3.55），并生成校准良好的预测区间；消融实验证明了注意力机制和概率建模的有效性。

Conclusion: PTTSD不仅提升了抑郁严重程度预测的准确性，还通过不确定性建模增强了模型的可解释性和临床实用性，为临床决策支持提供了可靠工具。

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [32] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: ThaiOCRBench 是首个针对泰语文本密集型视觉理解任务的综合评测基准，包含2,808个人工标注样本，涵盖13类任务，用于评估视觉语言模型（VLM）在低资源、复杂文字场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型评测基准主要聚焦高资源语言，泰语在文档结构理解等任务中严重缺乏代表性，亟需专门的评测数据集来推动相关研究。

Method: 构建包含2,808个样本、13个任务类别的泰语人工标注数据集 ThaiOCRBench，并在零样本设置下对多种开源与闭源视觉语言模型进行系统评估和错误分析。

Result: 闭源模型（如Gemini 2.5 Pro）显著优于开源模型；开源模型在细粒度文本识别和手写内容提取任务上表现最差；主要挑战包括语言偏见、结构错配和幻觉内容。

Conclusion: ThaiOCRBench 为低资源复杂文字环境下的视觉语言模型提供了标准化评测框架，并为提升泰语文档理解能力提供了可操作的改进方向。

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [33] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: RUST-BENCH 是一个包含7966个问题、基于2031张真实表格的新基准，涵盖科学（NSF资助记录）和体育（NBA统计数据）两个领域，旨在评估大语言模型在长表、异构结构、领域特异性和多跳推理等方面的表格推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有表格推理基准主要测试小而统一的表格，无法反映现实世界数据的复杂性，也无法全面评估大语言模型的真实推理能力。

Method: 构建 RUST-BENCH 基准，包含来自两个领域的真实表格，并设计涵盖规模、异构性、领域特异性和推理复杂性的评估任务；在开源与闭源大语言模型上进行实验。

Result: 实验表明，当前大语言模型在处理异构模式和复杂的多跳推理任务时表现不佳，揭示了现有架构和提示策略的持续弱点。

Conclusion: RUST-BENCH 为推动表格推理研究提供了一个具有挑战性的新测试平台。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [34] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 本文提出两种基于多轮简化的LLM提示方法（MRS-Rule和MRS-Joint）用于可读性控制的文本简化任务，在TSAR-2025共享任务中排名第7；后续改进表明以LLM简化结果为起点可进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 作者发现文本简化效果与源文本和目标文本之间的CEFR等级差距密切相关，由此启发设计更有效的多轮简化策略。

Method: 提出两种基于GPT-4o的多轮简化方法：基于规则的简化（MRS-Rule）和结合规则与LLM的联合简化（MRS-Joint）。

Result: 在TSAR-2025共享任务中，提交系统在20支队伍中排名第7；后续实验显示MRS-Joint通过以LLM简化结果为起点可进一步提升性能。

Conclusion: 利用LLM进行多轮文本简化时，结合规则与模型生成结果，并以模型输出为起点，能有效提升简化质量。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [35] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 本文系统评估了六种大语言模型（LLMs）在不同采样温度下基于大五人格量表（BFI-2）的人格特质表现，发现四个维度存在显著差异，且模型架构可能影响其人格稳定性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在以人为中心的应用中日益普及，理解其类人格行为对负责任的开发与部署至关重要。

Method: 应用大五人格量表（BFI-2）框架，在不同采样温度下评估六种大语言模型的人格特质表达，并通过层次聚类分析模型间的相似性。

Result: 在五个维度中的四个上观察到显著差异，其中神经质和外向性受温度调节影响较大；层次聚类揭示了不同的模型簇，表明模型架构可能影响其人格特征稳定性。

Conclusion: 研究揭示了大语言模型中类人格模式的涌现机制，为模型调优、选择及AI伦理治理提供了新视角。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [36] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 该论文提出一个两阶段框架，用于量化放射学报告中的显式和隐式不确定性，并构建了增强版的Lunguage++数据集以支持不确定性感知的医学图像分析与诊断推理。


<details>
  <summary>Details</summary>
Motivation: 放射学报告在临床决策中至关重要，但其中包含的不确定性（包括显式和隐式）阻碍了其在自动化分析中的有效利用。现有规则系统难以准确量化这些不确定性，因此需要更精细的建模方法。

Method: 作者通过两个部分处理不确定性：(1) 利用专家验证的大语言模型对常见模糊表达进行排序，将每个发现映射为概率值以量化显式不确定性；(2) 基于14种常见诊断的专家定义路径，系统性地扩展缺失的子发现，以建模隐式不确定性。

Result: 构建并发布了Lunguage++，这是Lunguage基准的扩展版本，具备细粒度结构化和不确定性感知能力，可用于不确定性感知的图像分类、可信诊断推理及临床不确定性影响研究。

Conclusion: 该工作通过建模放射学报告中的两类不确定性，显著提升了结构化报告的质量与可用性，为后续医学人工智能应用提供了重要资源和方法基础。

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [37] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 本文研究语言模型在生成过程中是否隐式表示其可能采取的多种推理路径，发现隐藏激活不仅能预测模型的不确定性，还能有效引导其生成方向，尤其在模型尚未确定最终答案时。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在生成文本时是否能够表征其推理过程中潜在的多种路径，并量化其不确定性。

Method: 通过干预和分析语言模型的隐藏激活，控制并预测其在链式思维推理过程中的不确定性。

Result: 发现模型在不同token处的不确定性与其被激活干预引导的难易程度显著相关；隐藏激活还能预测模型未来的输出分布。

Conclusion: 语言模型在其隐藏状态中隐式地编码了多种可能的推理路径，激活干预在模型尚未做出最终决策时最为有效。

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [38] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof 是一个基于大语言模型（LLM）的交互式系统，用于将议论文结构化为论证图，并通过可视化与自然语言工具提升用户对文章论证质量的理解。


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统缺乏对用户理解体验的关注，而 IntelliProof 旨在通过结合 LLM 与交互式可视化，帮助用户更直观地探索和评估议论文的论证结构与连贯性。

Method: 将议论文建模为论证图，其中节点表示主张、附带证据属性，边表示支持或攻击关系；利用 LLM 对关系进行分类与打分，并提供解释与量化指标；同时提供自然语言工具以增强用户理解。

Result: 系统实现了对议论文论证结构的可视化呈现、关系分类的可解释性输出以及连贯性的定量评估，支持用户快速探索并保留人工监督。

Conclusion: IntelliProof 有效弥合了议论文结构语义与用户理解之间的鸿沟，提升了人机协同下对论证质量的分析效率与体验。

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [39] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: DeReC 是一种轻量级事实验证框架，利用通用文本嵌入结合稠密检索与专用分类器，在显著提升效率的同时超越了基于大语言模型（LLM）的方法，在 RAWFC 数据集上 F1 分数达 65.58%，运行时间减少约 95%。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的事实验证方法存在计算开销大和幻觉风险高的问题，难以在现实场景中部署，因此需要更高效且可靠的替代方案。

Method: 提出 DeReC 框架，使用通用文本嵌入进行稠密检索，并结合专门设计的分类器完成事实验证，避免依赖自回归式 LLM 生成解释。

Result: 在 RAWFC 数据集上，DeReC 的 F1 分数为 65.58%，优于当前最优方法 L-Defense（61.20%）；运行时间相比 LLM 方法减少 95%（RAWFC）和 92%（LIAR-RAW）。

Conclusion: 精心设计的基于检索的事实验证系统可在特定任务中媲美甚至超越大语言模型，同时具备更高的实际部署可行性。

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [40] [From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies](https://arxiv.org/abs/2511.03944)
*Tong Zhang,Vikram Sharma Mailthody,Fei Sun,Linsen Ma,Chris J. Newburn,Teresa Zhang,Yang Liu,Jiangpeng Li,Hao Zhong,Wen-Mei Hwu*

Main category: cs.AR

TL;DR: 本文重新审视了经典的“五分钟规则”，结合现代AI平台的硬件特性（如GPU主机与高性能SSD），提出一个综合考虑主机成本、DRAM带宽/容量及SSD性能的新框架，发现DRAM到闪存的缓存阈值已缩短至几秒，并引入MQSim-Next模拟器和两个案例研究，为AI时代的存储层次结构研究奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统五分钟规则仅基于存储经济学，忽略了主机成本、可行性限制和工作负载行为；随着AI平台和新型SSD的发展，有必要从第一性原理出发，构建更全面、可行的缓存决策框架。

Method: 整合主机成本、DRAM带宽/容量限制以及基于物理模型的SSD性能与成本分析，构建一个包含约束条件和工作负载感知的缓存决策与资源配置框架，并开发校准后的SSD模拟器MQSim-Next用于验证与敏感性分析。

Result: 研究表明，在现代AI平台中，DRAM到闪存的缓存阈值从分钟级降至秒级；NAND闪存可作为活跃数据层；并通过两个软件系统案例展示了新内存层次结构带来的设计空间。

Conclusion: 本文将经典的五分钟规则发展为一个可行、可操作的分析与配置框架，揭示了AI时代内存层次结构的新研究方向。

Abstract: In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a
simple, storage-memory-economics-based heuristic for deciding when data should
live in DRAM rather than on storage. Subsequent revisits to the rule largely
retained that economics-only view, leaving host costs, feasibility limits, and
workload behavior out of scope. This paper revisits the rule from first
principles, integrating host costs, DRAM bandwidth/capacity, and
physics-grounded models of SSD performance and cost, and then embedding these
elements in a constraint- and workload-aware framework that yields actionable
provisioning guidance. We show that, for modern AI platforms, especially
GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained
random access, the DRAM-to-flash caching threshold collapses from minutes to a
few seconds. This shift reframes NAND flash memory as an active data tier and
exposes a broad research space across the hardware-software stack. We further
introduce MQSim-Next, a calibrated SSD simulator that supports validation and
sensitivity analysis and facilitates future architectural and system research.
Finally, we present two concrete case studies that showcase the software system
design space opened by such memory hierarchy paradigm shift. Overall, we turn a
classical heuristic into an actionable, feasibility-aware analysis and
provisioning framework and set the stage for further research on AI-era memory
hierarchy.

</details>


### [41] [AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM](https://arxiv.org/abs/2511.04321)
*Yuanpeng Zhang,Xing Hu,Xi Chen,Zhihang Yuan,Cong Li,Jingchen Zhu,Zhao Wang,Chenguang Zhang,Xin Si,Wei Gao,Qiang Wu,Runsheng Wang,Guangyu Sun*

Main category: cs.AR

TL;DR: 本文提出AIM，一种软硬件协同设计的架构级IR-drop缓解方案，用于高性能SRAM存内计算（PIM）芯片。通过建立工作负载与IR-drop之间的关联，并结合软件优化与硬件动态调节机制，在保持计算精度的同时显著降低IR-drop，提升能效与性能。


<details>
  <summary>Details</summary>
Motivation: 高性能SRAM存内计算（PIM）因追求更高性能而采用复杂电路和高频操作，导致严重的IR-drop问题，影响芯片性能与可靠性；传统电路级缓解方法代价高且牺牲PPA（功耗、性能、面积），亟需更高效的架构级解决方案。

Method: 提出AIM框架，包括：1）利用PIM的位串行与原位数据流特性，引入Rtog和HR指标以关联工作负载与IR-drop；2）设计LHR和WDS进行架构级IR-drop缓解探索并保持计算精度；3）开发IR-Booster机制，融合软件HR信息与硬件IR监测，动态调整PIM宏的电压-频率对；4）提出HR感知的任务映射方法，实现软硬件协同优化。

Result: 在7nm工艺、256-TOPS的PIM芯片上进行后仿真实验，AIM最多可减少69.2%的IR-drop，带来2.29倍的能效提升和1.152倍的加速比。

Conclusion: AIM通过软硬件协同的架构级设计，有效缓解了高性能PIM中的IR-drop问题，在不牺牲计算精度的前提下显著提升了能效与性能，为未来高密度存算一体芯片提供了可行的优化路径。

Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising
implementation for high-performance PIM, delivering superior computing density,
energy efficiency, and computational precision. However, the pursuit of higher
performance necessitates more complex circuit designs and increased operating
frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly
degrade chip performance and even threaten reliability. Conventional
circuit-level IR-drop mitigation methods, such as back-end optimizations, are
resource-intensive and often compromise power, performance, and area (PPA). To
address these challenges, we propose AIM, comprehensive software and hardware
co-design for architecture-level IR-drop mitigation in high-performance PIM.
Initially, leveraging the bit-serial and in-situ dataflow processing properties
of PIM, we introduce Rtog and HR, which establish a direct correlation between
PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,
enabling extensive exploration of architecture-level IR-drop mitigation while
maintaining computational accuracy through software optimization. Subsequently,
we develop IR-Booster, a dynamic adjustment mechanism that integrates
software-level HR information with hardware-based IR-drop monitoring to adapt
the V-f pairs of the PIM macro, achieving enhanced energy efficiency and
performance. Finally, we propose the HR-aware task mapping method, bridging
software and hardware designs to achieve optimal improvement. Post-layout
simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up
to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement
and 1.152x speedup.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Laugh, Relate, Engage: Stylized Comment Generation for Short Videos](https://arxiv.org/abs/2511.03757)
*Xuan Ouyang,Senan Wang,Bouzhou Wang,Siyuan Xiahou,Jinrong Zhou,Yuekang Li*

Main category: cs.LG

TL;DR: 本文提出LOLOGORITHM，一种模块化多智能体系统，用于在短视频平台上生成风格多样、上下文感知且符合平台规范的评论。该系统结合视频分割、情感分析与风格提示构建，支持六种评论风格，并通过大规模人工评估验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以生成既符合平台规范又具备风格多样性与上下文感知能力的短视频评论，限制了用户互动和内容再创作的潜力。

Method: LOLOGORITHM是一个基于多模态大语言模型（MLLM）的模块化多智能体系统，直接处理视频输入，通过显式提示标记和少量示例实现细粒度风格控制；同时构建了涵盖中英文的短视频评论数据集用于训练与评估。

Result: 在Douyin和YouTube上的实验表明，LOLOGORITHM在原创性、相关性和风格一致性方面显著优于基线模型，人工偏好研究显示其在Douyin上偏好率达90%以上，在YouTube上达87.55%。

Conclusion: LOLOGORITHM提供了一个可扩展且文化自适应的框架，有效提升短视频平台上的用户参与度与创意互动水平。

Abstract: Short-video platforms have become a central medium in the modern Internet
landscape, where efficient information delivery and strong interactivity are
reshaping user engagement and cultural dissemination. Among the various forms
of user interaction, comments play a vital role in fostering community
participation and enabling content re-creation. However, generating comments
that are both compliant with platform guidelines and capable of exhibiting
stylistic diversity and contextual awareness remains a significant challenge.
We introduce LOLGORITHM, a modular multi-agent system (MAS) designed for
controllable short-video comment generation. The system integrates video
segmentation, contextual and affective analysis, and style-aware prompt
construction. It supports six distinct comment styles: puns (homophones),
rhyming, meme application, sarcasm (irony), plain humor, and content
extraction. Powered by a multimodal large language model (MLLM), LOLGORITHM
directly processes video inputs and achieves fine-grained style control through
explicit prompt markers and few-shot examples. To support development and
evaluation, we construct a bilingual dataset using official APIs from Douyin
(Chinese) and YouTube (English), covering five popular video genres: comedy
skits, daily life jokes, funny animal clips, humorous commentary, and talk
shows. Evaluation combines automated metrics originality, relevance, and style
conformity with a large-scale human preference study involving 40 videos and
105 participants. Results show that LOLGORITHM significantly outperforms
baseline models, achieving preference rates of over 90% on Douyin and 87.55% on
YouTube. This work presents a scalable and culturally adaptive framework for
stylized comment generation on short-video platforms, offering a promising path
to enhance user engagement and creative interaction.

</details>


### [43] [FusionDP: Foundation Model-Assisted Differentially Private Learning for Partially Sensitive Features](https://arxiv.org/abs/2511.03806)
*Linghui Zeng,Ruixuan Liu,Atiquer Rahman Sarkar,Xiaoqian Jiang,Joyce C. Ho,Li Xiong*

Main category: cs.LG

TL;DR: FusionDP 是一种两阶段框架，通过利用大模型对敏感特征进行插补，并结合改进的 DP-SGD 算法，在保护特定特征隐私的同时显著提升模型效用。


<details>
  <summary>Details</summary>
Motivation: 传统 DP-SGD 对样本中所有特征统一施加隐私保护，导致噪声过多、模型效用下降；而实际场景中往往只需对部分高风险特征（如人口统计信息）进行隐私保护。

Method: FusionDP 首先利用大基础模型根据非敏感特征插补敏感特征，作为训练时的外部先验；然后采用改进的 DP-SGD 算法，在原始与插补特征上联合训练模型，同时严格保证原始敏感特征的差分隐私。

Result: 在 PhysioNet 的脓毒症预测任务和 MIMIC-III 的临床笔记分类任务上，FusionDP 相较于现有隐私保护方法显著提升了模型性能，同时维持了严格的特征级隐私保障。

Conclusion: FusionDP 有效缓解了特征级差分隐私下的隐私-效用权衡问题，展示了大模型驱动插补在多模态医疗数据隐私保护中的潜力。

Abstract: Ensuring the privacy of sensitive training data is crucial in
privacy-preserving machine learning. However, in practical scenarios, privacy
protection may be required for only a subset of features. For instance, in ICU
data, demographic attributes like age and gender pose higher privacy risks due
to their re-identification potential, whereas raw lab results are generally
less sensitive. Traditional DP-SGD enforces privacy protection on all features
in one sample, leading to excessive noise injection and significant utility
degradation. We propose FusionDP, a two-step framework that enhances model
utility under feature-level differential privacy. First, FusionDP leverages
large foundation models to impute sensitive features given non-sensitive
features, treating them as external priors that provide high-quality estimates
of sensitive attributes without accessing the true values during model
training. Second, we introduce a modified DP-SGD algorithm that trains models
on both original and imputed features while formally preserving the privacy of
the original sensitive features. We evaluate FusionDP on two modalities: a
sepsis prediction task on tabular data from PhysioNet and a clinical note
classification task from MIMIC-III. By comparing against privacy-preserving
baselines, our results show that FusionDP significantly improves model
performance while maintaining rigorous feature-level privacy, demonstrating the
potential of foundation model-driven imputation to enhance the privacy-utility
trade-off for various modalities.

</details>


### [44] [Optimizing Reasoning Efficiency through Prompt Difficulty Prediction](https://arxiv.org/abs/2511.03808)
*Bo Zhao,Berkcan Kapusuzoglu,Kartik Balasubramaniam,Sambit Sahu,Supriyo Chakraborty,Genta Indra Winata*

Main category: cs.LG

TL;DR: 提出一种基于问题难度的路由方法，将任务分配给能解决它的最小模型，在保持准确率的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 推理语言模型虽在复杂任务上表现优异，但因模型体积大、推理过程长而部署成本高，亟需更高效的部署策略。

Method: 利用s1.1-32B模型的中间表示训练轻量级预测器，用于估计问题难度或模型正确性，并据此在多个推理模型间进行任务路由。

Result: 在多个数学基准测试中，该路由方法相比随机分配显著提升效率，在大幅减少计算资源的同时达到与s1.1-32B相当的性能。

Conclusion: 基于难度感知的路由策略能有效实现推理模型的低成本高效部署。

Abstract: Reasoning language models perform well on complex tasks but are costly to
deploy due to their size and long reasoning traces. We propose a routing
approach that assigns each problem to the smallest model likely to solve it,
reducing compute without sacrificing accuracy. Using intermediate
representations from s1.1-32B, we train lightweight predictors of problem
difficulty or model correctness to guide routing across a pool of reasoning
models. On diverse math benchmarks, routing improves efficiency over random
assignment and matches s1.1-32B's performance while using significantly less
compute. Our results demonstrate that difficulty-aware routing is effective for
cost-efficient deployment of reasoning models.

</details>


### [45] [From Static to Dynamic: Enhancing Offline-to-Online Reinforcement Learning via Energy-Guided Diffusion Stratification](https://arxiv.org/abs/2511.03828)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为StratDiff的新方法，通过结合扩散模型与能量函数，对离线数据进行分层处理，从而在离线到在线强化学习的过渡中实现更平稳、稳定的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法很少显式评估或利用离线数据本身的分布结构，导致难以根据样本类型调整学习策略。为解决离线到在线强化学习过程中因策略分布偏移带来的挑战，作者提出新方法以更好利用离线数据先验。

Method: StratDiff使用扩散模型从离线数据集中学习先验知识，并通过基于能量的函数对其进行优化，以生成类似离线的动作；然后计算生成动作与采样动作之间的KL散度，据此将训练批次划分为“类离线”和“类在线”子集，并分别采用离线目标和在线策略进行更新。

Result: 在D4RL基准上的大量实验表明，将StratDiff与Cal-QL和IQL等现有方法结合后，其性能显著优于当前方法，在多种强化学习设置下展现出更强的适应性和稳定性。

Conclusion: StratDiff通过有效利用离线数据的分布结构并动态调整学习策略，成功缓解了离线到在线强化学习中的分布偏移问题，为该领域提供了一种高效且通用的解决方案。

Abstract: Transitioning from offline to online reinforcement learning (RL) poses
critical challenges due to distributional shifts between the fixed behavior
policy in the offline dataset and the evolving policy during online learning.
Although this issue is widely recognized, few methods attempt to explicitly
assess or utilize the distributional structure of the offline data itself,
leaving a research gap in adapting learning strategies to different types of
samples. To address this challenge, we propose an innovative method,
Energy-Guided Diffusion Stratification (StratDiff), which facilitates smoother
transitions in offline-to-online RL. StratDiff deploys a diffusion model to
learn prior knowledge from the offline dataset. It then refines this knowledge
through energy-based functions to improve policy imitation and generate
offline-like actions during online fine-tuning. The KL divergence between the
generated action and the corresponding sampled action is computed for each
sample and used to stratify the training batch into offline-like and
online-like subsets. Offline-like samples are updated using offline objectives,
while online-like samples follow online learning strategies. We demonstrate the
effectiveness of StratDiff by integrating it with off-the-shelf methods Cal-QL
and IQL. Extensive empirical evaluations on D4RL benchmarks show that StratDiff
significantly outperforms existing methods, achieving enhanced adaptability and
more stable performance across diverse RL settings.

</details>


### [46] [Higher-Order Causal Structure Learning with Additive Models](https://arxiv.org/abs/2511.03831)
*James Enouen,Yujia Zheng,Ignavier Ng,Yan Liu,Kun Zhang*

Main category: cs.LG

TL;DR: 本文将因果加性模型（CAM）扩展到包含高阶交互的加性模型，引入有向无环超图（hyper DAG）来表示此类结构，提供了可识别性理论，并开发了适用于该结构的贪婪学习算法，在合成实验中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法大多忽略变量间的高阶交互机制，而现实世界中的许多过程具有此类特性，因此需要一种能显式建模高阶交互的因果结构学习方法。

Method: 作者将传统CAM推广至包含高阶交互的加性模型，采用有向无环超图作为结构表示，提出相应的理论定义与可识别性结果，并设计了一种扩展的贪婪CAM算法用于搜索超图结构。

Result: 在合成数据实验中，所提出的算法能够有效学习高阶交互结构，并且由于模型假设更严格，反而在有限样本下表现出更好的性能。

Conclusion: 将高阶交互纳入因果结构学习不仅理论上可行，而且在实践中可能提升学习效果，为因果发现提供了更灵活和强大的建模框架。

Abstract: Causal structure learning has long been the central task of inferring causal
insights from data. Despite the abundance of real-world processes exhibiting
higher-order mechanisms, however, an explicit treatment of interactions in
causal discovery has received little attention. In this work, we focus on
extending the causal additive model (CAM) to additive models with higher-order
interactions. This second level of modularity we introduce to the structure
learning problem is most easily represented by a directed acyclic hypergraph
which extends the DAG. We introduce the necessary definitions and theoretical
tools to handle the novel structure we introduce and then provide
identifiability results for the hyper DAG, extending the typical Markov
equivalence classes. We next provide insights into why learning the more
complex hypergraph structure may actually lead to better empirical results. In
particular, more restrictive assumptions like CAM correspond to easier-to-learn
hyper DAGs and better finite sample complexity. We finally develop an extension
of the greedy CAM algorithm which can handle the more complex hyper DAG search
space and demonstrate its empirical usefulness in synthetic experiments.

</details>


### [47] [Enhancing Q-Value Updates in Deep Q-Learning via Successor-State Prediction](https://arxiv.org/abs/2511.03836)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为SADQ的新方法，通过建模环境动态并整合后继状态分布来改进DQN的学习稳定性与效率。


<details>
  <summary>Details</summary>
Motivation: DQN在目标更新时依赖于由过去可能次优策略生成的下一状态，导致学习信号信息量不足且更新方差高，尤其当采样转移与当前策略不一致时问题更严重。

Method: 提出Successor-state Aggregation Deep Q-Network（SADQ），利用随机转移模型显式建模环境动态，将后继状态分布整合进Q值估计，并设计更高效的行动选择策略。

Result: 在标准强化学习基准和真实世界的向量控制任务中，SADQ在稳定性和学习效率方面均优于多种DQN变体。

Conclusion: SADQ通过引入后继状态分布和环境动态建模，在保持无偏Q值估计的同时有效降低了训练方差，提升了DQN的性能。

Abstract: Deep Q-Networks (DQNs) estimate future returns by learning from transitions
sampled from a replay buffer. However, the target updates in DQN often rely on
next states generated by actions from past, potentially suboptimal, policy. As
a result, these states may not provide informative learning signals, causing
high variance into the update process. This issue is exacerbated when the
sampled transitions are poorly aligned with the agent's current policy. To
address this limitation, we propose the Successor-state Aggregation Deep
Q-Network (SADQ), which explicitly models environment dynamics using a
stochastic transition model. SADQ integrates successor-state distributions into
the Q-value estimation process, enabling more stable and policy-aligned value
updates. Additionally, it explores a more efficient action selection strategy
with the modeled transition structure. We provide theoretical guarantees that
SADQ maintains unbiased value estimates while reducing training variance. Our
extensive empirical results across standard RL benchmarks and real-world
vector-based control tasks demonstrate that SADQ consistently outperforms DQN
variants in both stability and learning efficiency.

</details>


### [48] [Benchmark Datasets for Lead-Lag Forecasting on Social Platforms](https://arxiv.org/abs/2511.03877)
*Kimia Kazemian,Zhenzhen Liu,Yangfanyu Yang,Katie Z Luo,Shuhan Gu,Audrey Du,Xinyu Yang,Jack Jansons,Kilian Q Weinberger,John Thickstun,Yian Yin,Sarah Dean*

Main category: cs.LG

TL;DR: 该论文提出了“领先-滞后预测”（Lead-Lag Forecasting, LLF）这一新的时间序列预测范式，用于根据早期用户行为（如浏览、点赞）预测后续高影响力事件（如引用、购买）。为推动该方向研究，作者发布了两个大规模基准数据集（arXiv 和 GitHub），并验证了其中存在的领先-滞后关系，同时提供了基线模型和完整数据文档。


<details>
  <summary>Details</summary>
Motivation: 尽管社交与协作平台中普遍存在早期互动与后期高影响力事件之间的时序关联，但时间序列社区尚未将此类问题统一为一个独立的预测任务，主要原因是缺乏标准化数据集。因此，作者旨在通过构建高质量基准数据集，正式定义并推动LLF研究。

Method: 作者构建并公开了两个大规模数据集（arXiv 论文访问→引用，GitHub 仓库推送/星标→分叉），详细记录了数据清洗与处理流程；通过统计检验和分类实验验证领先-滞后动态的存在；并评估了参数化与非参数化回归基线模型在该任务上的表现。

Result: 数据集成功捕捉了跨年度的长期领先-滞后动态，覆盖结果的全谱系且无幸存者偏差；实验证明早期信号可有效预测滞后结果，验证了LLF作为可行预测任务的合理性。

Conclusion: 该研究确立了领先-滞后预测（LLF）作为一种新颖的时间序列预测范式，并为未来在社交和使用数据中的系统性探索奠定了实证基础。

Abstract: Social and collaborative platforms emit multivariate time-series traces in
which early interactions-such as views, likes, or downloads-are followed,
sometimes months or years later, by higher impact like citations, sales, or
reviews. We formalize this setting as Lead-Lag Forecasting (LLF): given an
early usage channel (the lead), predict a correlated but temporally shifted
outcome channel (the lag). Despite the ubiquity of such patterns, LLF has not
been treated as a unified forecasting problem within the time-series community,
largely due to the absence of standardized datasets. To anchor research in LLF,
here we present two high-volume benchmark datasets-arXiv (accesses -> citations
of 2.3M papers) and GitHub (pushes/stars -> forks of 3M repositories)-and
outline additional domains with analogous lead-lag dynamics, including
Wikipedia (page views -> edits), Spotify (streams -> concert attendance),
e-commerce (click-throughs -> purchases), and LinkedIn profile (views ->
messages). Our datasets provide ideal testbeds for lead-lag forecasting, by
capturing long-horizon dynamics across years, spanning the full spectrum of
outcomes, and avoiding survivorship bias in sampling. We documented all
technical details of data curation and cleaning, verified the presence of
lead-lag dynamics through statistical and classification tests, and benchmarked
parametric and non-parametric baselines for regression. Our study establishes
LLF as a novel forecasting paradigm and lays an empirical foundation for its
systematic exploration in social and usage data. Our data portal with downloads
and documentation is available at https://lead-lag-forecasting.github.io/.

</details>


### [49] [DecoHD: Decomposed Hyperdimensional Classification under Extreme Memory Budgets](https://arxiv.org/abs/2511.03911)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: 本文提出DecoHD，一种用于超维计算（HDC）的新型分解方法，通过在类轴上压缩并保留原生HDC操作，在显著减少内存占用和参数量的同时保持高准确率和鲁棒性，并在硬件上实现显著能效与速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统HDC模型在压缩时通常沿特征轴缩减，导致表示集中性和鲁棒性下降；现有分解方法使用固定原子超向量，难以有效压缩学习得到的类别原型。

Method: DecoHD采用可学习的分解式HDC参数化方式：每层共享少量通道，通过跨层乘法绑定并在末端进行捆绑，构建紧凑但表达能力强的表示空间；同时引入轻量级捆绑头沿类别轴压缩，支持端到端训练且推理阶段保持纯HDC流程。

Result: DecoHD在极低部署预算下仅带来轻微准确率下降（平均差距0.1–0.15%，最差5.7%），对随机位翻转噪声更鲁棒，训练参数最多减少约97%，并在硬件上相比CPU、GPU和基准HDC ASIC分别实现高达277x/35x、13.5x/3.7x和2.0x/2.4x的能效/速度增益。

Conclusion: DecoHD是一种高效、鲁棒且硬件友好的HDC压缩方法，能够在大幅降低资源消耗的同时维持高性能，适用于边缘和近存计算场景。

Abstract: Decomposition is a proven way to shrink deep networks without changing I/O.
We bring this idea to hyperdimensional computing (HDC), where footprint cuts
usually shrink the feature axis and erode concentration and robustness. Prior
HDC decompositions decode via fixed atomic hypervectors, which are ill-suited
for compressing learned class prototypes. We introduce DecoHD, which learns
directly in a decomposed HDC parameterization: a small, shared set of per-layer
channels with multiplicative binding across layers and bundling at the end,
yielding a large representational space from compact factors. DecoHD compresses
along the class axis via a lightweight bundling head while preserving native
bind-bundle-score; training is end-to-end, and inference remains pure HDC,
aligning with in/near-memory accelerators. In evaluation, DecoHD attains
extreme memory savings with only minor accuracy degradation under tight
deployment budgets. On average it stays within about 0.1-0.15% of a strong
non-reduced HDC baseline (worst case 5.7%), is more robust to random bit-flip
noise, reaches its accuracy plateau with up to ~97% fewer trainable parameters,
and -- in hardware -- delivers roughly 277x/35x energy/speed gains over a CPU
(AMD Ryzen 9 9950X), 13.5x/3.7x over a GPU (NVIDIA RTX 4090), and 2.0x/2.4x
over a baseline HDC ASIC.

</details>


### [50] [On Predicting Sociodemographics from Mobility Signals](https://arxiv.org/abs/2511.03924)
*Ekin Uğurel,Cynthia Chen,Brian H. Y. Lee,Filipe Rodrigues*

Main category: cs.LG

TL;DR: 该论文通过引入基于有向移动图的高阶移动特征、开发衡量模型置信度与准确率一致性的诊断工具，以及构建多任务学习框架，提升了从移动数据中推断社会人口属性的准确性、可解释性、不确定性量化能力和跨情境泛化性能。


<details>
  <summary>Details</summary>
Motivation: 由于移动模式与社会人口特征之间的关联较弱且不一致，且模型在不同情境下泛化能力有限，从移动数据中推断社会人口属性仍具挑战性。

Method: 1）提出基于有向移动图的行为驱动高阶移动描述符，捕捉出行序列、交通方式和共行关系；2）设计衡量模型置信度与准确率均衡性的指标与可视化诊断工具；3）构建多任务学习框架，共享表征联合预测多个社会人口属性。

Result: 所提方法在预测年龄、性别、收入和家庭结构方面显著优于基线特征；多任务框架在训练数据有限或跨时间段应用时表现优于单任务模型。

Conclusion: 结合行为理论的高阶移动特征、不确定性量化工具和多任务学习能有效提升社会人口属性推断的性能与实用性，为交通规划者提供更可靠的数据支持。

Abstract: Inferring sociodemographic attributes from mobility data could help
transportation planners better leverage passively collected datasets, but this
task remains difficult due to weak and inconsistent relationships between
mobility patterns and sociodemographic traits, as well as limited
generalization across contexts. We address these challenges from three angles.
First, to improve predictive accuracy while retaining interpretability, we
introduce a behaviorally grounded set of higher-order mobility descriptors
based on directed mobility graphs. These features capture structured patterns
in trip sequences, travel modes, and social co-travel, and significantly
improve prediction of age, gender, income, and household structure over
baselines features. Second, we introduce metrics and visual diagnostic tools
that encourage evenness between model confidence and accuracy, enabling
planners to quantify uncertainty. Third, to improve generalization and sample
efficiency, we develop a multitask learning framework that jointly predicts
multiple sociodemographic attributes from a shared representation. This
approach outperforms single-task models, particularly when training data are
limited or when applying models across different time periods (i.e., when the
test set distribution differs from the training set).

</details>


### [51] [NVIDIA Nemotron Nano V2 VL](https://arxiv.org/abs/2511.03929)
*NVIDIA,:,Amala Sanjay Deshmukh,Kateryna Chumachenko,Tuomas Rintamaki,Matthieu Le,Tyler Poon,Danial Mohseni Taheri,Ilia Karmanov,Guilin Liu,Jarno Seppanen,Guo Chen,Karan Sapra,Zhiding Yu,Adi Renduchintala,Charles Wang,Peter Jin,Arushi Goel,Mike Ranzinger,Lukas Voegtle,Philipp Fischer,Timo Roman,Wei Ping,Boxin Wang,Zhuolin Yang,Nayeon Lee,Shaokun Zhang,Fuxiao Liu,Zhiqi Li,Di Zhang,Greg Heinrich,Hongxu,Yin,Song Han,Pavlo Molchanov,Parth Mannan,Yao Xu,Jane Polak Scowcroft,Tom Balough,Subhashree Radhakrishnan,Paris Zhang,Sean Cha,Ratnesh Kumar,Zaid Pervaiz Bhat,Jian Zhang,Darragh Hanley,Pritam Biswas,Jesse Oliver,Kevin Vasques,Roger Waleffe,Duncan Riach,Oluwatobi Olabiyi,Ameya Sunil Mahabaleshwarkar,Bilal Kartal,Pritam Gundecha,Khanh Nguyen,Alexandre Milesi,Eugene Khvedchenia,Ran Zilberstein,Ofri Masad,Natan Bagrov,Nave Assaf,Tomer Asida,Daniel Afrimi,Amit Zuker,Netanel Haber,Zhiyu Cheng,Jingyu,Xin,Di,Wu,Nik Spirin,Maryam Moosaei,Roman Ageev,Vanshil Atul Shah,Yuting Wu,Daniel Korzekwa,Unnikrishnan Kizhakkemadam Sreekumar,Wanli Jiang,Padmavathy Subramanian,Alejandra Rico,Sandip Bhaskar,Saeid Motiian,Kedi Wu,Annie Surla,Chia-Chih Chen,Hayden Wolff,Matthew Feinberg,Melissa Corpuz,Marek Wawrzos,Eileen Long,Aastha Jhunjhunwala,Paul Hendricks,Farzan Memarian,Benika Hall,Xin-Yu Wang,David Mosallanezhad,Soumye Singhal,Luis Vega,Katherine Cheung,Krzysztof Pawelec,Michael Evans,Katherine Luna,Jie Lou,Erick Galinkin,Akshay Hazare,Kaustubh Purandare,Ann Guan,Anna Warno,Chen Cui,Yoshi Suhara,Shibani Likhite,Seph Mard,Meredith Price,Laya Sleiman,Saori Kaji,Udi Karpas,Kari Briski,Joey Conway,Michael Lightstone,Jan Kautz,Mohammad Shoeybi,Mostofa Patwary,Jonathen Cohen,Oleksii Kuchaiev,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 本文介绍了Nemotron Nano V2 VL，这是Nemotron视觉语言系列的最新模型，在文档理解、长视频理解和推理任务方面表现优异。相比前代模型，它在架构、数据集和训练方法上均有显著改进，并采用混合Mamba-Transformer结构与创新的token压缩技术以提升长序列推理效率。作者开源了多种精度的模型权重及部分数据、训练代码和配方。


<details>
  <summary>Details</summary>
Motivation: 为提升模型在真实世界文档理解、长视频理解及复杂推理任务中的性能，需在模型架构、训练数据和训练策略上进行系统性优化。

Method: 基于Nemotron Nano V2（一种混合Mamba-Transformer大语言模型），引入创新的token压缩技术，并结合优化的数据集和训练方案进行训练。

Result: Nemotron Nano V2 VL在所有视觉与文本任务上均显著优于前代模型Llama-3.1-Nemotron-Nano-VL-8B，尤其在长文档和长视频场景中实现了更高的推理吞吐量。

Conclusion: Nemotron Nano V2 VL通过架构创新与高效训练策略，有效提升了多模态理解与推理能力，且通过开源促进社区发展。

Abstract: We introduce Nemotron Nano V2 VL, the latest model of the Nemotron
vision-language series designed for strong real-world document understanding,
long video comprehension, and reasoning tasks. Nemotron Nano V2 VL delivers
significant improvements over our previous model,
Llama-3.1-Nemotron-Nano-VL-8B, across all vision and text domains through major
enhancements in model architecture, datasets, and training recipes. Nemotron
Nano V2 VL builds on Nemotron Nano V2, a hybrid Mamba-Transformer LLM, and
innovative token reduction techniques to achieve higher inference throughput in
long document and video scenarios. We are releasing model checkpoints in BF16,
FP8, and FP4 formats and sharing large parts of our datasets, recipes and
training code.

</details>


### [52] [LogHD: Robust Compression of Hyperdimensional Classifiers via Logarithmic Class-Axis Reduction](https://arxiv.org/abs/2511.03938)
*Sanggeon Yun,Hyunwoo Oh,Ryozo Masukawa,Pietro Mercati,Nathaniel D. Bastian,Mohsen Imani*

Main category: cs.LG

TL;DR: 本文提出LogHD，一种在超维计算中通过类轴对数压缩减少内存占用的方法，在保持维度D不变的同时将内存需求从O(CD)降至O(D log_k C)，并展现出更高的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 标准的“每类一个原型”超维计算设计需要O(CD)内存，限制了其在资源受限系统中的应用；现有压缩方法虽减少维度D但牺牲了鲁棒性。

Method: LogHD用约⌈log_k C⌉个捆绑超向量替代C个每类原型，采用容量感知码本和基于轮廓的解码机制，并可与特征轴稀疏化结合。

Result: 在多个数据集和位翻转噪声下，LogHD在更小模型和相同内存下实现更高准确率和更强鲁棒性；ASIC实现相较CPU/GPU及特征轴压缩基线显著提升能效与速度。

Conclusion: LogHD通过类轴对数压缩有效降低内存开销，同时保持甚至提升模型性能与可靠性，适用于资源受限场景。

Abstract: Hyperdimensional computing (HDC) suits memory, energy, and
reliability-constrained systems, yet the standard "one prototype per class"
design requires $O(CD)$ memory (with $C$ classes and dimensionality $D$). Prior
compaction reduces $D$ (feature axis), improving storage/compute but weakening
robustness. We introduce LogHD, a logarithmic class-axis reduction that
replaces the $C$ per-class prototypes with $n\!\approx\!\lceil\log_k C\rceil$
bundle hypervectors (alphabet size $k$) and decodes in an $n$-dimensional
activation space, cutting memory to $O(D\log_k C)$ while preserving $D$. LogHD
uses a capacity-aware codebook and profile-based decoding, and composes with
feature-axis sparsification. Across datasets and injected bit flips, LogHD
attains competitive accuracy with smaller models and higher resilience at
matched memory. Under equal memory, it sustains target accuracy at roughly
$2.5$-$3.0\times$ higher bit-flip rates than feature-axis compression; an ASIC
instantiation delivers $498\times$ energy efficiency and $62.6\times$ speedup
over an AMD Ryzen 9 9950X and $24.3\times$/$6.58\times$ over an NVIDIA RTX
4090, and is $4.06\times$ more energy-efficient and $2.19\times$ faster than a
feature-axis HDC ASIC baseline.

</details>


### [53] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: 本文综述了大语言模型对齐研究的新前沿，涵盖多模态对齐、文化公平性和低延迟优化等方向，并系统比较了PPO、DPO和GRPO等基础与最新算法，为构建更鲁棒、高效和公平的AI系统提供路线图。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本的强化学习人类反馈（RLHF）方法已难以满足当前大语言模型对齐的复杂需求，亟需探索多模态、文化公平性及低延迟等新维度。

Method: 通过综述和比较分析现有对齐算法（如PPO、DPO、GRPO），系统梳理多模态对齐、文化公平性和低延迟优化等领域的最新进展。

Result: 明确了当前对齐技术在多模态、文化多样性和效率方面的关键进展与局限，识别出若干开放性挑战。

Conclusion: 该综述为未来构建更鲁棒、高效且公平的人工智能系统提供了重要的研究方向和技术路线图。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [54] [Conditional Score Learning for Quickest Change Detection in Markov Transition Kernels](https://arxiv.org/abs/2511.03953)
*Wuxia Chen,Taposh Banerjee,Vahid Tarokh*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件分数的CUSUM方法，用于在高维马尔可夫过程中实现无需显式似然计算的快速变点检测。


<details>
  <summary>Details</summary>
Motivation: 在马尔可夫过程的转移核未知且数据高维的情况下，传统基于似然的变点检测方法难以应用，因此需要一种不依赖显式似然评估的实用检测方法。

Method: 通过从样本对 $(\mathbf{x}, \mathbf{y})$ 直接学习条件分数 $\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$，构建基于条件Hyvärinen分数差的CUSUM统计量，并引入截断机制以保证增量有界；利用Hoeffding不等式分析其虚警时间和检测延迟。

Result: 证明了该方法在一致遍历马尔可夫过程下具有指数级的平均虚警时间下界和渐近的检测延迟上界。

Conclusion: 所提出的基于分数的检测方法在理论上具有性能保证，在实践中适用于高维马尔可夫模型中的快速变点检测。

Abstract: We address the problem of quickest change detection in Markov processes with
unknown transition kernels. The key idea is to learn the conditional score
$\nabla_{\mathbf{y}} \log p(\mathbf{y}|\mathbf{x})$ directly from sample pairs
$( \mathbf{x},\mathbf{y})$, where both $\mathbf{x}$ and $\mathbf{y}$ are
high-dimensional data generated by the same transition kernel. In this way, we
avoid explicit likelihood evaluation and provide a practical way to learn the
transition dynamics. Based on this estimation, we develop a score-based CUSUM
procedure that uses conditional Hyvarinen score differences to detect changes
in the kernel. To ensure bounded increments, we propose a truncated version of
the statistic. With Hoeffding's inequality for uniformly ergodic Markov
processes, we prove exponential lower bounds on the mean time to false alarm.
We also prove asymptotic upper bounds on detection delay. These results give
both theoretical guarantees and practical feasibility for score-based detection
in high-dimensional Markov models.

</details>


### [55] [PrivacyCD: Hierarchical Unlearning for Protecting Student Privacy in Cognitive Diagnosis](https://arxiv.org/abs/2511.03966)
*Mingliang Hou,Yinuo Wang,Teng Guo,Zitao Liu,Wenzhou Dou,Jiaqi Zheng,Renqiang Luo,Mi Tian,Weiqi Luo*

Main category: cs.LG

TL;DR: 本文首次系统研究认知诊断（CD）模型中的数据遗忘问题，提出了一种名为分层重要性引导遗忘（HIF）的新算法，通过结合个体与层级参数重要性，有效实现了对特定学生数据的精准移除，在保持模型性能的同时满足用户“被遗忘权”的需求。


<details>
  <summary>Details</summary>
Motivation: 随着用户对“被遗忘权”的重视，从认知诊断模型中删除特定学生数据成为迫切需求，但现有CD模型缺乏隐私保护机制和有效的数据遗忘方法；通用遗忘算法在CD模型异构结构下面临完整性、效用与效率难以兼顾的问题。

Method: 提出分层重要性引导遗忘（HIF）算法，利用CD模型中参数重要性具有分层特性的洞察，设计一种融合个体与层级重要性的平滑机制，以更精确识别并移除与目标数据相关的参数。

Result: 在三个真实数据集上的实验表明，HIF在关键指标上显著优于基线方法，能够高效完成数据遗忘任务，同时较好地保留模型性能。

Conclusion: HIF为认知诊断模型提供了首个有效的数据遗忘解决方案，有助于构建高性能且隐私保护的AI系统，响应用户的数据删除请求。

Abstract: The need to remove specific student data from cognitive diagnosis (CD) models
has become a pressing requirement, driven by users' growing assertion of their
"right to be forgotten". However, existing CD models are largely designed
without privacy considerations and lack effective data unlearning mechanisms.
Directly applying general purpose unlearning algorithms is suboptimal, as they
struggle to balance unlearning completeness, model utility, and efficiency when
confronted with the unique heterogeneous structure of CD models. To address
this, our paper presents the first systematic study of the data unlearning
problem for CD models, proposing a novel and efficient algorithm: hierarchical
importanceguided forgetting (HIF). Our key insight is that parameter importance
in CD models exhibits distinct layer wise characteristics. HIF leverages this
via an innovative smoothing mechanism that combines individual and layer, level
importance, enabling a more precise distinction of parameters associated with
the data to be unlearned. Experiments on three real world datasets show that
HIF significantly outperforms baselines on key metrics, offering the first
effective solution for CD models to respond to user data removal requests and
for deploying high-performance, privacy preserving AI systems

</details>


### [56] [PETRA: Pretrained Evolutionary Transformer for SARS-CoV-2 Mutation Prediction](https://arxiv.org/abs/2511.03976)
*Xu Zou*

Main category: cs.LG

TL;DR: 本文提出了PETRA，一种基于系统发育树进化轨迹而非原始RNA序列的新型Transformer模型，用于预测SARS-CoV-2未来的突变，在核苷酸和刺突蛋白氨基酸突变预测上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: SARS-CoV-2不断出现免疫逃逸变异株，对公共卫生和疫苗研发构成持续挑战；而现有大型生成式预训练模型在处理含噪声的病毒基因组序列时效果有限。

Method: 提出PETRA模型，利用从系统发育树中提取的进化轨迹作为输入，并采用加权训练框架以应对全球序列数据在地理和时间上的严重不平衡。

Result: PETRA在核苷酸突变和刺突氨基酸突变预测上的加权recall@1分别达到9.45%和17.10%，远超最佳基线（分别为0.49%和6.64%），并能有效支持对24F(XEC)和25A(LP.8.1)等主要分支的实时突变预测。

Conclusion: PETRA通过建模病毒进化轨迹而非原始序列，有效降低了测序噪声影响，显著提升了对未来SARS-CoV-2突变的预测能力，为病毒监测和疫苗设计提供了新工具。

Abstract: Since its emergence, SARS-CoV-2 has demonstrated a rapid and unpredictable
evolutionary trajectory, characterized by the continual emergence of
immune-evasive variants. This poses persistent challenges to public health and
vaccine development.
  While large-scale generative pre-trained transformers (GPTs) have
revolutionized the modeling of sequential data, their direct applications to
noisy viral genomic sequences are limited. In this paper, we introduce
PETRA(Pretrained Evolutionary TRAnsformer), a novel transformer approach based
on evolutionary trajectories derived from phylogenetic trees rather than raw
RNA sequences. This method effectively mitigates sequencing noise and captures
the hierarchical structure of viral evolution.
  With a weighted training framework to address substantial geographical and
temporal imbalances in global sequence data, PETRA excels in predicting future
SARS-CoV-2 mutations, achieving a weighted recall@1 of 9.45% for nucleotide
mutations and 17.10\% for spike amino-acid mutations, compared to 0.49% and
6.64% respectively for the best baseline. PETRA also demonstrates its ability
to aid in the real-time mutation prediction of major clades like 24F(XEC) and
25A(LP.8.1). The code is open sourced on https://github.com/xz-keg/PETra

</details>


### [57] [Structural Priors and Modular Adapters in the Composable Fine-Tuning Algorithm of Large-Scale Models](https://arxiv.org/abs/2511.03981)
*Yuxiao Wang,Di Wu,Feng Liu,Zhimin Qiu,Chenrui Hu*

Main category: cs.LG

TL;DR: 本文提出一种可组合微调方法，通过将图结构先验与模块化适配器结合，解决大规模预训练模型在多任务适应中面临的高计算成本和结构不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型在多任务微调中存在计算开销大、结构不稳定、路径冲突和冗余计算等问题，亟需一种兼顾效率与稳定性的新方法。

Method: 引入关系矩阵建模任务间依赖，将节点与路径相关性编码为图结构先验，用于指导模块化适配器的权重分配与路径选择；通过低秩映射和可插拔机制将适配器嵌入不同层，实现先验引导下的跨任务高效组合与复用。

Result: 实验表明该方法显著提升了任务预测准确率、适配器权重分配精度和整体计算效率，在保持轻量设计的同时展现出优越且稳定的性能。

Conclusion: 图结构先验与模块化机制的协同作用有效增强了多任务场景下可组合微调的性能、稳定性与参数效率。

Abstract: This paper proposes a composable fine-tuning method that integrates graph
structural priors with modular adapters to address the high computational cost
and structural instability faced by large-scale pre-trained models in
multi-task adaptation. The method introduces a relation matrix to model
dependencies among tasks, explicitly encoding correlations between nodes and
paths into graph structural priors, which provide unified structural
constraints for adapter weight allocation and path selection. Modular adapters
are embedded into different layers through low-rank mapping and a pluggable
mechanism, enabling efficient cross-task composition and reuse under prior
guidance. This mechanism not only improves parameter efficiency and training
stability but also alleviates path conflicts and redundant computation in
multi-task scenarios. Furthermore, experiments on hyperparameter sensitivity,
environmental sensitivity, and data sensitivity are conducted to systematically
analyze key factors such as routing temperature, gating thresholds, and
relation matrix regularization strength, verifying the consistency and superior
performance of the method under structural constraints. The results demonstrate
that the proposed framework significantly enhances task prediction accuracy,
adapter weight allocation precision, and overall computational efficiency while
maintaining model lightweight design, highlighting the synergistic advantages
of graph priors and modular mechanisms in composable fine-tuning.

</details>


### [58] [Use of Continuous Glucose Monitoring with Machine Learning to Identify Metabolic Subphenotypes and Inform Precision Lifestyle Changes](https://arxiv.org/abs/2511.03986)
*Ahmed A. Metwally,Heyjun Park,Yue Wu,Tracey McLaughlin,Michael P. Snyder*

Main category: cs.LG

TL;DR: 该综述提出，通过连续血糖监测（CGM）和可穿戴设备结合机器学习，可实现对糖尿病前期个体的动态代谢表型分型，从而支持精准预防。


<details>
  <summary>Details</summary>
Motivation: 传统基于静态血糖阈值的糖尿病和糖尿病前期分类忽略了由胰岛素抵抗、β细胞功能障碍和肠促胰岛素缺乏驱动的病理生理异质性。

Method: 利用家庭CGM支持的口服葡萄糖耐量试验获取高分辨率血糖数据，结合机器学习模型预测肌肉胰岛素抵抗和β细胞功能；同时整合可穿戴设备数据评估饮食、睡眠和体力活动模式与代谢功能障碍的关系。

Result: 研究发现个体对标准化餐食（如土豆 vs 葡萄）的餐后血糖反应（PPGR）可作为其代谢亚型的生物标志物；生活方式干预效果具有表型依赖性；CGM能将早期血糖异常分解为可操作的亚表型。

Conclusion: CGM与可穿戴技术结合机器学习，能够实现个体化代谢缺陷识别，推动精准营养、行为和药物干预，开启糖尿病精准预防新范式。

Abstract: The classification of diabetes and prediabetes by static glucose thresholds
obscures the pathophysiological dysglycemia heterogeneity, primarily driven by
insulin resistance (IR), beta-cell dysfunction, and incretin deficiency. This
review demonstrates that continuous glucose monitoring and wearable
technologies enable a paradigm shift towards non-invasive, dynamic metabolic
phenotyping. We show evidence that machine learning models can leverage
high-resolution glucose data from at-home, CGM-enabled oral glucose tolerance
tests to accurately predict gold-standard measures of muscle IR and beta-cell
function. This personalized characterization extends to real-world nutrition,
where an individual's unique postprandial glycemic response (PPGR) to
standardized meals, such as the relative glucose spike to potatoes versus
grapes, could serve as a biomarker for their metabolic subtype. Moreover,
integrating wearable data reveals that habitual diet, sleep, and physical
activity patterns, particularly their timing, are uniquely associated with
specific metabolic dysfunctions, informing precision lifestyle interventions.
The efficacy of dietary mitigators in attenuating PPGR is also shown to be
phenotype-dependent. Collectively, this evidence demonstrates that CGM can
deconstruct the complexity of early dysglycemia into distinct, actionable
subphenotypes. This approach moves beyond simple glycemic control, paving the
way for targeted nutritional, behavioral, and pharmacological strategies
tailored to an individual's core metabolic defects, thereby paving the way for
a new era of precision diabetes prevention.

</details>


### [59] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: 本文提出了一种高效可扩展的方法，通过合成近优决策树生成大规模预训练数据，用于决策树的元学习，在性能相当的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 由于决策树在金融和医疗等高风险领域因其可解释性而被广泛使用，但现有预训练方法依赖真实数据或计算代价高昂的最优决策树，限制了其可扩展性和灵活性。

Method: 通过合成采样近优决策树来生成大规模、逼真的预训练数据，并结合MetaTree Transformer架构进行决策树的元学习。

Result: 该方法在性能上与使用真实数据或昂贵最优决策树进行预训练的方法相当，同时大幅降低了计算开销并提升了数据生成的灵活性。

Conclusion: 所提出的方法为可解释决策树模型的高效、可扩展元学习提供了新路径，在保持性能的同时显著提升了效率和灵活性。

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [60] [Accelerating scientific discovery with the common task framework](https://arxiv.org/abs/2511.04001)
*J. Nathan Kutz,Peter Battaglia,Michael Brenner,Kevin Carlberg,Aric Hagberg,Shirley Ho,Stephan Hoyer,Henning Lange,Hod Lipson,Michael W. Mahoney,Frank Noe,Max Welling,Laure Zanna,Francis Zhu,Steven L. Brunton*

Main category: cs.LG

TL;DR: 本文提出了一种面向科学与工程领域的通用任务框架（CTF），用于在数据有限和测量噪声等现实条件下，对机器学习与人工智能算法在预测、状态重构、泛化和控制等目标上的性能进行客观比较。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习和人工智能算法在科学与工程领域迅速发展，但缺乏统一的评估标准来比较不同算法在多种实际任务中的表现，尤其是在数据稀缺和存在噪声的情况下。

Method: 引入通用任务框架（CTF），整合一系列具有多样化实际目标的挑战性数据集，为算法提供标准化的评估基准。

Result: 该框架为科学与工程中多样化的ML/AI算法提供了可比较的客观指标，有助于推动算法在这些领域的有效发展与应用。

Conclusion: 通用任务框架（CTF）是推动ML/AI算法在科学与工程领域进步的关键使能技术，能够支持对各类算法在实际任务中的性能进行系统性评估与比较。

Abstract: Machine learning (ML) and artificial intelligence (AI) algorithms are
transforming and empowering the characterization and control of dynamic systems
in the engineering, physical, and biological sciences. These emerging modeling
paradigms require comparative metrics to evaluate a diverse set of scientific
objectives, including forecasting, state reconstruction, generalization, and
control, while also considering limited data scenarios and noisy measurements.
We introduce a common task framework (CTF) for science and engineering, which
features a growing collection of challenge data sets with a diverse set of
practical and common objectives. The CTF is a critically enabling technology
that has contributed to the rapid advance of ML/AI algorithms in traditional
applications such as speech recognition, language processing, and computer
vision. There is a critical need for the objective metrics of a CTF to compare
the diverse algorithms being rapidly developed and deployed in practice today
across science and engineering.

</details>


### [61] [Memory- and Latency-Constrained Inference of Large Language Models via Adaptive Split Computing](https://arxiv.org/abs/2511.04002)
*Mingyu Sung,Vikas Palakonda,Suhwan Im,Sunghwan Moon,Il-Min Kim,Sangseok Yun,Jae-Mo Kang*

Main category: cs.LG

TL;DR: 本文提出首个面向边缘设备的自回归感知分割计算框架，通过混合精度量化、中间激活压缩和联合优化策略，在满足内存与延迟约束的同时显著提升大语言模型在物联网设备上的推理效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）虽在多种推理任务中表现优异，但其庞大的参数量和内存密集型的自回归解码使其难以部署于资源受限的物联网（IoT）设备。现有分割计算方法未能有效应对自回归推理中的迭代生成和不断增长的KV缓存等独特挑战。

Method: 作者提出三项关键技术：1）单点分割压缩（OPSC），采用混合精度量化避免内存溢出；2）两阶段中间压缩流程，结合阈值分割（TS）与逐token自适应比特量化（TAB-Q）以保留关键激活并降低通信开销；3）统一优化框架，联合选择最优分割点、量化配置和序列长度以满足严格资源约束。

Result: 在多种LLM和硬件平台上的实验表明，该框架优于SmoothQuant、OmniQuant和Atom等先进量化方法，实现1.49倍推理加速，并显著减少通信开销，同时保持或提升模型准确率。

Conclusion: 该工作首次实现了面向边缘设备的自回归感知分割计算，有效解决了LLM在资源受限环境下的部署难题，在推理速度、通信效率和准确性之间取得了良好平衡。

Abstract: Large language models (LLMs) have achieved near-human performance across
diverse reasoning tasks, yet their deployment on resource-constrained
Internet-of-Things (IoT) devices remains impractical due to massive parameter
footprints and memory-intensive autoregressive decoding. While split computing
offers a promising solution by partitioning model execution between edge
devices and cloud servers, existing approaches fail to address the unique
challenges of autoregressive inference, particularly the iterative token
generation process and expanding key-value (KV) cache requirements. This work
introduces the first autoregressive-aware split computing framework designed
explicitly for LLM deployment on edge devices. Our approach makes three key
contributions. First, we develop one-point split compression (OPSC), a
mixed-precision quantization scheme that prevents out-of-memory failures by
strategically partitioning models into front-end and back-end segments with
different precision levels. Second, we propose a two-stage intermediate
compression pipeline that combines threshold splitting (TS) and token-wise
adaptive bit quantization (TAB-Q) to preserve accuracy-critical activations
while dramatically reducing communication overhead. Third, we formulate a
unified optimization framework that jointly selects optimal split points,
quantization settings, and sequence lengths to satisfy strict memory and
latency constraints. Extensive evaluations across diverse LLMs and hardware
platforms demonstrate superior performance compared to state-of-the-art
quantization methods, including SmoothQuant, OmniQuant, and Atom. The framework
achieves a 1.49 inference speedup and significant communication overhead
reduction while maintaining or improving model accuracy.

</details>


### [62] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: DartQuant 是一种高效、分布感知的旋转校准方法，通过约束旋转后激活值的分布来简化旋转优化过程，显著降低计算开销与内存消耗，并首次在单张 3090 GPU 上完成 70B 大模型的旋转校准。


<details>
  <summary>Details</summary>
Motivation: 现有端到端微调旋转优化算法计算成本高且容易过拟合，限制了其在大规模模型量化中的应用。

Method: 提出 DartQuant 方法，通过约束旋转后激活分布来简化旋转优化，并引入 QR-Orth 优化方案替代昂贵的交替优化策略，减少对任务特定损失的依赖。

Result: 在多种模型量化实验中表现优越，相比现有方法，在 70B 模型上实现 47 倍加速和 10 倍内存节省，并首次在单张 3090 GPU 上完成该规模模型的旋转校准。

Conclusion: DartQuant 显著提升了大语言模型量化在资源受限环境下的可行性，兼具高效性与泛化能力。

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [63] [Pediatric Appendicitis Detection from Ultrasound Images](https://arxiv.org/abs/2511.04069)
*Fatemeh Hosseinabadi,Seyedhassan Sharifi*

Main category: cs.LG

TL;DR: 本文提出了一种基于预训练ResNet架构的深度学习模型，用于从儿童腹部超声图像中自动检测阑尾炎，在Regensburg儿科阑尾炎数据集上取得了93.44%的准确率。


<details>
  <summary>Details</summary>
Motivation: 儿童阑尾炎诊断因症状重叠和超声图像质量差异而具有挑战性，亟需自动化辅助诊断工具以提高准确性和效率。

Method: 研究采用预训练的ResNet模型，在包含超声图像、实验室数据和临床评分的Regensburg儿科数据集上进行微调；图像经过归一化、缩放和增强处理，用于区分阑尾炎与非阑尾炎病例。

Result: 模型在测试中达到93.44%的准确率、91.53%的精确率和89.8%的召回率，能有效识别不同视角下的阑尾炎，并克服低对比度、斑点噪声和解剖变异等成像难题。

Conclusion: 基于ResNet的深度学习模型在儿童阑尾炎超声诊断中表现出优异性能，具备临床辅助应用潜力。

Abstract: Pediatric appendicitis remains one of the most common causes of acute
abdominal pain in children, and its diagnosis continues to challenge clinicians
due to overlapping symptoms and variable imaging quality. This study aims to
develop and evaluate a deep learning model based on a pretrained ResNet
architecture for automated detection of appendicitis from ultrasound images. We
used the Regensburg Pediatric Appendicitis Dataset, which includes ultrasound
scans, laboratory data, and clinical scores from pediatric patients admitted
with abdominal pain to Children Hospital. Hedwig in Regensburg, Germany. Each
subject had 1 to 15 ultrasound views covering the right lower quadrant,
appendix, lymph nodes, and related structures. For the image based
classification task, ResNet was fine tuned to distinguish appendicitis from
non-appendicitis cases. Images were preprocessed by normalization, resizing,
and augmentation to enhance generalization. The proposed ResNet model achieved
an overall accuracy of 93.44, precision of 91.53, and recall of 89.8,
demonstrating strong performance in identifying appendicitis across
heterogeneous ultrasound views. The model effectively learned discriminative
spatial features, overcoming challenges posed by low contrast, speckle noise,
and anatomical variability in pediatric imaging.

</details>


### [64] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar,Suryansh Gupta,Ravishankar Krishnaswamy,Haiyang Xu,Aseem Rastogi,Gopal Srinivasa*

Main category: cs.LG

TL;DR: 本文提出一种数据驱动的方法，通过学习向量距离与标签过滤之间的最优权衡，改进了带过滤条件的近似最近邻搜索（Filtered ANN）的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的过滤近似最近邻搜索方法使用固定且与数据无关的惩罚项来处理标签过滤，难以在不同标签和向量分布的数据集上泛化。

Method: 将距离与过滤匹配的权衡建模为一个带约束的线性优化问题，从数据中学习权重，并用于指导搜索过程和图索引构建。

Result: 实验表明，相比固定惩罚方法，该方法在准确率上提升了5-10%，并展现出更强的灵活性和泛化能力。

Conclusion: 通过数据驱动地调整距离函数，能更有效地捕捉过滤条件的语义和分布，从而显著提升过滤ANN搜索的性能。

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest
vectors for a query vector from a dataset. It enforces that a specified set of
discrete labels $S$ for the query must be included in the labels of each
retrieved vector. Existing graph-based methods typically incorporate filter
awareness by assigning fixed penalties or prioritizing nodes based on filter
satisfaction. However, since these methods use fixed, data in- dependent
penalties, they often fail to generalize across datasets with diverse label and
vector distributions. In this work, we propose a principled alternative that
learns the optimal trade-off between vector distance and filter match directly
from the data, rather than relying on fixed penalties. We formulate this as a
constrained linear optimization problem, deriving weights that better reflect
the underlying filter distribution and more effectively address the filtered
ANN search problem. These learned weights guide both the search process and
index construction, leading to graph structures that more effectively capture
the underlying filter distribution and filter semantics. Our experiments
demonstrate that adapting the distance function to the data significantly im-
proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible
and generalizable framework for the filtered ANN search problem.

</details>


### [65] [KoTaP: A Panel Dataset for Corporate Tax Avoidance, Performance, and Governance in Korea](https://arxiv.org/abs/2511.04094)
*Hyungjong Na,Wonho Song,Seungyong Han,Donghyeon Jo,Sejin Myung,Hyungjoon Kim*

Main category: cs.LG

TL;DR: 本文构建了韩国避税面板数据集（KoTaP），涵盖2011至2024年间1,754家非金融上市公司的12,653个公司-年度观测值，旨在将企业避税作为预测变量，关联盈余管理、盈利能力、稳定性、成长性与治理等多个领域，并提供标准化、平衡的面板数据以支持实证研究与政策分析。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏一个既符合国际标准又能反映韩国制度特征的高质量、长期且结构平衡的企业避税面板数据集。为填补这一空白，作者构建KoTaP，以支持多领域交叉研究并提升实证结果的外部有效性。

Method: 作者从KOSPI和KOSDAQ市场筛选符合条件的非金融公司，剔除特定异常样本后构建平衡面板；采用现金实际税率（CETR）、GAAP实际税率（GETR）及账税差异指标（TSTA、TSDA）衡量避税程度，并对变量进行标准化处理，确保与国际文献可比，同时保留韩国企业所有权集中、外资持股高等制度特征。

Result: KoTaP包含12,653个公司-年度观测值，覆盖1,754家企业，具有良好的变量分布与相关性，兼具国际可比性与本土独特性，适用于计量模型基准测试、深度学习、可解释AI、政策评估、审计规划及投资分析等多种应用场景。

Conclusion: KoTaP是一个开放、标准化且具代表性的企业避税面板数据集，不仅促进会计与金融领域的学术研究，也为跨学科应用和实务决策提供了重要基础资源。

Abstract: This study introduces the Korean Tax Avoidance Panel (KoTaP), a long-term
panel dataset of non-financial firms listed on KOSPI and KOSDAQ between 2011
and 2024. After excluding financial firms, firms with non-December fiscal year
ends, capital impairment, and negative pre-tax income, the final dataset
consists of 12,653 firm-year observations from 1,754 firms. KoTaP is designed
to treat corporate tax avoidance as a predictor variable and link it to
multiple domains, including earnings management (accrual- and activity-based),
profitability (ROA, ROE, CFO, LOSS), stability (LEV, CUR, SIZE, PPE, AGE,
INVREC), growth (GRW, MB, TQ), and governance (BIG4, FORN, OWN). Tax avoidance
itself is measured using complementary indicators cash effective tax rate
(CETR), GAAP effective tax rate (GETR), and book-tax difference measures (TSTA,
TSDA) with adjustments to ensure interpretability. A key strength of KoTaP is
its balanced panel structure with standardized variables and its consistency
with international literature on the distribution and correlation of core
indicators. At the same time, it reflects distinctive institutional features of
Korean firms, such as concentrated ownership, high foreign shareholding, and
elevated liquidity ratios, providing both international comparability and
contextual uniqueness. KoTaP enables applications in benchmarking econometric
and deep learning models, external validity checks, and explainable AI
analyses. It further supports policy evaluation, audit planning, and investment
analysis, making it a critical open resource for accounting, finance, and
interdisciplinary research.

</details>


### [66] [Decomposable Neuro Symbolic Regression](https://arxiv.org/abs/2511.04124)
*Giorgio Morales,John W. Sheppard*

Main category: cs.LG

TL;DR: 本文提出了一种可解释的符号回归方法，结合Transformer、遗传算法和遗传编程，从“黑箱”回归模型中提取结构准确且可解释的数学表达式，在插值与外推误差上表现优异，并能还原原始数学结构。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法过于关注预测误差，忽视对真实控制方程的识别，常导致表达式复杂或结构错误；因此需要一种既能保持可解释性又能准确还原数学结构的新方法。

Method: 该方法首先利用Multi-Set Transformer生成多个单变量符号骨架，再通过遗传算法筛选高质量候选，随后采用基于遗传编程的级联过程将骨架逐步合并为多变量表达式，并保留原始结构，最后用遗传算法优化系数。

Result: 在含不同噪声水平的问题上，该方法在插值和外推误差方面优于或媲美多种现有方法（包括GP、神经符号回归及混合方法），且能一致地恢复原始数学表达式结构。

Conclusion: 所提方法在保证低预测误差的同时，显著提升了符号回归结果的可解释性和结构准确性，为从黑箱模型中提取可信数学表达式提供了有效途径。

Abstract: Symbolic regression (SR) models complex systems by discovering mathematical
expressions that capture underlying relationships in observed data. However,
most SR methods prioritize minimizing prediction error over identifying the
governing equations, often producing overly complex or inaccurate expressions.
To address this, we present a decomposable SR method that generates
interpretable multivariate expressions leveraging transformer models, genetic
algorithms (GAs), and genetic programming (GP). In particular, our explainable
SR method distills a trained ``opaque'' regression model into mathematical
expressions that serve as explanations of its computed function. Our method
employs a Multi-Set Transformer to generate multiple univariate symbolic
skeletons that characterize how each variable influences the opaque model's
response. We then evaluate the generated skeletons' performance using a
GA-based approach to select a subset of high-quality candidates before
incrementally merging them via a GP-based cascade procedure that preserves
their original skeleton structure. The final multivariate skeletons undergo
coefficient optimization via a GA. We evaluated our method on problems with
controlled and varying degrees of noise, demonstrating lower or comparable
interpolation and extrapolation errors compared to two GP-based methods, three
neural SR methods, and a hybrid approach. Unlike them, our approach
consistently learned expressions that matched the original mathematical
structure.

</details>


### [67] [Exploring the Feasibility of End-to-End Large Language Model as a Compiler](https://arxiv.org/abs/2511.04132)
*Hongbin Zhang,Shihao Gao,Yang Liu,Mingjie Xing,Yanjun Wu,Chen Zhao*

Main category: cs.LG

TL;DR: 本文探讨了大语言模型（LLM）作为端到端编译器（LaaC）的可行性，构建了CompilerEval数据集与评估框架，发现当前LLM虽具备基础编译能力但成功率较低，通过提示优化、模型扩展和推理方法可显著提升生成汇编代码质量，并提出了LaaC的架构设计与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM已被用于辅助编译器开发与维护，但其作为端到端编译器的潜力尚未被充分探索。本文旨在系统评估LLM在源码理解与汇编代码生成方面的能力，验证其作为编译器的可行性。

Method: 作者设计了CompilerEval数据集和评估框架，用于测试主流LLM在编译任务中的表现；通过分析错误类型、优化提示词、扩大模型规模及引入推理方法来提升LLM生成代码的质量，并评估其跨平台编译能力。

Result: 实验表明，当前LLM具备基本的编译能力，但编译成功率较低；通过提示优化、模型扩展和推理策略，可显著提高生成汇编代码的正确性与质量。

Conclusion: LLM作为编译器（LaaC）具有发展潜力，结合针对性训练、知识丰富的提示和专用基础设施，有望实现高质量汇编代码生成，推动编译领域范式变革。

Abstract: In recent years, end-to-end Large Language Model (LLM) technology has shown
substantial advantages across various domains. As critical system software and
infrastructure, compilers are responsible for transforming source code into
target code. While LLMs have been leveraged to assist in compiler development
and maintenance, their potential as an end-to-end compiler remains largely
unexplored. This paper explores the feasibility of LLM as a Compiler (LaaC) and
its future directions. We designed the CompilerEval dataset and framework
specifically to evaluate the capabilities of mainstream LLMs in source code
comprehension and assembly code generation. In the evaluation, we analyzed
various errors, explored multiple methods to improve LLM-generated code, and
evaluated cross-platform compilation capabilities. Experimental results
demonstrate that LLMs exhibit basic capabilities as compilers but currently
achieve low compilation success rates. By optimizing prompts, scaling up the
model, and incorporating reasoning methods, the quality of assembly code
generated by LLMs can be significantly enhanced. Based on these findings, we
maintain an optimistic outlook for LaaC and propose practical architectural
designs and future research directions. We believe that with targeted training,
knowledge-rich prompts, and specialized infrastructure, LaaC has the potential
to generate high-quality assembly code and drive a paradigm shift in the field
of compilation.

</details>


### [68] [Exchange Policy Optimization Algorithm for Semi-Infinite Safe Reinforcement Learning](https://arxiv.org/abs/2511.04147)
*Jiaming Zhang,Yujie Yang,Haoning Wang,Liping Zhang,Shengbo Eben Li*

Main category: cs.LG

TL;DR: 本文提出了一种名为交换策略优化（EPO）的算法框架，用于解决具有无限约束的安全强化学习问题，通过迭代更新有限约束子集，在保证策略性能的同时严格控制安全违规在预设范围内。


<details>
  <summary>Details</summary>
Motivation: 实际应用中存在大量需在连续参数空间上满足安全条件的问题，导致安全强化学习面临无限约束（即半无限安全RL）的挑战，现有方法难以兼顾性能与严格的安全保障。

Method: EPO通过迭代求解带有限约束的安全RL子问题，并利用约束扩展与删除机制动态调整活跃约束集：将超出容忍度的违规约束加入，同时移除拉格朗日乘子为零的约束，从而控制工作集规模并有效训练策略。

Result: 理论分析表明，在温和假设下，EPO训练出的策略性能接近最优，且全局约束违反严格控制在预设界限内。

Conclusion: EPO为半无限安全强化学习提供了一个兼具性能与确定性安全保障的有效算法框架。

Abstract: Safe reinforcement learning (safe RL) aims to respect safety requirements
while optimizing long-term performance. In many practical applications,
however, the problem involves an infinite number of constraints, known as
semi-infinite safe RL (SI-safe RL). Such constraints typically appear when
safety conditions must be enforced across an entire continuous parameter space,
such as ensuring adequate resource distribution at every spatial location. In
this paper, we propose exchange policy optimization (EPO), an algorithmic
framework that achieves optimal policy performance and deterministic bounded
safety. EPO works by iteratively solving safe RL subproblems with finite
constraint sets and adaptively adjusting the active set through constraint
expansion and deletion. At each iteration, constraints with violations
exceeding the predefined tolerance are added to refine the policy, while those
with zero Lagrange multipliers are removed after the policy update. This
exchange rule prevents uncontrolled growth of the working set and supports
effective policy training. Our theoretical analysis demonstrates that, under
mild assumptions, strategies trained via EPO achieve performance comparable to
optimal solutions with global constraint violations strictly remaining within a
prescribed bound.

</details>


### [69] [Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories](https://arxiv.org/abs/2511.04155)
*Olav Finne Praesteng Larsen,Massimiliano Ruocco,Michail Spitieris,Abdulmajid Murad,Martina Ragosta*

Main category: cs.LG

TL;DR: 本文研究了利用迁移学习将基于丰富数据机场训练的生成模型（如扩散模型和流匹配模型）迁移到数据稀缺机场（如都柏林）进行航迹生成的可行性，发现仅需5%的目标机场数据即可获得良好性能。


<details>
  <summary>Details</summary>
Motivation: 许多次要和区域性机场缺乏足够的航迹数据，限制了机器学习方法的应用以及大规模仿真和“假设”分析的能力，因此需要减少对大量本地数据的依赖。

Method: 作者将先进的扩散模型和流匹配架构适配到航空领域，先在苏黎世机场数据上预训练模型，然后在都柏林机场数据上进行不同比例（0%–100%）的微调，评估其迁移效果。

Result: 扩散模型仅用5%的都柏林数据即可达到有竞争力的性能，在20%时接近全量训练的基线水平，且始终优于从零训练的模型；潜在扩散和潜在流匹配模型也受益于预训练，但效果波动较大；标准流匹配模型泛化能力较弱。

Conclusion: 迁移学习能显著降低空中交通管理中航迹生成对本地数据的需求，即使在历史记录有限的环境中也能生成逼真的合成航迹数据。

Abstract: Access to trajectory data is a key requirement for developing and validating
Air Traffic Management (ATM) solutions, yet many secondary and regional
airports face severe data scarcity. This limits the applicability of machine
learning methods and the ability to perform large-scale simulations or
"what-if" analyses. In this paper, we investigate whether generative models
trained on data-rich airports can be efficiently adapted to data-scarce
airports using transfer learning. We adapt state-of-the-art diffusion- and
flow-matching-based architectures to the aviation domain and evaluate their
transferability between Zurich (source) and Dublin (target) landing trajectory
datasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying
amounts of local data, ranging from 0% to 100%. Results show that
diffusion-based models achieve competitive performance with as little as 5% of
the Dublin data and reach baseline-level performance around 20%, consistently
outperforming models trained from scratch across metrics and visual
inspections. Latent flow matching and latent diffusion models also benefit from
pretraining, though with more variable gains, while flow matching models show
weaker generalization. Despite challenges in capturing rare trajectory
patterns, these findings demonstrate the potential of transfer learning to
substantially reduce data requirements for trajectory generation in ATM,
enabling realistic synthetic data generation even in environments with limited
historical records.

</details>


### [70] [Deep Learning Approach for Clinical Risk Identification Using Transformer Modeling of Heterogeneous EHR Data](https://arxiv.org/abs/2511.04158)
*Anzhuo Xie,Wei-Chen Chang*

Main category: cs.LG

TL;DR: 本文提出一种基于Transformer的纵向建模方法，用于处理异构电子健康记录（EHR）数据中的临床风险分类问题，通过统一表示结构化与非结构化数据、引入可学习的时间编码机制和语义加权池化模块，在多源异构EHR环境中实现了更准确的风险识别。


<details>
  <summary>Details</summary>
Motivation: 临床风险分类面临电子健康记录（EHR）数据异构性强、时间采样不规则、模态差异大及语义结构复杂等挑战，现有方法难以有效建模长期趋势与短期波动并兼顾不同时间尺度下的关键医学事件。

Method: 该方法以多源医学特征为输入，通过特征嵌入层统一结构化与非结构化数据表示；引入可学习的时间编码机制处理不规则采样；采用多头自注意力结构对纵向序列进行全局依赖建模；设计语义加权池化模块自适应突出关键医学事件的重要性；最后通过线性映射层输出个体风险评分。

Result: 实验结果表明，所提模型在准确率、召回率、精确率和F1分数上均优于传统机器学习和时序深度学习模型，在多源异构EHR环境下实现了稳定且精准的风险识别。

Conclusion: 该研究提供了一个高效可靠的临床智能决策支持框架，能够有效应对异构EHR数据中的纵向建模挑战，提升临床风险分类性能。

Abstract: This study proposes a Transformer-based longitudinal modeling method to
address challenges in clinical risk classification with heterogeneous
Electronic Health Record (EHR) data, including irregular temporal patterns,
large modality differences, and complex semantic structures. The method takes
multi-source medical features as input and employs a feature embedding layer to
achieve a unified representation of structured and unstructured data. A
learnable temporal encoding mechanism is introduced to capture dynamic
evolution under uneven sampling intervals. The core model adopts a multi-head
self-attention structure to perform global dependency modeling on longitudinal
sequences, enabling the aggregation of long-term trends and short-term
fluctuations across different temporal scales. To enhance semantic
representation, a semantic-weighted pooling module is designed to assign
adaptive importance to key medical events, improving the discriminative ability
of risk-related features. Finally, a linear mapping layer generates
individual-level risk scores. Experimental results show that the proposed model
outperforms traditional machine learning and temporal deep learning models in
accuracy, recall, precision, and F1-Score, achieving stable and precise risk
identification in multi-source heterogeneous EHR environments and providing an
efficient and reliable framework for clinical intelligent decision-making.

</details>


### [71] [On Joint Regularization and Calibration in Deep Ensembles](https://arxiv.org/abs/2511.04160)
*Laurits Fredsgaard,Mikkel N. Schmidt*

Main category: cs.LG

TL;DR: 本文研究了在深度集成中联合调优权重衰减、温度缩放和早停策略对预测性能与不确定性量化的影响，并提出一种部分重叠的验证集策略，在联合评估与最大化训练数据利用之间取得实用平衡。


<details>
  <summary>Details</summary>
Motivation: 传统深度集成通常单独训练和调优每个模型，但有证据表明联合调优整个集成可能带来更优性能。作者旨在系统评估联合调优对性能和不确定性校准的实际影响，并解决联合调优所需验证策略带来的数据利用效率问题。

Method: 作者联合调优深度集成中的权重衰减、温度缩放和早停，并引入一种部分重叠的验证集策略，以支持联合评估的同时尽可能多地使用数据进行训练。

Result: 实验表明，联合调优通常能匹配或提升性能，但其效果大小在不同任务和指标间存在显著差异；部分重叠验证集策略在实践中表现良好。

Conclusion: 联合优化深度集成可带来性能收益，但需权衡个体与整体调优的利弊；所提出的部分重叠验证策略为实际应用提供了有效解决方案，对从业者具有指导意义。

Abstract: Deep ensembles are a powerful tool in machine learning, improving both model
performance and uncertainty calibration. While ensembles are typically formed
by training and tuning models individually, evidence suggests that jointly
tuning the ensemble can lead to better performance. This paper investigates the
impact of jointly tuning weight decay, temperature scaling, and early stopping
on both predictive performance and uncertainty quantification. Additionally, we
propose a partially overlapping holdout strategy as a practical compromise
between enabling joint evaluation and maximizing the use of data for training.
Our results demonstrate that jointly tuning the ensemble generally matches or
improves performance, with significant variation in effect size across
different tasks and metrics. We highlight the trade-offs between individual and
joint optimization in deep ensemble training, with the overlapping holdout
strategy offering an attractive practical solution. We believe our findings
provide valuable insights and guidance for practitioners looking to optimize
deep ensemble models. Code is available at:
https://github.com/lauritsf/ensemble-optimality-gap

</details>


### [72] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 本文系统评估了后训练量化（PTQ）方法在新兴MXFP4格式下的表现，发现主流旋转类方法与MXFP4存在严重不兼容问题，并提出一种适配的块旋转策略显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模迅速增长，其在内存、计算和能耗方面带来巨大开销。尽管后训练量化（PTQ）是高效部署的有前景方案，但实现准确的W4A4量化仍具挑战性，尤其在新型硬件支持的MXFP4格式下现有方法的有效性尚不明确。

Method: 作者构建了一个针对MXFP4格式的PTQ方法综合基准，系统评估多种现有方法；并通过深入分析揭示旋转类方法与MXFP4不兼容的根本原因——MXFP4基于2的幂次（PoT）块缩放与全局旋转导致的异常值能量重分布之间存在冲突；据此提出一种新的块旋转策略以适配MXFP4。

Result: 实验表明GPTQ等方法在MXFP4下表现稳健，而主流旋转类方法性能显著下降；所提出的块旋转策略有效缓解该问题，在多种大语言模型上带来显著精度提升。

Conclusion: 本研究不仅为MXFP4格式下的PTQ实践提供了明确指导，也为面向新兴低精度格式的PTQ研究奠定了基础。

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [73] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 当前大语言模型（LLM）的不确定性量化（UQ）方法在无歧义任务上表现良好，但在真实世界常见的歧义数据上性能显著下降；本文提出了首个带有真实答案分布的歧义问答数据集 MAQA* 和 AmbigQA*，并揭示了现有 UQ 方法在歧义场景下的根本局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的语言本质上具有歧义性（即存在随机不确定性），但现有的 LLM 不确定性量化方法大多在无歧义任务上进行评估，无法反映真实应用场景中的挑战。

Method: 构建了两个新的歧义问答数据集 MAQA* 和 AmbigQA*，其中包含基于事实共现估计的真实答案分布；并在多种不确定性估计范式（预测分布、内部表征、模型集成）下系统评估现有 UQ 方法在歧义数据上的表现；同时提供理论分析解释性能下降的原因。

Result: 现有 UQ 方法在歧义数据上性能严重退化，接近随机水平；该现象在不同估计范式中一致存在；理论分析表明基于预测分布和集成的方法在歧义条件下存在根本性局限。

Conclusion: 当前 LLM 的不确定性量化方法在面对语言歧义时存在关键缺陷，亟需重新思考现有建模范式以提升在真实场景中的可靠性。

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [74] [seqme: a Python library for evaluating biological sequence design](https://arxiv.org/abs/2511.04239)
*Rasmus Møller-Larsen,Adam Izdebski,Jan Olszewski,Pankhil Gawade,Michal Kmicikiewicz,Wojciech Zarzecki,Ewa Szczurek*

Main category: cs.LG

TL;DR: 本文介绍了 seqme，一个模块化、可扩展的开源 Python 库，用于评估生物序列设计方法的性能，涵盖序列、嵌入和属性三类指标，适用于多种生物序列类型。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏统一的软件库来实现评估生物序列设计方法性能的各类指标，因此需要开发一个通用、灵活且开源的工具。

Method: 开发了名为 seqme 的 Python 库，整合了模型无关的序列设计评估指标，包括基于序列、嵌入和属性的三类指标，并提供嵌入模型、属性预测模型以及结果诊断与可视化功能。

Result: seqme 支持对一次性（one-shot）和迭代式设计方法进行评估，适用于小分子、DNA、ncRNA、mRNA、肽段和蛋白质等多种生物序列。

Conclusion: seqme 填补了生物序列设计评估工具的空白，为研究人员提供了标准化、可扩展的评估框架，有助于推动计算生物序列设计领域的发展。

Abstract: Recent advances in computational methods for designing biological sequences
have sparked the development of metrics to evaluate these methods performance
in terms of the fidelity of the designed sequences to a target distribution and
their attainment of desired properties. However, a single software library
implementing these metrics was lacking. In this work we introduce seqme, a
modular and highly extendable open-source Python library, containing
model-agnostic metrics for evaluating computational methods for biological
sequence design. seqme considers three groups of metrics: sequence-based,
embedding-based, and property-based, and is applicable to a wide range of
biological sequences: small molecules, DNA, ncRNA, mRNA, peptides and proteins.
The library offers a number of embedding and property models for biological
sequences, as well as diagnostics and visualization functions to inspect the
results. seqme can be used to evaluate both one-shot and iterative
computational design methods.

</details>


### [75] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: 本文提出了SynthKGQA框架，用于从任意知识图谱自动生成高质量的合成问答数据集，并利用该框架基于Wikidata构建了GTSQA数据集，以评估知识图谱检索器在未见图结构和关系类型上的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏具有真实检索目标的、具有挑战性的问答数据集，导致难以对知识图谱检索方法进行有效比较。

Method: 提出SynthKGQA框架，可从任意知识图谱生成包含完整推理事实的合成KGQA数据集；并基于Wikidata生成新数据集GTSQA，用于评测KG增强型大语言模型的零样本泛化能力。

Result: 使用SynthKGQA生成的数据不仅支持更有效的检索器评测，还能用于训练性能更优的模型；在GTSQA上对主流KG增强LLM方法进行了基准测试。

Conclusion: SynthKGQA为知识图谱问答研究提供了可扩展的数据生成方案，并推动了KG检索器在零样本场景下的评估与改进。

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


### [76] [Guided by Stars: Interpretable Concept Learning Over Time Series via Temporal Logic Semantics](https://arxiv.org/abs/2511.04244)
*Irene Ferfoglia,Simone Silvetti,Gaia Saveri,Laura Nenzi,Luca Bortolussi*

Main category: cs.LG

TL;DR: 提出了一种名为STELLE的神经符号框架，通过将时间序列嵌入到时序逻辑概念空间中，实现高准确率分类的同时提供人类可读的局部与全局解释。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类常用于安全关键场景，但现有深度学习方法多为黑盒模型，缺乏可解释性，难以让人理解其决策依据。

Method: 设计了一种受信号时序逻辑（STL）启发的新核函数，将原始时间序列映射到预定义STL公式的对齐空间，构建神经符号框架STELLE，在优化分类准确率的同时生成基于逻辑的解释。

Result: 在多个真实世界基准数据集上验证表明，STELLE在保持有竞争力的分类准确率的同时，能够提供逻辑上忠实且人类可读的局部和全局解释。

Conclusion: STELLE成功将时间序列分类与逻辑驱动的解释统一起来，兼顾准确性与可解释性，适用于需要可信决策的安全关键应用。

Abstract: Time series classification is a task of paramount importance, as this kind of
data often arises in safety-critical applications. However, it is typically
tackled with black-box deep learning methods, making it hard for humans to
understand the rationale behind their output. To take on this challenge, we
propose a novel approach, STELLE (Signal Temporal logic Embedding for
Logically-grounded Learning and Explanation), a neuro-symbolic framework that
unifies classification and explanation through direct embedding of trajectories
into a space of temporal logic concepts. By introducing a novel STL-inspired
kernel that maps raw time series to their alignment with predefined STL
formulae, our model jointly optimises accuracy and interpretability, as each
prediction is accompanied by the most relevant logical concepts that
characterise it. This yields (i) local explanations as human-readable STL
conditions justifying individual predictions, and (ii) global explanations as
class-characterising formulae. Experiments demonstrate that STELLE achieves
competitive accuracy while providing logically faithful explanations, validated
on diverse real-world benchmarks.

</details>


### [77] [Efficient Reinforcement Learning from Human Feedback via Bayesian Preference Inference](https://arxiv.org/abs/2511.04286)
*Matteo Cercola,Valeria Capretti,Simone Formentin*

Main category: cs.LG

TL;DR: 本文提出了一种结合RLHF可扩展性与PBO查询效率的混合框架，通过在RLHF流程中引入主动查询模块，实现了更高效、样本利用率更高的偏好学习，并在高维偏好优化和大语言模型微调任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 收集人类偏好数据成本高且耗时，现有方法如RLHF虽可扩展但样本效率低，而PBO样本效率高但难以扩展。因此，需要一种兼顾两者优势的新范式。

Method: 提出一个混合框架，将基于获取策略（acquisition-driven）的主动查询模块整合进RLHF流程，从而在保持可扩展性的同时提升样本效率。

Result: 在高维偏好优化和大语言模型微调两个任务上的实验表明，该方法在样本效率和整体性能上均取得一致提升。

Conclusion: 所提出的混合框架成功融合了RLHF与PBO的优点，为高效、可扩展的人类偏好学习提供了一种有效解决方案。

Abstract: Learning from human preferences is a cornerstone of aligning machine learning
models with subjective human judgments. Yet, collecting such preference data is
often costly and time-consuming, motivating the need for more efficient
learning paradigms. Two established approaches offer complementary advantages:
RLHF scales effectively to high-dimensional tasks such as LLM fine-tuning,
while PBO achieves greater sample efficiency through active querying. We
propose a hybrid framework that unifies RLHF's scalability with PBO's query
efficiency by integrating an acquisition-driven module into the RLHF pipeline,
thereby enabling active and sample-efficient preference gathering. We validate
the proposed approach on two representative domains: (i) high-dimensional
preference optimization and (ii) LLM fine-tuning. Experimental results
demonstrate consistent improvements in both sample efficiency and overall
performance across these tasks.

</details>


### [78] [Differentially Private In-Context Learning with Nearest Neighbor Search](https://arxiv.org/abs/2511.04332)
*Antti Koskela,Tejas Kulkarni,Laith Zumot*

Main category: cs.LG

TL;DR: 本文提出了一种兼顾差分隐私与上下文学习的新框架，通过在检索相关示例时引入隐私感知的最近邻搜索和隐私过滤机制，在多个基准任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私上下文学习方法忽略了现代大语言模型流程中关键的相似性检索环节，存在隐私风险，因此需要一种能整合隐私保护与高效检索的新方法。

Method: 该方法结合了从上下文数据库中进行最近邻检索，并引入隐私过滤器追踪所选样本的累积隐私成本，以确保整体满足中心化差分隐私预算。

Result: 在文本分类和文档问答任务上的实验表明，该方法在隐私-效用权衡方面明显优于现有基线方法。

Conclusion: 将隐私感知的最近邻检索集成到上下文学习中，可有效提升差分隐私下的模型性能，为大语言模型的隐私保护提供了更优解决方案。

Abstract: Differentially private in-context learning (DP-ICL) has recently become an
active research topic due to the inherent privacy risks of in-context learning.
However, existing approaches overlook a critical component of modern large
language model (LLM) pipelines: the similarity search used to retrieve relevant
context data. In this work, we introduce a DP framework for in-context learning
that integrates nearest neighbor search of relevant examples in a privacy-aware
manner. Our method outperforms existing baselines by a substantial margin
across all evaluated benchmarks, achieving more favorable privacy-utility
trade-offs. To achieve this, we employ nearest neighbor retrieval from a
database of context data, combined with a privacy filter that tracks the
cumulative privacy cost of selected samples to ensure adherence to a central
differential privacy budget. Experimental results on text classification and
document question answering show a clear advantage of the proposed method over
existing baselines.

</details>


### [79] [Spurious Correlation-Aware Embedding Regularization for Worst-Group Robustness](https://arxiv.org/abs/2511.04401)
*Subeen Park,Joowang Kim,Hakyung Lee,Sunjae Yoo,Kyungwoo Song*

Main category: cs.LG

TL;DR: 本文提出了一种名为SCER的新方法，通过在嵌入空间中正则化特征表示来抑制虚假相关性，从而提升模型在最差子群体上的鲁棒性和准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型常依赖虚假相关性，在分布偏移（尤其是子群体偏移）下表现不佳；现有方法缺乏将嵌入表示与最差群体误差严格联系的理论框架。

Method: 提出SCER方法，通过分析不同域和类别间群体均值嵌入的差异，识别核心与虚假方向，并在嵌入层施加理论约束，使模型聚焦于核心特征、抑制虚假线索。

Result: 在多个视觉和语言任务上的系统评估表明，SCER在最差群体准确率上优于现有最先进的方法。

Conclusion: SCER通过嵌入层面的正则化有效提升了模型对子群体偏移的鲁棒性，为最差群体性能优化提供了理论支持和实践方案。

Abstract: Deep learning models achieve strong performance across various domains but
often rely on spurious correlations, making them vulnerable to distribution
shifts. This issue is particularly severe in subpopulation shift scenarios,
where models struggle in underrepresented groups. While existing methods have
made progress in mitigating this issue, their performance gains are still
constrained. They lack a rigorous theoretical framework connecting the
embedding space representations with worst-group error. To address this
limitation, we propose Spurious Correlation-Aware Embedding Regularization for
Worst-Group Robustness (SCER), a novel approach that directly regularizes
feature representations to suppress spurious cues. We show theoretically that
worst-group error is influenced by how strongly the classifier relies on
spurious versus core directions, identified from differences in group-wise mean
embeddings across domains and classes. By imposing theoretical constraints at
the embedding level, SCER encourages models to focus on core features while
reducing sensitivity to spurious patterns. Through systematic evaluation on
multiple vision and language, we show that SCER outperforms prior
state-of-the-art studies in worst-group accuracy. Our code is available at
\href{https://github.com/MLAI-Yonsei/SCER}{https://github.com/MLAI-Yonsei/SCER}.

</details>


### [80] [Federated Stochastic Minimax Optimization under Heavy-Tailed Noises](https://arxiv.org/abs/2511.04456)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中非凸-PL极小极大优化问题在重尾梯度噪声下的求解方法，提出了两种新算法Fed-NSGDA-M和FedMuon-DA，并证明其在较弱条件下具有理论收敛性，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 标准有界方差假设在非凸随机优化中不够现实，而大量实证研究表明重尾噪声更符合实际情况，因此本文旨在解决联邦学习中非凸-PL极小极大优化在重尾梯度噪声下的挑战。

Method: 提出两种新算法：Fed-NSGDA-M（采用归一化梯度）和FedMuon-DA（利用Muon优化器进行本地更新），以应对联邦极小极大优化中的重尾噪声。

Result: 两种算法在较温和的条件下均实现了$O({1}/{(TNp)^{\frac{s-1}{2s}}})$的收敛速率，是首个在重尾噪声下具有严格理论保证的联邦极小极大优化算法，实验结果也验证了其有效性。

Conclusion: 本文提出的Fed-NSGDA-M和FedMuon-DA算法有效解决了联邦学习中非凸-PL极小极大优化在重尾噪声下的收敛问题，兼具理论保证与实践效果。

Abstract: Heavy-tailed noise has attracted growing attention in nonconvex stochastic
optimization, as numerous empirical studies suggest it offers a more realistic
assumption than standard bounded variance assumption. In this work, we
investigate nonconvex-PL minimax optimization under heavy-tailed gradient noise
in federated learning. We propose two novel algorithms: Fed-NSGDA-M, which
integrates normalized gradients, and FedMuon-DA, which leverages the Muon
optimizer for local updates. Both algorithms are designed to effectively
address heavy-tailed noise in federated minimax optimization, under a milder
condition. We theoretically establish that both algorithms achieve a
convergence rate of $O({1}/{(TNp)^{\frac{s-1}{2s}}})$. To the best of our
knowledge, these are the first federated minimax optimization algorithms with
rigorous theoretical guarantees under heavy-tailed noise. Extensive experiments
further validate their effectiveness.

</details>


### [81] [Towards Causal Market Simulators](https://arxiv.org/abs/2511.04469)
*Dennis Thumm,Luis Ontaneda Mijares*

Main category: cs.LG

TL;DR: 提出了一种结合变分自编码器与结构因果模型的时间序列神经因果模型（TNCM-VAE），用于生成保留时间依赖性和因果关系的反事实金融时间序列。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度生成模型的市场生成器缺乏因果推理能力，难以支持反事实分析和风险评估。

Method: 在解码器架构中引入有向无环图以施加因果约束，并采用因果Wasserstein距离进行训练。

Result: 在受Ornstein-Uhlenbeck过程启发的合成自回归模型上验证，反事实概率估计的L1距离低至0.03–0.10，优于基线方法。

Conclusion: 该模型能生成符合底层因果机制的合理反事实市场轨迹，适用于金融压力测试、情景分析和增强回测。

Abstract: Market generators using deep generative models have shown promise for
synthetic financial data generation, but existing approaches lack causal
reasoning capabilities essential for counterfactual analysis and risk
assessment. We propose a Time-series Neural Causal Model VAE (TNCM-VAE) that
combines variational autoencoders with structural causal models to generate
counterfactual financial time series while preserving both temporal
dependencies and causal relationships. Our approach enforces causal constraints
through directed acyclic graphs in the decoder architecture and employs the
causal Wasserstein distance for training. We validate our method on synthetic
autoregressive models inspired by the Ornstein-Uhlenbeck process, demonstrating
superior performance in counterfactual probability estimation with L1 distances
as low as 0.03-0.10 compared to ground truth. The model enables financial
stress testing, scenario analysis, and enhanced backtesting by generating
plausible counterfactual market trajectories that respect underlying causal
mechanisms.

</details>


### [82] [Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training](https://arxiv.org/abs/2511.04485)
*Ipsita Ghosh,Ethan Nguyen,Christian Kümmerle*

Main category: cs.LG

TL;DR: 本文提出了一种名为Q3R的新型低秩诱导训练策略，通过二次重加权秩正则化，在保持低秩结构的同时实现高效参数训练，能够在显著减少模型参数的情况下维持与稠密模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于低秩优化的参数高效训练方法在低秩预训练任务中表现不佳，难以同时维持低秩结构和优化目标，因此需要一种新的训练策略来解决这一挑战。

Method: 提出Q3R（Quadratic Reweighted Rank Regularizer）方法，该方法基于二次正则项，对平滑后的log行列式进行控制，作为秩的替代目标，并受迭代重加权最小二乘（IRLS）框架启发，实现低秩诱导训练。

Result: 在ViT-Tiny模型上成功裁剪60%和80%的参数，仅带来约1.3%和4%的CIFAR-10准确率下降；在图像和语言任务的Transformer模型上均验证了Q3R的有效性，包括低秩微调场景。

Conclusion: Q3R是一种高效且兼容性强的低秩训练方法，能够在大幅压缩模型参数的同时保持高性能，适用于多种架构和任务。

Abstract: Parameter-efficient training, based on low-rank optimization, has become a
highly successful tool for fine-tuning large deep-learning models. However,
these methods fail at low-rank pre-training tasks where maintaining the
low-rank structure and the objective remains a challenging task. We propose the
Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel
low-rank inducing training strategy inspired by the iteratively reweighted
least squares (IRLS) framework. Q3R is based on a quadratic regularizer term
which majorizes a smoothed log determinant serving as rank surrogate objective.
Unlike other low-rank training techniques, Q3R is able to train weight matrices
with prescribed, low target ranks of models that achieve comparable predictive
performance as dense models, with small computational overhead, while remaining
fully compatible with existing architectures. For example, we demonstrated one
experiment where we are able to truncate $60\%$ and $80\%$ of the parameters of
a ViT-Tiny model with $~1.3\%$ and $~4\%$ accuracy drop in CIFAR-10 performance
respectively. The efficacy of Q3R is confirmed on Transformers across both
image and language tasks, including for low-rank fine-tuning.

</details>


### [83] [Alternative Fairness and Accuracy Optimization in Criminal Justice](https://arxiv.org/abs/2511.04505)
*Shaolong Wu,James Blume,Geshi Yeung*

Main category: cs.LG

TL;DR: 本文提出一种改进的群体公平方法，通过在容忍范围内控制假阴性率差异并最小化加权误差损失，以提升预测准确性并明确伦理权衡，同时结合实践框架指导公共决策系统中的算法部署。


<details>
  <summary>Details</summary>
Motivation: 算法公平性研究虽快速发展，但在刑事司法等领域核心概念仍未统一，不同公平定义之间常存在冲突，且现有方法在实践中面临数据偏差、次级群体约束爆炸等问题。

Method: 对标准群体公平进行修改，不再要求受保护群体间的完全均衡，而是在假阴性率差异不超过小容忍度的前提下，最小化加权误差损失；并构建一个包含需求导向决策、透明问责和精准定义三支柱的实践部署框架。

Result: 该方法更易找到可行解，可能提高预测准确率，并显式呈现误差成本的伦理选择；同时为使用风险评估工具的公共机构提供可操作的技术与制度结合方案。

Conclusion: 将技术设计与合法性相连接，通过兼顾公平性、准确性和可实施性的综合路径，为公共决策系统中的算法公平提供务实指导。

Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts
remain unsettled, especially in criminal justice. We review group, individual,
and process fairness and map the conditions under which they conflict. We then
develop a simple modification to standard group fairness. Rather than exact
parity across protected groups, we minimize a weighted error loss while keeping
differences in false negative rates within a small tolerance. This makes
solutions easier to find, can raise predictive accuracy, and surfaces the
ethical choice of error costs. We situate this proposal within three classes of
critique: biased and incomplete data, latent affirmative action, and the
explosion of subgroup constraints. Finally, we offer a practical framework for
deployment in public decision systems built on three pillars: need-based
decisions, Transparency and accountability, and narrowly tailored definitions
and solutions. Together, these elements link technical design to legitimacy and
provide actionable guidance for agencies that use risk assessment and related
tools.

</details>


### [84] [Linear Mode Connectivity under Data Shifts for Deep Ensembles of Image Classifiers](https://arxiv.org/abs/2511.04514)
*C. Hepburn,T. Zielke,A. P. Raulf*

Main category: cs.LG

TL;DR: 本文研究了数据分布偏移下线性模式连通性（LMC）的表现，发现小学习率和大批量可减轻偏移影响，使模型收敛至相似极小值；尽管LMC采样模型错误更趋一致，但其在训练效率与集成多样性之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 探究数据分布偏移对线性模式连通性（LMC）的影响，并寻找缓解该影响的条件，以更好地理解LMC在深度学习中的作用及其在实际应用中的鲁棒性。

Method: 通过实验分析不同学习率和批量大小设置下模型在数据偏移情形中的LMC行为，将数据偏移视为额外的随机梯度噪声来源，并考察模型是否收敛到相同或不同性质的损失景观区域。

Result: 小学习率和大批量能有效降低数据偏移对LMC的负面影响，促使模型收敛至具有相似平滑性和泛化能力的局部极小值；LMC采样模型虽错误一致性较高，但在训练效率与集成性能间取得较好权衡。

Conclusion: 线性模式连通性受数据偏移影响，但可通过优化训练超参数加以控制；LMC提供了一种高效构建模型集成的方法，在保持训练效率的同时仍具备一定泛化优势。

Abstract: The phenomenon of linear mode connectivity (LMC) links several aspects of
deep learning, including training stability under noisy stochastic gradients,
the smoothness and generalization of local minima (basins), the similarity and
functional diversity of sampled models, and architectural effects on data
processing. In this work, we experimentally study LMC under data shifts and
identify conditions that mitigate their impact. We interpret data shifts as an
additional source of stochastic gradient noise, which can be reduced through
small learning rates and large batch sizes. These parameters influence whether
models converge to the same local minimum or to regions of the loss landscape
with varying smoothness and generalization. Although models sampled via LMC
tend to make similar errors more frequently than those converging to different
basins, the benefit of LMC lies in balancing training efficiency against the
gains achieved from larger, more diverse ensembles. Code and supplementary
materials will be made publicly available at https://github.com/DLR-KI/LMC in
due course.

</details>


### [85] [Uncertainty Quantification for Reduced-Order Surrogate Models Applied to Cloud Microphysics](https://arxiv.org/abs/2511.04534)
*Jonas E. Katona,Emily K. de Jong,Nipun Gunawardena*

Main category: cs.LG

TL;DR: 本文提出了一种适用于降阶模型（ROM）的通用、事后不确定性量化框架，基于保形预测，无需修改模型结构或训练过程，能对潜空间动力学、重构及端到端预测提供准确的统计预测区间。


<details>
  <summary>Details</summary>
Motivation: 现有ROM缺乏通用且灵活的不确定性量化方法，多数方法依赖特定架构或训练流程，限制了其泛化能力。

Method: 采用保形预测（conformal prediction）构建一种模型无关的事后不确定性量化框架，在潜空间ROM中对多个组件（潜空间动力学、重构和端到端预测）估计统计预测区间。

Result: 在云微物理潜空间动力学模型上的实验表明，该方法能准确预测液滴尺寸分布的演化，并在整个ROM流程中有效量化不确定性。

Conclusion: 所提框架为ROM提供了一种灵活、通用且无需修改原有模型的不确定性量化手段，具有良好的实用性和推广潜力。

Abstract: Reduced-order models (ROMs) can efficiently simulate high-dimensional
physical systems, but lack robust uncertainty quantification methods. Existing
approaches are frequently architecture- or training-specific, which limits
flexibility and generalization. We introduce a post hoc, model-agnostic
framework for predictive uncertainty quantification in latent space ROMs that
requires no modification to the underlying architecture or training procedure.
Using conformal prediction, our approach estimates statistical prediction
intervals for multiple components of the ROM pipeline: latent dynamics,
reconstruction, and end-to-end predictions. We demonstrate the method on a
latent space dynamical model for cloud microphysics, where it accurately
predicts the evolution of droplet-size distributions and quantifies uncertainty
across the ROM pipeline.

</details>


### [86] [Integrating Temporal and Structural Context in Graph Transformers for Relational Deep Learning](https://arxiv.org/abs/2511.04557)
*Divyansha Lachi,Mahmoud Mohammadi,Joe Meyer,Vinam Arora,Tom Palczewski,Eva L. Dyer*

Main category: cs.LG

TL;DR: 本文提出了关系图感知器（RGP），一种结合结构与时间上下文的图Transformer架构，通过时间子图采样和交叉注意力机制，在多个任务上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有图模型主要关注空间结构，将时间信息仅作为过滤约束而非建模信号，且通常只支持单任务预测，难以处理医疗、金融和电商等领域中复杂的时空依赖和多任务需求。

Method: 提出时间子图采样器以捕获全局时序相关关系，并设计关系图感知器（RGP）架构，利用基于交叉注意力的潜在瓶颈整合结构与时间上下文，同时采用灵活的交叉注意力解码器支持多任务联合学习。

Result: 在RelBench、SALT和CTU数据集上的实验表明，RGP在多种预测任务中达到当前最优性能。

Conclusion: RGP提供了一种通用且可扩展的关系深度学习解决方案，能够有效建模复杂时空依赖并支持多样化的预测任务。

Abstract: In domains such as healthcare, finance, and e-commerce, the temporal dynamics
of relational data emerge from complex interactions-such as those between
patients and providers, or users and products across diverse categories. To be
broadly useful, models operating on these data must integrate long-range
spatial and temporal dependencies across diverse types of entities, while also
supporting multiple predictive tasks. However, existing graph models for
relational data primarily focus on spatial structure, treating temporal
information merely as a filtering constraint to exclude future events rather
than a modeling signal, and are typically designed for single-task prediction.
To address these gaps, we introduce a temporal subgraph sampler that enhances
global context by retrieving nodes beyond the immediate neighborhood to capture
temporally relevant relationships. In addition, we propose the Relational Graph
Perceiver (RGP), a graph transformer architecture for relational deep learning
that leverages a cross-attention-based latent bottleneck to efficiently
integrate information from both structural and temporal contexts. This latent
bottleneck integrates signals from different node and edge types into a common
latent space, enabling the model to build global context across the entire
relational system. RGP also incorporates a flexible cross-attention decoder
that supports joint learning across tasks with disjoint label spaces within a
single model. Experiments on RelBench, SALT, and CTU show that RGP delivers
state-of-the-art performance, offering a general and scalable solution for
relational deep learning with support for diverse predictive tasks.

</details>


### [87] [ARETE: an R package for Automated REtrieval from TExt with large language models](https://arxiv.org/abs/2511.04573)
*Vasco V. Branco,Jandó Benedek,Lidia Pivovarova,Luís Correia,Pedro Cardoso*

Main category: cs.LG

TL;DR: ARETE 是一个基于大语言模型（如 ChatGPT）的开源 R 包，用于自动从科学文献和灰色文献中提取物种出现数据，显著提升数据获取效率，并在蜘蛛物种案例中将已知分布范围平均扩大三个数量级。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏关键物种（尤其是出现记录）的数据，严格的保护措施难以实施；同时，人为活动加速了对新信息收集与处理的需求。现有文献中的相关数据多为非结构化文本，需大量人工提取。

Method: 开发了 ARETE R 包，整合光学字符识别（OCR）、大语言模型驱动的出现记录提取、异常值检测及表格化输出等步骤，并通过与人工标注结果的系统比较验证其准确性。

Result: 在100种蜘蛛的测试中，ARETE 自动提取的数据使已知分布范围（Extent of Occurrence）平均扩大三个数量级，揭示了以往未知的历史分布区域。

Conclusion: ARETE 能快速挖掘以往难以利用的物种出现数据，极大提升保护生物学研究效率，有助于优化资源分配、支持空间保护规划和灭绝风险评估。

Abstract: 1. A hard stop for the implementation of rigorous conservation initiatives is
our lack of key species data, especially occurrence data. Furthermore,
researchers have to contend with an accelerated speed at which new information
must be collected and processed due to anthropogenic activity. Publications
ranging from scientific papers to gray literature contain this crucial
information but their data are often not machine-readable, requiring extensive
human work to be retrieved. 2. We present the ARETE R package, an open-source
software aiming to automate data extraction of species occurrences powered by
large language models, namely using the chatGPT Application Programming
Interface. This R package integrates all steps of the data extraction and
validation process, from Optical Character Recognition to detection of outliers
and output in tabular format. Furthermore, we validate ARETE through systematic
comparison between what is modelled and the work of human annotators. 3. We
demonstrate the usefulness of the approach by comparing range maps produced
using GBIF data and with those automatically extracted for 100 species of
spiders. Newly extracted data allowed to expand the known Extent of Occurrence
by a mean three orders of magnitude, revealing new areas where the species were
found in the past, which mayhave important implications for spatial
conservation planning and extinction risk assessments. 4. ARETE allows faster
access to hitherto untapped occurrence data, a potential game changer in
projects requiring such data. Researchers will be able to better prioritize
resources, manually verifying selected species while maintaining automated
extraction for the majority. This workflow also allows predicting available
bibliographic data during project planning.

</details>


### [88] [Complexity as Advantage: A Regret-Based Perspective on Emergent Structure](https://arxiv.org/abs/2511.04590)
*Oshri Naparstek*

Main category: cs.LG

TL;DR: 该论文提出了“复杂性即优势”（CAA）框架，将系统的复杂性定义为不同观察者在预测系统时所产生的差异性后悔程度，而非系统的内在属性，并表明这种观点能统一多种涌现行为的概念。


<details>
  <summary>Details</summary>
Motivation: 传统复杂性度量往往将复杂性视为系统固有属性，忽略了观察者的作用；作者旨在引入一种以观察者为中心的复杂性定义，以解释为何某些系统在功能上具有价值。

Method: 通过定义系统对不同观察者产生的预测后悔（predictive regret）差异来衡量复杂性，并结合多尺度熵、预测信息等概念进行理论整合，辅以简单动力学模型进行演示。

Result: 该框架成功统一了多种涌现行为的度量方式，并指出“有趣”的系统正是那些能在不同观察者之间制造差异化后悔的系统。

Conclusion: 复杂性可被视为一种信息优势，其功能性价值源于观察者之间的预测能力差异，这一观点对学习、演化和人工智能等领域具有启示意义。

Abstract: We introduce Complexity as Advantage (CAA), a framework that defines the
complexity of a system relative to a family of observers. Instead of measuring
complexity as an intrinsic property, we evaluate how much predictive regret a
system induces for different observers attempting to model it. A system is
complex when it is easy for some observers and hard for others, creating an
information advantage. We show that this formulation unifies several notions of
emergent behavior, including multiscale entropy, predictive information, and
observer-dependent structure. The framework suggests that "interesting" systems
are those positioned to create differentiated regret across observers,
providing a quantitative grounding for why complexity can be functionally
valuable. We demonstrate the idea through simple dynamical models and discuss
implications for learning, evolution, and artificial agents.

</details>


### [89] [Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems](https://arxiv.org/abs/2511.04594)
*Utkarsh U. Chavan,Prashant Trivedi,Nandyala Hemachandra*

Main category: cs.LG

TL;DR: 本文研究了在使用线性函数近似的去中心化多智能体随机最短路径问题（Dec-MASSPs）中的学习复杂性，首次提出了该设定下的后悔下界 $\Omega(\sqrt{K})$，揭示了此类问题的固有学习难度。


<details>
  <summary>Details</summary>
Motivation: 现有对随机最短路径（SSP）问题的学习研究主要集中于单智能体场景，而多智能体去中心化设定下的学习问题尚未充分探索；本文旨在填补这一空白。

Method: 作者在Dec-MASSPs中引入线性函数近似来建模状态转移和代价，并利用新颖的对称性论证分析最优策略结构，进而通过构造对任意智能体数量 $n$ 都难以学习的实例，推导出后悔下界。

Result: 得到了Dec-MASSPs在 $K$ 个回合下的后悔下界为 $\Omega(\sqrt{K})$，表明该问题具有本质上的学习困难。

Conclusion: 该工作阐明了去中心化控制在多智能体系统中的学习复杂性，为未来设计高效多智能体学习算法提供了理论基础和方向指引。

Abstract: Multi-agent systems (MAS) are central to applications such as swarm robotics
and traffic routing, where agents must coordinate in a decentralized manner to
achieve a common objective. Stochastic Shortest Path (SSP) problems provide a
natural framework for modeling decentralized control in such settings. While
the problem of learning in SSP has been extensively studied in single-agent
settings, the decentralized multi-agent variant remains largely unexplored. In
this work, we take a step towards addressing that gap. We study decentralized
multi-agent SSPs (Dec-MASSPs) under linear function approximation, where the
transition dynamics and costs are represented using linear models. Applying
novel symmetry-based arguments, we identify the structure of optimal policies.
Our main contribution is the first regret lower bound for this setting based on
the construction of hard-to-learn instances for any number of agents, $n$. Our
regret lower bound of $\Omega(\sqrt{K})$, over $K$ episodes, highlights the
inherent learning difficulty in Dec-MASSPs. These insights clarify the learning
complexity of decentralized control and can further guide the design of
efficient learning algorithms in multi-agent systems.

</details>


### [90] [Addressing divergent representations from causal interventions on neural networks](https://arxiv.org/abs/2511.04638)
*Satchel Grant,Simon Jerome Han,Alexa Tartaglini,Christopher Potts*

Main category: cs.LG

TL;DR: 该论文研究了机制可解释性中常用的因果干预方法是否会导致模型内部表征偏离其自然分布，并区分了“无害”与“有害”的偏离类型，进而提出一种改进的反事实潜在损失（CL loss）来缓解有害偏离，提升解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 探究因果干预在机制可解释性中的使用是否因造成模型表征分布偏移而影响解释的忠实性。

Method: 通过实证分析验证常见干预技术引起的分布偏移；理论分析区分无害与有害偏移；改进Counterfactual Latent (CL) 损失以约束干预靠近自然分布。

Result: 发现常用干预确实导致分布偏移；识别出两类偏移：无害（如权重零空间内或决策边界内协方差引起）和有害（激活隐藏路径并引发行为变化）；改进后的CL损失能有效减少有害偏移同时保留解释能力。

Conclusion: 通过约束干预使其更贴近模型自然状态下的表征分布，可提升机制可解释方法的可靠性。

Abstract: A common approach to mechanistic interpretability is to causally manipulate
model representations via targeted interventions in order to understand what
those representations encode. Here we ask whether such interventions create
out-of-distribution (divergent) representations, and whether this raises
concerns about how faithful their resulting explanations are to the target
model in its natural state. First, we demonstrate empirically that common
causal intervention techniques often do shift internal representations away
from the natural distribution of the target model. Then, we provide a
theoretical analysis of two classes of such divergences: `harmless' divergences
that occur in the null-space of the weights and from covariance within
behavioral decision boundaries, and `pernicious' divergences that activate
hidden network pathways and cause dormant behavioral changes. Finally, in an
effort to mitigate the pernicious cases, we modify the Counterfactual Latent
(CL) loss from Grant (2025) that regularizes interventions to remain closer to
the natural distributions, reducing the likelihood of harmful divergences while
preserving the interpretive power of interventions. Together, these results
highlight a path towards more reliable interpretability methods.

</details>


### [91] [Environment Agnostic Goal-Conditioning, A Study of Reward-Free Autonomous Learning](https://arxiv.org/abs/2511.04598)
*Hampus Åström,Elin Anna Topp,Jacek Malec*

Main category: cs.LG

TL;DR: 本文提出一种将常规强化学习环境转换为目标条件环境的方法，使智能体能在无外部奖励的情况下自主选择目标进行学习，并在与外部引导强化学习相当的训练时间内掌握任务。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过将强化学习环境转化为目标条件形式，使智能体能够在没有外部奖励信号的情况下自主设定目标并完成任务，从而实现通用、前置的智能体训练。

Method: 将常规强化学习环境改造为目标条件环境，允许智能体在训练过程中自主选择目标，且该方法与底层的离策略学习算法无关，具有环境无关性。

Result: 实验表明，尽管单个目标的性能存在不稳定性，但平均目标成功率随训练提升并趋于稳定；训练后的智能体可被指示前往环境中任意观测状态。

Conclusion: 该方法支持智能体在无奖励、环境无关的条件下进行通用训练，为后续特定任务部署提供灵活基础。

Abstract: In this paper we study how transforming regular reinforcement learning
environments into goal-conditioned environments can let agents learn to solve
tasks autonomously and reward-free. We show that an agent can learn to solve
tasks by selecting its own goals in an environment-agnostic way, at training
times comparable to externally guided reinforcement learning. Our method is
independent of the underlying off-policy learning algorithm. Since our method
is environment-agnostic, the agent does not value any goals higher than others,
leading to instability in performance for individual goals. However, in our
experiments, we show that the average goal success rate improves and
stabilizes. An agent trained with this method can be instructed to seek any
observations made in the environment, enabling generic training of agents prior
to specific use cases.

</details>


### [92] [Efficient probabilistic surrogate modeling techniques for partially-observed large-scale dynamical systems](https://arxiv.org/abs/2511.04641)
*Hans Harder,Abhijeet Vishwasrao,Luca Guastoni,Ricardo Vinuesa,Sebastian Peitz*

Main category: cs.LG

TL;DR: 本文研究并比较了多种流匹配（flow matching）方法的扩展，旨在减少采样步数，用于预测由偏微分方程描述的动力系统，并在多个复杂系统上进行了实验，包括直接预测大规模3D模拟的2D切片。


<details>
  <summary>Details</summary>
Motivation: 传统流匹配方法在预测由偏微分方程（如Navier-Stokes方程）描述的动力系统时需要较多采样步数，效率较低。因此，作者希望探索和比较能减少采样步数的改进方法，以提升预测效率和实用性，特别是在大规模3D模拟中生成2D切片的应用场景。

Method: 文章对比了多种流匹配范式的扩展方法，包括直接蒸馏（direct distillation）、渐进蒸馏（progressive distillation）、对抗扩散蒸馏（adversarial diffusion distillation）、Wasserstein GANs 和修正流（rectified flows），并在多个具有挑战性的动力系统上进行实验验证。

Result: 实验结果表明，这些扩展方法在减少采样步数的同时仍能有效预测动力系统行为，并成功实现了对大规模3D模拟的2D切片的直接预测，为高效流入生成提供了可行路径。

Conclusion: 所研究的流匹配扩展方法在保持预测准确性的同时显著提升了计算效率，尤其适用于复杂偏微分方程系统的建模与仿真，展示了其在实际工程和科学计算中的潜力。

Abstract: This paper is concerned with probabilistic techniques for forecasting
dynamical systems described by partial differential equations (such as, for
example, the Navier-Stokes equations). In particular, it is investigating and
comparing various extensions to the flow matching paradigm that reduce the
number of sampling steps. In this regard, it compares direct distillation,
progressive distillation, adversarial diffusion distillation, Wasserstein GANs
and rectified flows. Moreover, experiments are conducted on a set of
challenging systems. In particular, we also address the challenge of directly
predicting 2D slices of large-scale 3D simulations, paving the way for
efficient inflow generation for solvers.

</details>


### [93] [Optimal Inference Schedules for Masked Diffusion Models](https://arxiv.org/abs/2511.04647)
*Sitan Chen,Kevin Cong,Jerry Li*

Main category: cs.LG

TL;DR: 本文研究了掩码扩散语言模型（MDM）在并行采样中的性能限制，通过将其与单变量函数逼近理论建立联系，给出了真实分布与采样分布之间期望散度的精确刻画，并基于分布的信息论性质（如总相关性和对偶总相关性）提出了新的上下界和采样调度策略，在某些自然设定下可在 $O(\log n)$ 步内完成采样而无明显性能损失。


<details>
  <summary>Details</summary>
Motivation: 标准自回归大语言模型推理过程固有地串行，导致推理时间长且成本高。为解决此问题，研究者提出了扩散语言模型（尤其是掩码扩散模型 MDM），其支持乱序甚至并行采样，但目前对其并行采样能力缺乏严格理解，尤其在不显著降低采样质量的前提下能并行多少仍不清楚。

Method: 作者建立了 MDM 采样过程中真实分布与采样分布之间期望散度的精确表达式，并揭示其与单变量函数逼近理论之间的优雅联系；在此基础上，利用信息论中分布的总相关性和对偶总相关性等性质，推导出新的上下界及采样调度策略。

Result: 1. 给出了任意分布和任意去掩码调度下期望散度的精确刻画；  
2. 证明了在缺乏先验知识的情况下，无法普遍达到最优调度性能；  
3. 提出了基于总相关性和对偶总相关性的新上界和采样调度，在某些自然场景下可实现 $O(\log n)$ 步高效采样而无可见性能损失。

Conclusion: 尽管掩码扩散语言模型理论上支持高度并行采样，但实际性能受限于对目标分布的先验知识；然而，借助信息论工具，可在特定条件下设计出接近最优的高效采样策略，显著减少推理步骤而不牺牲生成质量。

Abstract: A major bottleneck of standard auto-regressive large language models is that
their inference process is inherently sequential, resulting in very long and
costly inference times. To circumvent this, practitioners proposed a class of
language models called diffusion language models, of which the masked diffusion
model (MDM) is the most successful. The MDM is able to sample tokens
out-of-order and, ostensibly, many tokens at once and in parallel. However,
there is very limited rigorous understanding of how much parallel sampling
these models can perform without noticeable degradation in their sampling
performance. Prior work of Li and Cai obtained some preliminary bounds, but
these are not tight for many natural classes of distributions. In this work, we
give a new, exact characterization of the expected divergence between the true
distribution and the sampled distribution, for any distribution and any
unmasking schedule for the sampler, showing an elegant connection to the theory
of univariate function approximation.
  By leveraging this connection, we then attain a number of novel lower and
upper bounds for this problem. While the connection to function approximation
in principle gives the optimal unmasking schedule for any distribution, we show
that it is in general impossible to compete with it without strong a priori
knowledge of the distribution, even in seemingly benign settings. However, we
also demonstrate new upper bounds and new sampling schedules in terms of
well-studied information-theoretic properties of the base distribution, namely,
its total correlation and dual total correlation, which show that in some
natural settings, one can sample in $O(log n)$ steps without any visible loss
in performance, where $n$ is the total sequence length.

</details>


### [94] [TT-Prune: Joint Model Pruning and Resource Allocation for Communication-efficient Time-triggered Federated Learning](https://arxiv.org/abs/2511.04653)
*Xinlu Zhang,Yansha Deng,Toktam Mahmoodi*

Main category: cs.LG

TL;DR: 本文将自适应模型剪枝引入时间触发联邦学习（TT-Fed）系统，联合优化剪枝率与无线带宽分配，在满足延迟约束的前提下最小化训练损失，并通过KKT条件推导出闭式解，实验表明该方法可在保持模型性能的同时降低40%通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统时间触发联邦学习（TT-Fed）在用户设备数量增长和无线带宽受限的环境下，面临滞后节点和通信开销增大的问题，亟需一种能兼顾训练效率与通信成本的优化策略。

Method: 作者对TT-Fed模型在模型剪枝下的梯度l_2范数进行收敛性分析，基于所得收敛上界构建剪枝率与无线带宽的联合优化问题，并利用KKT条件求解闭式解。

Result: 仿真实验表明，所提方法可在维持模型性能不变的前提下，将通信成本降低40%。

Conclusion: 将自适应模型剪枝与无线资源分配联合优化可有效提升TT-Fed系统的通信效率和训练性能，为资源受限场景下的联邦学习提供了可行方案。

Abstract: Federated learning (FL) offers new opportunities in machine learning,
particularly in addressing data privacy concerns. In contrast to conventional
event-based federated learning, time-triggered federated learning (TT-Fed), as
a general form of both asynchronous and synchronous FL, clusters users into
different tiers based on fixed time intervals. However, the FL network consists
of a growing number of user devices with limited wireless bandwidth,
consequently magnifying issues such as stragglers and communication overhead.
In this paper, we introduce adaptive model pruning to wireless TT-Fed systems
and study the problem of jointly optimizing the pruning ratio and bandwidth
allocation to minimize the training loss while ensuring minimal learning
latency. To answer this question, we perform convergence analysis on the
gradient l_2 norm of the TT-Fed model based on model pruning. Based on the
obtained convergence upper bound, a joint optimization problem of pruning ratio
and wireless bandwidth is formulated to minimize the model training loss under
a given delay threshold. Then, we derive closed-form solutions for wireless
bandwidth and pruning ratio using Karush-Kuhn-Tucker(KKT) conditions. The
simulation results show that model pruning could reduce the communication cost
by 40% while maintaining the model performance at the same level.

</details>


### [95] [Nowcast3D: Reliable precipitation nowcasting via gray-box learning](https://arxiv.org/abs/2511.04659)
*Huaguan Chen,Wei Han,Haofei Sun,Ning Lin,Xingtao Song,Yunfan Yang,Jie Tian,Yang Liu,Ji-Rong Wen,Xiaoye Zhang,Xueshun Shen,Hao Sun*

Main category: cs.LG

TL;DR: 本文提出了一种全新的三维灰箱雷达回波临近预报框架，融合物理约束与数据驱动方法，在保持物理一致性的前提下显著提升了极端降水三小时内预报的准确性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有极端降水临近预报方法存在明显局限：数值天气预报及其深度学习替代方案时空分辨率不足、响应慢；外推法和纯数据驱动模型误差累积严重且过度平滑；二维混合方法忽略关键垂直结构信息，难以准确刻画高度依赖的动力过程。

Method: 构建了一个完全三维的灰箱临近预报框架，直接处理体扫雷达反射率数据。该框架结合物理约束神经算子与数据驱动学习：在守恒平流算子下学习垂直变化的三维平流场，参数化空间变化的扩散项，并引入受布朗运动启发的随机项以表征未解析运动；通过残差分支捕捉对流初生和微物理变率，利用基于扩散的随机模块估计不确定性。

Result: 该框架在多种降水类型下均能实现更准确的三小时临近预报，在由160名气象专家参与的盲评中，57%的案例中排名第一。

Conclusion: 通过恢复具备物理一致性的完整三维动力结构，该研究为极端降水的高技巧、可靠临近预报提供了一条可扩展且稳健的技术路径。

Abstract: Extreme precipitation nowcasting demands high spatiotemporal fidelity and
extended lead times, yet existing approaches remain limited. Numerical Weather
Prediction (NWP) and its deep-learning emulations are too slow and coarse for
rapidly evolving convection, while extrapolation and purely data-driven models
suffer from error accumulation and excessive smoothing. Hybrid 2D radar-based
methods discard crucial vertical information, preventing accurate
reconstruction of height-dependent dynamics. We introduce a gray-box, fully
three-dimensional nowcasting framework that directly processes volumetric radar
reflectivity and couples physically constrained neural operators with
datadriven learning. The model learns vertically varying 3D advection fields
under a conservative advection operator, parameterizes spatially varying
diffusion, and introduces a Brownian-motion--inspired stochastic term to
represent unresolved motions. A residual branch captures small-scale convective
initiation and microphysical variability, while a diffusion-based stochastic
module estimates uncertainty. The framework achieves more accurate forecasts up
to three-hour lead time across precipitation regimes and ranked first in 57\%
of cases in a blind evaluation by 160 meteorologists. By restoring full 3D
dynamics with physical consistency, it offers a scalable and robust pathway for
skillful and reliable nowcasting of extreme precipitation.

</details>


### [96] [Forgetting is Everywhere](https://arxiv.org/abs/2511.04666)
*Ben Sanati,Thomas L. Lee,Trevor McInroe,Aidan Scannell,Nikolay Malkin,David Abel,Amos Storkey*

Main category: cs.LG

TL;DR: 本文提出了一种与算法和任务无关的遗忘理论，将遗忘定义为学习者对未来经验预测分布缺乏自洽性，并通过多类实验验证了该理论在不同学习场景中的普适性和重要性。


<details>
  <summary>Details</summary>
Motivation: 现有通用学习算法在适应新数据时容易遗忘已有知识，而长期以来缺乏一个统一的遗忘定义来揭示其内在机制。

Method: 提出一种算法与任务无关的理论，将遗忘刻画为预测分布的自洽性缺失，并据此构建衡量算法遗忘倾向的通用度量；通过涵盖分类、回归、生成建模和强化学习的综合实验进行验证。

Result: 实验证明遗忘现象广泛存在于各类学习任务中，并显著影响学习效率，验证了所提理论的有效性。

Conclusion: 该研究建立了对遗忘的原理性理解，为分析和提升通用学习算法的信息保留能力奠定了基础。

Abstract: A fundamental challenge in developing general learning algorithms is their
tendency to forget past knowledge when adapting to new data. Addressing this
problem requires a principled understanding of forgetting; yet, despite decades
of study, no unified definition has emerged that provides insights into the
underlying dynamics of learning. We propose an algorithm- and task-agnostic
theory that characterises forgetting as a lack of self-consistency in a
learner's predictive distribution over future experiences, manifesting as a
loss of predictive information. Our theory naturally yields a general measure
of an algorithm's propensity to forget. To validate the theory, we design a
comprehensive set of experiments that span classification, regression,
generative modelling, and reinforcement learning. We empirically demonstrate
how forgetting is present across all learning settings and plays a significant
role in determining learning efficiency. Together, these results establish a
principled understanding of forgetting and lay the foundation for analysing and
improving the information retention capabilities of general learning
algorithms.

</details>


### [97] [Multi-Method Analysis of Mathematics Placement Assessments: Classical, Machine Learning, and Clustering Approaches](https://arxiv.org/abs/2511.04667)
*Julian D. Allagan,Dasia A. Singleton,Shanae N. Perry,Gabrielle C. Morgan,Essence A. Morgan*

Main category: cs.LG

TL;DR: 本研究结合经典测验理论、机器学习与无监督聚类方法，对一份40题数学分班考试进行多角度评估，发现部分题目区分度不佳需替换，并建议采用两阶段测评及引入可解释的随机森林预测模型以优化分班决策。


<details>
  <summary>Details</summary>
Motivation: 为提升数学分班考试的科学性与有效性，作者采用多方法融合框架，系统评估试题质量、学生能力结构及预测模型性能，旨在为分班政策提供实证依据。

Method: 综合运用经典测验理论（CTT）分析题目区分度、随机森林与梯度提升等机器学习算法评估预测准确率，以及K均值聚类识别学生能力的自然分组结构。

Result: 55%题目具有良好区分度，30%需替换；第6题为最佳区分题；机器学习模型准确率达96–97.5%；聚类结果揭示42.5%为更合理的分界线，低于现行55%标准；两簇解高度稳定（ARI=0.855）。

Conclusion: 多方法整合能为数学分班考试提供稳健的实证基础，建议替换低区分度题目、实施两阶段评估，并结合具透明度的机器学习预测以优化分班流程。

Abstract: This study evaluates a 40-item mathematics placement examination administered
to 198 students using a multi-method framework combining Classical Test Theory,
machine learning, and unsupervised clustering. Classical Test Theory analysis
reveals that 55\% of items achieve excellent discrimination ($D \geq 0.40$)
while 30\% demonstrate poor discrimination ($D < 0.20$) requiring replacement.
Question 6 (Graph Interpretation) emerges as the examination's most powerful
discriminator, achieving perfect discrimination ($D = 1.000$), highest ANOVA
F-statistic ($F = 4609.1$), and maximum Random Forest feature importance
(0.206), accounting for 20.6\% of predictive power. Machine learning algorithms
demonstrate exceptional performance, with Random Forest and Gradient Boosting
achieving 97.5\% and 96.0\% cross-validation accuracy. K-means clustering
identifies a natural binary competency structure with a boundary at 42.5\%,
diverging from the institutional threshold of 55\% and suggesting potential
overclassification into remedial categories. The two-cluster solution exhibits
exceptional stability (bootstrap ARI = 0.855) with perfect lower-cluster
purity. Convergent evidence across methods supports specific refinements:
replace poorly discriminating items, implement a two-stage assessment, and
integrate Random Forest predictions with transparency mechanisms. These
findings demonstrate that multi-method integration provides a robust empirical
foundation for evidence-based mathematics placement optimization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [98] [Scaling Agent Learning via Experience Synthesis](https://arxiv.org/abs/2511.03773)
*Zhaorun Chen,Zhuokai Zhao,Kai Zhang,Bo Liu,Qi Qi,Yifan Wu,Tarun Kalluri,Sara Cao,Yuanhao Xiong,Haibo Tong,Huaxiu Yao,Hengduo Li,Jiacheng Zhu,Xian Li,Dawn Song,Bo Li,Jason Weston,Dat Huynh*

Main category: cs.AI

TL;DR: DreamGym 是一个统一框架，通过基于推理的经验模型合成多样化、可扩展的交互数据，以支持大语言模型智能体的在线强化学习训练，显著减少对昂贵真实环境交互的依赖，并在多种任务中取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在大语言模型智能体中的应用受限于高成本的环境交互、任务多样性不足、奖励信号不可靠及基础设施复杂等问题，阻碍了可扩展经验数据的收集。

Method: DreamGym 构建了一个基于推理的经验模型，用以模拟环境动态并生成一致的状态转移和反馈信号；结合初始化自离线真实数据并持续更新的经验回放缓冲区，以及自适应生成挑战性新任务的课程学习机制，实现高效在线强化学习。

Result: 在 WebArena 等非 RL 就绪任务上，DreamGym 超越所有基线方法超过 30%；在 RL 就绪但高成本环境中，仅使用合成交互即可媲美 GRPO 和 PPO 的性能；纯合成经验训练的策略迁移到真实环境时也显著提升性能，同时大幅减少真实交互需求。

Conclusion: DreamGym 提供了一种可扩展、高效的强化学习训练范式，能够有效降低对真实环境交互的依赖，并为通用强化学习提供可行的预热策略。

Abstract: While reinforcement learning (RL) can empower large language model (LLM)
agents by enabling self-improvement through interaction, its practical adoption
remains challenging due to costly rollouts, limited task diversity, unreliable
reward signals, and infrastructure complexity, all of which obstruct the
collection of scalable experience data. To address these challenges, we
introduce DreamGym, the first unified framework designed to synthesize diverse
experiences with scalability in mind to enable effective online RL training for
autonomous agents. Rather than relying on expensive real-environment rollouts,
DreamGym distills environment dynamics into a reasoning-based experience model
that derives consistent state transitions and feedback signals through
step-by-step reasoning, enabling scalable agent rollout collection for RL. To
improve the stability and quality of transitions, DreamGym leverages an
experience replay buffer initialized with offline real-world data and
continuously enriched with fresh interactions to actively support agent
training. To improve knowledge acquisition, DreamGym adaptively generates new
tasks that challenge the current agent policy, enabling more effective online
curriculum learning. Experiments across diverse environments and agent
backbones demonstrate that DreamGym substantially improves RL training, both in
fully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready
tasks like WebArena, DreamGym outperforms all baselines by over 30%. And in
RL-ready but costly settings, it matches GRPO and PPO performance using only
synthetic interactions. When transferring a policy trained purely on synthetic
experiences to real-environment RL, DreamGym yields significant additional
performance gains while requiring far fewer real-world interactions, providing
a scalable warm-start strategy for general-purpose RL.

</details>


### [99] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 本文系统评估了自然语言处理中的分词模型在汇编代码分析中的适用性，研究了不同分词策略对内在属性（如词汇压缩、语义保真度）和下游任务（如函数签名预测）性能的影响，发现分词器选择显著影响实际效果，且内在指标无法完全预测外在表现。


<details>
  <summary>Details</summary>
Motivation: 汇编代码的分词方法对其分析至关重要，但目前该领域研究不足；作者旨在填补这一空白，探索适合汇编代码特性的分词策略。

Method: 作者对多种NLP分词模型进行系统评估，包括定制预处理与预分词规则，并在Llama 3.2、BERT和BART等预训练模型上测试其在函数签名预测等任务中的表现，同时通过内在指标（分词效率、词汇压缩、表征保真度）和外在指标进行综合分析。

Result: 实验表明分词器的选择显著影响下游任务性能，内在评估指标虽具一定预测能力，但无法完全反映外在表现，揭示了内在属性与实际效用之间的复杂权衡。

Conclusion: 本研究为优化面向低级代码分析的分词模型提供了重要见解，有助于提升基于自然语言模型的二进制分析流程的鲁棒性与可扩展性。

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [100] [To See or To Read: User Behavior Reasoning in Multimodal LLMs](https://arxiv.org/abs/2511.03845)
*Tianning Dong,Luyi Ma,Varun Vasudevan,Jason Cho,Sushant Kumar,Kannan Achan*

Main category: cs.AI

TL;DR: BehaviorLens 是一个用于评估多模态大语言模型（MLLMs）在用户行为数据推理中不同模态表现的基准框架，发现图像表示（如散点图、流程图）相比文本能显著提升预测准确率。


<details>
  <summary>Details</summary>
Motivation: 探索在用户行为数据推理任务中，文本与图像哪种模态更有利于提升 MLLMs 的性能。

Method: 提出 BehaviorLens 框架，在六个 MLLMs 上使用真实购买序列数据，将交易数据表示为文本段落、散点图和流程图三种形式进行对比实验。

Result: 图像表示使 MLLMs 的下一次购买预测准确率相比文本表示提升了 87.5%，且无需额外计算开销。

Conclusion: 图像模态在用户行为数据推理任务中显著优于文本模态，表明合理选择数据表示形式可有效提升 MLLM 性能。

Abstract: Multimodal Large Language Models (MLLMs) are reshaping how modern agentic
systems reason over sequential user-behavior data. However, whether textual or
image representations of user behavior data are more effective for maximizing
MLLM performance remains underexplored. We present \texttt{BehaviorLens}, a
systematic benchmarking framework for assessing modality trade-offs in
user-behavior reasoning across six MLLMs by representing transaction data as
(1) a text paragraph, (2) a scatter plot, and (3) a flowchart. Using a
real-world purchase-sequence dataset, we find that when data is represented as
images, MLLMs next-purchase prediction accuracy is improved by 87.5% compared
with an equivalent textual representation without any additional computational
cost.

</details>


### [101] [Extracting Causal Relations in Deep Knowledge Tracing](https://arxiv.org/abs/2511.03948)
*Kevin Hong,Kia Karbasi,Gregory Pottie*

Main category: cs.AI

TL;DR: 本文挑战了关于深度知识追踪（DKT）模型性能优势源于其建模知识点间双向关系的主流观点，提出并验证了DKT实际上是通过隐式学习知识点间的因果依赖结构（即先决条件关系）来提升预测效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究普遍认为DKT优于传统知识追踪方法是因为它能捕捉知识点之间的双向关系，但作者质疑这一解释，旨在揭示DKT真正有效的机制。

Method: 作者将练习关系图剪枝为有向无环图（DAG），在Assistments数据集的因果子集上训练DKT，并利用DKT学到的表征提出一种新的DAG提取方法。

Result: 实验表明DKT的预测能力与因果结构高度一致，且新提出的DAG提取方法提供了支持该论点的实证证据。

Conclusion: DKT的有效性主要源于其对知识点之间因果依赖关系的近似建模能力，而非简单的双向关系映射。

Abstract: A longstanding goal in computational educational research is to develop
explainable knowledge tracing (KT) models. Deep Knowledge Tracing (DKT), which
leverages a Recurrent Neural Network (RNN) to predict student knowledge and
performance on exercises, has been proposed as a major advancement over
traditional KT methods. Several studies suggest that its performance gains stem
from its ability to model bidirectional relationships between different
knowledge components (KCs) within a course, enabling the inference of a
student's understanding of one KC from their performance on others. In this
paper, we challenge this prevailing explanation and demonstrate that DKT's
strength lies in its implicit ability to model prerequisite relationships as a
causal structure, rather than bidirectional relationships. By pruning exercise
relation graphs into Directed Acyclic Graphs (DAGs) and training DKT on causal
subsets of the Assistments dataset, we show that DKT's predictive capabilities
align strongly with these causal structures. Furthermore, we propose an
alternative method for extracting exercise relation DAGs using DKT's learned
representations and provide empirical evidence supporting our claim. Our
findings suggest that DKT's effectiveness is largely driven by its capacity to
approximate causal dependencies between KCs rather than simple relational
mappings.

</details>


### [102] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: 该研究发现，尽管提示语言和文化框架能影响大语言模型（LLMs）的输出，使其在一定程度上贴近特定国家的价值观，但模型仍系统性偏向荷兰、德国、美国和日本的文化价值观，难以真正代表全球文化多样性。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型是否能够反映其全球用户群体的文化多样性，尤其是在训练数据和优化目标存在不平衡的情况下。

Method: 对10个大语言模型使用来自霍夫斯泰德价值观调查模块和世界价值观调查的63个问题，翻译成11种语言，并以有无明确文化视角的方式构建提示进行测试。

Result: 提示语言和文化视角均会影响模型输出，但模型整体仍偏向特定国家的价值观；显式文化视角比仅使用目标语言更有效，而两者结合并未进一步提升效果。

Conclusion: 大语言模型虽能响应提示变化产生差异化的回答，但受制于固有的文化默认设定，尚无法充分代表全球文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [103] [Detecting Silent Failures in Multi-Agentic AI Trajectories](https://arxiv.org/abs/2511.04032)
*Divya Pathak,Harshit Kumar,Anuska Roy,Felix George,Mudit Verma,Pratibha Moogi*

Main category: cs.AI

TL;DR: 本文首次系统研究多智能体AI系统中的异常检测问题，构建了包含4,275和894条轨迹的两个基准数据集，并评估了监督与半监督方法（如XGBoost和SVDD）在该任务上的表现，准确率分别高达98%和96%。


<details>
  <summary>Details</summary>
Motivation: 多智能体AI系统由于大语言模型的非确定性，容易出现难以察觉的静默故障（如输出漂移、循环或细节缺失），亟需有效手段进行异常检测。

Method: 提出一种数据集构建流程，用于捕捉用户行为、智能体非确定性和大语言模型变异，并基于此流程构建并标注两个基准数据集；在这些数据集上对监督（XGBoost）和半监督（SVDD）异常检测方法进行基准测试。

Result: 监督方法XGBoost和半监督方法SVDD在异常检测任务中表现相当，准确率分别达到98%和96%。

Conclusion: 本研究为多智能体AI系统中的异常检测提供了首个系统性探索，贡献了数据集、基准方法和关键洞见，为后续研究奠定基础。

Abstract: Multi-Agentic AI systems, powered by large language models (LLMs), are
inherently non-deterministic and prone to silent failures such as drift,
cycles, and missing details in outputs, which are difficult to detect. We
introduce the task of anomaly detection in agentic trajectories to identify
these failures and present a dataset curation pipeline that captures user
behavior, agent non-determinism, and LLM variation. Using this pipeline, we
curate and label two benchmark datasets comprising \textbf{4,275 and 894}
trajectories from Multi-Agentic AI systems. Benchmarking anomaly detection
methods on these datasets, we show that supervised (XGBoost) and
semi-supervised (SVDD) approaches perform comparably, achieving accuracies up
to 98% and 96%, respectively. This work provides the first systematic study of
anomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,
and insights to guide future research.

</details>


### [104] [Interpreting Multi-Attribute Confounding through Numerical Attributes in Large Language Models](https://arxiv.org/abs/2511.04053)
*Hirohane Takagi,Gouki Minegishi,Shota Kizawa,Issey Sukeda,Hitomi Yanaka*

Main category: cs.AI

TL;DR: 该研究发现大语言模型（LLMs）在内部表示中会系统性地放大现实世界中的数值相关性，且无关的数值上下文会一致地干扰其数值表征，进而影响下游输出，揭示了LLMs在多属性纠缠下的决策脆弱性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有行为研究表明大语言模型在数值推理中存在错误，但其背后的表征机制尚不清楚。作者旨在探究LLMs如何整合单个实体的多个数值属性，以及无关数值上下文如何干扰这些表征及其下游输出。

Method: 结合线性探针、偏相关分析和基于提示的脆弱性测试，在不同规模的模型上进行实验。

Result: LLMs确实编码了现实世界的数值相关性，但会系统性地放大这些相关性；无关上下文会引起数值表征的一致性偏移，且这种影响因模型规模而异。

Conclusion: 研究揭示了大语言模型在多属性纠缠情境下的决策脆弱性，并为实现更公平、基于表征感知的控制方法奠定了基础。

Abstract: Although behavioral studies have documented numerical reasoning errors in
large language models (LLMs), the underlying representational mechanisms remain
unclear. We hypothesize that numerical attributes occupy shared latent
subspaces and investigate two questions:(1) How do LLMs internally integrate
multiple numerical attributes of a single entity? (2)How does irrelevant
numerical context perturb these representations and their downstream outputs?
To address these questions, we combine linear probing with partial correlation
analysis and prompt-based vulnerability tests across models of varying sizes.
Our results show that LLMs encode real-world numerical correlations but tend to
systematically amplify them. Moreover, irrelevant context induces consistent
shifts in magnitude representations, with downstream effects that vary by model
size. These findings reveal a vulnerability in LLM decision-making and lay the
groundwork for fairer, representation-aware control under multi-attribute
entanglement.

</details>


### [105] [Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://arxiv.org/abs/2511.04076)
*Hao Li,Haotian Chen,Ruoyuan Gong,Juanjuan Wang,Hao Jiang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为Agentmandering的新框架，将重划选区过程建模为两个代表对立政治利益的智能体之间的回合制谈判，利用大语言模型（LLM）实现战略互动，显著降低了党派偏见和不公平性，并在摇摆州中展现出更强的稳定性和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有计算方法虽能生成大量合法选区划分方案，但忽视了选择过程中的策略性动态，导致党派行为者可挑选对其有利的地图。仅满足形式约束无法确保公平，因此需要将策略互动纳入重划选区过程。

Method: 作者受博弈论中“选择-冻结”（Choose-and-Freeze）协议启发，设计了一个基于大语言模型智能体的框架Agentmandering。两个对立政治立场的智能体轮流从少量候选地图中选择并冻结选区，逐步完成整个州的划分。

Result: 在2020年美国人口普查数据上的评估表明，Agentmandering显著减少了党派偏见和不公平性，其结果方差比标准基线低2至3个数量级，尤其在摇摆州表现优异。

Conclusion: 将重划选区视为多方策略互动过程，可有效提升公平性与稳定性；Agentmandering提供了一种可行且可解释的新范式，有望改善当前选区划分实践。

Abstract: Redistricting plays a central role in shaping how votes are translated into
political power. While existing computational methods primarily aim to generate
large ensembles of legally valid districting plans, they often neglect the
strategic dynamics involved in the selection process. This oversight creates
opportunities for partisan actors to cherry-pick maps that, while technically
compliant, are politically advantageous. Simply satisfying formal constraints
does not ensure fairness when the selection process itself can be manipulated.
We propose \textbf{Agentmandering}, a framework that reimagines redistricting
as a turn-based negotiation between two agents representing opposing political
interests. Drawing inspiration from game-theoretic ideas, particularly the
\textit{Choose-and-Freeze} protocol, our method embeds strategic interaction
into the redistricting process via large language model (LLM) agents. Agents
alternate between selecting and freezing districts from a small set of
candidate maps, gradually partitioning the state through constrained and
interpretable choices. Evaluation on post-2020 U.S. Census data across all
states shows that Agentmandering significantly reduces partisan bias and
unfairness, while achieving 2 to 3 orders of magnitude lower variance than
standard baselines. These results demonstrate both fairness and stability,
especially in swing-state scenarios. Our code is available at
https://github.com/Lihaogx/AgentMandering.

</details>


### [106] [When Empowerment Disempowers](https://arxiv.org/abs/2511.04177)
*Claire Yang,Maya Cakmak,Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 该论文指出，在多用户环境中，以单一人类赋权（empowerment）为目标的辅助AI可能导致其他用户的“去赋权”（disempowerment），并提出了一个开源的多用户网格世界测试平台Disempower-Grid来研究此问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于赋权的辅助方法仅考虑单个用户，但在家庭、医院等真实多用户场景中，这种假设可能引发对其他用户的负面影响，因此需要研究多用户环境下的赋权机制及其潜在风险。

Method: 作者构建了一个开源的多用户网格世界测试套件Disempower-Grid，并通过实证实验分析单一用户赋权优化如何导致其他用户的环境影响力和奖励下降；同时探讨了联合赋权（joint empowerment）作为缓解策略的效果。

Result: 实验表明，针对单一人类优化赋权的辅助智能体会显著降低其他人类的环境影响力与奖励，即出现“去赋权”现象；而采用联合赋权可在一定程度上缓解该问题，但会牺牲目标用户的奖励。

Conclusion: 目标无关的赋权目标在单用户设定下看似对齐，但在多用户情境中可能引发错位，这揭示了AI对齐领域在多智能体环境中的更广泛挑战。

Abstract: Empowerment, a measure of an agent's ability to control its environment, has
been proposed as a universal goal-agnostic objective for motivating assistive
behavior in AI agents. While multi-human settings like homes and hospitals are
promising for AI assistance, prior work on empowerment-based assistance assumes
that the agent assists one human in isolation. We introduce an open source
multi-human gridworld test suite Disempower-Grid. Using Disempower-Grid, we
empirically show that assistive RL agents optimizing for one human's
empowerment can significantly reduce another human's environmental influence
and rewards - a phenomenon we formalize as disempowerment. We characterize when
disempowerment occurs in these environments and show that joint empowerment
mitigates disempowerment at the cost of the user's reward. Our work reveals a
broader challenge for the AI alignment community: goal-agnostic objectives that
seem aligned in single-agent settings can become misaligned in multi-agent
contexts.

</details>


### [107] [Opus: A Quantitative Framework for Workflow Evaluation](https://arxiv.org/abs/2511.04220)
*Alan Seroul,Théo Fagnoni,Inès Adnani,Dana O. Mohamed,Phillip Kingston*

Main category: cs.AI

TL;DR: 本文提出了Opus工作流评估框架，通过结合奖励模型与规范惩罚机制，对工作流的质量与效率进行概率-规范化的量化评估与优化。


<details>
  <summary>Details</summary>
Motivation: 现有工作流缺乏统一的量化评估方法，难以在正确性、可靠性与成本之间进行权衡，因此需要一个能支持自动化评估、排序与优化的数学框架。

Method: 构建Opus工作流奖励模型（基于成功概率、资源使用和输出收益的期望）与Opus规范惩罚函数（涵盖内聚性、耦合性、可观测性和信息卫生），并将其整合为统一的优化公式。

Result: 实现了对工作流的自动化评估、排序和优化，并可集成到强化学习循环中，用于指导工作流的发现与改进。

Conclusion: Opus工作流评估框架提供了一种系统化、可量化的手段来衡量和优化工作流，在现代自动化系统中具有广泛应用潜力。

Abstract: This paper introduces the Opus Workflow Evaluation Framework, a
probabilistic-normative formulation for quantifying Workflow quality and
efficiency. It integrates notions of correctness, reliability, and cost into a
coherent mathematical model that enables direct comparison, scoring, and
optimization of Workflows. The framework combines the Opus Workflow Reward, a
probabilistic function estimating expected performance through success
likelihood, resource usage, and output gain, with the Opus Workflow Normative
Penalties, a set of measurable functions capturing structural and informational
quality across Cohesion, Coupling, Observability, and Information Hygiene. It
supports automated Workflow assessment, ranking, and optimization within modern
automation systems such as Opus and can be integrated into Reinforcement
Learning loops to guide Workflow discovery and refinement. In this paper, we
introduce the Opus Workflow Reward model that formalizes Workflow success as a
probabilistic expectation over costs and outcomes. We define measurable Opus
Workflow Normative Penalties capturing structural, semantic, and signal-related
properties of Workflows. Finally, we propose a unified optimization formulation
for identifying and ranking optimal Workflows under joint Reward-Penalty
trade-offs.

</details>


### [108] [Shared Spatial Memory Through Predictive Coding](https://arxiv.org/abs/2511.04235)
*Zhengru Fang,Yu Guo,Jingjing Wang,Yuang Zhang,Haonan An,Yinhai Wang,Yuguang Fang*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体预测编码框架，通过最小化智能体间的互不确定性实现高效协作，在带宽受限条件下显著优于基线方法，并展现出类似海马体社会位置细胞的神经表征。


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，由于局部可观测性和通信带宽有限，协调常常失败。因此需要一种机制使智能体能够高效共享和重建一致的空间记忆。

Method: 提出一个多智能体预测编码框架，将协调建模为最小化互不确定性的过程，采用信息瓶颈目标函数，促使智能体学习“何时、向谁、传递什么”；内部使用类网格细胞的空间编码进行自定位，并在此基础上发展出高效的通信机制与编码伙伴位置的神经群体（类社会位置细胞），结合分层强化学习策略主动探索以降低联合不确定性。

Result: 在Memory-Maze基准测试中，该方法在带宽从128降至4比特/步时，成功率仅从73.5%降至64.4%，而全广播基线则从67.6%骤降至28.6%，表现出更强的鲁棒性。

Conclusion: 该研究为复杂社会表征如何从统一的预测驱动中涌现提供了理论严谨且生物合理的基础，推动了社会集体智能的发展。

Abstract: Sharing and reconstructing a consistent spatial memory is a critical
challenge in multi-agent systems, where partial observability and limited
bandwidth often lead to catastrophic failures in coordination. We introduce a
multi-agent predictive coding framework that formulate coordination as the
minimization of mutual uncertainty among agents. Instantiated as an information
bottleneck objective, it prompts agents to learn not only who and what to
communicate but also when. At the foundation of this framework lies a
grid-cell-like metric as internal spatial coding for self-localization,
emerging spontaneously from self-supervised motion prediction. Building upon
this internal spatial code, agents gradually develop a bandwidth-efficient
communication mechanism and specialized neural populations that encode
partners' locations: an artificial analogue of hippocampal social place cells
(SPCs). These social representations are further enacted by a hierarchical
reinforcement learning policy that actively explores to reduce joint
uncertainty. On the Memory-Maze benchmark, our approach shows exceptional
resilience to bandwidth constraints: success degrades gracefully from 73.5% to
64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast
baseline collapses from 67.6% to 28.6%. Our findings establish a theoretically
principled and biologically plausible basis for how complex social
representations emerge from a unified predictive drive, leading to social
collective intelligence.

</details>


### [109] [RLoop: An Self-Improving Framework for Reinforcement Learning with Iterative Policy Initialization](https://arxiv.org/abs/2511.04285)
*Zeng Zhiyuan,Jiashuo Liu,Zhangyue Yin,Ge Zhang,Wenhao Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: RLoop is a self-improving reinforcement learning framework that combats overfitting and catastrophic forgetting by iteratively re-initializing policies using high-quality trajectories, significantly improving generalization and accuracy.


<details>
  <summary>Details</summary>
Motivation: Standard RLVR training suffers from policy over-specialization and catastrophic forgetting, leading to poor generalization despite high training rewards; valuable policy diversity across training steps is typically discarded.

Method: RLoop employs iterative policy initialization: it uses RL to explore solutions, filters successful trajectories into an expert dataset, and applies Rejection-sampling Fine-Tuning (RFT) to refine the initial policy for the next iteration.

Result: Experiments show RLoop reduces forgetting and enhances generalization, achieving a 9% increase in average accuracy and over 15% improvement in pass@32 compared to standard RL.

Conclusion: By converting transient policy variations into stable improvements through iterative exploration and exploitation, RLoop effectively addresses RL overfitting and boosts model performance in reasoning tasks.

Abstract: While Reinforcement Learning for Verifiable Rewards (RLVR) is powerful for
training large reasoning models, its training dynamics harbor a critical
challenge: RL overfitting, where models gain training rewards but lose
generalization. Our analysis reveals this is driven by policy
over-specialization and catastrophic forgetting of diverse solutions generated
during training. Standard optimization discards this valuable inter-step policy
diversity. To address this, we introduce RLoop, a self-improving framework
built on iterative policy initialization. RLoop transforms the standard
training process into a virtuous cycle: it first uses RL to explore the
solution space from a given policy, then filters the successful trajectories to
create an expert dataset. This dataset is used via Rejection-sampling
Fine-Tuning (RFT) to refine the initial policy, creating a superior starting
point for the next iteration. This loop of exploration and exploitation via
iterative re-initialization effectively converts transient policy variations
into robust performance gains. Our experiments show RLoop mitigates forgetting
and substantially improves generalization, boosting average accuracy by 9% and
pass@32 by over 15% compared to vanilla RL.

</details>


### [110] [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://arxiv.org/abs/2511.04307)
*Jian Mu,Chaoyun Zhang,Chiming Ni,Lu Wang,Bo Qiao,Kartik Mathur,Qianhui Wu,Yuhang Xie,Xiaojun Ma,Mengyu Zhou,Si Qin,Liqun Li,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.AI

TL;DR: 本文提出了GUI-360°，一个大规模、综合性的数据集与基准套件，旨在推动计算机使用智能体（CUAs）的发展。该数据集通过LLM增强的自动化流程构建，包含120万+操作步骤、截图、可访问性元数据、任务目标和推理轨迹，支持GUI定位、屏幕解析与动作预测三大任务，并揭示了当前视觉语言模型在这些任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用智能体（CUAs）研究面临三大挑战：缺乏真实世界的CUA任务、缺少用于多模态轨迹的自动收集与标注流程，以及缺乏统一的基准来联合评估GUI定位、屏幕解析和动作预测能力。

Method: 作者设计了一个基于大语言模型（LLM）增强的、高度自动化的数据构建流程，包括查询来源、环境模板构建、任务实例化、批量执行和LLM驱动的质量过滤。数据集涵盖数千条在Windows办公软件中执行的轨迹，包含高分辨率截图、可访问性元数据、任务目标、中间推理过程及成功/失败的操作轨迹。

Result: 在GUI-360°上对当前最先进的视觉语言模型进行基准测试，发现其在开箱即用情况下在GUI定位和动作预测方面存在显著不足；监督微调和强化学习能带来显著提升，但仍远未达到人类水平的可靠性。

Conclusion: GUI-360°填补了CUA研究中的关键空白，为GUI理解与交互提供了高质量、大规模的数据支持和统一评估基准。作者已公开发布该数据集及配套代码，以促进可复现研究并加速鲁棒桌面智能体的发展。

Abstract: We introduce GUI-360$^\circ$, a large-scale, comprehensive dataset and
benchmark suite designed to advance computer-using agents (CUAs). CUAs present
unique challenges and is constrained by three persistent gaps: a scarcity of
real-world CUA tasks, the lack of automated collection-and-annotation pipelines
for multi-modal trajectories, and the absence of a unified benchmark that
jointly evaluates GUI grounding, screen parsing, and action prediction.
  GUI-360$^\circ$ addresses these gaps with an LLM-augmented, largely automated
pipeline for query sourcing, environment-template construction, task
instantiation, batched execution, and LLM-driven quality filtering. The
released corpus contains over 1.2M executed action steps across thousands of
trajectories in popular Windows office applications, and includes
full-resolution screenshots, accessibility metadata when available,
instantiated goals, intermediate reasoning traces, and both successful and
failed action trajectories. The dataset supports three canonical tasks, GUI
grounding, screen parsing, and action prediction, and a hybrid GUI+API action
space that reflects modern agent designs. Benchmarking state-of-the-art
vision--language models on GUI-360$^\circ$ reveals substantial out-of-the-box
shortcomings in grounding and action prediction; supervised fine-tuning and
reinforcement learning yield significant gains but do not close the gap to
human-level reliability. We release GUI-360$^\circ$ and accompanying code to
facilitate reproducible research and accelerate progress on robust desktop
CUAs.
  The full dataset has been made public on
https://huggingface.co/datasets/vyokky/GUI-360.

</details>


### [111] [Probing the Probes: Methods and Metrics for Concept Alignment](https://arxiv.org/abs/2511.04312)
*Jacob Lysnæs-Larsen,Marte Eggen,Inga Strümke*

Main category: cs.AI

TL;DR: 该论文指出，在可解释AI中，仅依赖探针分类准确率来评估概念激活向量（CAV）是否忠实表示目标概念是不可靠的，因为高准确率可能源于对虚假相关性的捕捉。作者提出一种基于空间线性归因的新概念定位方法，并引入三类定量评估指标（硬准确率、分割得分和增强鲁棒性）来衡量概念对齐程度，强调应采用对齐导向的评估方式并根据模型架构与概念特性定制探针。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常以探针分类准确率作为判断CAV是否准确表示目标概念的标准，但作者发现这一指标不可靠，因为探针容易学习到虚假相关性而非真正目标概念，从而误导对模型可解释性的理解。

Method: 作者构建故意错位的探针以验证准确率的不可靠性；提出一种基于空间线性归因的新概念定位方法；引入三类定量指标（硬准确率、分割得分、增强鲁棒性）用于评估概念对齐；并与现有特征可视化技术进行比较。

Result: 实验表明，即使探针被故意设计为利用虚假相关性，其准确率仍接近标准探针；而具备平移不变性和空间对齐特性的探针能显著提升概念对齐度；所提新方法和评估指标能更可靠地检测和缓解概念错位问题。

Conclusion: 仅依赖探针准确率不足以评估CAV的有效性，应采用基于概念对齐的评估指标，并根据模型结构和目标概念特性定制探针设计，以提升可解释AI的可靠性。

Abstract: In explainable AI, Concept Activation Vectors (CAVs) are typically obtained
by training linear classifier probes to detect human-understandable concepts as
directions in the activation space of deep neural networks. It is widely
assumed that a high probe accuracy indicates a CAV faithfully representing its
target concept. However, we show that the probe's classification accuracy alone
is an unreliable measure of concept alignment, i.e., the degree to which a CAV
captures the intended concept. In fact, we argue that probes are more likely to
capture spurious correlations than they are to represent only the intended
concept. As part of our analysis, we demonstrate that deliberately misaligned
probes constructed to exploit spurious correlations, achieve an accuracy close
to that of standard probes. To address this severe problem, we introduce a
novel concept localization method based on spatial linear attribution, and
provide a comprehensive comparison of it to existing feature visualization
techniques for detecting and mitigating concept misalignment. We further
propose three classes of metrics for quantitatively assessing concept
alignment: hard accuracy, segmentation scores, and augmentation robustness. Our
analysis shows that probes with translation invariance and spatial alignment
consistently increase concept alignment. These findings highlight the need for
alignment-based evaluation metrics rather than probe accuracy, and the
importance of tailoring probes to both the model architecture and the nature of
the target concept.

</details>


### [112] [AdversariaLLM: A Unified and Modular Toolbox for LLM Robustness Research](https://arxiv.org/abs/2511.04316)
*Tim Beyer,Jonas Dornbusch,Jakob Steimle,Moritz Ladenburger,Leo Schwinn,Stephan Günnemann*

Main category: cs.AI

TL;DR: 为解决大语言模型（LLM）安全与鲁棒性研究中因实现、数据集和评估方法碎片化导致的可复现性和可比性难题，本文提出了 AdversariaLLM 工具箱，集成12种对抗攻击算法、7个基准数据集和多种开源LLM，并通过资源追踪、确定性结果和分布评估等功能提升研究的透明度与可复现性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全与鲁棒性研究生态存在实现碎片化、代码错误频发、评估方法不统一等问题，严重影响了研究成果的可复现性和横向比较，阻碍了领域进展。

Method: 开发名为 AdversariaLLM 的开源工具箱，整合12种对抗攻击算法、7个涵盖有害性、过度拒绝和实用性评估的基准数据集，支持通过 Hugging Face 接入多种开源LLM，并引入计算资源追踪、确定性执行和分布评估等机制；同时配套 JudgeZoo 评判包用于自动化评估。

Result: AdversariaLLM 提供了一个可扩展、可复现且功能完整的平台，显著提升了 LLM 对抗鲁棒性研究的标准化水平和实验一致性。

Conclusion: AdversariaLLM 通过统一框架和严谨设计，为 LLM 安全研究建立了可靠基础，有助于推动该领域向更透明、可比和可复现的方向发展。

Abstract: The rapid expansion of research on Large Language Model (LLM) safety and
robustness has produced a fragmented and oftentimes buggy ecosystem of
implementations, datasets, and evaluation methods. This fragmentation makes
reproducibility and comparability across studies challenging, hindering
meaningful progress. To address these issues, we introduce AdversariaLLM, a
toolbox for conducting LLM jailbreak robustness research. Its design centers on
reproducibility, correctness, and extensibility. The framework implements
twelve adversarial attack algorithms, integrates seven benchmark datasets
spanning harmfulness, over-refusal, and utility evaluation, and provides access
to a wide range of open-weight LLMs via Hugging Face. The implementation
includes advanced features for comparability and reproducibility such as
compute-resource tracking, deterministic results, and distributional evaluation
techniques. \name also integrates judging through the companion package
JudgeZoo, which can also be used independently. Together, these components aim
to establish a robust foundation for transparent, comparable, and reproducible
research in LLM safety.

</details>


### [113] [RxSafeBench: Identifying Medication Safety Issues of Large Language Models in Simulated Consultation](https://arxiv.org/abs/2511.04328)
*Jiahao Zhao,Luxin Xu,Minghuan Tan,Lichao Zhang,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: 本文提出了一个用于评估大语言模型（LLM）在临床问诊中用药安全性的框架，构建了包含药物禁忌、相互作用和适应症-药物对的数据库RxRisk DB，并在此基础上建立了高质量基准测试集RxSafeBench。实验表明当前LLM在整合隐含用药风险信息方面仍存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏真实世界数据来评估大语言模型在用药安全性方面的表现，且在模拟真实临床问诊场景中的相关评估仍不充分。

Method: 提出一个模拟和评估临床问诊的框架，生成嵌入用药风险的问诊对话，构建包含6,725个禁忌、28,781个药物相互作用和14,906个适应症-药物对的RxRisk DB数据库，并通过两阶段过滤策略构建包含2,443个高质量问诊场景的基准RxSafeBench；使用结构化多选题评估主流开源与闭源LLM在模拟患者情境下推荐安全药物的能力。

Result: 当前大语言模型在整合禁忌和药物相互作用知识方面表现不佳，尤其在风险信息为隐含而非显式时更为明显。

Conclusion: 该研究揭示了LLM在保障用药安全方面面临的关键挑战，为通过改进提示工程和任务特定微调提升系统可靠性提供了洞见，并首次提供了全面评估LLM用药安全性的基准RxSafeBench。

Abstract: Numerous medical systems powered by Large Language Models (LLMs) have
achieved remarkable progress in diverse healthcare tasks. However, research on
their medication safety remains limited due to the lack of real world datasets,
constrained by privacy and accessibility issues. Moreover, evaluation of LLMs
in realistic clinical consultation settings, particularly regarding medication
safety, is still underexplored. To address these gaps, we propose a framework
that simulates and evaluates clinical consultations to systematically assess
the medication safety capabilities of LLMs. Within this framework, we generate
inquiry diagnosis dialogues with embedded medication risks and construct a
dedicated medication safety database, RxRisk DB, containing 6,725
contraindications, 28,781 drug interactions, and 14,906 indication-drug pairs.
A two-stage filtering strategy ensures clinical realism and professional
quality, resulting in the benchmark RxSafeBench with 2,443 high-quality
consultation scenarios. We evaluate leading open-source and proprietary LLMs
using structured multiple choice questions that test their ability to recommend
safe medications under simulated patient contexts. Results show that current
LLMs struggle to integrate contraindication and interaction knowledge,
especially when risks are implied rather than explicit. Our findings highlight
key challenges in ensuring medication safety in LLM-based systems and provide
insights into improving reliability through better prompting and task-specific
tuning. RxSafeBench offers the first comprehensive benchmark for evaluating
medication safety in LLMs, advancing safer and more trustworthy AI-driven
clinical decision support.

</details>


### [114] [Monitor-Generate-Verify (MGV):Formalising Metacognitive Theory for Language Model Reasoning](https://arxiv.org/abs/2511.04341)
*Nick Oh,Fernand Gobet*

Main category: cs.AI

TL;DR: 本文提出Monitor-Generate-Verify（MGV）框架，将元认知理论形式化为计算规范，通过在生成前引入显式监控机制来弥补现有测试时推理架构中缺乏监控过程的缺陷，从而避免模型陷入次优推理路径。


<details>
  <summary>Details</summary>
Motivation: 现有Generate-Verify范式忽略了决定何时以及如何开始推理的监控过程，容易导致“前缀主导陷阱”，使模型过早锁定次优推理路径且难以恢复，造成约20%的准确率损失。

Method: 将Flavell及Nelson和Narens的元认知理论形式化为计算规范，构建包含监控（Monitor）、生成（Generate）和验证（Verify）三个阶段的MGV框架，在生成前进行难度评估与信心判断，并利用验证反馈优化后续监控。

Result: 虽未提供实证验证，但首次系统地将经典元认知理论转化为计算模型，为理解推理系统失败机制提供了理论词汇，并指出了未来测试时推理架构的具体改进方向。

Conclusion: 引入显式监控机制是提升测试时推理系统鲁棒性的关键，MGV框架为下一代推理架构设计提供了理论基础和结构化指导。

Abstract: Test-time reasoning architectures such as those following the Generate-Verify
paradigm -- where a model iteratively refines or verifies its own generated
outputs -- prioritise generation and verification but exclude the monitoring
processes that determine when and how reasoning should begin. This omission may
contribute to the prefix dominance trap, in which models commit early to
suboptimal reasoning paths and seldom recover, yielding roughly 20% accuracy
loss. We address this architectural gap by formalising Flavell's and Nelson and
Narens' metacognitive theories into computational specifications, proposing the
Monitor-Generate-Verify (MGV) framework. MGV extends the Generate-Verify
paradigm by adding explicit monitoring that captures metacognitive experiences
(from difficulty assessments to confidence judgements) before generation begins
and refines future monitoring through verification feedback. Though we present
no empirical validation, this work provides the first systematic computational
translation of foundational metacognitive theories, offering a principled
vocabulary for understanding reasoning system failures and suggesting specific
architectural interventions for future test-time reasoning designs.

</details>


### [115] [Post-Training LLMs as Better Decision-Making Agents: A Regret-Minimization Approach](https://arxiv.org/abs/2511.04393)
*Chanwoo Park,Ziyang Chen,Asuman Ozdaglar,Kaiqing Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为迭代后悔最小化微调（Iterative RMFT）的后训练方法，通过反复将低后悔决策轨迹蒸馏回基础大语言模型（LLM），显著提升了LLM在动态交互环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）虽被广泛用于决策任务，但其原始设计并非面向决策，导致在基本在线决策问题中表现不佳，难以实现低后悔或有效平衡探索与利用。

Method: Iterative RMFT是一种迭代式后训练流程：每轮中，模型生成多条决策轨迹，选取其中后悔值最低的k条，并以此对模型进行微调。该方法不依赖外部算法的动作序列或人工设计的思维链模板，而是直接利用后悔指标激发模型自身的决策能力和推理逻辑。

Result: 实验表明，Iterative RMFT在多种模型（包括数值输入/输出的Transformer、开源LLM及GPT-4o mini等闭源模型）上均显著提升决策性能，并能泛化至不同时间跨度、动作空间、奖励机制和自然语言上下文的任务。理论分析还证明，在简化设定下单层Transformer可实现无后悔学习。

Conclusion: Iterative RMFT提供了一个原则性强且通用的后训练框架，有效增强大语言模型的决策能力，同时避免了僵化的输出工程，支持灵活的自然语言推理训练信号。

Abstract: Large language models (LLMs) are increasingly deployed as "agents" for
decision-making (DM) in interactive and dynamic environments. Yet, since they
were not originally designed for DM, recent studies show that LLMs can struggle
even in basic online DM problems, failing to achieve low regret or an effective
exploration-exploitation tradeoff. To address this, we introduce Iterative
Regret-Minimization Fine-Tuning (Iterative RMFT), a post-training procedure
that repeatedly distills low-regret decision trajectories back into the base
model. At each iteration, the model rolls out multiple decision trajectories,
selects the k-lowest regret ones, and fine-tunes itself on them. Unlike prior
methods that (a) distill action sequences from known DM algorithms or (b) rely
on manually crafted chain-of-thought templates, our approach leverages the
regret metric to elicit the model's own DM ability and reasoning rationales.
This reliance on model-generated reasoning avoids rigid output engineering and
provides more flexible, natural-language training signals. Empirical results
show that Iterative RMFT improves LLMs' DM performance across diverse models -
from Transformers with numerical input/output, to open-weight LLMs, and
advanced closed-weight models like GPT-4o mini. Its flexibility in output and
reasoning formats enables generalization across tasks with varying horizons,
action spaces, reward processes, and natural-language contexts. Finally, we
provide theoretical insight showing that a single-layer Transformer under this
paradigm can act as a no-regret learner in a simplified setting. Overall,
Iterative RMFT offers a principled and general post-training framework for
enhancing LLMs' decision-making capabilities.

</details>


### [116] [The Peril of Preference: Why GRPO fails on Ordinal Rewards](https://arxiv.org/abs/2511.04439)
*Anisha Garg,Ganesh Venkatesh*

Main category: cs.AI

TL;DR: 本文提出了一种名为Correctness Relative Policy Optimization（CoRPO）的新算法，通过引入自适应基线机制，在使用序数奖励进行强化学习时避免对失败轨迹的错误正向强化，从而改进了Group-relative Policy Optimization（GRPO）在非二元反馈下的不足。实验表明，CoRPO在代码验证任务中具有更稳定的收敛性和更强的域外泛化能力。


<details>
  <summary>Details</summary>
Motivation: GRPO虽然结构简单、易于应用，但在利用非二元（如序数）奖励信号进行强化学习时存在缺陷：其基于组平均的基线可能导致对失败轨迹赋予正优势，从而强化错误行为。因此，亟需一种能更好处理丰富反馈信号的策略优化方法。

Method: 作者提出了CoRPO方法，该方法采用一个自适应基线：首先设定最低质量阈值，确保只有达到基本正确性的轨迹才可能被正向强化；当策略稳定满足该阈值后，基线自动切换为相对偏好模式，以进一步优化解的质量。

Result: 在代码验证任务上的实验表明，CoRPO相比GRPO展现出更稳定的训练收敛性以及更好的域外泛化性能。

Conclusion: CoRPO有效解决了GRPO在使用序数奖励时的缺陷，是推动大语言模型通过强化学习从丰富、多维反馈中学习新能力的重要一步，为未来实现更密集的逐步骤监督奠定了基础。

Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly
desirable for adapting LLMs to become experts at specific tasks. But this
simplicity also makes it ill-specified as we seek to enhance RL training with
richer, non-binary feedback. When using ordinal rewards to give partial credit,
GRPO's simplicity starts to hurt, as its group-average baseline often assigns a
positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new
formulation that solves this flaw. CoRPO uses an adaptive baseline that
enforces a minimum quality threshold, ensuring failed solutions are never
positively reinforced. Once the policy consistently meets this threshold, the
baseline automatically transitions to a relative preference mode, pushing the
model to find optimal solutions rather than just "acceptable" ones. We
empirically validate CoRPO on a code verification task, where it demonstrates
more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to
enable LLMs to learn genuinely new capabilities through reinforcement learning.
We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback
- progressing from binary to ordinal rewards in this work, and onward to
denser, per-step supervision.

</details>


### [117] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 该研究通过构建博弈论实验的数字孪生系统，评估了Llama、Mistral和Qwen三个开源大语言模型在模拟人类决策行为方面的表现，发现Llama能高保真地复现人类合作模式，而Qwen更符合纳什均衡预测；研究还展示了无需角色提示即可实现群体层面行为复制，并能生成可验证的新假设。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在健康、教育和法律等领域被用于决策，并用于模拟人类行为，但其与真实人类决策的契合程度尚不清楚。这种认知差距可能导致实际应用中的有害后果，或使LLMs在社会模拟中失效。

Method: 开发了一个博弈论实验的数字孪生系统，并引入了一套系统的提示与探测框架来评估机器行为。在未使用角色提示的情况下，对Llama、Mistral和Qwen三个开源模型进行了测试，并扩展至原始人类实验范围之外的新博弈配置，生成并预注册了可检验的假设。

Result: Llama模型高保真地再现了人类的合作行为模式，包括对理性选择理论的偏离；Qwen则更贴近纳什均衡的预测；研究成功在群体层面复现人类行为，并能系统探索未被实证检验的实验空间。

Conclusion: 经过适当校准的大语言模型能够有效复现人类群体行为模式，并为社会科学提供一种补充传统研究的新方法，可生成关于人类社会决策的新实证预测。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [118] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 该论文提出将自然语言查询中的歧义视为用户与系统协作解析查询的特性，而非缺陷，并构建了一个区分“协作性查询”与“非协作性查询”的框架。通过对15个流行数据集的分析，发现现有评估混合了不同类型的查询，不利于准确评估系统的执行或解释能力。研究倡导在表格数据自然语言接口的设计与评估中重视协作机制。


<details>
  <summary>Details</summary>
Motivation: 传统方法将自然语言查询中的歧义视为需要消除的问题，但作者认为应将其视为人机协作解析查询的机会。当前评估方法未能区分可协作解析的查询与不可解析的查询，导致对系统能力的误判。

Method: 提出一个原则性框架，用于区分协作性查询（可被合理解析）与非协作性查询（无法解析），并应用该框架对15个常用表格问答和分析数据集中的查询进行分类与分析。

Result: 分析发现现有数据集中存在大量未加区分的查询类型，这些混合类型既不适合评估系统执行准确性，也不适合评估其语义解释能力。

Conclusion: 应转变对歧义的看法，从“修复歧义”转向“通过协作解决查询”，从而推动更合理的自然语言表格接口设计与评估方法，并为未来研究指明方向。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [119] [Question the Questions: Auditing Representation in Online Deliberative Processes](https://arxiv.org/abs/2511.04588)
*Soham De,Lodewijk Gelauff,Ashish Goel,Smitha Milli,Ariel Procaccia,Alice Siu*

Main category: cs.AI

TL;DR: 本文提出了一种基于“正当代表”（JR）概念的审计框架，用于评估在审议过程中专家问答环节所选问题对参与者兴趣的代表性，并开发了高效算法实现该审计，同时比较了实际选择、整数线性规划和大语言模型生成的问题集的代表性。


<details>
  <summary>Details</summary>
Motivation: 在公民大会等审议过程中，由于时间限制只能从参与者提出的大量问题中选出少量问题向专家提问，如何选出最具代表性的子集是一个关键挑战。

Method: 引入基于社会选择理论中“正当代表”（JR）概念的审计框架；设计并实现了在通用效用设定下审计JR的算法，最优算法复杂度为O(mn log n)；将该方法应用于历史审议数据，对比实际问题、整数线性规划选取问题和大语言模型生成问题的代表性；并将方法集成到一个已广泛使用的在线审议平台中。

Result: 算法能有效审计问题集的代表性；实证分析显示大语言模型在支持审议过程方面既有潜力也存在当前局限；所提方法已在覆盖50多个国家的在线平台部署，便于实践者提升未来审议的代表性。

Conclusion: 通过结合计算社会科学与社会选择理论，本文为提升审议民主中问题选择的公平性和代表性提供了可扩展、实用的工具，并揭示了AI技术（如LLMs）在该场景中的应用边界。

Abstract: A central feature of many deliberative processes, such as citizens'
assemblies and deliberative polls, is the opportunity for participants to
engage directly with experts. While participants are typically invited to
propose questions for expert panels, only a limited number can be selected due
to time constraints. This raises the challenge of how to choose a small set of
questions that best represent the interests of all participants. We introduce
an auditing framework for measuring the level of representation provided by a
slate of questions, based on the social choice concept known as justified
representation (JR). We present the first algorithms for auditing JR in the
general utility setting, with our most efficient algorithm achieving a runtime
of $O(mn\log n)$, where $n$ is the number of participants and $m$ is the number
of proposed questions. We apply our auditing methods to historical
deliberations, comparing the representativeness of (a) the actual questions
posed to the expert panel (chosen by a moderator), (b) participants' questions
chosen via integer linear programming, (c) summary questions generated by large
language models (LLMs). Our results highlight both the promise and current
limitations of LLMs in supporting deliberative processes. By integrating our
methods into an online deliberation platform that has been used for over
hundreds of deliberations across more than 50 countries, we make it easy for
practitioners to audit and improve representation in future deliberations.

</details>


### [120] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: DR. WELL 是一个去中心化的神经符号框架，通过两阶段协商协议实现多智能体协作规划：先提出角色建议，再达成共识分配；各智能体随后独立生成并执行符号化计划，借助共享世界模型进行状态更新与自适应优化，在积木推动任务中显著提升任务完成率与协作效率。


<details>
  <summary>Details</summary>
Motivation: 在多智能体协作规划中，基于轨迹的协调容易因微小偏差引发冲突，而符号化规划可通过提升抽象层次、提供最小动作词汇来实现同步与集体进展。现有方法缺乏在部分可观测和有限通信条件下高效、可解释且可重用的协作机制。

Method: 提出 DR. WELL 框架，采用两阶段协商协议：1）智能体通过推理提出候选角色；2）在共识和环境约束下确定联合角色分配。分配后，各智能体独立生成并执行其角色的符号计划，不共享详细轨迹。通过共享世界模型对执行结果进行状态编码与更新，实现符号计划的动态调整与自优化。

Result: 在协作积木推动任务中，DR. WELL 能够跨回合自适应，动态世界模型捕捉到可重用模式，显著提高了任务完成率和执行效率；尽管引入一定时间开销，但通过协商与自我精炼演化出更高效的协作策略。

Conclusion: DR. WELL 通过符号化规划与去中心化协商机制，有效避免了轨迹级对齐的脆弱性，实现了可重用、可同步且可解释的高层协作，在部分可观测环境中展现出优越的适应性和效率。

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [121] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: 本文提出VeriCoT，一种神经符号方法，用于从链式思维（CoT）推理中提取并验证形式化逻辑论证，以识别错误推理并提升大语言模型在关键场景中的可信度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽能通过CoT进行多步推理，但无法可靠地自我验证其逻辑，即使答案正确，推理过程也可能存在缺陷，这在高风险场景中会削弱可信度。

Method: VeriCoT将CoT的每一步推理形式化为一阶逻辑，并识别支撑该推理的前提（来自源上下文、常识知识或先前推理步骤），利用符号表示供自动求解器验证逻辑有效性，同时保留自然语言前提以便人工或系统识别无根据或谬误的推理步骤。

Result: 在ProofWriter、LegalBench和BioASQ数据集上的实验表明，VeriCoT能有效识别错误推理，并可作为最终答案正确性的强预测指标；此外，其验证信号还可用于推理时自省、监督微调和基于偏好的优化，进一步提升推理的有效性与准确性。

Conclusion: VeriCoT通过结合神经与符号方法，显著提升了大语言模型推理过程的可验证性与可靠性，为高风险应用提供了更可信的推理机制。

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [122] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: 本文提出了OMPILOT，一种专用于将C++代码翻译为OpenMP并行代码的领域特定编码器-解码器Transformer模型，并引入新评估指标OMPBLEU。


<details>
  <summary>Details</summary>
Motivation: 传统规则系统在代码翻译中准确性和灵活性不足，而现有大语言模型虽有进展，但在OpenMP并行化任务上缺乏对函数级语义和并行结构语义的专门建模。

Method: OMPILOT采用定制的预训练目标，融合并行构造的语义信息，结合无监督与有监督学习策略，在函数级别进行代码翻译；同时提出OMPBLEU指标以更准确评估OpenMP翻译质量。

Result: OMPILOT在C++到OpenMP的翻译任务中展现出更强的鲁棒性和上下文理解能力，OMPBLEU能有效衡量并行构造的正确性与质量。

Conclusion: 通过领域定制的模型架构与评估指标，OMPILOT显著提升了代码并行化翻译的效果，为遗留代码迁移和跨语言并行优化提供了新思路。

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [123] [Stochastic Modeling for Energy-Efficient Edge Infrastructure](https://arxiv.org/abs/2511.03941)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 本文提出一种基于马尔可夫链的随机建模方法，用于分析边缘计算中的电源状态转换，并通过AI驱动的预测性电源调节策略提升能效。


<details>
  <summary>Details</summary>
Motivation: 边缘计算虽支持低延迟实时应用，但其分布式特性和设备有限的能源资源带来了电源管理挑战。

Method: 采用马尔可夫链对边缘计算中的电源状态转换进行建模，推导稳态概率并评估能耗；结合蒙特卡洛仿真与敏感性分析验证模型。

Result: AI驱动的预测性电源调节相比传统响应式方法能显著减少不必要的状态切换，优化异构边缘节点间的工作负载分配，降低设备间能耗差异，提升整体能效与系统响应能力。

Conclusion: 基于AI的电源管理策略通过预判工作负载需求并优化状态转换，显著提升了边缘计算环境下的能源效率和多节点协同能力。

Abstract: Edge Computing enables low-latency processing for real-time applications but
introduces challenges in power management due to the distributed nature of edge
devices and their limited energy resources. This paper proposes a stochastic
modeling approach using Markov Chains to analyze power state transitions in
Edge Computing. By deriving steady-state probabilities and evaluating energy
consumption, we demonstrate the benefits of AI-driven predictive power scaling
over conventional reactive methods. Monte Carlo simulations validate the model,
showing strong alignment between theoretical and empirical results. Sensitivity
analysis highlights how varying transition probabilities affect power
efficiency, confirming that predictive scaling minimizes unnecessary
transitions and improves overall system responsiveness. Our findings suggest
that AI-based power management strategies significantly enhance energy
efficiency by anticipating workload demands and optimizing state transitions.
Experimental results indicate that AI-based power management optimizes workload
distribution across heterogeneous edge nodes, reducing energy consumption
disparities between devices, improving overall efficiency, and enhancing
adaptive power coordination in multi-node environments.

</details>


### [124] [Parallel Spawning Strategies for Dynamic-Aware MPI Applications](https://arxiv.org/abs/2511.04268)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo,Sergio Iserte*

Main category: cs.DC

TL;DR: 本文提出了一种新颖的并行启动策略，通过让所有进程在资源重分配前协同启动，显著降低了MPI应用在动态伸缩（尤其是收缩）时的开销，在保持扩展性能的同时将收缩成本降低至少20倍。


<details>
  <summary>Details</summary>
Motivation: 动态资源管理对高性能计算系统至关重要，但现有MPI应用的可变性（malleability）方法存在重配置成本高或无法有效释放节点资源的问题。

Method: 提出一种新型并行启动策略：所有进程在资源重分配前协作完成启动，从而优化扩展与收缩过程。

Result: 该方法在扩展时最多仅有1.25倍开销，而在收缩时成本降低至少20倍，并在同构与异构系统及共享资源环境中均验证有效。

Conclusion: 所提策略有效克服了现有MPI可变性方法的局限性，显著提升了动态资源调整效率，有助于减少作业等待时间和系统整体完工时间。

Abstract: Dynamic resource management is an increasingly important capability of High
Performance Computing systems, as it enables jobs to adjust their resource
allocation at runtime. This capability has been shown to reduce workload
makespan, substantially decrease job waiting times and improve overall system
utilization. In this context, malleability refers to the ability of
applications to adapt to new resource allocations during execution. Although
beneficial, malleability incurs significant reconfiguration costs, making the
reduction of these costs an important research topic.
  Some existing methods for MPI applications respawn the entire application,
which is an expensive solution that avoids the reuse of original processes.
Other MPI methods reuse them, but fail to fully release unneeded processes when
shrinking, since some ranks within the same communicator remain active across
nodes, preventing the application from returning those nodes to the system.
This work overcomes both limitations by proposing a novel parallel spawning
strategy, in which all processes cooperate in spawning before redistribution,
thereby reducing execution time. Additionally, it removes shrinkage
limitations, allowing better adaptation of parallel systems to workload and
reducing their makespan. As a result, it preserves competitive expansion times
with at most a $1.25\times$ overhead, while enabling fast shrink operations
that reduce their cost by at least $20\times$. This strategy has been validated
on both homogeneous and heterogeneous systems and can also be applied in
shared-resource environments.

</details>


### [125] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: 本文提出了一种在低比特量化下实现动态稀疏推理的方法，通过zigzag量化布局、专用GEMV核和紧凑的运行时索引机制，在保持精度的同时显著提升LLM在端侧设备上的推理速度。


<details>
  <summary>Details</summary>
Motivation: 在终端设备上部署大语言模型（LLM）具有响应快、隐私保护和成本低等优势，但受限于移动和桌面GPU的内存与算力，高效执行困难。尽管LLM内部激活具有动态稀疏性，可减少计算量，但其与主流的分组量化方法不兼容，阻碍了在资源受限硬件上的应用。

Method: 提出一套技术方案，使动态稀疏推理能在低比特量化下有效运行：(1) 采用zigzag模式的量化布局，使权重排布与激活稀疏性一致并提升GPU内存局部性；(2) 设计适配该布局的专用GEMV核以充分利用并行计算单元；(3) 构建轻量级运行时机制，以极低开销收集稀疏索引。

Result: 在多种模型规模和硬件配置下，该方法在保持与稠密量化推理相当精度的同时，解码吞吐量最高提升1.55倍。

Conclusion: 结构化稀疏性与量化可以在商用GPU上有效共存，所提方法为在资源受限设备上高效部署大语言模型提供了可行路径。

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>


### [126] [A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems](https://arxiv.org/abs/2511.04523)
*Silvia Bonomi,Giovanni Farina,Roy Friedman,Eviatar B. Procaccia,Sebastien Tixeuil*

Main category: cs.DC

TL;DR: 本文提出一种基于MAPE-K架构的自保护分布式系统模型，引入新的概率性移动拜占庭故障（MBF）机制，用于刻画攻击动态并驱动系统自保护与重构策略，并通过数学分析和仿真验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统面临日益增长的安全威胁，传统拜占庭容错模型难以准确反映真实攻击场景，因此需要更贴近实际的动态安全建模方法。

Method: 在MAPE-K自适应架构的分析组件中嵌入一种新的概率性移动拜占庭故障（MBF）模型，结合感染传播率与自恢复率，对系统状态演化进行数学建模与仿真。

Result: 通过数学分析得出了拜占庭节点数量跨越阈值或系统恢复至安全状态所需的时间，并通过仿真实验展示了系统在不同参数下的行为特征。

Conclusion: 所提出的概率性MBF模型能更真实地刻画攻击动态，有助于提升分布式系统的自保护能力与安全韧性。

Abstract: Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.

</details>


### [127] [Resolving Conflicts with Grace: Dynamically Concurrent Universality](https://arxiv.org/abs/2511.04631)
*Petr Kuznetsov,Nathan Josia Schrodt*

Main category: cs.DC

TL;DR: 本文提出动态并发的概念，通过根据系统当前状态动态调整同步机制，仅在必要时使用强同步原语，并给出一种动态并发的通用构造方法。


<details>
  <summary>Details</summary>
Motivation: 传统分布式计算中，同步是可扩展性的主要障碍。现有方法往往静态处理冲突，而实际上许多操作仅在特定罕见状态下才发生冲突，因此需要一种能根据系统状态动态检测和处理冲突的机制。

Method: 定义“动态并发”概念：操作仅在当前系统状态下确实需要与并发操作仲裁时，才使用强同步原语；并基于此构建一个动态并发的通用构造。

Result: 提出了动态并发的形式化定义，并实现了一种能够根据运行时状态自适应调整同步强度的通用构造方法。

Conclusion: 动态并发能有效减少不必要的同步开销，提升分布式系统的可扩展性，为冲突检测和同步机制提供了更灵活、高效的新思路。

Abstract: Synchronization is the major obstacle to scalability in distributed
computing. Concurrent operations on the shared data engage in synchronization
when they encounter a \emph{conflict}, i.e., their effects depend on the order
in which they are applied. Ideally, one would like to detect conflicts in a
\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it
is very common that two concurrent operations conflict only in some rarely
occurring states. In this paper, we define the notion of \emph{dynamic
concurrency}: an operation employs strong synchronization primitives only if it
\emph{has} to arbitrate with concurrent operations, given the current system
state. We then present a dynamically concurrent universal construction.

</details>
