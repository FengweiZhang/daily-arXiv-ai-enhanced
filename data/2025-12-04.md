<div id=toc></div>

# Table of Contents

- [cs.CE](#cs.CE) [Total: 2]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [1] [State Transition Block Diagram of the Generalized Maxwell Slip Friction Model](https://arxiv.org/abs/2512.03049)
*Kirk Roffi*

Main category: cs.CE

TL;DR: 本文提出了一种广义麦克斯韦滑移（GMS）摩擦模型的框图表示方法，以提升复杂动态摩擦模型的可访问性和可实现性，并通过仿真验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 现代动态摩擦模型日益复杂，缺乏传统的框图表示，限制了其可理解性与可复现性。

Method: 构建GMS摩擦模型的框图，并在MATLAB-Simulink中通过Stateflow或嵌入式if-else逻辑实现状态转移逻辑。

Result: 仿真结果表明该框图能准确再现非漂移行为和粘滑摩擦现象，并与LuGre模型进行了基准对比。

Conclusion: 所提出的框图为工程界提供了一个实用工具，有助于提升高级动态摩擦模型在仿真与控制中的可及性与应用性。

Abstract: Dynamic friction models (DFMs) encode essential information for the simulation and control of systems with friction. Traditionally, DFMs have been published with conceptual block diagrams, promoting clarity and reproducibility in simulation. However, modern DFMs have grown increasingly complex and block diagrams are now rarely presented, limiting accessibility. This letter presents a block diagram representation of the Generalized Maxwell Slip (GMS) friction model, an advanced multi-state DFM capable of simulating a wide range of nonlinear friction phenomena. The diagram can be implemented in the MATLAB-Simulink environment using a Stateflow chart or embedded if-else logic to represent the state transition criteria, but it is not limited to this platform. Closed-loop and open-loop simulations were conducted to verify that the block diagram reproduces non-drifting behavior and stick-slip friction, including benchmarking against the LuGre model. The proposed diagram improves accessibility to advanced dynamic friction models and provides the engineering community with a practical tool for the simulation and control of systems with friction.

</details>


### [2] [A 3D virtual geographic environment for flood representation towards risk communication](https://arxiv.org/abs/2512.03839)
*Weilian Li,Jun Zhu,Saied Pirasteh,Qing Zhu,Yukun Guo,Lan Luo,Youness Dehbi*

Main category: cs.CE

TL;DR: 本文提出了一种用于洪水风险沟通的三维虚拟地理环境，集成了洪水建模、并行计算与三维可视化，显著提升了非专业利益相关者对洪水过程的理解。


<details>
  <summary>Details</summary>
Motivation: 现有风险沟通研究过度依赖专业数值模型，导致非研究人员难以理解和使用，阻碍了有效的公众参与和应急响应。

Method: 构建一个集成洪水建模、并行计算和三维表示的流水线式3D虚拟地理环境，并在德国波恩莱茵河段进行实验验证。

Result: 该方法可在数小时内完成洪水建模与三维可视化，并行加速比达6.45；直观的三维城市洪水场景有助于提升公众尤其是无亲历经验者对洪水时空过程的理解，并可嵌入GeoIME云平台。

Conclusion: 所提出的3D虚拟地理环境有效促进了洪水风险沟通，具有良好的实用性与可扩展性，适用于智能洪水系统。

Abstract: Risk communication seeks to develop a shared understanding of disaster among stakeholders, thereby amplifying public awareness and empowering them to respond more effectively to emergencies. However, existing studies have overemphasized specialized numerical modelling, making the professional output challenging to understand and use by non-research stakeholders. In this context, this article proposes a 3D virtual geographic environment for flood representation towards risk communication, which integrates flood modelling, parallel computation, and 3D representation in a pipeline. Finally, a section of the Rhine River in Bonn, Germany, is selected for experiment analysis. The experimental results show that the proposed approach is capable of flood modelling and 3D representation within a few hours, the parallel speedup ratio reached 6.45. The intuitive flood scene with 3D city models is beneficial for promoting flood risk communication and is particularly helpful for participants without direct experience of floods to understand its spatiotemporal process. It also can be embedded in the Geospatial Infrastructure Management Ecosystem (GeoIME) cloud application for intelligent flood systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [3] [Accelerating Detailed Routing Convergence through Offline Reinforcement Learning](https://arxiv.org/abs/2512.03594)
*Afsara Khan,Austin Rovinski*

Main category: cs.AR

TL;DR: 本文提出利用强化学习（特别是保守Q学习，CQL）动态调整详细布线中的成本权重，从而显著加速收敛过程。相比基线布线器，该方法在ISPD19基准测试中平均提速1.56倍、最高达3.01倍，同时保持或减少了设计规则违规（DRV）数量，并展现出跨工艺节点的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着特征尺寸缩小和设计规则趋严，详细布线成为物理设计中最复杂且耗时的步骤之一。现有详细布线器依赖迭代路径搜索算法，但其静态设定的成本权重无法适应不同设计或工艺，导致收敛至零DRV解所需时间过长。

Method: 作者采用保守Q学习（CQL）训练一个强化学习模型，根据历史设计经验动态选择最优的布线成本权重，以最小化算法迭代次数，从而加速详细布线过程。

Result: 在ISPD19基准测试中，所提方法平均运行速度提升1.56倍，最高达3.01倍，同时所有测试案例的DRV数量均未增加，部分甚至有所改善；此外，该方法在不同工艺节点间表现出一定的泛化能力。

Conclusion: 通过强化学习动态调整成本权重可有效提升详细布线效率，在保证布线质量的同时显著缩短运行时间，并具备跨技术节点迁移学习的潜力。

Abstract: Detailed routing remains one of the most complex and time-consuming steps in modern physical design due to the challenges posed by shrinking feature sizes and stricter design rules. Prior detailed routers achieve state-of-the-art results by leveraging iterative pathfinding algorithms to route each net. However, runtimes are a major issue in detailed routers, as converging to a solution with zero design rule violations (DRVs) can be prohibitively expensive.
  In this paper, we propose leveraging reinforcement learning (RL) to enable rapid convergence in detailed routing by learning from previous designs. We make the key observation that prior detailed routers statically schedule the cost weights used in their routing algorithms, meaning they do not change in response to the design or technology. By training a conservative Q-learning (CQL) model to dynamically select the routing cost weights which minimize the number of algorithm iterations, we find that our work completes the ISPD19 benchmarks with 1.56x average and up to 3.01x faster runtime than the baseline router while maintaining or improving the DRV count in all cases. We also find that this learning shows signs of generalization across technologies, meaning that learning designs in one technology can translate to improved outcomes in other technologies.

</details>


### [4] [KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing](https://arxiv.org/abs/2512.03608)
*Lishuo Deng,Shaojie Xu,Jinwu Chen,Changwei Yan,Jiajie Wang,Zhe Jiang,Weiwei Shan*

Main category: cs.AR

TL;DR: KVNAND 是一种基于计算型 3D NAND 闪存的无 DRAM 架构，首次将大语言模型（LLM）的权重和键值（KV）缓存全部存储在闪存中，通过利用片上计算、头组并行性和页级 KV 映射等技术，显著提升长上下文推理性能并避免内存溢出问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备部署大语言模型面临内存带宽与容量瓶颈：传统方案依赖 DRAM 存储 KV 缓存，而随着上下文长度增加，KV 缓存体积甚至超过模型权重，导致 DRAM 成本和容量需求过高；若将 KV 缓存卸载至闪存，则会带来严重性能损失。

Method: 提出 KVNAND 架构，将模型权重与 KV 缓存全部置于支持计算的 3D NAND 闪存中；采用片上计算减少数据搬运、引入头组并行性提高吞吐量、设计页级 KV 缓存映射以匹配闪存访问特性，并构建设计空间探索框架自动优化权重与 KV 缓存布局。

Result: 在 MHA 7B 和 GQA 70B 模型上评估表明，KVNAND 在 128/1K/10K token 上下文中相比带 DRAM 的 IFC 方案分别实现 1.98×/1.94×/2.05× 的几何平均加速，并成功支持 100K token 上下文而不会发生内存溢出。

Conclusion: KVNAND 有效缓解了闪存在密集 KV 缓存访问下的延迟、能耗与可靠性问题，使闪存成为长上下文 LLM 推理中实用的存储介质，为资源受限平台上的高效 LLM 部署提供了新路径。

Abstract: Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.
  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\(\times\)/1.94\(\times\)/2.05\(\times\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.

</details>


### [5] [Lightweight Unified Sha-3/Shake Architecture with a Fault-Resilient State](https://arxiv.org/abs/2512.03616)
*Christian Ewert,Amrit Sharma Poudel,Mouadh Ayache,Andrija Neskovic,Rainer Buchty,Mladen Berekovic,Sebastian Berndt,Saleh Mulhem*

Main category: cs.AR

TL;DR: 本文提出了一种支持SHA-3和SHAKE的统一哈希引擎，采用字节级原位划分Keccak状态，并引入基于二维奇偶校验的故障检测机制，在保证高故障覆盖率的同时显著降低面积开销，适用于资源受限的后量子密码应用。


<details>
  <summary>Details</summary>
Motivation: 在后量子密码（PQC）系统中，哈希函数（如SHA-3和SHAKE）至关重要，但其硬件实现需兼顾轻量性与可靠性。现有方案在支持标准配置、故障检测能力或面积效率方面存在不足，因此亟需一种既能覆盖所有标准哈希配置又具备高效容错能力的轻量级设计。

Method: 作者提出：a) 一种统一的哈希引擎，通过字节级原位划分机制处理Keccak状态，同时支持SHA-3和SHAKE；b) 一种利用Keccak状态立方结构的二维交叉奇偶校验机制，用于检测寄存器级故障。

Result: 该设计在ASIC和FPGA上实现了比现有技术小4.5倍的整体容错引擎，面积开销降低3.7倍；对Keccak状态中三个故障实现100%检测，对更多故障仍接近100%检测率；集成到RISC-V环境中仅带来不到8%的面积开销，并全面支持所有标准哈希配置。

Conclusion: 所提出的统一哈希引擎结合多维交叉奇偶校验机制，为资源受限的后量子密码应用提供了一种高效、轻量且高可靠性的哈希函数容错解决方案。

Abstract: Hash functions have become a key part of standard Post-quantum cryptography (PQC) schemes, especially Sha-3 and Shake, calling arXiv:submit/7045552 [cs.AR] 3 Dec 2025 for lightweight implementation. A fault-resilient design is always desirable to make the whole PQC system reliable. We, therefore, propose a) a unified hash engine supporting Sha-3 and Shake that follows a byte-wise in-place partitioning mechanism of the so-called Keccak state, and b) an according fault detection for Keccak state protection exploiting its cube structure by deploying two-dimensional parity checks. It outperforms the state-of-the-art (SoA) regarding area requirements at competitive register-level fault detection by achieving 100% detection of three and still near 100% of higher numbers of Keccak state faults. Unlike SoA solutions, the proposed unified hash engine covers all standard hash configurations. Moreover, the introduced multidimensional cross-parity check mechanism achieves a 3.7x improvement in area overhead, with an overall 4.5x smaller fault-resilient engine design as demonstrated in ASIC and FPGA implementations. Integrated into a RISC-V environment, the unified hash engine with the integrated fault-resilient mechanism introduced less than 8% area overhead. Our approach thus provides a robust and lightweight fault-detection solution for protecting hash functions deployed in resource-constrained PQC applications.

</details>


### [6] [The BrainScaleS-2 multi-chip system: Interconnecting continuous-time neuromorphic compute substrates](https://arxiv.org/abs/2512.03781)
*Joscha Ilmberger,Johannes Schemmel*

Main category: cs.AR

TL;DR: 本文介绍了BrainScaleS-2 SoC系统通过FPGA互连架构的扩展方案，利用Aggregator单元实现多芯片低延迟通信，并集成到标准机架中。


<details>
  <summary>Details</summary>
Motivation: 为支持大规模神经形态计算，需扩展BrainScaleS-2 SoC的计算基底，实现高效、低延迟的芯片间通信。

Method: 采用基于FPGA的互连架构，引入Aggregator单元连接多个Node-FPGA，构建包含12条收发器链路的背板系统，并集成至19英寸4U机架中。

Result: 在每个背板内，跨三个FPGA的四跳通信实现了低于1.3微秒的芯片间延迟，适用于所有脉冲发放率。

Conclusion: 所提出的互连架构有效支持了BrainScaleS-2系统的可扩展性，并保持了极低的通信延迟，为大规模神经形态硬件提供了可行方案。

Abstract: The BrainScaleS-2 SoC integrates analog neuron and synapse circuits with digital periphery, including two CPUs with SIMD extensions. Each ASIC is connected to a Node-FPGA, providing experiment control and Ethernet connectivity. This work details the scaling of the compute substrate through FPGA-based interconnection via an additional Aggregator unit. The Aggregator provides up to 12 transceiver links to a backplane of Node-FPGAs, as well as 4 transceiver lanes for further extension. Two such interconnected backplanes are integrated into a standard 19in rack case with 4U height together with an Ethernet switch, system controller and power supplies. For all spike rates, chip-to-chip latencies -- consisting of four hops across three FPGAs -- below 1.3$μ$s are achieved within each backplane.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [7] [TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity](https://arxiv.org/abs/2512.03416)
*Ruiqi Lai,Hongrui Liu,Chengzhi Lu,Zonghao Liu,Siyu Cao,Siyang Shao,Yixin Zhang,Luo Mai,Dmitrii Ustiugov*

Main category: cs.DC

TL;DR: TokenScale 是一种面向预填充/解码（PD）分离架构的大语言模型（LLM）服务自动扩缩框架，通过引入 Token Velocity 预测指标和可转换解码器设计，显著提升 SLO 达成率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于 GPU 利用率或粗粒度请求计数的自动扩缩策略在应对突发负载时反应迟缓，导致 TTFT 和 TPOT 服务等级目标（SLO）严重违反，并造成资源过度配置。

Method: 提出 Token Velocity 指标作为系统背压的领先指标以实现主动扩缩，并设计 Convertible Decoders 机制，使解码 GPU 能在流量高峰时动态执行预填充任务，快速吸收突发请求。

Result: 在真实生产负载下的 GPU 集群评估表明，TokenScale 将 SLO 达成率从 50–88% 提升至 80–96%，相比 DistServe、BlitzScale 和 AIBrix 等先进系统，成本降低 4–14%。

Conclusion: TokenScale 通过结合预测性指标与灵活的系统架构，有效提升了 PD 分离式 LLM 服务系统的性能与资源效率。

Abstract: The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.

</details>


### [8] [Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas](https://arxiv.org/abs/2512.03565)
*Luis Gall,Samuel James Newcome,Fabio Alexander Gratl,Markus Mühlhäußer,Manish Kumar Mishra,Hans-Joachim Bungartz*

Main category: cs.DC

TL;DR: 本文研究了在分子动力学模拟中通过SIMD向量化优化粒子间作用力计算的方法，重点在于向量寄存器中粒子数据的加载顺序，并结合AutoPas库的动态调优机制，在运行时根据粒子密度和邻域识别算法等因素选择最优向量化顺序，从而显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 分子动力学（MD）模拟在原子尺度上提供重要物理过程洞察，但其计算开销大。已有研究表明，最优MD算法可能在运行时发生变化，因此有必要探索能动态适应不同模拟条件（如粒子密度、邻域识别算法）的向量化策略，以优化执行时间或能耗。

Method: 本文在AutoPas粒子模拟库中实现并评估多种SIMD向量化技术，重点关注粒子值加载到向量寄存器的顺序；同时扩展AutoPas的动态调优机制，使其能在运行时根据模拟参数（如粒子密度和邻域识别算法）自动选择最优的向量化顺序。

Result: 基准测试表明，相较于AutoPas之前的方法，在运行时考虑不同的粒子相互作用顺序可显著提升力计算的性能。

Conclusion: 通过在运行时动态选择最优的SIMD向量化顺序，结合粒子密度和邻域算法等模拟特定参数，能够有效提升分子动力学模拟中力计算的性能，验证了该方法的有效性和实用性。

Abstract: Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.
  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.
  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.

</details>


### [9] [On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs](https://arxiv.org/abs/2512.03697)
*Rafael Ravedutti Lucio Machado,Jan Eitzinger,Georg Hager,Gerhard Wellein*

Main category: cs.DC

TL;DR: 本文研究了在Fritz和Alex高性能计算集群上分析合成基准测试和Gromacs软件包能效时所面临的挑战，使用Intel Ice Lake、Sapphire Rapids CPU及Nvidia A40、A100 GPU进行MPI并行实验，并基于Likwid与Nvidia性能分析工具提出测量结果、问题剖析及最佳实践建议。


<details>
  <summary>Details</summary>
Motivation: 当前对高性能计算系统能效的评估缺乏统一标准，且在实际测量中存在诸多技术挑战和陷阱，因此需要系统性地识别这些问题并提出可行的最佳实践。

Method: 在Fritz和Alex HPC集群上，利用MPI在Intel Ice Lake和Sapphire Rapids CPU全插槽以及Nvidia A40和A100 GPU上运行合成基准和Gromacs；使用Likwid和Nvidia性能分析工具收集能效相关指标，并对实验过程中的问题进行分析。

Result: 获得了不同硬件配置下的能效测量数据，揭示了在能效分析过程中常见的挑战与误区，如工具兼容性、测量粒度和系统干扰等问题。

Conclusion: 为未来高性能计算能效研究提供了实用的最佳实践指南，强调需谨慎选择测量工具、控制实验环境，并充分理解硬件与软件交互对能效结果的影响。

Abstract: This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.

</details>


### [10] [Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods](https://arxiv.org/abs/2512.03825)
*Aingeru Ramos,Jose A Pascual,Javier Navaridas,Ivan Coluzza*

Main category: cs.DC

TL;DR: 本文提出了一种基于OpenMP和CUDA的并行化Metropolis-Hastings与Parallel Tempering算法实现，在CPU和GPU上分别实现了最高52倍和986倍的加速，并为未来量子实现提供基准。


<details>
  <summary>Details</summary>
Motivation: 传统马尔可夫链蒙特卡洛（MCMC）方法在处理复杂构型空间时采样精度不足，而提升精度的技术（如Parallel Tempering）通常带来高昂计算开销，因此需要高效的并行化策略以提升性能。

Method: 采用OpenMP在多核CPU上、CUDA在GPU上对Metropolis-Hastings算法结合Parallel Tempering进行并行实现。

Result: 在48核CPU上使用OpenMP获得最高52倍加速，在GPU上使用CUDA获得最高986倍加速；结果可作为未来量子实现的基准。

Conclusion: 通过现代硬件的并行化，显著提升了MCMC与Parallel Tempering的计算效率，使得更大规模物理/化学模型的模拟成为可能，并为后续量子算法比较奠定基础。

Abstract: Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.

</details>


### [11] [OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference](https://arxiv.org/abs/2512.03927)
*Liujianfu Wang,Yuyang Du,Yuchen Pan,Soung Chang Liew,Jiacheng Liu,Kexin Chen*

Main category: cs.DC

TL;DR: 本文提出OD-MoE，一种无需专家缓存的分布式MoE推理框架，通过按需加载专家和高精度预测机制，在仅1/3 GPU内存下实现接近全缓存部署75%的解码速度，并支持低于1GB显存的边缘设备运行MoE模型。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的低成本边缘设备上部署Mixture-of-Experts（MoE）大语言模型面临GPU内存不足的问题。现有专家卸载方法虽能缓解内存压力，但专家缓存利用率低，限制了性能与部署灵活性。

Method: OD-MoE采用两种核心技术：1）在分布式边缘节点上并行执行专家加载与计算；2）使用高精度模拟预测器，在当前层计算过程中提前多层预测后续专家激活情况，从而实现按需即时加载与释放专家参数，完全消除对专家缓存的需求。

Result: 实验表明，OD-MoE的专家激活预测准确率达99.94%，显著优于现有方法；在仅使用1/3 GPU内存的情况下，达到全缓存部署约75%的解码速度，并可在GPU内存小于1GB的边缘设备上运行MoE模型。

Conclusion: OD-MoE通过消除专家缓存需求，显著降低MoE模型对边缘设备GPU内存的要求，为在低成本物联网设备上实际部署大语言模型中的MoE架构提供了可行方案。

Abstract: Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization](https://arxiv.org/abs/2512.03421)
*Hexiang Xu,Hengyuan Liu,Yonghao Wu,Xiaolan Kang,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 该研究评估了13个大语言模型（LLM）在程序错误定位任务中的表现，发现具备推理能力的先进模型（如OpenAI o3和DeepSeekR1）在无需复杂提示工程的情况下即可实现高准确率，尤其在新构建的BugT数据集上表现稳健；尽管LLM对初学者调试具有显著辅助价值，但其在难题上的性能下降、过度推理及高计算成本仍是实际应用的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 初学者程序员因经验不足难以进行有效的错误定位，传统方法（如SBFL和MBFL）缺乏对代码上下文的理解，而大语言模型（LLM）有望通过理解程序语法与语义提供更准确、上下文感知的错误定位支持。

Method: 研究在Codeflaws、Condefects和新构建的BugT三个数据集上评估了6个闭源和7个开源LLM的错误定位能力，分析不同模型在有无推理能力、提示工程依赖性、问题难度变化等情况下的表现，并考察其解释对初学者的帮助程度。

Result: 具备推理能力的模型（如OpenAI o3、DeepSeekR1）在错误定位任务中准确率更高且对提示工程依赖较小；LLM在简单任务中表现优异，但在难题中性能下降，其中顶尖模型在BugT数据集上仍保持稳健；存在过度推理和高计算成本问题；LLM生成的解释被初学者高度评价。

Conclusion: LLM在提升初学者调试效率方面具有巨大潜力，但需进一步优化其推理能力、减少过度解释并降低计算开销，以实现实际部署和广泛应用。

Abstract: Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.

</details>


### [13] [Runnable Directories: The Solution to the Monorepo vs. Multi-repo Debate](https://arxiv.org/abs/2512.03815)
*Shayan Ghasemnezhad,Samarth KaPatel,Sofia Nikiforova,Giacinto Paolo Saggese,Paul Smith,Heanh Sok*

Main category: cs.SE

TL;DR: Causify Dev 提出了一种混合代码库组织方法，通过“可运行目录”结合单体仓库与多仓库的优点，以提升大型复杂代码库的可靠性与可维护性。


<details>
  <summary>Details</summary>
Motivation: 传统单体仓库存在扩展性和工具复杂性问题，而多仓库则面临协调和依赖管理挑战。为解决这一权衡，作者提出一种兼顾两者优势的新方案。

Method: 引入“可运行目录”概念，每个目录具备独立的开发、测试和部署生命周期，并依托统一的轻量环境、共享工具及基于 Docker 的容器化工作流。

Result: 该方法实现了配置一致性、依赖隔离以及高效的 CI/CD 流程，在实践中为代码库组织提供了可行的中间路线。

Conclusion: Causify Dev 在单体与多仓库策略之间取得平衡，有效提升了复杂系统代码库的可靠性和可维护性。

Abstract: Modern software systems increasingly strain traditional codebase organization strategies. Monorepos offer consistency but often suffer from scalability issues and tooling complexity, while multi-repos provide modularity at the cost of coordination and dependency management challenges. As an answer to this trade-off, we present the Causify Dev system, a hybrid approach that integrates key benefits of both. Its central concept is the runnable directory -- a self-contained, independently executable unit with its own development, testing, and deployment lifecycles. Backed by a unified thin environment, shared helper utilities, and containerized Docker-based workflows, runnable directories enable consistent setups, isolated dependencies, and efficient CI/CD processes. The Causify Dev approach provides a practical middle ground between monorepo and multi-repo strategies, improving reliability and maintainability for growing, complex codebases.

</details>


### [14] [A Comprehensive Study on the Impact of Vulnerable Dependencies on Open-Source Software](https://arxiv.org/abs/2512.03868)
*Shree Hari Bittugondanahalli Indra Kumar,Lilia Rodrigues Sampaio,André Martin,Andrey Brito,Christof Fetzer*

Main category: cs.SE

TL;DR: 该研究分析了1000多个开源项目、约5万次发布中依赖漏洞的严重性、持续时间和分布情况，发现大多数语言中的漏洞依赖是传递性的，且关键漏洞平均需一年以上才能修复。


<details>
  <summary>Details</summary>
Motivation: 随着开源库的广泛使用，其引入的安全漏洞（如Log4Shell）对软件供应链构成威胁，因此亟需深入理解并解决依赖漏洞问题。

Method: 利用自研的SCA工具VODA，从GitHub爬取2013至2023年间1000多个多语言开源项目的版本历史，分析其依赖版本、依赖深度、已知漏洞及其在开发周期中的演变，并关联项目指标（如团队规模、活跃度等）进行综合研究。

Result: 研究发现：多数编程语言中的漏洞依赖为传递性依赖；关键漏洞平均持续存在超过一年才被修复；该数据集比以往研究更广泛多样，提升了结果的代表性与泛化能力。

Conclusion: 开源项目中依赖漏洞普遍存在且修复缓慢，尤其在传递依赖中更为显著，需加强SCA工具的应用与漏洞响应机制以提升软件供应链安全。

Abstract: Open-source libraries are widely used by software developers to speed up the development of products, however, they can introduce security vulnerabilities, leading to incidents like Log4Shell. With the expanding usage of open-source libraries, it becomes even more imperative to comprehend and address these dependency vulnerabilities. The use of Software Composition Analysis (SCA) tools does greatly help here as they provide a deep insight on what dependencies are used in a project, enhancing the security and integrity in the software supply chain. In order to learn how wide spread vulnerabilities are and how quickly they are being fixed, we conducted a study on over 1k open-source software projects with about 50k releases comprising several languages such as Java, Python, Rust, Go, Ruby, PHP, and JavaScript. Our objective is to investigate the severity, persistence, and distribution of these vulnerabilities, as well as their correlation with project metrics such as team and contributors size, activity and release cycles. In order to perform such analysis, we crawled over 1k projects from github including their version history ranging from 2013 to 2023 using VODA, our SCA tool. Using our approach, we can provide information such as library versions, dependency depth, and known vulnerabilities, and how they evolved over the software development cycle. Being larger and more diverse than datasets used in earlier works and studies, ours provides better insights and generalizability of the gained results. The data collected answers several research questions about the dependency depth and the average time a vulnerability persists. Among other findings, we observed that for most programming languages, vulnerable dependencies are transitive, and a critical vulnerability persists in average for over a year before being fixed.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [15] [Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases](https://arxiv.org/abs/2512.03278)
*Michael Theologitis,Dan Suciu*

Main category: cs.DB

TL;DR: 本文提出了Thucy，首个跨数据库、跨表的多智能体声明验证系统，能自动发现并推理多个关系数据库以验证声明，并提供支持其判断的具体SQL查询作为证据，在TabFact数据集上准确率超越此前最优方法5.6个百分点。


<details>
  <summary>Details</summary>
Motivation: 当前事实核查系统大多局限于小型单表数据库，无法处理现实中复杂的跨数据库、跨表验证任务；而尽管大语言模型和智能体技术已取得显著进展，现有工作尚未充分展现其在结构化数据验证方面的潜力。

Method: Thucy是一个多智能体系统，能在部署前对底层数据源完全无先验知识的情况下，自主发现、检查并推理所有可用的关系数据库以验证声明，并输出支持其判断的精确SQL查询。

Result: 在标准基准TabFact数据集上，Thucy的准确率达到94.3%，比之前最先进的方法（88.7%）高出5.6个百分点。

Conclusion: Thucy展示了利用多智能体架构和LLM实现复杂结构化数据事实核查的可行性，同时通过提供可解释的SQL证据增强了系统的透明度和可信度。

Abstract: In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims$\unicode{x2014}$often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases$\unicode{x2014}$typically a few hundred rows$\unicode{x2014}$that conveniently fit within an LLM's context window.
  In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset$\unicode{x2014}$the standard benchmark for fact verification over structured data$\unicode{x2014}$Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).

</details>


### [16] [Continuous Prompts: LLM-Augmented Pipeline Processing over Unstructured Streams](https://arxiv.org/abs/2512.03389)
*Shu Chen,Deepti Raghavan,Uğur Çetintemel*

Main category: cs.DB

TL;DR: 本文提出了Continuous Prompts（CPs）框架，首次将大语言模型（LLM）推理引入持续流处理中，通过扩展RAG、定义语义算子并结合LLM与嵌入方法，支持对非结构化数据流进行持久语义查询；同时提出动态优化机制，在精度与吞吐之间实现自适应权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM框架多为无状态的一次性调用，难以支持对非结构化数据流进行持续、语义感知的长期分析，限制了其在流式场景中的应用。

Method: 提出Continuous Prompts框架，将RAG扩展至流式环境，定义连续语义算子，并实现基于LLM和嵌入的多种算子；引入元组批处理与算子融合两种优化策略，并结合轻量级影子执行与多目标贝叶斯优化（MOBO）构建动态优化框架，以在精度与效率间自适应调整。

Result: 在VectraFlow系统中实现CPs，通过微基准测试和真实数据集上的流处理管道验证了系统能有效适应工作负载变化，在保证语义查询持续性的同时，灵活权衡精度与吞吐性能。

Conclusion: Continuous Prompts为LLM在持续流处理中的应用提供了可行框架，通过动态优化机制实现了对非结构化数据流的高效、持久语义分析能力。

Abstract: Monitoring unstructured streams increasingly requires persistent, semantics-aware computation, yet today's LLM frameworks remain stateless and one-shot, limiting their usefulness for long-running analytics. We introduce Continuous Prompts (CPs), the first framework that brings LLM reasoning into continuous stream processing. CPs extend RAG to streaming settings, define continuous semantic operators, and provide multiple implementations, primarily focusing on LLM-based approaches but also reporting one embedding-based variants. Furthermore, we study two LLM-centric optimizations, tuple batching and operator fusion, to significantly improve efficiency while managing accuracy loss.
  Because these optimizations inherently trade accuracy for speed, we present a dynamic optimization framework that uses lightweight shadow executions and cost-aware multi-objective Bayesian optimization (MOBO) to learn throughput-accuracy frontiers and adapt plans under probing budgets.
  We implement CPs in the VectraFlow stream processing system. Using operator-level microbenchmarks and streaming pipelines on real datasets, we show that VectraFlow can adapt to workload dynamics, navigate accuracy-efficiency trade-offs, and sustain persistent semantic queries over evolving unstructured streams.

</details>


### [17] [Enterprise Data Science Platform: A Unified Architecture for Federated Data Access](https://arxiv.org/abs/2512.03401)
*Ryoto Miyamoto,Akira Kasuga*

Main category: cs.DB

TL;DR: 该论文提出了一种名为企业数据科学平台（EDSP）的解决方案，基于湖仓一体架构，采用“一次写入、随处读取”原则，通过四层架构实现跨多个查询引擎的联邦数据访问，有效减少数据冗余和厂商锁定，在实际部署中显著降低操作步骤，虽略有增加查询延迟，但整体仍满足分析型场景的性能需求。


<details>
  <summary>Details</summary>
Motivation: 组织在不同部门使用异构数据分析平台时，面临数据孤岛、重复存储、不一致性和高昂成本等问题；传统数据仓库因依赖厂商特定存储，难以支持跨平台访问。

Method: 提出EDSP平台，基于湖仓一体架构，采用四层设计（数据准备、数据存储、访问接口、查询引擎），实现集中式数据管理与多查询引擎的联邦访问，避免数据复制和迁移。

Result: 实验和生产部署表明，主流云数仓和编程环境可直接查询EDSP管理的数据；相比传统方法，操作步骤减少33–44%；查询延迟最多增加2.6倍，但端到端响应仍在秒级，满足实际分析需求。

Conclusion: EDSP为多查询引擎环境下的数据孤岛问题提供了可行且实用的架构方案，兼顾互操作性、成本效益与性能，具有推广价值。

Abstract: Organizations struggle to share data across departments that have adopted different data analytics platforms. If n datasets must serve m environments, up to n*m replicas can emerge, increasing inconsistency and cost. Traditional warehouses copy data into vendor-specific stores; cross-platform access is hard. This study proposes the Enterprise Data Science Platform (EDSP), which builds on data lakehouse architecture and follows a Write-Once, Read-Anywhere principle. EDSP enables federated data access for multi-query engine environments, targeting data science workloads with periodic data updates and query response times ranging from seconds to minutes. By providing centralized data management with federated access from multiple query engines to the same data sources, EDSP eliminates data duplication and vendor lock-in inherent in traditional data warehouses. The platform employs a four-layer architecture: Data Preparation, Data Store, Access Interface, and Query Engines. This design enforces separation of concerns and reduces the need for data migration when integrating additional analytical environments. Experimental results demonstrate that major cloud data warehouses and programming environments can directly query EDSP-managed datasets. We implemented and deployed EDSP in production, confirming interoperability across multiple query engines. For data sharing across different analytical environments, EDSP achieves a 33-44% reduction in operational steps compared with conventional approaches requiring data migration. Although query latency may increase by up to a factor of 2.6 compared with native tables, end-to-end completion times remain on the order of seconds, maintaining practical performance for analytical use cases. Based on our production experience, EDSP provides practical design guidelines for addressing the data-silo problem in multi-query engine environments.

</details>


### [18] [ExOAR: Expert-Guided Object and Activity Recognition from Textual Data](https://arxiv.org/abs/2512.03790)
*Iris Beerepoot,Vinicius Stein Dani,Xixi Lu*

Main category: cs.DB

TL;DR: ExOAR is an interactive method that combines large language models with human verification to extract structured object-centric process mining data from unstructured text.


<details>
  <summary>Details</summary>
Motivation: Object-centric process mining needs structured data, yet extracting such data from unstructured textual sources is difficult; existing automated methods often lack semantic clarity or human interpretability.

Method: ExOAR uses an LLM guided by user context (e.g., profession) to propose object types, activities, and instances from text; users iteratively review and refine these suggestions in a staged workflow.

Result: Evaluation with real-world Active Window Tracking data from five users shows ExOAR successfully produces structured logs with clear semantics suitable for object-centric process mining.

Conclusion: ExOAR effectively bridges unstructured text and structured object-centric logs while preserving flexibility and enabling essential human oversight.

Abstract: Object-centric process mining requires structured data, but extracting it from unstructured text remains a challenge. We introduce ExOAR (Expert-Guided Object and Activity Recognition), an interactive method that combines large language models (LLMs) with human verification to identify objects and activities from textual data. ExOAR guides users through consecutive stages in which an LLM generates candidate object types, activities, and object instances based on contextual input, such as a user's profession, and textual data. Users review and refine these suggestions before proceeding to the next stage. Implemented as a practical tool, ExOAR is initially validated through a demonstration and then evaluated with real-world Active Window Tracking data from five users. Our results show that ExOAR can effectively bridge the gap between unstructured textual data and the structured log with clear semantics needed for object-centric process analysis, while it maintains flexibility and human oversight.

</details>


### [19] [IBM Multilevel Process Mining vs de facto Object-Centric Process Mining approaches](https://arxiv.org/abs/2512.03906)
*Alberto Ronzoni,Anina Antony,Anjana M R,Francesca De Leo,Jesna Jose,Mattia Freda,Nandini Narayanankutty,Rafflesia Khan,Raji RV,Thomas Diacci*

Main category: cs.DB

TL;DR: 本文比较了对象中心流程挖掘与IBM的多级流程挖掘方法，结合两者优势提出了组织挖掘这一新功能，并通过实例展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 随着流程挖掘向对象中心范式演进，IBM希望评估自身多级流程挖掘方法与新兴方法的优劣，以指导产品创新和功能演进。

Method: 对对象中心流程挖掘与IBM多级流程挖掘进行描述与对比分析，提炼各自优缺点，并融合二者优势开发出新的“组织挖掘”功能。

Result: 成功开发并展示了“组织挖掘”这一创新功能，体现了结合两种方法优势所带来的新方法论潜力。

Conclusion: 融合对象中心流程挖掘与多级流程挖掘的优势可催生更强大的分析能力，IBM通过组织挖掘功能验证了这一整合路径的有效性与创新价值。

Abstract: The academic evolution of process mining is moving toward object centric process mining, marking a significant shift in how processes are modeled and analyzed. IBM has developed its own distinctive approach called Multilevel Process Mining. This paper provides a description of the two approaches and presents a comparative analysis of their respective advantages and limitations. IBM leveraged this comparison to drive the evolution of IBM Process Mining product, creating the new Organizational Mining feature, an innovation that combines the best of the two approaches. Demonstrate the potential of this novel, innovative and distinct methodology with an example.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [20] [Performance Evaluation of Parallel Wi-Fi Redundancy with Deferral Techniques](https://arxiv.org/abs/2512.03569)
*Gianluca Cena,Pietro Chiavassa,Stefano Scanzio*

Main category: cs.NI

TL;DR: 本文从定量角度评估了在工业无线通信中采用并行冗余（特别是延迟并行冗余）对软实时应用性能的提升，基于真实实验数据表明其能显著改善最坏情况下的传输延迟，且频谱开销有限。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi虽在工业环境中因高性能和低成本而具有吸引力，但其可靠性常被认为不适用于实时控制系统，因此需探索提升其可靠性的方法。

Method: 通过真实实验平台采集大量数据，从定量角度评估并行冗余策略在多个与软实时应用相关的性能指标上的表现。

Result: 延迟并行冗余在最坏情况传输延迟方面带来明显优势，同时仅消耗有限的频谱资源。

Conclusion: 延迟并行冗余是一种实用的方法，可在无线连接被纳入控制回路时有效提升系统性能。

Abstract: Wireless communication is increasingly used in industrial environments, since it supports mobility of interconnected devices. Among the transmission technologies operating in unlicensed bands available to this purpose, Wi-Fi is certainly one of the most interesting, because of its high performance and the relatively low deployment costs. Unfortunately, its dependability is often deemed unsuitable for real-time control systems. In this paper, the use of parallel redundancy is evaluated from a quantitative viewpoint, by considering a number of performance indices that are relevant for soft real-time applications. Analysis is carried out on a large dataset acquired from a real setup, to provide realistic insights on the advantages this kind of approaches can provide. As will be seen, deferred parallel redundancy provides clear advantages in terms of the worst-case transmission latency, at limited costs concerning the amount of consumed spectrum. Hence, it can be practically exploited every time a wireless connection is included in a control loop.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [21] [Getting the MOST out of your Storage Hierarchy with Mirror-Optimized Storage Tiering](https://arxiv.org/abs/2512.03279)
*Kaiwei Tu,Kan Wu,Andrea C. Arpaci-Dusseau,Remzi H. Arpaci-Dusseau*

Main category: cs.OS

TL;DR: MOST 是一种结合镜像与分层存储优势的新方法，在保持空间效率的同时，通过动态镜像热点数据提升 I/O 密集型负载下的带宽利用率。


<details>
  <summary>Details</summary>
Motivation: 传统存储分层在 I/O 密集型或动态工作负载下难以高效平衡负载，而镜像虽能负载均衡但空间开销大；MOST 旨在融合两者优点，兼顾空间效率与性能。

Method: MOST 动态地在存储层级间对少量热点数据进行镜像，避免昂贵的数据迁移，并在 CacheLib 基础上实现为用户态存储管理层 Cerberus。

Result: 在多种静态和动态工作负载下，Cerberus（MOST 的实现）相比现有方法在现代存储层次结构中实现了更高的吞吐量，尤其在 I/O 密集型场景中表现更优。

Conclusion: MOST 成功结合了镜像的负载均衡优势与分层的空间效率，在不增加显著存储开销的前提下，显著提升了 I/O 性能。

Abstract: We present Mirror-Optimized Storage Tiering (MOST), a novel tiering-based approach optimized for modern storage hierarchies. The key idea of MOST is to combine the load balancing advantages of mirroring with the space-efficiency advantages of tiering. Specifically, MOST dynamically mirrors a small amount of hot data across storage tiers to efficiently balance load, avoiding costly migrations. As a result, MOST is as space-efficient as classic tiering while achieving better bandwidth utilization under I/O-intensive workloads. We implement MOST in Cerberus, a user-level storage management layer based on CacheLib. We show the efficacy of Cerberus through a comprehensive empirical study: across a range of static and dynamic workloads, Cerberus achieves better throughput than competing approaches on modern storage hierarchies especially under I/O-intensive and dynamic workloads.

</details>
