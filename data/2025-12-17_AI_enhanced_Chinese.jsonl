{"id": "2512.13922", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.13922", "abs": "https://arxiv.org/abs/2512.13922", "authors": ["Anselme Ndikumana", "Kim Khoa Nguyen", "Adel Larabi", "Mohamed Cheriet"], "title": "Energy-Efficient Multi-Radio Microwave and IAB-Based Fixed Wireless Access for Rural Areas", "comment": null, "summary": "Deploying fiber optics as a last-mile solution in rural areas is not economically viable due to low population density. Nevertheless, providing high-speed internet access in these regions is essential to promote digital inclusion. 5G Fixed Wireless Access (5G FWA) has emerged as a promising alternative; however, its one-hop topology limits coverage. To overcome this limitation, a multi-hop architecture is required. This work proposes a unified multi-hop framework that integrates long-haul microwave, Integrated Access and Backhaul (IAB), and FWA to provide wide coverage and high capacity in rural areas. As the number of hops increases, total energy consumption also rises, a challenge often overlooked in existing literature. To address this, we propose an energy-efficient multi-radio microwave and IAB-based FWA framework for rural area connectivity. When the network is underutilized, the proposed approach dynamically operates at reduced capacity to minimize energy consumption. We optimize the off, start-up, serving, deep sleep, and wake-up sates of microwave radios to balance energy use and satisfying data rate requirements. Additionally, we optimize resource block allocation for IAB-based FWA nodes connected to microwave backhaul. The formulated optimization problems aim to minimize the energy consumption of long-haul microwave and multi-hop IAB-based network while satisfying data rate constraints. These problems are solved using dual decomposition and multi-convex programming, supported by dynamic programming. Simulation results demonstrates", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u519c\u6751\u5730\u533a\u7684\u8282\u80fd\u578b\u591a\u8df35G\u56fa\u5b9a\u65e0\u7ebf\u63a5\u5165\uff08FWA\uff09\u67b6\u6784\uff0c\u878d\u5408\u5fae\u6ce2\u3001IAB\u4e0eFWA\u6280\u672f\uff0c\u5728\u4fdd\u8bc1\u6570\u636e\u901f\u7387\u7684\u524d\u63d0\u4e0b\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u8bbe\u5907\u72b6\u6001\u548c\u8d44\u6e90\u5206\u914d\u4ee5\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u5728\u519c\u6751\u5730\u533a\u90e8\u7f72\u5149\u7ea4\u6210\u672c\u8fc7\u9ad8\uff0c\u800c5G FWA\u867d\u5177\u6f5c\u529b\u4f46\u53d7\u9650\u4e8e\u5355\u8df3\u8986\u76d6\u8303\u56f4\uff1b\u540c\u65f6\uff0c\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u591a\u8df3\u67b6\u6784\u4e2d\u968f\u8df3\u6570\u589e\u52a0\u800c\u4e0a\u5347\u7684\u80fd\u8017\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u8df3\u6846\u67b6\uff0c\u7ed3\u5408\u957f\u8ddd\u79bb\u5fae\u6ce2\u3001IAB\u548cFWA\uff1b\u901a\u8fc7\u4f18\u5316\u5fae\u6ce2\u5c04\u9891\u5355\u5143\u7684\u5f00\u5173\u72b6\u6001\uff08\u5173\u95ed\u3001\u542f\u52a8\u3001\u670d\u52a1\u3001\u6df1\u5ea6\u7761\u7720\u3001\u5524\u9192\uff09\u53caIAB\u8282\u70b9\u7684\u8d44\u6e90\u5757\u5206\u914d\uff0c\u6784\u5efa\u4ee5\u6700\u5c0f\u5316\u80fd\u8017\u4e3a\u76ee\u6807\u3001\u6ee1\u8db3\u6570\u636e\u901f\u7387\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u5bf9\u5076\u5206\u89e3\u3001\u591a\u51f8\u89c4\u5212\u4e0e\u52a8\u6001\u89c4\u5212\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u969c\u519c\u6751\u533a\u57df\u9ad8\u5bb9\u91cf\u8986\u76d6\u7684\u540c\u65f6\u6709\u6548\u964d\u4f4e\u4e86\u6574\u4f53\u80fd\u8017\u3002", "conclusion": "\u8be5\u8282\u80fd\u578b\u591a\u8df3FWA\u6846\u67b6\u4e3a\u519c\u6751\u5bbd\u5e26\u63a5\u5165\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u987e\u8986\u76d6\u3001\u5bb9\u91cf\u4e0e\u80fd\u6548\u3002"}}
{"id": "2512.14425", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.14425", "abs": "https://arxiv.org/abs/2512.14425", "authors": ["Hosna Hooshyar", "Mattia Fumagalli", "Marco Montali", "Giancarlo Guizzardi"], "title": "Time and Relations into Focus: Ontological Foundations of Object-Centric Event Data", "comment": null, "summary": "Object-centric process mining is a new branch of process mining where events are associated with multiple objects, and where object-to-object interactions are essential to understand the process dynamics. Traditional event data models, also called case-centric, are unable to cope with the complexity introduced by these more refined relationships. Several models have been made to move from case-centric to Object-Centric Event Data (OCED), trying to retain simplicity as much as possible. Still, these suffer from inherent ambiguities, and lack a comprehensive support of essential dimensions related to time and (dynamic) relations. In this work, we propose to fill this gap by leveraging a well-founded ontology of events and bringing ontological foundations to OCED, with a three-step approach. First, we start from key open issues reported in the literature regarding current OCED metamodels, and witness their ambiguity and expressiveness limitations on illustrative and representative examples proposed therein. Second, we consider the OCED Core Model, currently proposed as the basis for defining a new standard for object-centric event data, and we enhance it by grounding it on a lightweight version of UFO-B called gUFO, a well-known foundational ontology tailored to the representation of objects, events, time, and their (dynamic) relations. This results in a new metamodel, which we call gOCED. The third contribution then shows how gOCED at once covers the features of existing metamodels preserving their simplicity, and extends them with the essential features needed to overcome the ambiguity and expressiveness issues reported in the literature.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u672c\u4f53\u8bba\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8f7b\u91cf\u7ea7\u672c\u4f53gUFO\u6765\u589e\u5f3a\u73b0\u6709\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u6570\u636e\uff08OCED\uff09\u5143\u6a21\u578b\uff0c\u4ece\u800c\u89e3\u51b3\u5176\u5728\u65f6\u95f4\u4e0e\u52a8\u6001\u5173\u7cfb\u8868\u8fbe\u4e0a\u7684\u6a21\u7cca\u6027\u548c\u8868\u8fbe\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u5f62\u6210\u65b0\u5143\u6a21\u578bgOCED\u3002", "motivation": "\u4f20\u7edf\u4ee5\u6848\u4f8b\u4e3a\u4e2d\u5fc3\u7684\u4e8b\u4ef6\u6570\u636e\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5bf9\u8c61\u95f4\u590d\u6742\u4ea4\u4e92\uff0c\u800c\u73b0\u6709\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u6570\u636e\uff08OCED\uff09\u6a21\u578b\u5728\u65f6\u95f4\u7ef4\u5ea6\u548c\u52a8\u6001\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u6a21\u7cca\u6027\u4e0e\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4e09\u6b65\u6cd5\uff1a\u9996\u5148\u5206\u6790\u6587\u732e\u4e2dOCED\u5143\u6a21\u578b\u7684\u5173\u952e\u95ee\u9898\u5e76\u7528\u793a\u4f8b\u8bf4\u660e\u5176\u5c40\u9650\uff1b\u5176\u6b21\u5c06\u5f53\u524dOCED\u6838\u5fc3\u6a21\u578b\u4e0e\u8f7b\u91cf\u7ea7\u672c\u4f53gUFO\u7ed3\u5408\uff0c\u6784\u5efa\u65b0\u7684\u5143\u6a21\u578bgOCED\uff1b\u6700\u540e\u9a8c\u8bc1gOCED\u5728\u4fdd\u6301\u7b80\u6d01\u6027\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u7684gOCED\u5143\u6a21\u578b\u4e0d\u4ec5\u4fdd\u7559\u4e86\u73b0\u6709\u6a21\u578b\u7684\u7b80\u6d01\u6027\uff0c\u8fd8\u6709\u6548\u89e3\u51b3\u4e86\u6587\u732e\u4e2d\u6307\u51fa\u7684\u6a21\u7cca\u6027\u548c\u8868\u8fbe\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u652f\u6301\u66f4\u5168\u9762\u7684\u65f6\u95f4\u548c\u52a8\u6001\u5173\u7cfb\u5efa\u6a21\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u672c\u4f53\u8bba\u57fa\u7840\uff0cgOCED\u4e3a\u5bf9\u8c61\u4e2d\u5fc3\u8fc7\u7a0b\u6316\u6398\u63d0\u4f9b\u4e86\u66f4\u4e25\u8c28\u3001\u66f4\u5177\u8868\u8fbe\u529b\u7684\u6570\u636e\u6a21\u578b\uff0c\u6709\u671b\u6210\u4e3a\u672a\u6765\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u6570\u636e\u6807\u51c6\u7684\u57fa\u7840\u3002"}}
{"id": "2512.13830", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.13830", "abs": "https://arxiv.org/abs/2512.13830", "authors": ["Chaima Boufaied", "Thanh Nguyen", "Ronnie de Souza Santos"], "title": "Practitioner Insights on Fairness Requirements in the AI Development Life Cycle: An Interview Study", "comment": null, "summary": "Nowadays, Artificial Intelligence (AI), particularly Machine Learning (ML) and Large Language Models (LLMs), is widely applied across various contexts. However, the corresponding models often operate as black boxes, leading them to unintentionally act unfairly towards different demographic groups. This has led to a growing focus on fairness in AI software recently, alongside the traditional focus on the effectiveness of AI models. Through 26 semi-structured interviews with practitioners from different application domains and with varied backgrounds across 23 countries, we conducted research on fairness requirements in AI from software engineering perspective. Our study assesses the participants' awareness of fairness in AI / ML software and its application within the Software Development Life Cycle (SDLC), from translating fairness concerns into requirements to assessing their arising early in the SDLC. It also examines fairness through the key assessment dimensions of implementation, validation, evaluation, and how it is balanced with trade-offs involving other priorities, such as addressing all the software functionalities and meeting critical delivery deadlines. Findings of our thematic qualitative analysis show that while our participants recognize the aforementioned AI fairness dimensions, practices are inconsistent, and fairness is often deprioritized with noticeable knowledge gaps. This highlights the need for agreement with relevant stakeholders on well-defined, contextually appropriate fairness definitions, the corresponding evaluation metrics, and formalized processes to better integrate fairness into AI/ML projects.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc726\u4f4d\u6765\u81ea23\u4e2a\u56fd\u5bb6\u3001\u4e0d\u540c\u80cc\u666f\u7684\u4ece\u4e1a\u8005\u7684\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u4ece\u8f6f\u4ef6\u5de5\u7a0b\u89c6\u89d2\u63a2\u8ba8\u4e86AI/ML\u7cfb\u7edf\u4e2d\u7684\u516c\u5e73\u6027\u9700\u6c42\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u4ece\u4e1a\u8005\u666e\u904d\u8ba4\u8bc6\u5230\u516c\u5e73\u6027\u7684\u91cd\u8981\u6027\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u505a\u6cd5\u4e0d\u4e00\u81f4\u3001\u4f18\u5148\u7ea7\u4f4e\u548c\u77e5\u8bc6\u7f3a\u53e3\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u5728\u5229\u76ca\u76f8\u5173\u8005\u4e4b\u95f4\u5c31\u516c\u5e73\u6027\u5b9a\u4e49\u3001\u8bc4\u4f30\u6307\u6807\u548c\u6d41\u7a0b\u8fbe\u6210\u5171\u8bc6\u3002", "motivation": "\u968f\u7740AI\uff08\u5c24\u5176\u662f\u673a\u5668\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u201c\u9ed1\u7bb1\u201d\u7279\u6027\u53ef\u80fd\u5bfc\u81f4\u5bf9\u4e0d\u540c\u4eba\u7fa4\u7684\u4e0d\u516c\u5e73\u5bf9\u5f85\u3002\u56e0\u6b64\uff0c\u9664\u4e86\u4f20\u7edf\u5173\u6ce8\u6a21\u578b\u6709\u6548\u6027\u5916\uff0cAI\u516c\u5e73\u6027\u65e5\u76ca\u53d7\u5230\u91cd\u89c6\u3002\u7136\u800c\uff0c\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\uff08SDLC\uff09\u4e2d\u5982\u4f55\u6709\u6548\u878d\u5165\u516c\u5e73\u6027\u4ecd\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5b9e\u8df5\uff0c\u4fc3\u4f7f\u4f5c\u8005\u4ece\u8f6f\u4ef6\u5de5\u7a0b\u89d2\u5ea6\u5f00\u5c55\u6b64\u9879\u7814\u7a76\u3002", "method": "\u91c7\u7528\u5b9a\u6027\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf9\u6765\u81ea23\u4e2a\u56fd\u5bb6\u3001\u4e0d\u540c\u5e94\u7528\u9886\u57df\u548c\u80cc\u666f\u768426\u540dAI/ML\u4ece\u4e1a\u8005\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u56f4\u7ed5\u4ed6\u4eec\u5728\u8f6f\u4ef6\u5f00\u53d1\u751f\u547d\u5468\u671f\u4e2d\u5bf9\u516c\u5e73\u6027\u7684\u8ba4\u77e5\u3001\u5b9e\u8df5\u3001\u8bc4\u4f30\u7ef4\u5ea6\uff08\u5b9e\u73b0\u3001\u9a8c\u8bc1\u3001\u8bc4\u4ef7\uff09\u4ee5\u53ca\u4e0e\u5176\u4ed6\u5f00\u53d1\u76ee\u6807\uff08\u5982\u529f\u80fd\u5b8c\u6574\u6027\u3001\u4ea4\u4ed8\u671f\u9650\uff09\u4e4b\u95f4\u7684\u6743\u8861\u8fdb\u884c\u6df1\u5165\u63a2\u8ba8\uff0c\u5e76\u901a\u8fc7\u4e3b\u9898\u5206\u6790\u63d0\u70bc\u5173\u952e\u53d1\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\uff081\uff09\u4ece\u4e1a\u8005\u666e\u904d\u610f\u8bc6\u5230AI\u516c\u5e73\u6027\u7684\u591a\u4e2a\u7ef4\u5ea6\uff1b\uff082\uff09\u5b9e\u9645\u64cd\u4f5c\u4e2d\u516c\u5e73\u6027\u5b9e\u8df5\u4e0d\u4e00\u81f4\u4e14\u5e38\u88ab\u964d\u7ea7\u5904\u7406\uff1b\uff083\uff09\u5b58\u5728\u660e\u663e\u7684\u516c\u5e73\u6027\u77e5\u8bc6\u7f3a\u53e3\uff1b\uff084\uff09\u5728\u5c06\u516c\u5e73\u6027\u7eb3\u5165\u9700\u6c42\u3001\u65e9\u671f\u8bc4\u4f30\u53ca\u4e0e\u5176\u4ed6\u4f18\u5148\u7ea7\u5e73\u8861\u65b9\u9762\u7f3a\u4e4f\u6807\u51c6\u5316\u6d41\u7a0b\u3002", "conclusion": "\u4e3a\u6709\u6548\u5c06\u516c\u5e73\u6027\u878d\u5165AI/ML\u9879\u76ee\uff0c\u9700\u5728\u76f8\u5173\u5229\u76ca\u65b9\u4e4b\u95f4\u5c31\u5177\u4f53\u573a\u666f\u4e0b\u7684\u516c\u5e73\u6027\u5b9a\u4e49\u3001\u5bf9\u5e94\u7684\u8bc4\u4f30\u6307\u6807\u4ee5\u53ca\u6b63\u5f0f\u7684\u96c6\u6210\u6d41\u7a0b\u8fbe\u6210\u5171\u8bc6\uff0c\u5e76\u52a0\u5f3a\u4ece\u4e1a\u8005\u5728\u516c\u5e73\u6027\u65b9\u9762\u7684\u77e5\u8bc6\u4e0e\u80fd\u529b\u5efa\u8bbe\u3002"}}
{"id": "2512.14290", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.14290", "abs": "https://arxiv.org/abs/2512.14290", "authors": ["Suhrid Gupta", "Muhammed Tawfiqul Islam", "Rajkumar Buyya"], "title": "A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing", "comment": null, "summary": "Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u4e0e\u5b9e\u65f6\u53cd\u9988\u7684\u6df7\u5408\u81ea\u52a8\u6269\u7f29\u5bb9\u7b97\u6cd5\uff0c\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u6ee1\u8db3\u670d\u52a1\u7b49\u7ea7\u534f\u8bae\uff08SLA\uff09\u7684\u5e94\u7528\uff0c\u5728Kubernetes\u4e2d\u5b9e\u73b0\u5e76\u663e\u8457\u964d\u4f4e\u4e86SLA\u8fdd\u89c4\u7387\u3002", "motivation": "\u73b0\u6709\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u7b56\u7565\u5728\u5e94\u5bf9SLA\u8981\u6c42\u65f6\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u548c\u914d\u7f6e\u590d\u6742\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6709\u6548\u4fdd\u969c\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u53ef\u9760\u6027\u7684\u670d\u52a1\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u6df7\u5408\u81ea\u52a8\u6269\u7f29\u5bb9\u7b97\u6cd5\uff0c\u96c6\u6210\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u6027\u6269\u7f29\u5bb9\u4e0e\u57fa\u4e8e\u5f53\u524d\u8d44\u6e90\u4f7f\u7528\u548cSLA\u7ea6\u675f\u7684\u53cd\u5e94\u5f0f\u6269\u7f29\u5bb9\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3aKubernetes\u6269\u5c55\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06SLA\u8fdd\u89c4\u7387\u4ece\u73b0\u6709\u65b9\u6848\u7684\u6700\u9ad823%\u964d\u4f4e\u81f36%\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u8fb9\u7f18\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u7684SLA\u5408\u89c4\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u81ea\u52a8\u6269\u7f29\u5bb9\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e0bSLA\u7684\u9075\u5b88\u7a0b\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2512.13845", "categories": ["cs.CE", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.13845", "abs": "https://arxiv.org/abs/2512.13845", "authors": ["Lars T. Kyllingstad"], "title": "Co-simulation errors due to step size changes", "comment": "17 pages, 10 figures. Code to perform simulations and produce plots is available at https://doi.org/10.60609/6p8m-0713", "summary": "When two simulation units in a continuous-time co-simulation are connected via some variable $q$, and both simulation units have an internal state which represents the time integral of $q$, there will generally be a discrepancy between those states due to extrapolation errors. Normally, such extrapolation errors diminish if the macro time step size is reduced. Here we show that, under certain circumstances, step size changes can cause such discrepancies to increase even when the change is towards smaller steps.", "AI": {"tldr": "\u5728\u8fde\u7eed\u65f6\u95f4\u534f\u540c\u4eff\u771f\u4e2d\uff0c\u5f53\u4e24\u4e2a\u4eff\u771f\u5355\u5143\u901a\u8fc7\u53d8\u91cf $q$ \u8fde\u63a5\u4e14\u5404\u81ea\u5185\u90e8\u72b6\u6001\u8868\u793a $q$ \u7684\u65f6\u95f4\u79ef\u5206\u65f6\uff0c\u7531\u4e8e\u5916\u63a8\u8bef\u5dee\u4f1a\u5bfc\u81f4\u72b6\u6001\u4e0d\u4e00\u81f4\uff1b\u901a\u5e38\u51cf\u5c0f\u5b8f\u89c2\u65f6\u95f4\u6b65\u957f\u53ef\u51cf\u5c0f\u8be5\u8bef\u5dee\uff0c\u4f46\u672c\u6587\u6307\u51fa\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u51cf\u5c0f\u6b65\u957f\u4e5f\u53ef\u80fd\u5bfc\u81f4\u8bef\u5dee\u589e\u5927\u3002", "motivation": "\u7814\u7a76\u8fde\u7eed\u65f6\u95f4\u534f\u540c\u4eff\u771f\u4e2d\u5916\u63a8\u8bef\u5dee\u5bf9\u7cfb\u7edf\u72b6\u6001\u4e00\u81f4\u6027\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u51cf\u5c0f\u65f6\u95f4\u6b65\u957f\u65f6\u8bef\u5dee\u53cd\u800c\u589e\u5927\u7684\u53cd\u5e38\u73b0\u8c61\u3002", "method": "\u7406\u8bba\u5206\u6790\u4e0e\u6570\u503c\u5b9e\u9a8c\u76f8\u7ed3\u5408\uff0c\u63a2\u8ba8\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u65f6\u95f4\u6b65\u957f\u53d8\u5316\u5982\u4f55\u5f71\u54cd\u7531\u5916\u63a8\u5f15\u8d77\u7684\u79ef\u5206\u72b6\u6001\u504f\u5dee\u3002", "result": "\u53d1\u73b0\u5e76\u9a8c\u8bc1\u4e86\u5728\u67d0\u4e9b\u60c5\u5f62\u4e0b\uff0c\u51cf\u5c0f\u5b8f\u89c2\u65f6\u95f4\u6b65\u957f\u53cd\u800c\u4f1a\u5bfc\u81f4\u4e24\u4e2a\u4eff\u771f\u5355\u5143\u95f4\u72b6\u6001\u5dee\u5f02\u589e\u5927\u3002", "conclusion": "\u5916\u63a8\u8bef\u5dee\u5728\u534f\u540c\u4eff\u771f\u4e2d\u53ef\u80fd\u5bfc\u81f4\u975e\u76f4\u89c2\u7684\u884c\u4e3a\uff0c\u51cf\u5c0f\u6b65\u957f\u5e76\u4e0d\u603b\u80fd\u6539\u5584\u72b6\u6001\u4e00\u81f4\u6027\uff0c\u9700\u8c28\u614e\u5904\u7406\u65f6\u95f4\u6b65\u957f\u9009\u62e9\u548c\u72b6\u6001\u540c\u6b65\u7b56\u7565\u3002"}}
{"id": "2512.14622", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.14622", "abs": "https://arxiv.org/abs/2512.14622", "authors": ["Ostap Vykhopen", "Viktoria Skorik", "Maxim Tereschenko", "Veronika Solopova"], "title": "Beyond Text-to-SQL: Autonomous Research-Driven Database Exploration with DAR", "comment": null, "summary": "Large language models can already query databases, yet most existing systems remain reactive: they rely on explicit user prompts and do not actively explore data. We introduce DAR (Data Agnostic Researcher), a multi-agent system that performs end-to-end database research without human-initiated queries. DAR orchestrates specialized AI agents across three layers: initialization (intent inference and metadata extraction), execution (SQL and AI-based query synthesis with iterative validation), and synthesis (report generation with built-in quality control). All reasoning is executed directly inside BigQuery using native generative AI functions, eliminating data movement and preserving data governance. On a realistic asset-incident dataset, DAR completes the full analytical task in 16 minutes, compared to 8.5 hours for a professional analyst (approximately 32x times faster), while producing useful pattern-based insights and evidence-grounded recommendations. Although human experts continue to offer deeper contextual interpretation, DAR excels at rapid exploratory analysis. Overall, this work shifts database interaction from query-driven assistance toward autonomous, research-driven exploration within cloud data warehouses.", "AI": {"tldr": "DAR \u662f\u4e00\u4e2a\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u53ef\u5728\u4e91\u6570\u636e\u4ed3\u5e93\u5185\u81ea\u4e3b\u5b8c\u6210\u7aef\u5230\u7aef\u6570\u636e\u5e93\u7814\u7a76\uff0c\u6bd4\u4eba\u7c7b\u5206\u6790\u5e08\u5feb\u7ea632\u500d\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u867d\u80fd\u67e5\u8be2\u6570\u636e\u5e93\uff0c\u4f46\u5927\u591a\u4f9d\u8d56\u7528\u6237\u663e\u5f0f\u63d0\u793a\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u63a2\u7d22\u6570\u636e\u7684\u80fd\u529b\u3002", "method": "DAR \u91c7\u7528\u4e09\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u521d\u59cb\u5316\uff08\u610f\u56fe\u63a8\u65ad\u4e0e\u5143\u6570\u636e\u63d0\u53d6\uff09\u3001\u6267\u884c\uff08SQL \u4e0e AI \u67e5\u8be2\u751f\u6210\u53ca\u8fed\u4ee3\u9a8c\u8bc1\uff09\u3001\u5408\u6210\uff08\u5e26\u8d28\u91cf\u63a7\u5236\u7684\u62a5\u544a\u751f\u6210\uff09\uff0c\u6240\u6709\u63a8\u7406\u5747\u5728 BigQuery \u5185\u901a\u8fc7\u539f\u751f\u751f\u6210\u5f0f AI \u51fd\u6570\u5b8c\u6210\u3002", "result": "\u5728\u771f\u5b9e\u8d44\u4ea7-\u4e8b\u4ef6\u6570\u636e\u96c6\u4e0a\uff0cDAR \u572816\u5206\u949f\u5185\u5b8c\u6210\u5206\u6790\u4efb\u52a1\uff0c\u800c\u4e13\u4e1a\u5206\u6790\u5e08\u97008.5\u5c0f\u65f6\uff0c\u4e14 DAR \u80fd\u4ea7\u51fa\u57fa\u4e8e\u6a21\u5f0f\u7684\u6d1e\u89c1\u548c\u6709\u8bc1\u636e\u652f\u6301\u7684\u5efa\u8bae\u3002", "conclusion": "DAR \u5c06\u6570\u636e\u5e93\u4ea4\u4e92\u4ece\u88ab\u52a8\u67e5\u8be2\u8f85\u52a9\u8f6c\u53d8\u4e3a\u5728\u4e91\u6570\u636e\u4ed3\u5e93\u4e2d\u81ea\u4e3b\u3001\u7814\u7a76\u9a71\u52a8\u7684\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u63a2\u7d22\u6027\u5206\u6790\u6548\u7387\u3002"}}
{"id": "2512.13860", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13860", "abs": "https://arxiv.org/abs/2512.13860", "authors": ["Henger Li", "Shuangjie You", "Flavio Di Palo", "Yiyue Qian", "Ayush Jain"], "title": "Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors", "comment": "Accepted by AAAI 2026 Workshop on Agentic AI Benchmarks and Applications for Enterprise Tasks", "summary": "Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVGCO\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u7f16\u8f91\u5668\u81ea\u52a8\u4f18\u5316\u5de5\u5177\u8c03\u7528\u6240\u9700\u7684\u6587\u6863\u548c\u77e5\u8bc6\u5e93\u4e0a\u4e0b\u6587\uff0c\u4ee5\u63d0\u5347LLM\u5728\u5355\u8f6e\u3001\u5927\u89c4\u6a21\u5de5\u5177\u8c03\u7528\u573a\u666f\u4e2d\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u8c03\u7528\u4f9d\u8d56\u4e3a\u4eba\u7c7b\u7f16\u5199\u7684\u6587\u6863\u548c\u77e5\u8bc6\u5e93\uff0c\u8fd9\u4e9b\u5185\u5bb9\u4e0eLLM\u7684\u7406\u89e3\u65b9\u5f0f\u4e0d\u4e00\u81f4\uff0c\u5c24\u5176\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u5b58\u5728\u5de5\u5177\u6570\u91cf\u591a\u3001\u529f\u80fd\u91cd\u53e0\u3001\u4e0a\u4e0b\u6587\u6a21\u7cca\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5de5\u5177\u8c03\u7528\u7684\u6548\u679c\u3002", "method": "VGCO\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u8bc4\u4f30\u9636\u6bb5\uff0c\u6536\u96c6\u771f\u5b9e\u5931\u8d25\u6848\u4f8b\u5e76\u8bc6\u522b\u5de5\u5177\u4e0e\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\uff1b2\uff09\u4f18\u5316\u9636\u6bb5\uff0c\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u8fdb\u884c\u5177\u6709\u7ed3\u6784\u611f\u77e5\u80fd\u529b\u7684\u4e0a\u4e0b\u6587\u5185\u5206\u5c42\u7f16\u8f91\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u72b6\u6001\u611f\u77e5\u3001\u52a8\u4f5c\u7279\u5b9a\u4e14\u9a8c\u8bc1\u5f15\u5bfc\u7684LLM\u7f16\u8f91\u5668\uff0c\u5728\u5355\u8f6e\u8c03\u7528\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9a\u5411\u7684\u4e0a\u4e0b\u6587\u4f18\u5316\u3002", "result": "VGCO\u5728\u591a\u4e2aLLM\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u8c03\u7528\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5355\u8f6e\u3001\u5927\u89c4\u6a21\u7684\u5de5\u4e1a\u7ea7\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u901a\u8fc7\u5c06LLM\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u7f16\u8f91\u5668\uff0c\u5e76\u7ed3\u5408\u9a8c\u8bc1\u5f15\u5bfc\u7684\u5206\u5c42\u4f18\u5316\u7b56\u7565\uff0cVGCO\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u5177\u6587\u6863\u4e0eLLM\u7406\u89e3\u4e4b\u95f4\u7684\u9519\u4f4d\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21\u5de5\u5177\u8c03\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.14172", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.14172", "abs": "https://arxiv.org/abs/2512.14172", "authors": ["Qijun Zhang", "Shang Liu", "Yao Lu", "Mengming Li", "Zhiyao Xie"], "title": "ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework", "comment": "Accepted by ASP-DAC'26", "summary": "Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u578b\u529f\u8017\u5efa\u6a21\u6846\u67b6ReadyPower\uff0c\u901a\u8fc7\u5f15\u5165\u67b6\u6784\u7ea7\u3001\u5b9e\u73b0\u7ea7\u548c\u6280\u672f\u7ea7\u53c2\u6570\u6539\u8fdb\u4f20\u7edf\u6a21\u578b\uff08\u5982McPAT\uff09\uff0c\u5728\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u529f\u8017\u6a21\u578b\uff0c\u5e76\u5728BOOM\u548c\u9999\u5c71CPU\u67b6\u6784\u4e0a\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u548c\u66f4\u9ad8\u7684\u76f8\u5173\u7cfb\u6570\u3002", "motivation": "\u73b0\u4ee3\u5904\u7406\u5668\u8bbe\u8ba1\u4e2d\uff0c\u529f\u8017\u662f\u6838\u5fc3\u76ee\u6807\u4e4b\u4e00\uff0c\u9700\u8981\u65e2\u51c6\u786e\u53c8\u9ad8\u6548\u7684\u529f\u8017\u5efa\u6a21\u65b9\u6cd5\u3002\u4f20\u7edf\u7684\u5206\u6790\u578b\u67b6\u6784\u7ea7\u529f\u8017\u6a21\u578b\uff08\u5982McPAT\uff09\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u65b0\u5174\u7684\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6a21\u578b\u867d\u7cbe\u5ea6\u9ad8\u4f46\u5b58\u5728\u53ef\u9760\u6027\u5dee\u3001\u53ef\u89e3\u91ca\u6027\u5f31\u548c\u4f7f\u7528\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5de5\u4e1a\u5e94\u7528\u3002", "method": "\u4f5c\u8005\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684McPAT\u5206\u6790\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e09\u4e2a\u5c42\u7ea7\uff08\u67b6\u6784\u7ea7\u3001\u5b9e\u73b0\u7ea7\u3001\u6280\u672f\u7ea7\uff09\u7684\u53c2\u6570\u4ee5\u5f25\u5408\u5b9e\u9645\u5904\u7406\u5668\u5b9e\u73b0\u4e0e\u5206\u6790\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6784\u5efa\u51fa\u540d\u4e3aReadyPower\u7684\u65b0\u5206\u6790\u578b\u529f\u8017\u5efa\u6a21\u6846\u67b6\u3002\u5404\u5c42\u7ea7\u53c2\u6570\u91c7\u7528\u4e0d\u540c\u65b9\u5f0f\u786e\u5b9a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u8bad\u7ec3\u573a\u666f\u4e0b\uff0cReadyPower\u76f8\u6bd4\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728BOOM\u548c\u9999\u5c71CPU\u67b6\u6784\u4e0a\u5e73\u5747\u5b9e\u73b0\u4e86\u8d85\u8fc720%\u66f4\u4f4e\u7684MAPE\u548c\u8d85\u8fc70.2\u66f4\u9ad8\u7684\u76f8\u5173\u7cfb\u6570R\u3002", "conclusion": "ReadyPower\u662f\u4e00\u79cd\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u5206\u6790\u578b\u529f\u8017\u5efa\u6a21\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4e86\u67b6\u6784\u7ea7\u529f\u8017\u6a21\u578b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u514b\u670d\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u7684\u4e3b\u8981\u969c\u788d\u3002"}}
{"id": "2512.14445", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.14445", "abs": "https://arxiv.org/abs/2512.14445", "authors": ["Brenton Walker", "Markus Fidler"], "title": "Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs", "comment": null, "summary": "In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.\n  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5e76\u884c\u8ba1\u7b97\u4e2d\u5f15\u5165\u540c\u6b65\u5c4f\u969c\uff08\u5982Apache Spark\u7684Barrier Execution Mode\uff09\u5bf9\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u9020\u6210\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5206\u6790\u4e86$(s,k,l)$\u7c7b\u5c4f\u969c\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u63a8\u5bfc\u4e86\u6df7\u5408\u4efb\u52a1\u8d1f\u8f7d\u4e0b\u7684\u6027\u80fd\u8fb9\u754c\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9eSpark\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6a21\u578b\u3002", "motivation": "\u5728\u8bb8\u591a\u5e76\u884c\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u4efb\u52a1\u4e4b\u95f4\u9700\u8981\u540c\u6b65\u542f\u52a8\u6216\u7ed3\u675f\uff0c\u8fd9\u79cd\u540c\u6b65\u673a\u5236\uff08\u5982Spark\u7684Barrier Execution Mode\uff09\u4f1a\u5f15\u53d1\u5de5\u4f5c\u8282\u70b9\u7a7a\u95f2\uff0c\u4ece\u800c\u964d\u4f4e\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002\u56e0\u6b64\u6709\u5fc5\u8981\u91cf\u5316\u548c\u5206\u6790\u6b64\u7c7b\u5c4f\u969c\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86$(s,k,l)$\u5c4f\u969c\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u63a8\u5bfc\u6df7\u5408\u4efb\u52a1\uff08\u542b\u5c4f\u969c\u4e0e\u4e0d\u542b\u5c4f\u969c\uff09\u7cfb\u7edf\u7684\u6027\u80fd\u8fb9\u754c\uff1b\u9488\u5bf9\u7eaf1-barrier\u60c5\u5f62\uff0c\u7ed3\u5408Spark\u5b9e\u6d4b\u6570\u636e\uff0c\u901a\u8fc7\u4eff\u771f\u5bf9\u6bd4\u7406\u8bba\u8fb9\u754c\uff1b\u5e76\u57fa\u4e8e\u5b9e\u6d4b\u5f00\u9500\u5206\u5e03\uff0c\u6784\u5efa\u8c03\u5ea6\u673a\u5236\u76f8\u5173\u7684\u5f00\u9500\u6a21\u578b\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u4e86\u5c4f\u969c\u7cfb\u7edf\u5bf9\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u7684\u5f71\u54cd\u8fb9\u754c\uff0c\u4eff\u771f\u7ed3\u679c\u4e0eSpark\u5b9e\u6d4b\u6570\u636e\u543b\u5408\uff1b\u53d1\u73b0\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u5f00\u9500\u4e3b\u8981\u6e90\u4e8e\u4e8b\u4ef6\u9a71\u52a8\u4e0e\u8f6e\u8be2\u8c03\u5ea6\u673a\u5236\u5171\u5b58\uff0c\u5e76\u636e\u6b64\u5efa\u7acb\u4e86\u6709\u6548\u7684\u5f00\u9500\u6a21\u578b\u3002", "conclusion": "\u540c\u6b65\u5c4f\u969c\u867d\u80fd\u652f\u6301\u7279\u5b9a\u5e76\u884c\u4efb\u52a1\u9700\u6c42\uff0c\u4f46\u4f1a\u5e26\u6765\u663e\u8457\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u4ee3\u4ef7\uff1b\u901a\u8fc7\u5efa\u6a21\u548c\u5206\u6790\u53ef\u6709\u6548\u7406\u89e3\u548c\u9884\u6d4b\u6b64\u7c7b\u5f00\u9500\uff0c\u4e3a\u4f18\u5316Barrier Execution Mode\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2512.13914", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13914", "abs": "https://arxiv.org/abs/2512.13914", "authors": ["Bhargav Chickmagalur Nanjundappa", "Spandan Maaheshwari"], "title": "Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming", "comment": "11 pages, 4 figures, 2 tables, 1 code snippet, 4 algorithms", "summary": "Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.\n  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faContextBranch\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u7c7b\u4f3c\u7248\u672c\u63a7\u5236\u7684\u5206\u652f\u673a\u5236\u7ba1\u7406\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u591a\u8f6e\u5bf9\u8bdd\uff0c\u6709\u6548\u7f13\u89e3\u56e0\u4e0a\u4e0b\u6587\u6c61\u67d3\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5728\u590d\u6742\u63a2\u7d22\u6027\u7f16\u7a0b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\u5e76\u51cf\u5c11\u65e0\u5173\u4e0a\u4e0b\u6587\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u5e73\u5747\u4e0b\u964d39%\uff09\uff0c\u5c24\u5176\u5728\u9700\u8981\u63a2\u7d22\u591a\u79cd\u65b9\u6848\u7684\u7f16\u7a0b\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u65b9\u6cd5\u8feb\u4f7f\u7528\u6237\u5728\u6df7\u4e71\u4e0a\u4e0b\u6587\u548c\u4e22\u5931\u5386\u53f2\u4e4b\u95f4\u505a\u9009\u62e9\u3002", "method": "\u63d0\u51faContextBranch\u7cfb\u7edf\uff0c\u63d0\u4f9bcheckpoint\u3001branch\u3001switch\u548cinject\u56db\u4e2a\u539f\u8bed\uff0c\u5141\u8bb8\u7528\u6237\u4fdd\u5b58\u5bf9\u8bdd\u72b6\u6001\u3001\u9694\u79bb\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u6709\u9009\u62e9\u5730\u5408\u5e76\u89c1\u89e3\u3002", "result": "\u572830\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u573a\u666f\u7684\u5b9e\u9a8c\u4e2d\uff0c\u5206\u652f\u5bf9\u8bdd\u76f8\u6bd4\u7ebf\u6027\u5bf9\u8bdd\u663e\u8457\u63d0\u5347\u54cd\u5e94\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u6982\u5ff5\u8ddd\u79bb\u8f83\u8fdc\u7684\u590d\u6742\u63a2\u7d22\u4e2d\uff1b\u4e0a\u4e0b\u6587\u6d88\u606f\u6570\u51cf\u5c1158.1%\uff08\u4ece31.0\u964d\u81f313.0\uff09\u3002", "conclusion": "\u5bf9\u8bdd\u5206\u652f\u662f\u4e00\u79cd\u57fa\u7840\u6027\u673a\u5236\uff0c\u80fd\u6709\u6548\u9632\u6b62\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\u65f6\u7684\u4e0a\u4e0b\u6587\u6c61\u67d3\uff0c\u63d0\u5347AI\u8f85\u52a9\u63a2\u7d22\u6027\u5de5\u4f5c\u7684\u6548\u679c\u3002"}}
{"id": "2512.14256", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.14256", "abs": "https://arxiv.org/abs/2512.14256", "authors": ["Huizheng Wang", "Taiquan Wei", "Zichuan Wang", "Dingcheng Jiang", "Qize Yang", "Jiaxin Liu", "Jingxiang Hou", "Chao Li", "Jinyi Deng", "Yang Hu", "Shouyi Yin"], "title": "TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips", "comment": "Accepted by HPCA 2026", "summary": "Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.\n  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.\n  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TEMP \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6676\u5706\u7ea7\u82af\u7247\uff08WSC\uff09\u4e0a\u9ad8\u6548\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u901a\u8fc7\u62d3\u6251\u611f\u77e5\u7684\u5f20\u91cf\u6d41\u5206\u533a\u3001\u6d41\u91cf\u611f\u77e5\u6620\u5c04\u548c\u53cc\u5c42\u7ea7\u6676\u5706\u6c42\u89e3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u7f13\u89e3\u7247\u4e0a\u5185\u5b58\u9650\u5236\u3002", "motivation": "\u6676\u5706\u7ea7\u82af\u7247\u867d\u5177\u5907\u9ad8\u7b97\u529b\u548c\u9ad8\u5e26\u5bbd\uff0c\u4f46\u53d7\u9650\u4e8e\u6676\u5706\u9762\u79ef\uff0c\u5728\u7247\u4e0a\u5185\u5b58\u4e0e\u8ba1\u7b97\u8d44\u6e90\u4e4b\u95f4\u5b58\u5728\u72ec\u7279\u6743\u8861\u3002\u73b0\u6709\u5f20\u91cf\u5e76\u884c\u7b56\u7565\u672a\u80fd\u6709\u6548\u5229\u7528\u5176\u901a\u4fe1\u4f18\u52bf\u540c\u65f6\u4fdd\u6301\u5185\u5b58\u6548\u7387\uff0c\u96be\u4ee5\u5145\u5206\u53d1\u6325 WSC \u6027\u80fd\u3002", "method": "\u63d0\u51fa\u5f20\u91cf\u6d41\u5206\u533a\u8303\u5f0f\uff08TSPP\uff09\uff0c\u5e76\u6784\u5efa TEMP \u6846\u67b6\uff0c\u5305\u542b\u62d3\u6251\u611f\u77e5\u7684\u5f20\u91cf\u6d41\u5206\u533a\u3001\u6d41\u91cf\u611f\u77e5\u7684\u6620\u5c04\u7b56\u7565\u4ee5\u53ca\u53cc\u5c42\u7ea7\u6676\u5706\u6c42\u89e3\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9 WSC \u4e0a LLM \u8bad\u7ec3\u4e2d\u7684\u786c\u4ef6\u7ea6\u675f\u548c\u5e76\u884c\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTEMP \u5728\u591a\u79cd\u6a21\u578b\u4e0a\u5e73\u5747\u541e\u5410\u91cf\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684 LLM \u8bad\u7ec3\u7cfb\u7edf\u63d0\u5347 1.7 \u500d\u3002", "conclusion": "TEMP \u6210\u529f\u89e3\u51b3\u4e86\u6676\u5706\u7ea7\u82af\u7247\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u4e0e\u901a\u4fe1\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u9a8c\u8bc1\u4e86 TSPP \u8303\u5f0f\u5728 WSC \u5e73\u53f0\u4e0a\u7684\u6709\u6548\u6027\u4e0e\u6f5c\u529b\u3002"}}
{"id": "2512.14628", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.14628", "abs": "https://arxiv.org/abs/2512.14628", "authors": ["Alireza Olama", "Andreas Lundell", "Izzat El Hajj", "Johan Lilius", "Jerker Bj\u00f6rkqvist"], "title": "PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning", "comment": null, "summary": "Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).", "AI": {"tldr": "PruneX \u662f\u4e00\u79cd\u7ed3\u5408\u526a\u679d\u7b97\u6cd5\u4e0e\u96c6\u7fa4\u5c42\u6b21\u7ed3\u6784\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u5206\u5c42\u7ed3\u6784\u5316 ADMM \u7b97\u6cd5\uff0c\u5728\u8282\u70b9\u95f4\u540c\u6b65\u524d\u65bd\u52a0\u8282\u70b9\u7ea7\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u5728\u591a\u8282\u70b9 GPU \u96c6\u7fa4\u4e0a\u8fdb\u884c\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u65f6\uff0c\u8282\u70b9\u95f4\u901a\u4fe1\u5e26\u5bbd\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff1b\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u56e0\u65e0\u6cd5\u6709\u6548\u5229\u7528\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u96be\u4ee5\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u63d0\u51fa PruneX \u7cfb\u7edf\uff0c\u91c7\u7528\u5206\u5c42\u7ed3\u6784\u5316 ADMM\uff08H-SADMM\uff09\u7b97\u6cd5\uff0c\u5728\u8282\u70b9\u7ea7\u5f3a\u5236\u7ed3\u6784\u5316\u7a00\u758f\uff0c\u5e76\u7ed3\u5408\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u6267\u884c\u6a21\u578b\uff0c\u5c06\u5bc6\u96c6\u96c6\u5408\u64cd\u4f5c\u5e94\u7528\u4e8e\u538b\u7f29\u5f20\u91cf\uff0c\u540c\u65f6\u5c06\u5b8c\u6574\u540c\u6b65\u9650\u5236\u5728\u9ad8\u5e26\u5bbd\u7684\u8282\u70b9\u5185\u4e92\u8fde\u4e2d\u3002", "result": "\u5728 64 \u4e2a GPU \u4e0a\u5bf9 ResNet \u67b6\u6784\u7684\u8bc4\u4f30\u8868\u660e\uff0cPruneX \u5c06\u8282\u70b9\u95f4\u901a\u4fe1\u91cf\u51cf\u5c11\u4e86\u7ea6 60%\uff0c\u5e76\u5b9e\u73b0\u4e86 6.75 \u500d\u7684\u5f3a\u6269\u5c55\u52a0\u901f\u6bd4\uff0c\u4f18\u4e8e\u7a20\u5bc6\u57fa\u7ebf\uff085.81 \u500d\uff09\u548c Top-K \u68af\u5ea6\u538b\u7f29\uff083.71 \u500d\uff09\u3002", "conclusion": "PruneX \u901a\u8fc7\u534f\u540c\u8bbe\u8ba1\u526a\u679d\u7b56\u7565\u4e0e\u7cfb\u7edf\u67b6\u6784\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6269\u5c55\u6548\u7387\u3002"}}
{"id": "2512.14012", "categories": ["cs.SE", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.14012", "abs": "https://arxiv.org/abs/2512.14012", "authors": ["Ruanqianqian Huang", "Avery Reyna", "Sorin Lerner", "Haijun Xia", "Brian Hempel"], "title": "Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025", "comment": null, "summary": "The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u5730\u89c2\u5bdf\u548c\u95ee\u5377\u8c03\u67e5\u7814\u7a76\u4e86\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u53d1\u8005\u5982\u4f55\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u4f7f\u7528AI\u667a\u80fd\u4f53\uff0c\u53d1\u73b0\u4ed6\u4eec\u867d\u8ba4\u53ef\u667a\u80fd\u4f53\u5bf9\u6548\u7387\u7684\u63d0\u5347\uff0c\u4f46\u4ecd\u4e3b\u5bfc\u5173\u952e\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u4ee5\u4fdd\u969c\u8f6f\u4ef6\u8d28\u91cf\uff0c\u5e76\u5bf9\u667a\u80fd\u4f53\u6301\u79ef\u6781\u6001\u5ea6\u3002", "motivation": "\u63a2\u8ba8AI\u667a\u80fd\u4f53\u5728\u4e13\u4e1a\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b9e\u9645\u89d2\u8272\uff0c\u4e86\u89e3\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u53d1\u8005\u4f7f\u7528\u667a\u80fd\u4f53\u7684\u52a8\u673a\u3001\u7b56\u7565\u3001\u4efb\u52a1\u9002\u7528\u6027\u53ca\u6001\u5ea6\u3002", "method": "\u7ed3\u5408\u5b9e\u5730\u89c2\u5bdf\uff08N=13\uff09\u4e0e\u5b9a\u6027\u95ee\u5377\u8c03\u67e5\uff08N=99\uff09\uff0c\u5206\u6790\u5f00\u53d1\u8005\u5728\u771f\u5b9e\u5f00\u53d1\u73af\u5883\u4e2d\u4f7f\u7528AI\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u4e0e\u89c2\u70b9\u3002", "result": "\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u53d1\u8005\u5c06\u667a\u80fd\u4f53\u89c6\u4e3a\u751f\u4ea7\u529b\u5de5\u5177\uff0c\u4f46\u5728\u8f6f\u4ef6\u8bbe\u8ba1\u4e0e\u5b9e\u73b0\u4e2d\u4fdd\u6301\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4ee5\u786e\u4fdd\u8f6f\u4ef6\u8d28\u91cf\uff1b\u4ed6\u4eec\u91c7\u7528\u57fa\u4e8e\u81ea\u8eab\u4e13\u4e1a\u77e5\u8bc6\u7684\u7b56\u7565\u63a7\u5236\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u5e76\u5bf9\u5176\u6301\u603b\u4f53\u79ef\u6781\u6001\u5ea6\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8f6f\u4ef6\u5f00\u53d1\u6700\u4f73\u5b9e\u8df5\u5728\u6709\u6548\u4f7f\u7528AI\u667a\u80fd\u4f53\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u6307\u51fa\u4e86\u9002\u5408\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u7c7b\u578b\uff0c\u5e76\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u754c\u9762\u8bbe\u8ba1\u4e0e\u4f7f\u7528\u6307\u5357\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.14322", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2512.14322", "abs": "https://arxiv.org/abs/2512.14322", "authors": ["Huizheng Wang", "Hongbin Wang", "Zichuan Wang", "Zhiheng Yue", "Yang Wang", "Chao Li", "Yang Hu", "Shouyi Yin"], "title": "PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion", "comment": "Accepted by HPCA 2026", "summary": "Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.\n  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.\n  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPADE\uff0c\u4e00\u79cd\u65e0\u9700\u989d\u5916\u7a00\u758f\u6027\u9884\u6d4b\u5668\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4f4d\u4e32\u884c\u4f7f\u80fd\u9636\u6bb5\u878d\u5408\u673a\u5236\u5b9e\u73b0\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\uff0c\u572822\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4Nvidia H100 GPU\u5b9e\u73b07.43\u500d\u52a0\u901f\u548c31.1\u500d\u80fd\u6548\u63d0\u5347\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u5927\uff0c\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u56e0\u5f15\u5165\u9ad8\u5f00\u9500\u7684\u7a00\u758f\u6027\u9884\u6d4b\u5668\u800c\u7f3a\u4e4f\u5b9e\u7528\u6027\uff0c\u5bfc\u81f4\u786c\u4ef6\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faPADE\u6846\u67b6\uff0c\u5305\u542b\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a1\uff09\u57fa\u4e8e\u4f4d\u7ea7\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u7684\u5b88\u536b\u8fc7\u6ee4\u7b56\u7565\uff08BUI-GF\uff09\uff1b2\uff09\u57fa\u4e8e\u53cc\u5411\u7a00\u758f\u6027\u7684\u4e71\u5e8f\u6267\u884c\u673a\u5236\uff08BS-OOE\uff09\uff1b3\uff09\u57fa\u4e8e\u4ea4\u7ec7\u7684\u7a00\u758f\u5206\u5757\u6ce8\u610f\u529b\u673a\u5236\uff08ISTA\uff09\uff0c\u5e76\u7ed3\u5408\u5b9a\u5236\u52a0\u901f\u5668\u8bbe\u8ba1\u3002", "result": "\u572822\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPADE\u76f8\u6bd4Nvidia H100 GPU\u5b9e\u73b07.43\u500d\u901f\u5ea6\u63d0\u5347\u548c31.1\u500d\u80fd\u6548\u63d0\u5347\uff1b\u76f8\u6bd4SOTA\u52a0\u901f\u5668Sanger\u3001DOTA\u548cSOFA\uff0c\u5206\u522b\u5b9e\u73b05.1\u500d\u30014.3\u500d\u548c3.4\u500d\u7684\u80fd\u8017\u8282\u7701\u3002", "conclusion": "PADE\u901a\u8fc7\u7b97\u6cd5\u4e0e\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7a00\u758f\u6ce8\u610f\u529b\u4e2d\u9884\u6d4b\u5668\u5f00\u9500\u5927\u3001\u786c\u4ef6\u5229\u7528\u7387\u4f4e\u548c\u5206\u5757\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u52a0\u901f\u3002"}}
{"id": "2512.14329", "categories": ["cs.CE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14329", "abs": "https://arxiv.org/abs/2512.14329", "authors": ["Yanning Dai", "Chenyu Tang", "Ruizhi Zhang", "Wenyu Yang", "Yilan Zhang", "Yuhui Wang", "Junliang Chen", "Xuhang Chen", "Ruimou Xie", "Yangyue Cao", "Qiaoying Li", "Jin Cao", "Tao Li", "Hubin Zhao", "Yu Pan", "Arokia Nathan", "Xin Gao", "Peter Smielewski", "Shuo Gao"], "title": "A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data", "comment": "26 pages, 6 figures", "summary": "Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e-\u7269\u7406\u6df7\u5408\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b21\u5e73\u5730\u6b65\u884c\u8bd5\u9a8c\u91cd\u5efa\u5352\u4e2d\u60a3\u8005\u7684\u795e\u7ecf\u808c\u8089\u63a7\u5236\uff0c\u5e76\u9884\u6d4b\u5176\u5728\u659c\u5761\u884c\u8d70\u548c\u722c\u697c\u68af\u7b49\u5eb7\u590d\u4efb\u52a1\u4e2d\u7684\u8fd0\u52a8\u8868\u73b0\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u3001\u7269\u7406\u63a7\u5236\u5668\u3001\u5065\u5eb7\u8fd0\u52a8\u56fe\u8c31\u53ca\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff0c\u5728\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5eb7\u590d\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5352\u4e2d\u540e\u8fd0\u52a8\u80fd\u529b\u8bc4\u4f30\u4ec5\u63d0\u4f9b\u9759\u6001\u635f\u4f24\u8bc4\u5206\uff0c\u65e0\u6cd5\u52a8\u6001\u9884\u6d4b\u60a3\u8005\u662f\u5426\u80fd\u5b89\u5168\u6267\u884c\u7279\u5b9a\u4efb\u52a1\uff08\u5982\u4e0a\u4e0b\u5761\u6216\u697c\u68af\uff09\uff0c\u9650\u5236\u4e86\u5eb7\u590d\u65b9\u6848\u7684\u4e2a\u6027\u5316\u5236\u5b9a\u3002", "method": "\u7ed3\u5408\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u8fd0\u52a8\u5b66\u6570\u636e\u3001\u6bd4\u4f8b-\u5fae\u5206\u7269\u7406\u63a7\u5236\u5668\u3001\u7fa4\u4f53\u5065\u5eb7\u8fd0\u52a8\u56fe\u8c31\uff0c\u4ee5\u53ca\u76ee\u6807\u6761\u4ef6\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08\u542b\u884c\u4e3a\u514b\u9686\u4e0e\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\uff09\uff0c\u6784\u5efa\u4e2a\u4f53\u5316\u751f\u6210\u6a21\u578b\uff0c\u6a21\u62df\u60a3\u8005\u5728\u4e0d\u540c\u4efb\u52a1\u4e0b\u7684\u6b65\u6001\u3002", "result": "\u572811\u540d\u5352\u4e2d\u60a3\u8005\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u7559\u4e2a\u4f53\u6b65\u6001\u7279\u5f81\u7684\u540c\u65f6\uff0c\u5173\u8282\u89d2\u5ea6\u548c\u672b\u7aef\u70b9\u4fdd\u771f\u5ea6\u5206\u522b\u63d0\u53474.73%\u548c12.10%\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u81f3\u7269\u7406\u57fa\u7ebf\u768425.56%\uff1b\u572821\u540d\u4f4f\u9662\u60a3\u8005\u7684\u591a\u4e2d\u5fc3\u8bd5\u70b9\u4e2d\uff0c\u4f7f\u7528\u8be5\u9884\u6d4b\u6307\u5bfc\u5eb7\u590d\u7684\u4e34\u5e8a\u7ec4Fugl-Meyer\u4e0b\u80a2\u8bc4\u5206\u5e73\u5747\u63d0\u9ad86.0\u5206\uff0c\u663e\u8457\u4f18\u4e8e\u5bf9\u7167\u7ec4\u76843.7\u5206\u3002", "conclusion": "\u8be5\u751f\u6210\u5f0f\u3001\u4efb\u52a1\u9884\u6d4b\u6846\u67b6\u53ef\u6709\u6548\u8f85\u52a9\u4e34\u5e8a\u51b3\u7b56\uff0c\u4e3a\u5352\u4e2d\u540e\u6b65\u6001\u5eb7\u590d\u63d0\u4f9b\u52a8\u6001\u4e2a\u6027\u5316\u7b56\u7565\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2512.14018", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.14018", "abs": "https://arxiv.org/abs/2512.14018", "authors": ["Jiuding Yang", "Shengyao Lu", "Hongxuan Liu", "Shayan Shirahmad Gale Bagi", "Zahra Fazel", "Tomasz Czajkowski", "Di Niu"], "title": "PerfCoder: Large Language Models for Interpretable Code Performance Optimization", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PerfCoder\uff0c\u4e00\u79cd\u4e13\u4e3a\u751f\u6210\u9ad8\u6027\u80fd\u4ee3\u7801\u800c\u8bbe\u8ba1\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u5e26\u6709\u53ef\u89e3\u91ca\u6ce8\u91ca\u7684\u771f\u5b9e\u4f18\u5316\u8f68\u8ff9\u4e0a\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408\u8fd0\u884c\u65f6\u6307\u6807\u8fdb\u884c\u5f3a\u5316\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u6027\u80fd\u4f18\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u4ee3\u7801\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e0d\u4ec5\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u7f3a\uff0c\u66f4\u7f3a\u4e4f\u5f15\u5bfc\u5176\u8fdb\u884c\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u6027\u80fd\u4f18\u5316\u7684\u76d1\u7763\u4fe1\u53f7\u3002", "method": "PerfCoder\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u4e2d\u5e26\u6709\u4eba\u7c7b\u53ef\u8bfb\u6ce8\u91ca\u7684\u4f18\u5316\u8f68\u8ff9\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u8fd0\u884c\u65f6\u6d4b\u91cf\u6307\u6807\u8fdb\u884c\u5f3a\u5316\u5fae\u8c03\uff0c\u4f7f\u5176\u80fd\u591f\u63d0\u51fa\u9488\u5bf9\u7279\u5b9a\u8f93\u5165\u7684\u4f18\u5316\u7b56\u7565\u5e76\u76f4\u63a5\u5e94\u7528\uff0c\u65e0\u9700\u8fed\u4ee3\u4f18\u5316\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u53cd\u9988\uff0c\u7528\u4e8e\u4e0e\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u3002", "result": "\u5728PIE\u4ee3\u7801\u6027\u80fd\u57fa\u51c6\u4e0a\uff0cPerfCoder\u5728\u8fd0\u884c\u65f6\u52a0\u901f\u6bd4\u548c\u6709\u6548\u4f18\u5316\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff1b\u540c\u65f6\uff0c\u5176\u751f\u6210\u7684\u53ef\u89e3\u91ca\u53cd\u9988\u80fd\u663e\u8457\u63d0\u534732B\u6a21\u578b\u548cGPT-5\u5728\u4ee3\u7801\u4f18\u5316\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u4ec5\u9760\u6a21\u578b\u89c4\u6a21\u65e0\u6cd5\u5b9e\u73b0\u6709\u6548\u7684\u6027\u80fd\u4f18\u5316\uff0c\u5fc5\u987b\u5f15\u5165\u5bf9\u4f18\u5316\u7b56\u7565\u7684\u7406\u89e3\uff1bPerfCoder\u901a\u8fc7\u53ef\u89e3\u91ca\u4f18\u5316\u8def\u5f84\u548c\u534f\u540c\u5de5\u4f5c\u6a21\u5f0f\uff0c\u4e3a\u9ad8\u6027\u80fd\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.14453", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.14453", "abs": "https://arxiv.org/abs/2512.14453", "authors": ["Fabiola Moy\u00f3n", "Florian Angermeir", "Daniel Mendez", "Tony Gorschek", "Markus Voggenreiter", "Pierre-Louis Bonvin"], "title": "Aligning Security Compliance and DevOps: A Longitudinal Study", "comment": null, "summary": "Companies adopt agile methodologies and DevOps to facilitate efficient development and deployment of software-intensive products. This, in turn, introduces challenges in relation to security standard compliance traditionally following a more linear workflow. This is especially a challenge for the engineering of products and services associated with critical infrastructures. To support companies in their transition towards DevOps, this paper presents an adaptation of DevOps according to security regulations and standards. We report on our longitudinal study at Siemens AG, consisting of several individual sub-studies in the inception, validation, and initial adoption of our framework based on RefA as well as the implications for practice. RefA is a prescriptive model of a security compliant DevOps lifecycle based on the IEC 62443-4-1 standard. The overall framework is aimed at professionals, not only security experts, being able to use it on implementing DevOps processes while remaining compliant with security norms. We demonstrate how RefA facilitates the transfer of security compliance knowledge to product development teams. This knowledge transfer supports the agility aim of ensuring that cross-functional teams have all the skills needed to deliver the compliant products.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eIEC 62443-4-1\u6807\u51c6\u7684\u5b89\u5168\u5408\u89c4DevOps\u6846\u67b6\uff08RefA\uff09\uff0c\u5e76\u901a\u8fc7\u897f\u95e8\u5b50\u516c\u53f8\u7684\u7eb5\u5411\u7814\u7a76\u9a8c\u8bc1\u5176\u5728\u652f\u6301\u975e\u5b89\u5168\u4e13\u5bb6\u56e2\u961f\u5b9e\u65bdDevOps\u7684\u540c\u65f6\u6ee1\u8db3\u5b89\u5168\u89c4\u8303\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f01\u4e1a\u5728\u91c7\u7528\u654f\u6377\u548cDevOps\u65b9\u6cd5\u65f6\u9762\u4e34\u4f20\u7edf\u7ebf\u6027\u5b89\u5168\u5408\u89c4\u6d41\u7a0b\u7684\u6311\u6218\uff0c\u5c24\u5176\u5728\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u76f8\u5173\u4ea7\u54c1\u5f00\u53d1\u4e2d\u66f4\u4e3a\u7a81\u51fa\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u878d\u5408\u5b89\u5168\u5408\u89c4\u8981\u6c42\u4e0eDevOps\u5b9e\u8df5\u7684\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u57fa\u4e8eIEC 62443-4-1\u6807\u51c6\u6784\u5efa\u4e86\u540d\u4e3aRefA\u7684\u89c4\u8303\u6027\u5b89\u5168\u5408\u89c4DevOps\u751f\u547d\u5468\u671f\u6a21\u578b\uff0c\u5e76\u5728\u897f\u95e8\u5b50\u5f00\u5c55\u7eb5\u5411\u7814\u7a76\uff0c\u6db5\u76d6\u6846\u67b6\u7684\u6784\u601d\u3001\u9a8c\u8bc1\u4e0e\u521d\u6b65\u5e94\u7528\u3002", "result": "\u7814\u7a76\u8868\u660eRefA\u80fd\u591f\u6709\u6548\u5c06\u5b89\u5168\u5408\u89c4\u77e5\u8bc6\u4f20\u9012\u7ed9\u4ea7\u54c1\u5f00\u53d1\u56e2\u961f\uff0c\u4f7f\u8de8\u804c\u80fd\u56e2\u961f\u5177\u5907\u4ea4\u4ed8\u5408\u89c4\u4ea7\u54c1\u6240\u9700\u7684\u80fd\u529b\uff0c\u4ece\u800c\u517c\u987e\u654f\u6377\u6027\u4e0e\u5b89\u5168\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684RefA\u6846\u67b6\u6709\u52a9\u4e8e\u4f01\u4e1a\u5728\u5411DevOps\u8f6c\u578b\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5bf9\u5b89\u5168\u6807\u51c6\u7684\u5408\u89c4\uff0c\u540c\u65f6\u63d0\u5347\u56e2\u961f\u6574\u4f53\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u5305\u62ec\u975e\u5b89\u5168\u4e13\u5bb6\u5728\u5185\u7684\u5404\u7c7b\u4e13\u4e1a\u4eba\u5458\u3002"}}
{"id": "2512.14475", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.14475", "abs": "https://arxiv.org/abs/2512.14475", "authors": ["Johann Glock", "Clemens Bauer", "Martin Pinzger"], "title": "Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests", "comment": null, "summary": "Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.\n  We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.\n  Artifacts available at: https://doi.org/10.5281/zenodo.17950381", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u7684\u65b9\u6cd5 Teralizer\uff0c\u901a\u8fc7\u5355\u8def\u5f84\u7b26\u53f7\u5206\u6790\u81ea\u52a8\u5c06\u5355\u5143\u6d4b\u8bd5\u8f6c\u5316\u4e3a\u5c5e\u6027\u6d4b\u8bd5\uff0c\u65e0\u9700\u4eba\u5de5\u5b9a\u4e49\u5c5e\u6027\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u5728\u90e8\u5206\u6570\u636e\u96c6\u4e0a\u80fd\u63d0\u5347\u53d8\u5f02\u6d4b\u8bd5\u5f97\u5206\uff0c\u4f46\u5728\u771f\u5b9e\u9879\u76ee\u4e2d\u53d7\u9650\u4e8e\u7b26\u53f7\u5206\u6790\u548c\u9759\u6001\u5206\u6790\u7684\u80fd\u529b\uff0c\u9002\u7528\u6027\u6709\u9650\u3002", "motivation": "\u4f20\u7edf\u5355\u5143\u6d4b\u8bd5\u4ec5\u9a8c\u8bc1\u5355\u4e00\u8f93\u5165\u8f93\u51fa\u5bf9\uff0c\u65e0\u6cd5\u8986\u76d6\u6267\u884c\u8def\u5f84\u4e2d\u7684\u591a\u6570\u8f93\u5165\uff1b\u800c\u5c5e\u6027\u6d4b\u8bd5\u867d\u80fd\u751f\u6210\u6ee1\u8db3\u5c5e\u6027\u7684\u591a\u7ec4\u8f93\u5165\uff0c\u5374\u9700\u5927\u91cf\u4eba\u5de5\u5b9a\u4e49\u5c5e\u6027\u53ca\u5176\u7ea6\u675f\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u5c06\u5355\u5143\u6d4b\u8bd5\u6cdb\u5316\u4e3a\u5c5e\u6027\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa Teralizer \u65b9\u6cd5\uff0c\u5229\u7528\u5355\u8def\u5f84\u7b26\u53f7\u5206\u6790\u4ece\u5b9e\u73b0\u4e2d\u63d0\u53d6\u89c4\u8303\uff0c\u5e76\u81ea\u52a8\u5c06 JUnit \u5355\u5143\u6d4b\u8bd5\u8f6c\u6362\u4e3a jqwik \u5c5e\u6027\u6d4b\u8bd5\uff0c\u907f\u514d\u4f9d\u8d56\u4eba\u5de5\u7f16\u5199\u5c5e\u6027\u3002", "result": "\u5728 EvoSuite \u751f\u6210\u7684\u6d4b\u8bd5\u4e0a\uff0cTeralizer \u5c06\u53d8\u5f02\u5206\u6570\u63d0\u9ad8\u4e86 1\u20134 \u4e2a\u767e\u5206\u70b9\uff1b\u5728\u5f00\u53d1\u8005\u7f16\u5199\u7684\u6210\u719f\u6d4b\u8bd5\u4e0a\u4ec5\u63d0\u5347 0.05\u20130.07 \u4e2a\u767e\u5206\u70b9\uff1b\u5728 632 \u4e2a\u771f\u5b9e Java \u9879\u76ee\u4e2d\uff0c\u4ec5\u6709 1.7% \u6210\u529f\u5b8c\u6210\u6cdb\u5316\u6d41\u7a0b\uff0c\u5931\u8d25\u4e3b\u56e0\u662f\u539f\u578b\u5728\u7c7b\u578b\u652f\u6301\u548c\u9759\u6001\u5206\u6790\u65b9\u9762\u7684\u5c40\u9650\u3002", "conclusion": "Teralizer \u5c55\u793a\u4e86\u4ece\u7a0b\u5e8f\u8bed\u4e49\u81ea\u52a8\u63a8\u5bfc\u6d4b\u8bd5\u5c5e\u6027\u7684\u53ef\u884c\u6027\uff0c\u4f46\u8981\u5e7f\u6cdb\u5e94\u7528\u4e8e\u771f\u5b9e\u9879\u76ee\uff0c\u4ecd\u9700\u514b\u670d\u7b26\u53f7\u6267\u884c\u4e0e\u9759\u6001\u5206\u6790\u4e2d\u7684\u591a\u9879\u7814\u7a76\u4e0e\u5de5\u7a0b\u6311\u6218\u3002"}}
{"id": "2512.14613", "categories": ["cs.SE", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.14613", "abs": "https://arxiv.org/abs/2512.14613", "authors": ["Cristiano Welter", "Kleinner Farias"], "title": "MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development", "comment": "20 pages, 23 figures, 4 tables", "summary": "The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u7269\u6a21\u578b\u201d\uff08Model of Things, MoT\uff09\u7684\u57fa\u4e8e\u6a21\u578b\u4e14\u878d\u5408\u4f4e\u4ee3\u7801\u7406\u5ff5\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7b80\u5316\u4e91\u7269\u878d\u5408\uff08Cloud-of-Things, CoT\uff09\u5e94\u7528\u7684\u5f00\u53d1\u3002\u901a\u8fc7\u5b9a\u5236UML\u914d\u7f6e\u6587\u4ef6\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u4e0e\u6280\u672f\u63a5\u53d7\u6a21\u578b\uff08TAM\uff09\u95ee\u5377\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u4e0e\u6613\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u4e91\u7269\u878d\u5408\u5e94\u7528\u5f00\u53d1\u9762\u4e34\u6280\u672f\u95e8\u69db\u9ad8\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u5efa\u6a21\u65b9\u6cd5\u3001\u4e92\u64cd\u4f5c\u6027\u5dee\u3001\u81ea\u52a8\u5316\u7a0b\u5ea6\u4f4e\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u6613\u7528\u7684\u5f00\u53d1\u8303\u5f0f\u3002", "method": "\u63d0\u51faMoT\u65b9\u6cd5\uff0c\u7ed3\u5408\u4f4e\u4ee3\u7801\u7406\u5ff5\u4e0e\u5b9a\u5236\u5316\u7684UML Profile\uff0c\u652f\u6301\u5bf9IoT\u4e0e\u4e91\u670d\u52a1\u8fdb\u884c\u5efa\u6a21\uff1b\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548cTAM\u95ee\u5377\u8bc4\u4f30\u5176\u53ef\u7528\u6027\u4e0e\u6709\u6548\u6027\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793aMoT\u80fd\u6709\u6548\u7b80\u5316CoT\u5e94\u7528\u7684\u5f00\u53d1\u4e0e\u90e8\u7f72\u6d41\u7a0b\uff0c\u7528\u6237\u5373\u4f7f\u7f3a\u4e4fIoT\u7ecf\u9a8c\u4e5f\u8ba4\u4e3a\u5176\u6613\u4e8e\u4f7f\u7528\u4e14\u5b9e\u7528\u6027\u5f3a\uff0c\u5b9a\u6027\u53cd\u9988\u8868\u660e\u5176\u663e\u8457\u964d\u4f4e\u4e86\u5f00\u53d1\u590d\u6742\u5ea6\u5e76\u52a0\u5feb\u4e86\u5f00\u53d1\u901f\u5ea6\u3002", "conclusion": "MoT\u4e3aCoT\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u6a21\u578b\u9a71\u52a8\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u964d\u4f4e\u6280\u672f\u95e8\u69db\u548c\u63d0\u5347\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u589e\u5f3a\u4e86\u5f00\u53d1\u6548\u7387\u4e0e\u7075\u6d3b\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8CoT\u6280\u672f\u7684\u5e7f\u6cdb\u91c7\u7528\u3002"}}
{"id": "2512.14673", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.14673", "abs": "https://arxiv.org/abs/2512.14673", "authors": ["Ronnie de Souza Santos", "Cleyton Magalh\u00e3es", "Italo Santos"], "title": "Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI", "comment": null, "summary": "LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.", "AI": {"tldr": "\u672c\u6587\u6307\u51fa\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u804a\u5929\u673a\u5668\u4eba\u5728\u7528\u6237\u4ea4\u4e92\u5c42\u9762\u7684\u884c\u4e3a\u663e\u8457\u5f71\u54cd\u5176\u80fd\u8017\u4e0e\u73af\u5883\u8db3\u8ff9\uff0c\u4f46\u8fd9\u4e00\u56e0\u7d20\u5e38\u88ab\u5ffd\u89c6\u3002\u4f5c\u8005\u4ece\u5bf9\u8bdd\u957f\u5ea6\u3001\u5373\u65f6\u54cd\u5e94\u671f\u5f85\u3001\u7528\u6237\u65e5\u5e38\u4e60\u60ef\u548c\u4e0a\u4e0b\u6587\u7d2f\u79ef\u56db\u4e2a\u7ef4\u5ea6\u5206\u6790\u4e86\u4ea4\u4e92\u884c\u4e3a\u5bf9\u7cfb\u7edf\u53ef\u6301\u7eed\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u547c\u5401\u91cd\u65b0\u8bbe\u8ba1\u5bf9\u8bdd\u4ea4\u4e92\u4ee5\u7eb3\u5165\u53ef\u6301\u7eed\u8003\u91cf\u3002", "motivation": "\u5f53\u524d\u5bf9LLM\u7cfb\u7edf\u53ef\u6301\u7eed\u6027\u7684\u8bc4\u4f30\u4e3b\u8981\u805a\u7126\u4e8e\u6a21\u578b\u67b6\u6784\u3001\u786c\u4ef6\u6548\u7387\u548c\u90e8\u7f72\u57fa\u7840\u8bbe\u65bd\uff0c\u5374\u5ffd\u7565\u4e86\u7528\u6237\u4ea4\u4e92\u884c\u4e3a\u5bf9\u80fd\u8017\u7684\u6f5c\u5728\u5f71\u54cd\u3002\u4f5c\u8005\u65e8\u5728\u63ed\u793a\u5e76\u5f3a\u8c03\u4ea4\u4e92\u5c42\u9762\u56e0\u7d20\u5728\u5851\u9020LLM\u73af\u5883\u5f71\u54cd\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "method": "\u672c\u6587\u4e3a\u4e00\u7bc7\u89c2\u70b9\u6027\u8bba\u6587\uff08vision paper\uff09\uff0c\u901a\u8fc7\u6982\u5ff5\u5206\u6790\u548c\u7ef4\u5ea6\u5212\u5206\uff0c\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u7528\u6237\u4e0eLLM\u804a\u5929\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u884c\u4e3a\u5982\u4f55\u4ece\u56db\u4e2a\u65b9\u9762\u5f71\u54cd\u7cfb\u7edf\u7684\u80fd\u6e90\u6d88\u8017\u548c\u53ef\u6301\u7eed\u6027\u3002", "result": "\u8bba\u6587\u8bc6\u522b\u51fa\u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a1\uff09\u5ef6\u957f\u7684\u5bf9\u8bdd\u589e\u52a0token\u751f\u6210\u548c\u63a8\u7406\u8ba1\u7b97\u6210\u672c\uff1b2\uff09\u5bf9\u5373\u65f6\u54cd\u5e94\u7684\u671f\u5f85\u9650\u5236\u4e86\u8282\u80fd\u8c03\u5ea6\uff1b3\uff09\u7528\u6237\u65e5\u5e38\u4e60\u60ef\u7d2f\u79ef\u4ea7\u751f\u672a\u88ab\u91cf\u5316\u7684\u8fd0\u8425\u9700\u6c42\uff1b4\uff09\u4e0a\u4e0b\u6587\u7d2f\u79ef\u589e\u52a0\u5185\u5b58\u8d1f\u62c5\u5e76\u964d\u4f4e\u957f\u5bf9\u8bdd\u6548\u7387\u3002", "conclusion": "\u8981\u63d0\u5347LLM\u7cfb\u7edf\u7684\u53ef\u6301\u7eed\u6027\uff0c\u9700\u91cd\u65b0\u601d\u8003\u804a\u5929\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u8bbe\u8ba1\u7406\u5ff5\uff0c\u5c06\u53ef\u6301\u7eed\u6027\u89c6\u4e3a\u90e8\u5206\u4f9d\u8d56\u4e8e\u7528\u6237\u5bf9\u8bdd\u89c4\u8303\u7684\u95ee\u9898\uff0c\u5e76\u5728\u4ea4\u4e92\u8bbe\u8ba1\u4e2d\u878d\u5165\u8282\u80fd\u610f\u8bc6\u3002"}}
