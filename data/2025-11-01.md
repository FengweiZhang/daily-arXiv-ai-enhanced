<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 70]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [ReSpec: Towards Optimizing Speculative Decoding in Reinforcement Learning Systems](https://arxiv.org/abs/2510.26475)
*Qiaoling Chen,Zijun Liu,Peng Sun,Shenggui Li,Guoteng Wang,Ziming Liu,Yonggang Wen,Siyuan Feng,Tianwei Zhang*

Main category: cs.LG

TL;DR: 本文提出ReSpec系统，通过动态调整推测解码配置、基于知识蒸馏更新草稿模型以及按回报加权更新策略，解决了在强化学习训练中直接应用推测解码存在的三大问题，在Qwen模型上实现了最高4.5倍的训练加速，同时保持了奖励收敛性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 将大语言模型通过强化学习进行微调时，生成阶段常成为瓶颈，占训练时间75%以上。虽然推测解码（SD）在推理服务中能加速自回归生成，但其在强化学习训练中的适用性尚未被充分探索，且存在速度提升随批量增大而减弱、草稿模型因持续更新而过时、以及草稿模型导致策略退化三大关键问题。

Method: 作者提出了ReSpec系统，包含三个互补机制：动态调整SD配置以适应不同训练阶段；通过知识蒸馏持续演化草稿模型；根据rollout回报对更新进行加权，从而缓解策略退化。

Result: 在Qwen系列模型（3B–14B）上的实验表明，ReSpec最多可实现4.5倍的训练加速，同时保持奖励收敛性和训练稳定性。

Conclusion: ReSpec为基于强化学习的大语言模型高效微调提供了一种实用且有效的解决方案，成功将推测解码技术适配到强化学习训练流程中。

Abstract: Adapting large language models (LLMs) via reinforcement learning (RL) is
often bottlenecked by the generation stage, which can consume over 75\% of the
training time. Speculative decoding (SD) accelerates autoregressive generation
in serving systems, but its behavior under RL training remains largely
unexplored. We identify three critical gaps that hinder the naive integration
of SD into RL systems: diminishing speedups at large batch sizes, drafter
staleness under continual actor updates, and drafter-induced policy
degradation.
  To address these gaps, we present ReSpec, a system that adapts SD to RL
through three complementary mechanisms: dynamically tuning SD configurations,
evolving the drafter via knowledge distillation, and weighting updates by
rollout rewards. On Qwen models (3B--14B), ReSpec achieves up to 4.5x speedup
while preserving reward convergence and training stability, providing a
practical solution for efficient RL-based LLM adaptation.

</details>


### [2] [HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series](https://arxiv.org/abs/2510.25785)
*Simon A. Lee,Cyrus Tanade,Hao Zhou,Juhyeon Lee,Megha Thukral,Minji Han,Rachel Choi,Md Sazzad Hissain Khan,Baiying Lu,Migyeong Gwak,Mehrab Bin Morshed,Viswam Nathan,Md Mahbubur Rahman,Li Zhu,Subramaniam Venkatraman,Sharanya Arcot Desai*

Main category: cs.LG

TL;DR: HiMAE 是一种高效的自监督学习框架，通过分层掩码自编码器生成多尺度时间序列表示，在可穿戴健康数据中实现优越性能与边缘部署能力。


<details>
  <summary>Details</summary>
Motivation: 探索时间分辨率在可穿戴传感器生理时间序列预测中的作用，验证不同临床和行为结果依赖于不同时间尺度结构的假设。

Method: 提出 HiMAE（分层掩码自编码器），结合掩码自编码与分层卷积编解码器，生成多分辨率嵌入以系统评估各时间尺度的预测信号。

Result: HiMAE 在分类、回归和生成任务上均优于现有忽略尺度的基线模型，且模型体积小数个数量级，可在智能手表上实现亚毫秒级推理。

Conclusion: HiMAE 不仅是一种高效的自监督表示学习方法，还可作为发现可穿戴健康数据中尺度敏感结构的工具。

Abstract: Wearable sensors provide abundant physiological time series, yet the
principles governing their predictive utility remain unclear. We hypothesize
that temporal resolution is a fundamental axis of representation learning, with
different clinical and behavioral outcomes relying on structure at distinct
scales. To test this resolution hypothesis, we introduce HiMAE (Hierarchical
Masked Autoencoder), a self supervised framework that combines masked
autoencoding with a hierarchical convolutional encoder decoder. HiMAE produces
multi resolution embeddings that enable systematic evaluation of which temporal
scales carry predictive signal, transforming resolution from a hyperparameter
into a probe for interpretability. Across classification, regression, and
generative benchmarks, HiMAE consistently outperforms state of the art
foundation models that collapse scale, while being orders of magnitude smaller.
HiMAE is an efficient representation learner compact enough to run entirely on
watch, achieving sub millisecond inference on smartwatch class CPUs for true
edge inference. Together, these contributions position HiMAE as both an
efficient self supervised learning method and a discovery tool for scale
sensitive structure in wearable health.

</details>


### [3] [An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning](https://arxiv.org/abs/2510.26709)
*Chuyan Chen,Chenyang Ma,Zhangxin Li,Yutong He,Yanjie Dong,Kun Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种名为ARC-Top-K的梯度压缩方法，该方法在保留Top-K性能优势的同时，通过轻量级梯度草图对齐各节点的稀疏模式，实现无需索引的All-Reduce通信，并具备收缩性。结合动量误差反馈（EF21M），该方法在理论上实现线性加速和更优收敛速率，实验中在保持Top-K精度的同时最多减少60.7%的训练时间。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式机器学习中通信是主要瓶颈，现有梯度压缩方法如Rand-K丢失结构信息、性能差，而Top-K虽保留重要信息但缺乏收缩性且需昂贵的All-Gather操作，因此需要一种兼顾性能与通信效率的新压缩方法。

Method: 提出ARC-Top-K压缩器，利用梯度的轻量级草图在各节点间对齐稀疏模式，实现兼容All-Reduce的Top-K压缩，无需传输索引；并结合动量误差反馈机制（EF21M）进行优化。

Result: 理论证明ARC-Top-K具有收缩性，结合EF21M可实现线性加速和更优收敛速率；实验表明其精度与Top-K相当，训练时间最多减少60.7%。

Conclusion: ARC-Top-K是一种高效可扩展的梯度压缩方法，兼具Rand-K的鲁棒性和Top-K的高性能，显著提升分布式训练效率。

Abstract: Communication remains a central bottleneck in large-scale distributed machine
learning, and gradient sparsification has emerged as a promising strategy to
alleviate this challenge. However, existing gradient compressors face notable
limitations: Rand-$K$\ discards structural information and performs poorly in
practice, while Top-$K$\ preserves informative entries but loses the
contraction property and requires costly All-Gather operations. In this paper,
we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that
aligns sparsity patterns across nodes using a lightweight sketch of the
gradient, enabling index-free All-Reduce while preserving globally significant
information. ARC-Top-$K$\ is provably contractive and, when combined with
momentum error feedback (EF21M), achieves linear speedup and sharper
convergence rates than the original EF21M under standard assumptions.
Empirically, ARC-Top-$K$\ matches the accuracy of Top-$K$\ while reducing
wall-clock training time by up to 60.7\%, offering an efficient and scalable
solution that combines the robustness of Rand-$K$\ with the strong performance
of Top-$K$.

</details>


### [4] [Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off](https://arxiv.org/abs/2510.26722)
*Muhammad Faraz Ul Abrar,Nicolò Michelusi*

Main category: cs.LG

TL;DR: 本文研究了在无线信道异构条件下，面向非凸目标的空中计算联邦学习（OTA-FL）问题，提出一种允许结构化时间不变模型偏差的新SGD更新机制，并通过优化偏差-方差权衡设计了一种仅需统计信道状态信息的功率控制算法，实验证明其能加速收敛并提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有OTA-FL方法在异构无线环境下受限于最弱设备，导致更新方差增大；同时，以往对有偏OTA-FL的分析主要针对凸目标，难以适用于现代非凸AI模型。

Method: 提出一种新型OTA-FL SGD更新机制，允许结构化、时间不变的模型偏差以降低方差；推导有限时间平稳性界，并基于此构建非凸联合功率控制问题，采用仅需统计CSI的逐次凸逼近（SCA）算法求解。

Result: 在非凸图像分类任务上的实验表明，所提出的SCA功率控制设计通过优化偏差有效加速了收敛，并优于现有OTA-FL基线方法的泛化性能。

Conclusion: 在异构无线条件下，允许结构化偏差的OTA-FL设计能有效平衡偏差与方差，提升非凸联邦学习的收敛速度与泛化能力，且仅需统计信道信息即可实现高效优化。

Abstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a
scalable paradigm that exploits the waveform superposition of the wireless
multiple-access channel to aggregate model updates in a single use. Existing
OTA-FL designs largely enforce zero-bias model updates by either assuming
\emph{homogeneous} wireless conditions (equal path loss across devices) or
forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous}
wireless scenarios, however, such designs are constrained by the weakest device
and inflate the update variance. Moreover, prior analyses of biased OTA-FL
largely address convex objectives, while most modern AI models are highly
non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient
descent (SGD) for general smooth non-convex objectives under wireless
heterogeneity. We develop novel OTA-FL SGD updates that allow a structured,
time-invariant model bias while facilitating reduced variance updates. We
derive a finite-time stationarity bound (expected time average squared gradient
norm) that explicitly reveals a bias-variance trade-off. To optimize this
trade-off, we pose a non-convex joint OTA power-control design and develop an
efficient successive convex approximation (SCA) algorithm that requires only
statistical CSI at the base station. Experiments on a non-convex image
classification task validate the approach: the SCA-based design accelerates
convergence via an optimized bias and improves generalization over prior OTA-FL
baselines.

</details>


### [5] [The Kinetics of Reasoning: How Chain-of-Thought Shapes Learning in Transformers?](https://arxiv.org/abs/2510.25791)
*Zihan Pengmei,Costas Mavromatis,Zhengyuan Shen,Yunyi Zhang,Vassilis N. Ioannidis,Huzefa Rangwala*

Main category: cs.LG

TL;DR: 本文通过符号推理任务研究思维链（CoT）监督对Transformer模型学习动态的影响，发现CoT虽能加速泛化，但无法克服高算法复杂度任务，并揭示了推理轨迹与答案对齐的阶段性过程。


<details>
  <summary>Details</summary>
Motivation: 理解思维链（CoT）如何被模型学习并提升性能的机制尚不清楚，因此作者通过可控的符号推理任务和数据组成，从“grokking”视角探究CoT对模型泛化和内部计算的影响。

Method: 在两类训练设置下（仅输出最终答案 vs. 输出显式CoT轨迹）预训练Transformer模型，使用可调算法复杂度的符号推理任务；通过三参数逻辑曲线建模训练步数与准确率的关系，并分析轨迹忠实性与内部计算机制。

Result: (1) CoT加速泛化但无法解决高复杂度任务（如列表交集）；(2) 提出用于理解Transformer学习的动态建模框架；(3) 发现轨迹忠实性是训练过程中逐渐形成的动态属性；(4) CoT会机制性地改变Transformer的内部计算。

Conclusion: CoT监督虽能提升模型性能并改变其内部推理机制，但其效果受限于任务的算法复杂度，且推理轨迹的忠实性是在训练后期才逐步建立的。

Abstract: Chain-of-thought (CoT) supervision can substantially improve transformer
performance, yet the mechanisms by which models learn to follow and benefit
from CoT remain poorly understood. We investigate these learning dynamics
through the lens of grokking by pretraining transformers on symbolic reasoning
tasks with tunable algorithmic complexity and controllable data composition to
study their generalization. Models were trained under two settings: (i)
producing only final answers, and (ii) emitting explicit CoT traces before
answering. Our results show that while CoT generally improves task performance,
its benefits depend on task complexity. To quantify these effects, we model the
accuracy of the logarithmic training steps with a three-parameter logistic
curve, revealing how the learning speed and shape vary with task complexity,
data distribution, and the presence of CoT supervision. We also uncover a
transient trace unfaithfulness phase: early in training, models often produce
correct answers while skipping or contradicting CoT steps, before later
aligning their reasoning traces with answers. Empirically, we (1) demonstrate
that CoT accelerates generalization but does not overcome tasks with higher
algorithmic complexity, such as finding list intersections; (2) introduce a
kinetic modeling framework for understanding transformer learning; (3)
characterize trace faithfulness as a dynamic property that emerges over
training; and (4) show CoT alters internal transformer computation
mechanistically.

</details>


### [6] [Non-myopic Matching and Rebalancing in Large-Scale On-Demand Ride-Pooling Systems Using Simulation-Informed Reinforcement Learning](https://arxiv.org/abs/2510.25796)
*Farnoosh Namdarpour,Joseph Y. J. Chow*

Main category: cs.LG

TL;DR: 本文提出一种结合仿真的强化学习方法，用于提升拼车系统的非短视决策能力，在匹配和车辆再平衡策略上显著提升服务率、减少乘客等待与行程时间，并降低车队规模。


<details>
  <summary>Details</summary>
Motivation: 现有拼车系统在调度决策中多采用短视策略，忽视了长期影响，限制了系统整体效率；因此需要引入非短视的决策机制以优化性能。

Method: 将拼车仿真嵌入强化学习框架中，扩展Xu等人（2018）的规划与学习方法，采用n步时序差分学习获取时空状态价值，并设计配套的空闲车辆再平衡策略。

Result: 相比短视策略，所提非短视匹配策略可提升服务率最高8.4%，减少乘客等待时间和车内时间，并减少超过25%的车队规模；加入再平衡策略后，服务率进一步提升15.1%，等待时间减少27.3%，车内时间减少12.5%。

Conclusion: 通过仿真驱动的强化学习实现非短视决策，能显著提升拼车系统效率与成本效益，为运营商和乘客带来双重收益。

Abstract: Ride-pooling, also known as ride-sharing, shared ride-hailing, or
microtransit, is a service wherein passengers share rides. This service can
reduce costs for both passengers and operators and reduce congestion and
environmental impacts. A key limitation, however, is its myopic
decision-making, which overlooks long-term effects of dispatch decisions. To
address this, we propose a simulation-informed reinforcement learning (RL)
approach. While RL has been widely studied in the context of ride-hailing
systems, its application in ride-pooling systems has been less explored. In
this study, we extend the learning and planning framework of Xu et al. (2018)
from ride-hailing to ride-pooling by embedding a ride-pooling simulation within
the learning mechanism to enable non-myopic decision-making. In addition, we
propose a complementary policy for rebalancing idle vehicles. By employing
n-step temporal difference learning on simulated experiences, we derive
spatiotemporal state values and subsequently evaluate the effectiveness of the
non-myopic policy using NYC taxi request data. Results demonstrate that the
non-myopic policy for matching can increase the service rate by up to 8.4%
versus a myopic policy while reducing both in-vehicle and wait times for
passengers. Furthermore, the proposed non-myopic policy can decrease fleet size
by over 25% compared to a myopic policy, while maintaining the same level of
performance, thereby offering significant cost savings for operators.
Incorporating rebalancing operations into the proposed framework cuts wait time
by up to 27.3%, in-vehicle time by 12.5%, and raises service rate by 15.1%
compared to using the framework for matching decisions alone at the cost of
increased vehicle minutes traveled per passenger.

</details>


### [7] [MemEIC: A Step Toward Continual and Compositional Knowledge Editing](https://arxiv.org/abs/2510.25798)
*Jin Seong,Jiyun Park,Wencke Liermann,Hongseok Choi,Yoonji Nam,Hyun Kim,Soojong Lim,Namhoon Lee*

Main category: cs.LG

TL;DR: 本文提出MemEIC方法，用于大视觉-语言模型（LVLMs）的持续与组合式知识编辑（CCKE），通过双外部记忆和双LoRA适配器实现视觉与文本知识的组合编辑，并引入受大脑启发的知识连接器以支持跨模态推理，在复杂多模态问题上表现优异且能有效保留先前编辑。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑技术通常仅针对单一模态（视觉或语言）进行编辑，忽视了LVLMs的多模态本质以及知识持续更新的需求，导致在模态交互和持续知识精炼方面效果不佳。

Method: MemEIC采用混合外部-内部编辑器，包括用于跨模态证据检索的双外部记忆、用于各模态解耦参数更新的双LoRA适配器，以及一个受大脑启发、可选择性激活以实现组合推理的知识连接器。

Result: 实验表明，MemEIC在复杂多模态问题上显著提升性能，并能有效保留先前的知识编辑。

Conclusion: MemEIC为LVLMs中的持续与组合式知识编辑设立了新基准，有效解决了多模态协同编辑与知识持续更新的挑战。

Abstract: The dynamic nature of information necessitates continuously updating large
vision-language models (LVLMs). While recent knowledge editing techniques hint
at promising directions, they often focus on editing a single modality (vision
or language) in isolation. This prevalent practice neglects the inherent
multimodality of LVLMs and the continuous nature of knowledge updates,
potentially leading to suboptimal editing outcomes when considering the
interplay between modalities and the need for ongoing knowledge refinement. To
address these limitations, we propose MemEIC, a novel method for Continual and
Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional
editing of both visual and textual knowledge sequentially. Our approach employs
a hybrid external-internal editor featuring a dual external memory for
cross-modal evidence retrieval and dual LoRA adapters that facilitate
disentangled parameter updates for each modality. A key component is a
brain-inspired knowledge connector, activated selectively for compositional
reasoning, that integrates information across different modalities. Experiments
demonstrate that MemEIC significantly improves performance on complex
multimodal questions and effectively preserves prior edits, setting a new
benchmark for CCKE in LVLMs.

</details>


### [8] [Metis-SPECS: Decoupling Multimodal Learning via Self-distilled Preference-based Cold Start](https://arxiv.org/abs/2510.25801)
*Kun Chen,Peng Shi,Haibo Qiu,Zhixiong Zeng,Siqi Yang,Wenji Mao,Lin Ma*

Main category: cs.LG

TL;DR: 本文提出了一种名为SPECS的新型冷启动框架，通过基于偏好的自蒸馏方法解耦多模态学习，从而提升视觉语言模型在强化学习中的泛化能力和下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调（SFT）的冷启动方法容易导致指令风格过拟合，削弱分布外泛化能力，进而影响后续强化学习效果。作者旨在通过改进冷启动阶段的训练方法和数据构建方式，提升模型泛化性。

Method: 提出SPECS框架：（1）通过自蒸馏生成内省式偏好数据对，无需依赖更大教师模型或人工标注；（2）采用偏好训练聚焦于可迁移的表层形式标准（如格式、结构、风格）；（3）将训练结果交由具有可验证奖励的强化学习进行深度推理。

Result: 在多个多模态基准上取得一致性能提升，MEGA-Bench提升4.1%，MathVista提升12.2%；此外还缓解了分布内“卡顿”现象，改善探索能力、训练稳定性，并提升性能上限。

Conclusion: 基于偏好的冷启动方法（如SPECS）相比传统SFT更有利于泛化，能有效提升视觉语言模型在强化学习中的整体表现。

Abstract: Reinforcement learning (RL) with verifiable rewards has recently catalyzed a
wave of "MLLM-r1" approaches that bring RL to vision language models. Most
representative paradigms begin with a cold start, typically employing
supervised fine-tuning (SFT), to initialize the policy before RL. However,
SFT-based cold start adopts the reasoning paradigm intertwined with task
solution and output format, which may induce instruction-style overfitting,
weakens out-of-distribution generalization, and ultimately affects downstream
RL. We revisit the cold start along two views, its training method and data
construction, and introduce the Generalization Factor (GF) coefficient to
quantify the generalization capability under different methods. Our empirical
study finds that preference-based training methods (e.g. DPO) generalizes
better than SFT-based methods in cold start. Motivated by this, we propose
SPECS-a Self-distilled, Preference-based Cold Start framework that decouples
multimodal learning: (1) generates introspective preference data pairs via
self-distillation, avoiding reliance on larger teachers or manual annotation;
(2) performs preference-based training to learn, focusing on shallow,
transferable surface-form criteria (format, structure, style) rather than
memorizing content; and (3) hands off to RL with verifiable rewards for deep
reasoning results. Experimental results across multiple multimodal benchmarks
show that our decoupling learning framework yields consistent performance gains
over strong baselines, improving MEGA-Bench by 4.1% and MathVista by 12.2%.
Additional experiments indicate that SPECS contributes to reducing
in-distribution "stuckness," improving exploration, stabilizing training, and
raising the performance ceiling.

</details>


### [9] [PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs](https://arxiv.org/abs/2510.25808)
*Jaewon Chu,Seunghun Lee,Hyunwoo J. Kim*

Main category: cs.LG

TL;DR: 本文提出PRESTO框架，通过利用软提示的原像结构（preimage）来提升黑盒大语言模型指令优化的效率，显著减少冗余查询并在相同查询预算下获得更优性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在使用白盒大语言模型为黑盒模型生成候选指令时，常因多个软提示映射到同一指令而产生冗余查询，降低优化效率。作者认为这种多对一映射可作为先验知识加以利用，而非仅视为障碍。

Method: 提出PRESTO框架，包含三个核心组件：(1) 分数共享，将评估分数共享给同一原像中的所有软提示；(2) 基于原像的初始化，利用原像信息选择覆盖搜索空间最大的初始点；(3) 分数一致性正则化，强制同一原像内预测一致性。

Result: 在33个指令优化任务上的实验表明，PRESTO在相同查询预算下相当于获得了14倍的有效评分数据，显著提升了优化效率和性能。

Conclusion: 利用软提示的原像结构可有效提升黑盒大语言模型指令优化的效率，PRESTO为此提供了一个高效且实用的解决方案。

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
domains, due to their strong instruction-following capabilities. This has led
to increasing interest in optimizing instructions for black-box LLMs, whose
internal parameters are inaccessible but widely used due to their strong
performance. To optimize instructions for black-box LLMs, recent methods employ
white-box LLMs to generate candidate instructions from optimized soft prompts.
However, white-box LLMs often map different soft prompts to the same
instruction, leading to redundant queries. While previous studies regarded this
many-to-one mapping as a structure that hinders optimization efficiency, we
reinterpret it as a useful prior knowledge that can accelerate the
optimization. To this end, we introduce PREimage-informed inSTruction
Optimization (PRESTO), a novel framework that leverages the preimage structure
of soft prompts for efficient optimization. PRESTO consists of three key
components: (1) score sharing, which shares the evaluation score with all soft
prompts in a preimage; (2) preimage-based initialization, which selects initial
data points that maximize search space coverage using preimage information; and
(3) score consistency regularization, which enforces prediction consistency
within each preimage. By leveraging preimages, PRESTO achieves the effect of
effectively obtaining 14 times more scored data under the same query budget,
resulting in more efficient optimization. Experimental results on 33
instruction optimization tasks demonstrate the superior performance of PRESTO.
Code is available at https://github.com/mlvlab/PRESTO

</details>


### [10] [ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion](https://arxiv.org/abs/2510.25818)
*Sungho Koh,SeungJu Cha,Hyunwoo Oh,Kwanyoung Lee,Dong-Jin Kim*

Main category: cs.LG

TL;DR: 本文提出ScaleDiff，一种无需训练、模型无关且高效的框架，用于扩展预训练扩散模型的生成分辨率，通过引入邻域块注意力（NPA）、潜在频率混合（LFM）和结构引导，在图像质量和推理速度上达到训练免费方法中的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在生成超出其训练分辨率的图像时性能下降，现有无需训练的方法要么计算开销大，要么不兼容最新的Diffusion Transformer模型。

Method: 提出ScaleDiff框架，包含邻域块注意力（NPA）以减少自注意力层的计算冗余，结合SDEdit流程，引入潜在频率混合（LFM）生成细节，并采用结构引导增强全局结构。

Result: 实验表明，ScaleDiff在U-Net和Diffusion Transformer架构上均实现了训练免费方法中领先的图像质量和推理速度。

Conclusion: ScaleDiff是一种高效、模型无关且无需额外训练的高分辨率图像生成方案，在性能和速度方面优于现有训练免费方法。

Abstract: Text-to-image diffusion models often exhibit degraded performance when
generating images beyond their training resolution. Recent training-free
methods can mitigate this limitation, but they often require substantial
computation or are incompatible with recent Diffusion Transformer models. In
this paper, we propose ScaleDiff, a model-agnostic and highly efficient
framework for extending the resolution of pretrained diffusion models without
any additional training. A core component of our framework is Neighborhood
Patch Attention (NPA), an efficient mechanism that reduces computational
redundancy in the self-attention layer with non-overlapping patches. We
integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing
(LFM) to better generate fine details. Furthermore, we apply Structure Guidance
to enhance global structure during the denoising process. Experimental results
demonstrate that ScaleDiff achieves state-of-the-art performance among
training-free methods in terms of both image quality and inference speed on
both U-Net and Diffusion Transformer architectures.

</details>


### [11] [Topology-Aware Active Learning on Graphs](https://arxiv.org/abs/2510.25892)
*Harris Hardiman-Mostow,Jack Mauro,Adrien Weihs,Andrea L. Bertozzi*

Main category: cs.LG

TL;DR: 本文提出一种基于图拓扑的主动学习方法，利用平衡福尔曼曲率（BFC）指导探索阶段的样本选择并动态切换至利用阶段，同时引入局部图重连策略提升标签传播效果，在低标签率下显著优于现有图半监督方法。


<details>
  <summary>Details</summary>
Motivation: 主动学习在标签预算有限的情况下面临探索与利用之间的权衡挑战，现有方法多依赖人工调参的启发式策略，缺乏对图结构信息的有效利用。

Method: 提出基于平衡福尔曼曲率（BFC）的coreset构建算法用于初始标签选择，并设计数据驱动的停止准则；利用BFC动态触发从探索到利用的转换；引入局部图重连策略融合多尺度信息以增强标签传播。

Result: 在多个基准分类任务上的实验表明，所提方法在低标签率条件下始终优于现有的图半监督学习基线。

Conclusion: 结合图拓扑结构与曲率信息的主动学习框架能有效平衡探索与利用，在标签稀缺场景下显著提升性能。

Abstract: We propose a graph-topological approach to active learning that directly
targets the core challenge of exploration versus exploitation under scarce
label budgets. To guide exploration, we introduce a coreset construction
algorithm based on Balanced Forman Curvature (BFC), which selects
representative initial labels that reflect the graph's cluster structure. This
method includes a data-driven stopping criterion that signals when the graph
has been sufficiently explored. We further use BFC to dynamically trigger the
shift from exploration to exploitation within active learning routines,
replacing hand-tuned heuristics. To improve exploitation, we introduce a
localized graph rewiring strategy that efficiently incorporates multiscale
information around labeled nodes, enhancing label propagation while preserving
sparsity. Experiments on benchmark classification tasks show that our methods
consistently outperform existing graph-based semi-supervised baselines at low
label rates.

</details>


### [12] [Active Learning with Task-Driven Representations for Messy Pools](https://arxiv.org/abs/2510.25926)
*Kianoosh Ashouritaklimi,Tom Rainforth*

Main category: cs.LG

TL;DR: 本文指出当前主动学习方法在处理杂乱数据池时依赖固定无监督表示的局限性，并提出通过任务驱动的动态更新表示来提升性能，实验表明所提两种策略均显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法在处理杂乱、未经整理的数据池时，通常使用固定的无监督表示，这可能无法充分捕捉与目标任务相关的重要信息，从而限制了其有效性。

Method: 提出在主动学习过程中利用已收集的标签定期更新任务驱动的表示，并引入两种具体策略：一是直接学习半监督表示，二是对初始无监督表示进行监督微调。

Result: 两种策略在实验中均显著优于使用无监督或预训练表示的方法。

Conclusion: 使用任务驱动且动态更新的表示能有效提升主动学习在杂乱数据池中的性能，优于依赖固定无监督表示的现有方法。

Abstract: Active learning has the potential to be especially useful for messy,
uncurated pools where datapoints vary in relevance to the target task. However,
state-of-the-art approaches to this problem currently rely on using fixed,
unsupervised representations of the pool, focusing on modifying the acquisition
function instead. We show that this model setup can undermine their
effectiveness at dealing with messy pools, as such representations can fail to
capture important information relevant to the task. To address this, we propose
using task-driven representations that are periodically updated during the
active learning process using the previously collected labels. We introduce two
specific strategies for learning these representations, one based on directly
learning semi-supervised representations and the other based on supervised
fine-tuning of an initial unsupervised representation. We find that both
significantly improve empirical performance over using unsupervised or
pretrained representations.

</details>


### [13] [Modular Linear Tokenization (MLT)](https://arxiv.org/abs/2510.25952)
*Tcharlies Schmitz*

Main category: cs.LG

TL;DR: 本文提出了一种名为模块化线性标记化（MLT）的新方法，用于将高基数类别标识符可逆地编码为紧凑的数值向量，在保持完全可逆性的同时，显著减少参数量和训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统哈希或独热编码方法在处理高基数类别变量时存在信息损失或维度爆炸问题，且通常不可逆。作者旨在设计一种既能保持双射映射、又具备良好可扩展性和维度可控性的编码方法。

Method: MLT利用有限域上的模运算和可逆线性变换，实现对高基数类别标识符的可逆、确定性编码，支持显式控制输出维度，并具备计算可扩展性。

Result: 在MovieLens 20M数据集上的实验表明，MLT在预测性能上与监督嵌入方法相当，但参数量更少、训练成本更低。

Conclusion: MLT是一种高效、可逆且可扩展的高基数类别变量编码方法，适用于大规模机器学习任务，并已开源提供使用。

Abstract: This paper introduces Modular Linear Tokenization (MLT), a reversible and
deterministic technique for encoding high-cardinality categorical identifiers
into compact numerical vectors. Unlike traditional hashing or one-hot
encodings, MLT preserves bijective mappings by leveraging modular arithmetic
over finite fields and invertible linear transformations. The method offers
explicit control of dimensionality and computational scalability while
maintaining full reversibility, even for millions of identifiers. Experimental
results on the MovieLens 20M dataset show that MLT achieves comparable
predictive performance to supervised embeddings while requiring significantly
fewer parameters and lower training cost. An open-source implementation of MLT
is available on PyPI (https://pypi.org/project/light-mlt/) and GitHub
(https://github.com/tcharliesschmitz/light-mlt).

</details>


### [14] [Application and Validation of Geospatial Foundation Model Data for the Prediction of Health Facility Programmatic Outputs -- A Case Study in Malawi](https://arxiv.org/abs/2510.25954)
*Lynn Metz,Rachel Haggard,Michael Moszczynski,Samer Asbah,Chris Mwase,Patricia Khomani,Tyler Smith,Hannah Cooper,Annie Mwale,Arbaaz Muslim,Gautam Prasad,Mimi Sun,Tomer Shekel,Joydeep Paul,Anna Carter,Shravya Shetty,Dylan Green*

Main category: cs.LG

TL;DR: 本研究评估了三种地理空间基础模型（GeoFM）嵌入——Google人口动态基础模型（PDFM）、Google AlphaEarth（基于卫星图像）和手机通话记录（CDR）——在马拉维预测15项常规卫生项目产出的表现。结果表明，相比传统地理空间插值方法，GeoFM嵌入在15项指标中的13项上表现更优，其中融合三种嵌入的多GeoFM模型效果最佳，尤其在人口密度、新发HIV病例和儿童疫苗接种方面预测效果显著，但在结核病和营养不良等数据稀缺指标上表现较差。


<details>
  <summary>Details</summary>
Motivation: 低收入和中等收入国家（LMICs）的常规卫生数据常受限于报告延迟和覆盖不全，亟需探索新的数据源与分析方法以提升数据可靠性。

Method: 研究使用XGBoost模型，基于马拉维552个卫生服务区域2021年1月至2023年5月的数据，对三种GeoFM嵌入（PDFM、AlphaEarth、CDR）进行评估，并与传统地理空间插值方法比较。采用80/20训练测试划分及5折交叉验证，以R²作为性能指标。

Result: GeoFM嵌入方法在15项指标中的13项优于传统方法；多GeoFM融合模型表现最优，例如在人口密度、新HIV病例和儿童疫苗接种上的交叉验证R²分别为0.63、0.57和0.47，测试集R²分别为0.64、0.68和0.55；但在结核病和营养不良等数据稀疏指标上预测效果不佳。

Conclusion: GeoFM嵌入能为LMIC中部分卫生与人口指标提供适度但有价值的预测提升，多源GeoFM融合是一种高效且有前景的工具，可用于补充和强化受限的常规卫生信息系统。

Abstract: The reliability of routine health data in low and middle-income countries
(LMICs) is often constrained by reporting delays and incomplete coverage,
necessitating the exploration of novel data sources and analytics. Geospatial
Foundation Models (GeoFMs) offer a promising avenue by synthesizing diverse
spatial, temporal, and behavioral data into mathematical embeddings that can be
efficiently used for downstream prediction tasks. This study evaluated the
predictive performance of three GeoFM embedding sources - Google Population
Dynamics Foundation Model (PDFM), Google AlphaEarth (derived from satellite
imagery), and mobile phone call detail records (CDR) - for modeling 15 routine
health programmatic outputs in Malawi, and compared their utility to
traditional geospatial interpolation methods. We used XGBoost models on data
from 552 health catchment areas (January 2021-May 2023), assessing performance
with R2, and using an 80/20 training and test data split with 5-fold
cross-validation used in training. While predictive performance was mixed, the
embedding-based approaches improved upon baseline geostatistical methods in 13
of 15 (87%) indicators tested. A Multi-GeoFM model integrating all three
embedding sources produced the most robust predictions, achieving average
5-fold cross validated R2 values for indicators like population density (0.63),
new HIV cases (0.57), and child vaccinations (0.47) and test set R2 of 0.64,
0.68, and 0.55, respectively. Prediction was poor for prediction targets with
low primary data availability, such as TB and malnutrition cases. These results
demonstrate that GeoFM embeddings imbue a modest predictive improvement for
select health and demographic outcomes in an LMIC context. We conclude that the
integration of multiple GeoFM sources is an efficient and valuable tool for
supplementing and strengthening constrained routine health information systems.

</details>


### [15] [Contrastive Predictive Coding Done Right for Mutual Information Estimation](https://arxiv.org/abs/2510.25983)
*J. Jon Ryu,Pavan Yeddanapudi,Xiangxiang Xu,Gregory W. Wornell*

Main category: cs.LG

TL;DR: 本文指出InfoNCE并非有效的互信息（MI）估计器，并提出改进方法InfoNCE-anchor，通过引入辅助锚点类实现更准确的MI估计；同时基于恰当评分规则构建统一框架，涵盖多种对比学习目标。实验表明该方法能显著降低MI估计偏差，但在自监督表示学习中并未提升下游任务性能，说明对比学习的优势源于结构化密度比的学习而非精确MI估计。


<details>
  <summary>Details</summary>
Motivation: InfoNCE虽广泛用于互信息估计，但其与互信息并无直接联系，作者旨在澄清其作为MI估计器的局限性并提出更准确的替代方案。

Method: 提出InfoNCE-anchor方法，引入辅助锚点类以实现一致的密度比估计，并基于恰当评分规则构建统一框架，将InfoNCE、NCE及f-散度变体等纳入其中。

Result: InfoNCE-anchor在MI估计任务中显著降低偏差，表现最优；但在自监督表示学习中未提升下游性能。

Conclusion: 对比表示学习的有效性并非来自准确的MI估计，而是源于对结构化密度比的学习。

Abstract: The InfoNCE objective, originally introduced for contrastive representation
learning, has become a popular choice for mutual information (MI) estimation,
despite its indirect connection to MI. In this paper, we demonstrate why
InfoNCE should not be regarded as a valid MI estimator, and we introduce a
simple modification, which we refer to as InfoNCE-anchor, for accurate MI
estimation. Our modification introduces an auxiliary anchor class, enabling
consistent density ratio estimation and yielding a plug-in MI estimator with
significantly reduced bias. Beyond this, we generalize our framework using
proper scoring rules, which recover InfoNCE-anchor as a special case when the
log score is employed. This formulation unifies a broad spectrum of contrastive
objectives, including NCE, InfoNCE, and $f$-divergence variants, under a single
principled framework. Empirically, we find that InfoNCE-anchor with the log
score achieves the most accurate MI estimates; however, in self-supervised
representation learning experiments, we find that the anchor does not improve
the downstream task performance. These findings corroborate that contrastive
representation learning benefits not from accurate MI estimation per se, but
from the learning of structured density ratios.

</details>


### [16] [Infrequent Exploration in Linear Bandits](https://arxiv.org/abs/2510.26000)
*Harin Lee,Min-hwan Oh*

Main category: cs.LG

TL;DR: 本文提出了一种名为INFEX的简单实用框架，用于在稀疏探索设置下解决线性bandit问题，在保证与现有高效算法相当的遗憾界的同时，显著降低探索频率和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有完全自适应探索方法（如UCB、汤普森采样）在每一步都可能进行探索，这在安全关键或高成本场景中不切实际甚至不道德；而纯贪婪策略则依赖强上下文多样性假设，通常难以成功。因此，需要一种介于两者之间的稀疏探索方法。

Method: INFEX框架按照预设调度执行基础探索策略，其余时间主要采取贪婪动作。该框架具有模块化设计，可集成任意完全自适应探索方法，并将高强度探索计算限制在稀疏时间点。

Result: 理论分析表明，只要探索频率超过对数阈值，INFEX即可达到与标准高效算法相匹配的实例依赖遗憾界；实验验证了其在遗憾性能和运行时间方面优于现有方法。

Conclusion: INFEX提供了一种高效、实用且通用的稀疏探索解决方案，在保持理论性能的同时提升了计算效率，适用于多种实际应用场景。

Abstract: We study the problem of infrequent exploration in linear bandits, addressing
a significant yet overlooked gap between fully adaptive exploratory methods
(e.g., UCB and Thompson Sampling), which explore potentially at every time
step, and purely greedy approaches, which require stringent diversity
assumptions to succeed. Continuous exploration can be impractical or unethical
in safety-critical or costly domains, while purely greedy strategies typically
fail without adequate contextual diversity. To bridge these extremes, we
introduce a simple and practical framework, INFEX, explicitly designed for
infrequent exploration. INFEX executes a base exploratory policy according to a
given schedule while predominantly choosing greedy actions in between. Despite
its simplicity, our theoretical analysis demonstrates that INFEX achieves
instance-dependent regret matching standard provably efficient algorithms,
provided the exploration frequency exceeds a logarithmic threshold.
Additionally, INFEX is a general, modular framework that allows seamless
integration of any fully adaptive exploration method, enabling wide
applicability and ease of adoption. By restricting intensive exploratory
computations to infrequent intervals, our approach can also enhance
computational efficiency. Empirical evaluations confirm our theoretical
findings, showing state-of-the-art regret performance and runtime improvements
over existing methods.

</details>


### [17] [Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis](https://arxiv.org/abs/2510.26014)
*Hyeonjun Lee,Hyungseob Shin,Gunhee Nam,Hyeonsoo Lee*

Main category: cs.LG

TL;DR: 本文提出了一种用于离散时间生存分析的双混合专家（dual-MoE）框架，通过结合特征编码器MoE和风险MoE，分别处理患者亚群表示学习和时序动态建模，在乳腺癌数据集上显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 生存分析需同时建模患者异质性、个体特征与时间动态对风险预测的影响，现有方法在灵活整合这些因素方面存在挑战。

Method: 提出双混合专家（dual-MoE）架构：一个用于亚群感知的特征表示学习（feature-encoder MoE），另一个结合患者特征与时间嵌入以捕捉时序动态（hazard MoE），并可灵活集成到现有深度学习生存分析流程中。

Result: 在METABRIC和GBSG乳腺癌数据集上，该方法在测试集上将时变C-index最多提升0.04，并在集成到Consurv框架时进一步提升性能。

Conclusion: 所提出的双MoE框架有效提升了离散时间生存分析的预测准确性，展示了其在建模患者异质性和时间动态方面的优势。

Abstract: Survival analysis is a task to model the time until an event of interest
occurs, widely used in clinical and biomedical research. A key challenge is to
model patient heterogeneity while also adapting risk predictions to both
individual characteristics and temporal dynamics. We propose a dual
mixture-of-experts (MoE) framework for discrete-time survival analysis. Our
approach combines a feature-encoder MoE for subgroup-aware representation
learning with a hazard MoE that leverages patient features and time embeddings
to capture temporal dynamics. This dual-MoE design flexibly integrates with
existing deep learning based survival pipelines. On METABRIC and GBSG breast
cancer datasets, our method consistently improves performance, boosting the
time-dependent C-index up to 0.04 on the test sets, and yields further gains
when incorporated into the Consurv framework.

</details>


### [18] [Do Students Debias Like Teachers? On the Distillability of Bias Mitigation Methods](https://arxiv.org/abs/2510.26038)
*Jiali Cheng,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 本文首次系统研究了知识蒸馏（KD）对模型去偏能力迁移的影响，发现KD通常会削弱学生模型的去偏效果，并揭示了其内在注意力机制原因，进而提出了三种提升去偏能力可蒸馏性的有效策略。


<details>
  <summary>Details</summary>
Motivation: 尽管知识蒸馏广泛用于模型压缩和知识迁移，但其对模型在分布外数据上鲁棒性（尤其是对抗虚假相关性/偏见的能力）的影响尚不明确。作者旨在探究KD是否以及如何影响教师模型向学生模型传递“去偏”能力。

Method: 通过在自然语言推理（NLI）和图像分类任务上进行大量实验，评估不同去偏方法经KD后的效果；分析学生模型在KD后的鲁棒性变化及内部注意力模式与电路，以理解其行为差异；并提出三种改进策略：高质量数据增强、迭代式知识蒸馏、用教师模型权重初始化学生模型。

Result: 实验发现：(i) KD总体上削弱了模型的去偏能力；(ii) 对教师模型进行去偏训练并不能通过KD使学生受益；(iii) 虽然整体鲁棒性可能保持稳定，但不同类型偏见的表现存在显著差异；(iv) 定位到了导致KD后行为差异的内部注意力模式和电路。所提三种策略能有效提升去偏能力的可蒸馏性。

Conclusion: 知识蒸馏通常不利于去偏能力的迁移，但通过针对性设计（如数据增强、迭代蒸馏和权重初始化），可以改善这一问题。本研究首次揭示了KD对去偏的影响及其内在机制，为设计更优的去偏方法和理解KD的工作原理提供了重要见解。

Abstract: Knowledge distillation (KD) is an effective method for model compression and
transferring knowledge between models. However, its effect on model's
robustness against spurious correlations that degrade performance on
out-of-distribution data remains underexplored. This study investigates the
effect of knowledge distillation on the transferability of ``debiasing''
capabilities from teacher models to student models on natural language
inference (NLI) and image classification tasks. Through extensive experiments,
we illustrate several key findings: (i) overall the debiasing capability of a
model is undermined post-KD; (ii) training a debiased model does not benefit
from injecting teacher knowledge; (iii) although the overall robustness of a
model may remain stable post-distillation, significant variations can occur
across different types of biases; and (iv) we pin-point the internal attention
pattern and circuit that causes the distinct behavior post-KD. Given the above
findings, we propose three effective solutions to improve the distillability of
debiasing methods: developing high quality data for augmentation, implementing
iterative knowledge distillation, and initializing student models with weights
obtained from teacher models. To the best of our knowledge, this is the first
study on the effect of KD on debiasing and its interenal mechanism at scale.
Our findings provide understandings on how KD works and how to design better
debiasing methods.

</details>


### [19] [Towards Scaling Laws for Symbolic Regression](https://arxiv.org/abs/2510.26064)
*David Otte,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 本文首次系统研究了符号回归（SR）中的缩放规律，发现验证损失和求解率随计算量呈幂律变化，并确定了最优超参数缩放策略。


<details>
  <summary>Details</summary>
Motivation: 符号回归有望揭示数据背后的数学表达式，提供科学洞察并构建可解释、可泛化的表格数据模型；然而，尽管基于深度学习的SR方法已与遗传编程方法相媲美，计算规模对SR性能的影响尚未被系统探索。

Method: 作者构建了一个可扩展的端到端Transformer流水线，并使用精心生成的训练数据，系统地研究了五种不同模型规模（跨越三个数量级的计算量）下的缩放行为。

Result: 实验发现验证损失和求解率均随计算量呈现清晰的幂律趋势；同时确定了计算最优的超参数缩放策略：最优批大小和学习率随模型规模增长，且在当前实验范围内，token与参数比约为15时性能最优，并随计算量略有上升趋势。

Conclusion: 符号回归的性能在很大程度上可通过计算量预测，该研究为下一代SR模型的训练提供了重要指导。

Abstract: Symbolic regression (SR) aims to discover the underlying mathematical
expressions that explain observed data. This holds promise for both gaining
scientific insight and for producing inherently interpretable and generalizable
models for tabular data. In this work we focus on the basics of SR. Deep
learning-based SR has recently become competitive with genetic programming
approaches, but the role of scale has remained largely unexplored. Inspired by
scaling laws in language modeling, we present the first systematic
investigation of scaling in SR, using a scalable end-to-end transformer
pipeline and carefully generated training data. Across five different model
sizes and spanning three orders of magnitude in compute, we find that both
validation loss and solved rate follow clear power-law trends with compute. We
further identify compute-optimal hyperparameter scaling: optimal batch size and
learning rate grow with model size, and a token-to-parameter ratio of
$\approx$15 is optimal in our regime, with a slight upward trend as compute
increases. These results demonstrate that SR performance is largely predictable
from compute and offer important insights for training the next generation of
SR models.

</details>


### [20] [Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization](https://arxiv.org/abs/2510.26068)
*Di Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种新的机器学习范式，通过优化流形上的度量张量场来动态塑造模型的几何结构，而非在固定几何空间中优化参数。该方法结合数据保真度与几何复杂度构建变分框架，并利用离散微分几何实现高效计算，展现出比传统固定几何模型更强的表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在固定几何空间中优化参数，难以灵活适应复杂数据结构。作者希望将模型本身视为可塑的几何实体，通过动态调整其几何结构以提升表达能力和泛化性能。

Method: 构建一个变分框架，其损失函数平衡数据拟合与流形内在几何复杂度；采用离散微分几何方法，将连续流形离散为三角网格，并以边长参数化度量张量，利用自动微分进行优化。

Result: 理论分析揭示了该框架与广义相对论中爱因斯坦-希尔伯特作用量的深刻类比；即使拓扑结构固定，度量优化仍显著提升模型表达能力，并为构建能自主演化几何与拓扑的“元学习器”奠定基础。

Conclusion: 该工作为数据驱动的动态几何建模提供了新思路，在科学模型发现和鲁棒表示学习等领域具有广阔应用前景。

Abstract: This paper proposes a novel paradigm for machine learning that moves beyond
traditional parameter optimization. Unlike conventional approaches that search
for optimal parameters within a fixed geometric space, our core idea is to
treat the model itself as a malleable geometric entity. Specifically, we
optimize the metric tensor field on a manifold with a predefined topology,
thereby dynamically shaping the geometric structure of the model space. To
achieve this, we construct a variational framework whose loss function
carefully balances data fidelity against the intrinsic geometric complexity of
the manifold. The former ensures the model effectively explains observed data,
while the latter acts as a regularizer, penalizing overly curved or irregular
geometries to encourage simpler models and prevent overfitting. To address the
computational challenges of this infinite-dimensional optimization problem, we
introduce a practical method based on discrete differential geometry: the
continuous manifold is discretized into a triangular mesh, and the metric
tensor is parameterized by edge lengths, enabling efficient optimization using
automatic differentiation tools. Theoretical analysis reveals a profound
analogy between our framework and the Einstein-Hilbert action in general
relativity, providing an elegant physical interpretation for the concept of
"data-driven geometry". We further argue that even with fixed topology, metric
optimization offers significantly greater expressive power than models with
fixed geometry. This work lays a solid foundation for constructing fully
dynamic "meta-learners" capable of autonomously evolving their geometry and
topology, and it points to broad application prospects in areas such as
scientific model discovery and robust representation learning.

</details>


### [21] [New Money: A Systematic Review of Synthetic Data Generation for Finance](https://arxiv.org/abs/2510.26076)
*James Meldrum,Basem Suleiman,Fethi Rabhi,Muhammad Johan Alibasa*

Main category: cs.LG

TL;DR: 本文系统综述了2018年以来72项关于金融合成数据生成的研究，总结了常用的数据类型、生成方法（以GAN为主）和评估策略，并指出当前研究在隐私保障评估方面存在明显不足。


<details>
  <summary>Details</summary>
Motivation: 为应对在机器学习应用中使用敏感金融数据所带来的隐私风险与监管限制，合成数据生成成为一种有前景的解决方案；然而，该领域缺乏对现有研究的系统性综述。

Method: 对2018年以来发表的72篇聚焦于金融合成数据生成的研究进行系统性综述，分类分析所合成的金融信息类型、使用的生成模型（如GAN、VAE）以及评估数据效用与隐私的方法。

Result: GAN类方法在文献中占主导地位，尤其适用于生成时间序列市场数据和表格型信用数据；尽管已有若干创新技术提升了数据真实性和隐私保护能力，但多数研究缺乏对隐私保障机制的严格评估。

Conclusion: 本综述整合了金融合成数据领域的生成技术、应用场景与评估方法，揭示了关键研究空白，并为未来开发稳健且隐私安全的合成数据方案提供了方向。

Abstract: Synthetic data generation has emerged as a promising approach to address the
challenges of using sensitive financial data in machine learning applications.
By leveraging generative models, such as Generative Adversarial Networks (GANs)
and Variational Autoencoders (VAEs), it is possible to create artificial
datasets that preserve the statistical properties of real financial records
while mitigating privacy risks and regulatory constraints. Despite the rapid
growth of this field, a comprehensive synthesis of the current research
landscape has been lacking. This systematic review consolidates and analyses 72
studies published since 2018 that focus on synthetic financial data generation.
We categorise the types of financial information synthesised, the generative
methods employed, and the evaluation strategies used to assess data utility and
privacy. The findings indicate that GAN-based approaches dominate the
literature, particularly for generating time-series market data and tabular
credit data. While several innovative techniques demonstrate potential for
improved realism and privacy preservation, there remains a notable lack of
rigorous evaluation of privacy safeguards across studies. By providing an
integrated overview of generative techniques, applications, and evaluation
methods, this review highlights critical research gaps and offers guidance for
future work aimed at developing robust, privacy-preserving synthetic data
solutions for the financial domain.

</details>


### [22] [LLMBisect: Breaking Barriers in Bug Bisection with A Comparative Analysis Pipeline](https://arxiv.org/abs/2510.26086)
*Zheng Zhang,Haonan Li,Xingyu Li,Hang Zhang,Zhiyun Qian*

Main category: cs.LG

TL;DR: 本文提出一种基于大语言模型（LLM）的多阶段漏洞引入提交（BIC）识别方法，显著优于现有技术，准确率提升超过38%。


<details>
  <summary>Details</summary>
Motivation: 传统基于补丁的二分法存在多个局限：假设漏洞引入提交与补丁修改相同函数（未必成立）、仅依赖代码变更而忽略提交信息中的文本线索、使用简单启发式规则而缺乏对漏洞逻辑的深入分析。

Method: 设计了一个多阶段流水线，利用LLM综合分析补丁信息、在上下文中比较多个候选提交，并通过逐步筛选缩小候选范围。

Result: 实验表明，该方法比当前最先进的方案准确率高出38%以上；相比基础的LLM二分法，多阶段流程使准确率提升60%。

Conclusion: 利用LLM构建的多阶段BIC识别流程能有效克服传统方法的局限，在漏洞二分任务中显著提升准确性。

Abstract: Bug bisection has been an important security task that aims to understand the
range of software versions impacted by a bug, i.e., identifying the commit that
introduced the bug. However, traditional patch-based bisection methods are
faced with several significant barriers: For example, they assume that the
bug-inducing commit (BIC) and the patch commit modify the same functions, which
is not always true. They often rely solely on code changes, while the commit
message frequently contains a wealth of vulnerability-related information. They
are also based on simple heuristics (e.g., assuming the BIC initializes lines
deleted in the patch) and lack any logical analysis of the vulnerability.
  In this paper, we make the observation that Large Language Models (LLMs) are
well-positioned to break the barriers of existing solutions, e.g., comprehend
both textual data and code in patches and commits. Unlike previous BIC
identification approaches, which yield poor results, we propose a comprehensive
multi-stage pipeline that leverages LLMs to: (1) fully utilize patch
information, (2) compare multiple candidate commits in context, and (3)
progressively narrow down the candidates through a series of down-selection
steps. In our evaluation, we demonstrate that our approach achieves
significantly better accuracy than the state-of-the-art solution by more than
38\%. Our results further confirm that the comprehensive multi-stage pipeline
is essential, as it improves accuracy by 60\% over a baseline LLM-based
bisection method.

</details>


### [23] [SAFE: A Novel Approach to AI Weather Evaluation through Stratified Assessments of Forecasts over Earth](https://arxiv.org/abs/2510.26099)
*Nick Masi,Randall Balestriero*

Main category: cs.LG

TL;DR: 该论文提出了一种名为 SAFE（Stratified Assessments of Forecasts over Earth）的新评估框架，用于分析地球范围内天气和气候预测模型在不同地理和社会经济分层（如国家、收入水平、陆地/水域）下的性能差异，揭示了现有先进AI天气模型在各类分层中存在显著性能不均，并倡导以分层公平性替代传统的全局平均评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习评估方法依赖于测试集上的平均损失，忽视了地球表面人类发展和地理特征分布的非均匀性，无法揭示模型在不同区域或人群中的表现差异。

Method: 开发并应用 SAFE 工具包，整合多源地理和社会经济数据，按国家、全球子区域、收入水平和土地覆盖类型等属性对预测结果进行分层评估。

Result: 对多个前沿AI天气预测模型的评估显示，它们在所有分层属性上均存在显著的预测技能差异；SAFE 能有效识别模型表现最佳与最差的区域，并支持对模型公平性的量化分析。

Conclusion: 仅依赖全局平均指标会掩盖模型在特定区域或群体中的性能缺陷，SAFE 提供了一种更细致、公平的评估范式，有助于推动更具包容性和可靠性的气候与天气预测模型发展。

Abstract: The dominant paradigm in machine learning is to assess model performance
based on average loss across all samples in some test set. This amounts to
averaging performance geospatially across the Earth in weather and climate
settings, failing to account for the non-uniform distribution of human
development and geography. We introduce Stratified Assessments of Forecasts
over Earth (SAFE), a package for elucidating the stratified performance of a
set of predictions made over Earth. SAFE integrates various data domains to
stratify by different attributes associated with geospatial gridpoints:
territory (usually country), global subregion, income, and landcover (land or
water). This allows us to examine the performance of models for each individual
stratum of the different attributes (e.g., the accuracy in every individual
country). To demonstrate its importance, we utilize SAFE to benchmark a zoo of
state-of-the-art AI-based weather prediction models, finding that they all
exhibit disparities in forecasting skill across every attribute. We use this to
seed a benchmark of model forecast fairness through stratification at different
lead times for various climatic variables. By moving beyond globally-averaged
metrics, we for the first time ask: where do models perform best or worst, and
which models are most fair? To support further work in this direction, the SAFE
package is open source and available at https://github.com/N-Masi/safe

</details>


### [24] [Do Not Step Into the Same River Twice: Learning to Reason from Trial and Error](https://arxiv.org/abs/2510.26109)
*Chenming Tang,Hsiu-Yuan Huang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: 本文提出LTE（Learning to reason from Trial and Error）方法，通过利用大语言模型自身生成的错误答案和过长回答作为提示，无需外部专家指导即可缓解强化学习中的探索停滞问题，在多个数学基准上显著优于标准GRPO方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于可验证奖励的强化学习（RLVR）方法受限于大语言模型初始能力，易陷入探索停滞，而依赖外部专家指导的离策略方法又面临专家资源稀缺的问题。

Method: 提出LTE方法，将模型先前自生成的错误答案和过长响应作为提示信号，融入训练过程，以增强探索与利用，且无需任何外部专家干预。

Result: 在六个数学基准上，LTE相比标准GRPO方法平均提升Pass@1达6.38、Pass@k达9.00（以Qwen3-4B-Base为基座模型），并有效缓解了探索停滞问题。

Conclusion: LTE是一种无需外部指导的有效训练策略，能显著提升大语言模型在推理任务中的性能，并改善强化学习过程中的探索与利用平衡。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has significantly
boosted the reasoning capability of large language models (LLMs) recently.
However, existing RLVR approaches merely train LLMs based on their own
generated responses and are constrained by the initial capability of LLMs, thus
prone to exploration stagnation, in which LLMs fail to solve more training
problems and cannot further learn from the training data. Some work tries to
address this by leveraging off-policy solutions to training problems but
requires external guidance from experts which suffers from limited
availability. In this work, we propose LTE (Learning to reason from Trial and
Error), an approach hinting LLMs with their previously self-generated incorrect
answers and problem of overlong responses, which does not require any external
expert guidance. Experiments validate the effectiveness of LTE, which
outperforms the normal group relative policy optimization (GRPO) by 6.38 in
Pass@1 and 9.00 in Pass@k on average across six mathematics benchmarks for
Qwen3-4B-Base. Further analysis confirms that LTE successfully mitigates the
problem of exploration stagnation and enhances both exploitation and
exploration during training.

</details>


### [25] [maxVSTAR: Maximally Adaptive Vision-Guided CSI Sensing with Closed-Loop Edge Model Adaptation for Robust Human Activity Recognition](https://arxiv.org/abs/2510.26146)
*Kexing Liu*

Main category: cs.LG

TL;DR: 本文提出maxVSTAR，一种闭环视觉引导的模型自适应框架，用于解决WiFi信道状态信息（CSI）在边缘设备上进行人体活动识别时因环境和硬件变化导致的域偏移问题。


<details>
  <summary>Details</summary>
Motivation: WiFi CSI-based HAR在边缘部署时面临域偏移挑战，即在不同环境和硬件条件下识别性能显著下降，限制了其实际应用。

Method: 提出maxVSTAR框架，采用跨模态教师-学生架构：以高精度YOLO视觉模型作为教师，为CSI数据流提供实时活动标签；轻量级CSI模型STAR作为学生，在边缘端利用这些标签进行在线自主微调，实现闭环自适应。

Result: 实验表明，在未校准硬件上，基线STAR模型准确率从93.52%降至49.14%，而经过一次视觉引导的自适应后，maxVSTAR将准确率恢复至81.51%。

Conclusion: maxVSTAR实现了在隐私保护的物联网环境中对CSI感知系统的动态、自监督模型自适应，为边缘端长期自主的人体活动识别提供了可扩展且实用的范式。

Abstract: WiFi Channel State Information (CSI)-based human activity recognition (HAR)
provides a privacy-preserving, device-free sensing solution for smart
environments. However, its deployment on edge devices is severely constrained
by domain shift, where recognition performance deteriorates under varying
environmental and hardware conditions. This study presents maxVSTAR (maximally
adaptive Vision-guided Sensing Technology for Activity Recognition), a
closed-loop, vision-guided model adaptation framework that autonomously
mitigates domain shift for edge-deployed CSI sensing systems. The proposed
system integrates a cross-modal teacher-student architecture, where a
high-accuracy YOLO-based vision model serves as a dynamic supervisory signal,
delivering real-time activity labels for the CSI data stream. These labels
enable autonomous, online fine-tuning of a lightweight CSI-based HAR model,
termed Sensing Technology for Activity Recognition (STAR), directly at the
edge. This closed-loop retraining mechanism allows STAR to continuously adapt
to environmental changes without manual intervention. Extensive experiments
demonstrate the effectiveness of maxVSTAR. When deployed on uncalibrated
hardware, the baseline STAR model's recognition accuracy declined from 93.52%
to 49.14%. Following a single vision-guided adaptation cycle, maxVSTAR restored
the accuracy to 81.51%. These results confirm the system's capacity for
dynamic, self-supervised model adaptation in privacy-conscious IoT
environments, establishing a scalable and practical paradigm for long-term
autonomous HAR using CSI sensing at the network edge.

</details>


### [26] [Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment](https://arxiv.org/abs/2510.26157)
*Hyuntae Park,Yeachan Kim,SangKeun Lee*

Main category: cs.LG

TL;DR: MolBridge 是一种基于子结构对齐的分子-文本学习框架，通过引入子结构与化学短语之间的细粒度对齐信号，并结合对比学习与自优化机制，显著提升了分子表示学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉分子与其文本描述之间的细微差异，缺乏对分子子结构与化学短语之间细粒度对齐的学习能力。

Method: 提出 MolBridge 框架，通过增强原始分子-描述对，引入来自分子子结构和化学短语的对齐信号，并采用子结构感知的对比学习与自优化机制来过滤噪声对齐。

Result: MolBridge 在多种分子基准任务上优于当前最先进的方法，有效捕捉了分子与文本间的细粒度对应关系。

Conclusion: 子结构感知的对齐机制对提升分子-文本表示学习具有重要意义，MolBridge 为此提供了一个有效解决方案。

Abstract: Molecule and text representation learning has gained increasing interest due
to its potential for enhancing the understanding of chemical information.
However, existing models often struggle to capture subtle differences between
molecules and their descriptions, as they lack the ability to learn
fine-grained alignments between molecular substructures and chemical phrases.
To address this limitation, we introduce MolBridge, a novel molecule-text
learning framework based on substructure-aware alignments. Specifically, we
augment the original molecule-description pairs with additional alignment
signals derived from molecular substructures and chemical phrases. To
effectively learn from these enriched alignments, MolBridge employs
substructure-aware contrastive learning, coupled with a self-refinement
mechanism that filters out noisy alignment signals. Experimental results show
that MolBridge effectively captures fine-grained correspondences and
outperforms state-of-the-art baselines on a wide range of molecular benchmarks,
highlighting the significance of substructure-aware alignment in molecule-text
learning.

</details>


### [27] [Segmentation over Complexity: Evaluating Ensemble and Hybrid Approaches for Anomaly Detection in Industrial Time Series](https://arxiv.org/abs/2510.26159)
*Emilio Mastriani,Alessandro Costa,Federico Incardona,Kevin Munari,Sebastiano Spinello*

Main category: cs.LG

TL;DR: 在针对蒸汽轮机系统的多元工业时序异常检测任务中，尽管采用了基于变点统计特征、聚类子结构表示和混合学习策略等复杂方法，其性能仍不及一个基于分段数据训练的简单Random Forest + XGBoost集成模型。该集成模型取得了0.976的AUC-ROC、0.41的F1分数和100%的早期检测率，表明在高度不平衡且时间不确定的数据场景下，模型简洁性与优化分段策略更具优势。


<details>
  <summary>Details</summary>
Motivation: 探索先进特征工程与混合模型架构在多元工业时序异常检测中的有效性，尤其是在数据高度不平衡和时间不确定性强的实际工业场景中，是否复杂方法一定优于简单模型。

Method: 对比多种复杂方法（包括基于变点的统计特征、聚类子结构表示和混合学习策略）与一个基于分段数据训练的Random Forest + XGBoost集成模型的性能。

Result: 复杂方法表现一致不如简单集成模型；该集成模型在测试中达到AUC-ROC为0.976、F1-score为0.41，并实现100%的早期异常检测。

Conclusion: 在高度不平衡和时间不确定的工业时序数据中，模型简洁性结合优化的数据分段策略可提供更优的鲁棒性、可解释性和实用价值，优于复杂的架构设计。

Abstract: In this study, we investigate the effectiveness of advanced feature
engineering and hybrid model architectures for anomaly detection in a
multivariate industrial time series, focusing on a steam turbine system. We
evaluate the impact of change point-derived statistical features,
clustering-based substructure representations, and hybrid learning strategies
on detection performance. Despite their theoretical appeal, these complex
approaches consistently underperformed compared to a simple Random Forest +
XGBoost ensemble trained on segmented data. The ensemble achieved an AUC-ROC of
0.976, F1-score of 0.41, and 100% early detection within the defined time
window. Our findings highlight that, in scenarios with highly imbalanced and
temporally uncertain data, model simplicity combined with optimized
segmentation can outperform more sophisticated architectures, offering greater
robustness, interpretability, and operational utility.

</details>


### [28] [A Game-Theoretic Spatio-Temporal Reinforcement Learning Framework for Collaborative Public Resource Allocation](https://arxiv.org/abs/2510.26184)
*Songxin Lei,Qiongyan Wang,Yanchen Zhu,Hanyu Yao,Sijie Ruan,Weilin Ruan,Yuyu Luo,Huaming Wu,Yuxuan Liang*

Main category: cs.LG

TL;DR: 本文提出了协作式公共资源分配（CPRA）问题，并设计了一种基于博弈论与时空强化学习的框架（GSTRL）来解决该问题，在真实数据集上验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有公共资源分配方法通常独立优化单个资源的调度，忽略了资源容量限制和时空动态特性，难以满足现实场景中的复杂需求。

Method: 作者将CPRA建模为势博弈问题，并提出Game-Theoretic Spatio-Temporal Reinforcement Learning（GSTRL）框架，结合势博弈理论与强化学习，以逼近NP难问题的纳什均衡。

Result: 在两个真实世界数据集上的实验表明，所提出的GSTRL框架在性能上优于现有方法。

Conclusion: 该研究通过引入容量约束和时空动态，构建了更贴近实际的公共资源分配模型，并通过理论分析与实验验证了所提方法的有效性。

Abstract: Public resource allocation involves the efficient distribution of resources,
including urban infrastructure, energy, and transportation, to effectively meet
societal demands. However, existing methods focus on optimizing the movement of
individual resources independently, without considering their capacity
constraints. To address this limitation, we propose a novel and more practical
problem: Collaborative Public Resource Allocation (CPRA), which explicitly
incorporates capacity constraints and spatio-temporal dynamics in real-world
scenarios. We propose a new framework called Game-Theoretic Spatio-Temporal
Reinforcement Learning (GSTRL) for solving CPRA. Our contributions are twofold:
1) We formulate the CPRA problem as a potential game and demonstrate that there
is no gap between the potential function and the optimal target, laying a solid
theoretical foundation for approximating the Nash equilibrium of this NP-hard
problem; and 2) Our designed GSTRL framework effectively captures the
spatio-temporal dynamics of the overall system. We evaluate GSTRL on two
real-world datasets, where experiments show its superior performance. Our
source codes are available in the supplementary materials.

</details>


### [29] [Predicting All-Cause Hospital Readmissions from Medical Claims Data of Hospitalised Patients](https://arxiv.org/abs/2510.26188)
*Avinash Kadimisetty,Arun Rajagopalan,Vijendra SK*

Main category: cs.LG

TL;DR: 该研究利用逻辑回归、随机森林和支持向量机等机器学习方法，结合主成分分析降维，对高维健康理赔数据进行建模，以预测全因再入院风险。结果显示随机森林模型性能最优，有助于识别关键影响因素和高风险患者，从而降低再入院率、成本并提升医疗质量。


<details>
  <summary>Details</summary>
Motivation: 降低可预防的医院再入院率是支付方、医疗服务提供者和政策制定者提高医疗质量并降低成本的重要目标，而再入院率也被用作评估医院医疗质量的指标。

Method: 使用逻辑回归、随机森林和支持向量机等机器学习方法分析健康理赔数据，并采用主成分分析（PCA）进行降维，随后基于降维后的数据构建回归模型，并以AUC指标评估模型性能。

Result: 随机森林模型表现最佳，其次为逻辑回归和支持向量机模型，这些模型能够有效识别导致再入院的关键人口统计学和医学因素。

Conclusion: 所构建的模型可用于识别再入院的关键因素和高风险患者，从而帮助降低再入院率、医疗成本，并提升患者所接受的医疗服务质量。

Abstract: Reducing preventable hospital readmissions is a national priority for payers,
providers, and policymakers seeking to improve health care and lower costs. The
rate of readmission is being used as a benchmark to determine the quality of
healthcare provided by the hospitals. In thisproject, we have used machine
learning techniques like Logistic Regression, Random Forest and Support Vector
Machines to analyze the health claims data and identify demographic and medical
factors that play a crucial role in predicting all-cause readmissions. As the
health claims data is high dimensional, we have used Principal Component
Analysis as a dimension reduction technique and used the results for building
regression models. We compared and evaluated these models based on the Area
Under Curve (AUC) metric. Random Forest model gave the highest performance
followed by Logistic Regression and Support Vector Machine models. These models
can be used to identify the crucial factors causing readmissions and help
identify patients to focus on to reduce the chances of readmission, ultimately
bringing down the cost and increasing the quality of healthcare provided to the
patients.

</details>


### [30] [Test-Time Alignment of LLMs via Sampling-Based Optimal Control in pre-logit space](https://arxiv.org/abs/2510.26219)
*Sekitoshi Kanai,Tsukasa Yoshida,Hiroshi Takahashi,Haru Kuroki,Kazumune Hashimoto*

Main category: cs.LG

TL;DR: 本文提出了一种新的测试时对齐方法AISP，通过在预logits上应用高斯扰动并利用重要性采样优化扰动均值，从而在样本效率和奖励得分上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型微调的计算成本高昂，测试时对齐成为研究热点；现有方法如best-of-n采样在样本效率和奖励优化方面存在不足。

Method: 提出自适应重要性采样预logits（AISP）方法，基于采样模型预测控制框架，在预logits（倒数第二层输出）上施加高斯扰动，并通过重要性采样利用采样奖励优化扰动均值以最大化期望奖励。

Result: AISP在相同样本数量下获得比best-of-n采样更高的奖励，并优于其他基于奖励的测试时对齐方法。

Conclusion: AISP是一种高效且有效的测试时对齐方法，能够在不进行微调的情况下提升大语言模型的对齐性能。

Abstract: Test-time alignment of large language models (LLMs) attracts attention
because fine-tuning LLMs requires high computational costs. In this paper, we
propose a new test-time alignment method called adaptive importance sampling on
pre-logits (AISP) on the basis of the sampling-based model predictive control
with the stochastic control input. AISP applies the Gaussian perturbation into
pre-logits, which are outputs of the penultimate layer, so as to maximize
expected rewards with respect to the mean of the perturbation. We demonstrate
that the optimal mean is obtained by importance sampling with sampled rewards.
AISP outperforms best-of-n sampling in terms of rewards over the number of used
samples and achieves higher rewards than other reward-based test-time alignment
methods.

</details>


### [31] [MPRU: Modular Projection-Redistribution Unlearning as Output Filter for Classification Pipelines](https://arxiv.org/abs/2510.26230)
*Minyi Peng,Darian Gunamardi,Ivan Tjuawinata,Kwok-Yan Lam*

Main category: cs.LG

TL;DR: 本文提出一种新的机器遗忘方法，通过将分类训练视为顺序学习过程，并在模型末端添加投影-重分布层来实现高效遗忘，无需访问原始数据或完整模型，具有模块化、模型无关性和高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在实际部署中面临可扩展性差、需完全访问原始数据和模型等挑战，难以满足现实场景需求。

Method: 将分类训练建模为顺序学习过程（归纳式方法），通过逆向最后的训练步骤实现遗忘，具体是在模型末端附加一个投影-重分布层，作为输出过滤器集成到现有分类流程中。

Result: 在多个图像（CIFAR-10/100）和表格（Covertype）数据集上的实验表明，该方法输出结果与完全重训练模型高度一致，同时显著降低计算开销。

Conclusion: 所提方法在保持输出性能的同时，具备良好的实用性、可扩展性和系统兼容性，克服了现有机器遗忘方法的局限。

Abstract: As a new and promising approach, existing machine unlearning (MU) works
typically emphasize theoretical formulations or optimization objectives to
achieve knowledge removal. However, when deployed in real-world scenarios, such
solutions typically face scalability issues and have to address practical
requirements such as full access to original datasets and model. In contrast to
the existing approaches, we regard classification training as a sequential
process where classes are learned sequentially, which we call \emph{inductive
approach}. Unlearning can then be done by reversing the last training sequence.
This is implemented by appending a projection-redistribution layer in the end
of the model. Such an approach does not require full access to the original
dataset or the model, addressing the challenges of existing methods. This
enables modular and model-agnostic deployment as an output filter into existing
classification pipelines with minimal alterations. We conducted multiple
experiments across multiple datasets including image (CIFAR-10/100 using
CNN-based model) and tabular datasets (Covertype using tree-based model).
Experiment results show consistently similar output to a fully retrained model
with a high computational cost reduction. This demonstrates the applicability,
scalability, and system compatibility of our solution while maintaining the
performance of the output in a more practical setting.

</details>


### [32] [Angular Steering: Behavior Control via Rotation in Activation Space](https://arxiv.org/abs/2510.26243)
*Hieu M. Vu,Tan M. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种名为 Angular Steering 的新方法，通过在固定二维子空间中旋转向量来精细调控大语言模型的特定行为（如拒绝或顺从），并在保持模型通用能力的同时提升控制的稳定性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有行为调控方法（如向量加法和方向消融）受限于二维子空间，对参数选择敏感且可能干扰无关特征，难以在保持模型通用能力的同时精准控制特定行为。

Method: Angular Steering 将行为调控建模为在激活空间中围绕目标行为方向的几何旋转；进一步提出 Adaptive Angular Steering，仅对与目标特征对齐的激活进行旋转，以增强稳定性。

Result: 在多个模型家族和规模上的实验表明，Angular Steering 能有效实现鲁棒的行为控制，同时保持语言建模性能，并统一了现有方法（如加法和正交化）于一个几何旋转框架下。

Conclusion: Angular Steering 提供了一种灵活、稳定且通用的行为调控方法，显著优于现有技术，在安全可靠的人工智能部署中具有重要应用价值。

Abstract: Controlling specific behaviors in large language models while preserving
their general capabilities is a central challenge for safe and reliable
artificial intelligence deployment. Current steering methods, such as vector
addition and directional ablation, are constrained within a two-dimensional
subspace defined by the activation and feature direction, making them sensitive
to chosen parameters and potentially affecting unrelated features due to
unintended interactions in activation space. We introduce Angular Steering, a
novel and flexible method for behavior modulation that operates by rotating
activations within a fixed two-dimensional subspace. By formulating steering as
a geometric rotation toward or away from a target behavior direction, Angular
Steering provides continuous, fine-grained control over behaviors such as
refusal and compliance. We demonstrate this method using refusal steering
emotion steering as use cases. Additionally, we propose Adaptive Angular
Steering, a selective variant that rotates only activations aligned with the
target feature, further enhancing stability and coherence. Angular Steering
generalizes existing addition and orthogonalization techniques under a unified
geometric rotation framework, simplifying parameter selection and maintaining
model stability across a broader range of adjustments. Experiments across
multiple model families and sizes show that Angular Steering achieves robust
behavioral control while maintaining general language modeling performance,
underscoring its flexibility, generalization, and robustness compared to prior
approaches. Code and artifacts are available at
https://github.com/lone17/angular-steering/.

</details>


### [33] [Likely Interpolants of Generative Models](https://arxiv.org/abs/2510.26266)
*Frederik Möbius Rygaard,Shen Zhu,Yinzhu Jin,Søren Hauberg,Tom Fletcher*

Main category: cs.LG

TL;DR: 本文提出了一种通用的插值方法，通过在数据分布约束下模拟测地线路径，无需额外训练即可生成高密度区域的插值结果，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型大多缺乏一种普适的插值概念，且通常对模型或数据维度有严格假设，限制了插值在可控生成和模型分析中的应用。

Method: 作者构建了一种类测地线的插值方案，该方案在合适的黎曼度量下约束于数据分布，并提出了一种无需额外训练的新算法来计算这些插值曲线。

Result: 实验表明，该方法在多种模型和数据集上生成的插值路径经过的概率密度区域显著高于基线方法。

Conclusion: 所提出的插值方法具有通用性和理论基础，能有效提升插值质量，适用于多种生成模型和数据分布。

Abstract: Interpolation in generative models allows for controlled generation, model
inspection, and more. Unfortunately, most generative models lack a principal
notion of interpolants without restrictive assumptions on either the model or
data dimension. In this paper, we develop a general interpolation scheme that
targets likely transition paths compatible with different metrics and
probability distributions. We consider interpolants analogous to a geodesic
constrained to a suitable data distribution and derive a novel algorithm for
computing these curves, which requires no additional training. Theoretically,
we show that our method locally can be considered as a geodesic under a
suitable Riemannian metric. We quantitatively show that our interpolation
scheme traverses higher density regions than baselines across a range of models
and datasets.

</details>


### [34] [Distributional Multi-objective Black-box Optimization for Diffusion-model Inference-time Multi-Target Generation](https://arxiv.org/abs/2510.26278)
*Kim Yong Tan,Yueming Lyu,Ivor Tsang,Yew-Soon Ong*

Main category: cs.LG

TL;DR: 本文提出了一种名为推理时多目标生成（IMG）的新算法，通过在扩散过程中进行加权重采样，使生成样本符合多目标玻尔兹曼分布，从而在单次生成中高效实现多目标优化，显著优于需多次生成的现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的多目标黑盒优化方法通常将扩散模型视为黑盒精炼器，忽略了其内部生成过程中的分布转移，导致效率受限。

Method: 提出IMG算法，在扩散生成过程中根据多目标值的加权进行重采样，使生成样本服从目标多目标玻尔兹曼分布，并揭示该分布具有对数似然解释，是最优分布解。

Result: 在多目标分子生成任务中，IMG仅需一次生成即可获得显著高于基线方法的超体积指标，而基线通常需数百次扩散生成。

Conclusion: IMG通过优化扩散过程本身实现高效多目标优化，不仅性能优越，还可作为模块集成到现有方法中进一步提升效果。

Abstract: Diffusion models have been successful in learning complex data distributions.
This capability has driven their application to high-dimensional
multi-objective black-box optimization problem. Existing approaches often
employ an external optimization loop, such as an evolutionary algorithm, to the
diffusion model. However, these approaches treat the diffusion model as a
black-box refiner, which overlooks the internal distribution transition of the
diffusion generation process, limiting their efficiency. To address these
challenges, we propose the Inference-time Multi-target Generation (IMG)
algorithm, which optimizes the diffusion process at inference-time to generate
samples that simultaneously satisfy multiple objectives. Specifically, our IMG
performs weighted resampling during the diffusion generation process according
to the expected aggregated multi-objective values. This weighted resampling
strategy ensures the diffusion-generated samples are distributed according to
our desired multi-target Boltzmann distribution. We further derive that the
multi-target Boltzmann distribution has an interesting log-likelihood
interpretation, where it is the optimal solution to the distributional
multi-objective optimization problem. We implemented IMG for a multi-objective
molecule generation task. Experiments show that IMG, requiring only a single
generation pass, achieves a significantly higher hypervolume than baseline
optimization algorithms that often require hundreds of diffusion generations.
Notably, our algorithm can be viewed as an optimized diffusion process and can
be integrated into existing methods to further improve their performance.

</details>


### [35] [Empirical Bayesian Multi-Bandit Learning](https://arxiv.org/abs/2510.26284)
*Xia Jiang,Rong J. B. Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种用于多臂老虎机多任务学习的分层贝叶斯框架，通过引入经验贝叶斯方法估计先验协方差矩阵，设计了ebmTS和ebmUCB两种高效算法，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多臂老虎机多任务学习中忽略了跨任务协方差结构的学习，限制了信息共享的有效性和模型灵活性。

Method: 提出一种分层贝叶斯框架，利用经验贝叶斯方法估计先验分布的协方差矩阵，并在此基础上开发ebmTS和ebmUCB两种算法，将估计的先验融入决策过程。

Result: 在合成和真实数据集上的实验表明，所提算法在复杂环境中具有更低的累积后悔值，优于现有方法；同时提供了算法的频域后悔上界。

Conclusion: 该方法有效结合了任务间的相关性与异质性，在多臂老虎机多任务学习中实现了更优的探索-利用平衡，提升了整体决策性能。

Abstract: Multi-task learning in contextual bandits has attracted significant research
interest due to its potential to enhance decision-making across multiple
related tasks by leveraging shared structures and task-specific heterogeneity.
In this article, we propose a novel hierarchical Bayesian framework for
learning in various bandit instances. This framework captures both the
heterogeneity and the correlations among different bandit instances through a
hierarchical Bayesian model, enabling effective information sharing while
accommodating instance-specific variations. Unlike previous methods that
overlook the learning of the covariance structure across bandits, we introduce
an empirical Bayesian approach to estimate the covariance matrix of the prior
distribution.This enhances both the practicality and flexibility of learning
across multi-bandits. Building on this approach, we develop two efficient
algorithms: ebmTS (Empirical Bayesian Multi-Bandit Thompson Sampling) and
ebmUCB (Empirical Bayesian Multi-Bandit Upper Confidence Bound), both of which
incorporate the estimated prior into the decision-making process. We provide
the frequentist regret upper bounds for the proposed algorithms, thereby
filling a research gap in the field of multi-bandit problems. Extensive
experiments on both synthetic and real-world datasets demonstrate the superior
performance of our algorithms, particularly in complex environments. Our
methods achieve lower cumulative regret compared to existing techniques,
highlighting their effectiveness in balancing exploration and exploitation
across multi-bandits.

</details>


### [36] [Offline Clustering of Preference Learning with Active-data Augmentation](https://arxiv.org/abs/2510.26301)
*Jingyuan Liu,Fatemeh Ghaffari,Xuchuang Wang,Mohammad Hajiesmaili,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 本文研究离线偏好学习中多用户偏好差异与数据不平衡问题，提出Off-C²PL算法进行用户聚类以聚合数据，并进一步引入A²-Off-C²PL算法通过主动数据增强优化对测试用户的效用。


<details>
  <summary>Details</summary>
Motivation: 现实中的偏好学习常涉及具有不同偏好的用户，且离线数据往往在用户间和偏好维度上分布不均，导致难以有效聚合数据并准确建模目标用户的偏好。

Method: 提出Off-C²PL算法，在纯离线设置下基于用户偏好聚类；并进一步设计A²-Off-C²PL算法，在聚类结构基础上主动选择信息量最大的样本进行有限的数据增强。

Result: 理论分析给出了子优性界，揭示了样本噪声与偏差之间的权衡；证明主动采集的样本比离线样本更有效；在合成和真实数据集上的实验验证了方法的有效性。

Conclusion: 通过聚类与主动数据增强相结合，能有效应对离线偏好学习中用户异质性与数据不平衡的挑战，提升对目标用户的偏好建模性能。

Abstract: Preference learning from pairwise feedback is a widely adopted framework in
applications such as reinforcement learning with human feedback and
recommendations. In many practical settings, however, user interactions are
limited or costly, making offline preference learning necessary. Moreover,
real-world preference learning often involves users with different preferences.
For example, annotators from different backgrounds may rank the same responses
differently. This setting presents two central challenges: (1) identifying
similarity across users to effectively aggregate data, especially under
scenarios where offline data is imbalanced across dimensions, and (2) handling
the imbalanced offline data where some preference dimensions are
underrepresented. To address these challenges, we study the Offline Clustering
of Preference Learning problem, where the learner has access to fixed datasets
from multiple users with potentially different preferences and aims to maximize
utility for a test user. To tackle the first challenge, we first propose
Off-C$^2$PL for the pure offline setting, where the learner relies solely on
offline data. Our theoretical analysis provides a suboptimality bound that
explicitly captures the tradeoff between sample noise and bias. To address the
second challenge of inbalanced data, we extend our framework to the setting
with active-data augmentation where the learner is allowed to select a limited
number of additional active-data for the test user based on the cluster
structure learned by Off-C$^2$PL. In this setting, our second algorithm,
A$^2$-Off-C$^2$PL, actively selects samples that target the least-informative
dimensions of the test user's preference. We prove that these actively
collected samples contribute more effectively than offline ones. Finally, we
validate our theoretical results through simulations on synthetic and
real-world datasets.

</details>


### [37] [Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens](https://arxiv.org/abs/2510.26302)
*Ziliang Chen,Tianang Xiao,Jusheng Zhang,Yongsen Zheng,Xipeng Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于语言标记的因果表征学习框架，揭示了CLIP模型在组合推理上的脆弱性源于“组合不可识别性”，并解释了其对硬负样本敏感的原因。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP模型虽在跨模态对齐上表现优异，但在对象、属性和关系的组合推理上存在明显缺陷，且以往因果分析方法将文本视为单一向量，无法解释提示敏感性和硬负样本失败等问题。

Method: 作者构建了一个基于语言标记的结构因果模型（SCM），将CLIP的对比学习目标与标记级因果表征学习相结合，并扩展了块可识别性理论至分词文本，分析组合不可识别性问题。

Result: 研究证明存在伪最优文本编码器，虽能实现模态不变对齐，却对原子概念的SWAP、REPLACE和ADD操作不敏感，导致无法区分正确描述与硬负样本；同时揭示了语言侧不可识别性如何通过模态间隙影响视觉侧性能。

Conclusion: CLIP的组合推理失败源于其对比目标下的组合不可识别性，该工作为理解此类模型局限性提供了首个原则性解释，并启发了更优的负样本挖掘策略。

Abstract: Contrastive Language-Image Pre-training (CLIP) delivers strong cross modal
generalization by aligning images and texts in a shared embedding space, yet it
persistently fails at compositional reasoning over objects, attributes, and
relations often behaving like a bag-of-words matcher. Prior causal accounts
typically model text as a single vector, obscuring token-level structure and
leaving core phenomena-such as prompt sensitivity and failures on hard
negatives unexplained. We address this gap with a token-aware causal
representation learning (CRL) framework grounded in a sequential,
language-token SCM. Our theory extends block identifiability to tokenized text,
proving that CLIP's contrastive objective can recover the modal-invariant
latent variable under both sentence-level and token-level SCMs. Crucially,
token granularity yields the first principled explanation of CLIP's
compositional brittleness: composition nonidentifiability. We show the
existence of pseudo-optimal text encoders that achieve perfect modal-invariant
alignment yet are provably insensitive to SWAP, REPLACE, and ADD operations
over atomic concepts, thereby failing to distinguish correct captions from hard
negatives despite optimizing the same training objective as true-optimal
encoders. The analysis further links language-side nonidentifiability to
visual-side failures via the modality gap and shows how iterated composition
operators compound hardness, motivating improved negative mining strategies.

</details>


### [38] [Model Inversion with Layer-Specific Modeling and Alignment for Data-Free Continual Learning](https://arxiv.org/abs/2510.26311)
*Ruilin Tong,Haodong Lu,Yuhang Liu,Dong Gong*

Main category: cs.LG

TL;DR: 本文提出一种数据无关的持续学习方法，通过逐层模型反演（PMI）高效生成伪样本，并结合高斯分布建模与对比学习对齐特征，有效缓解知识遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 在持续学习中，由于隐私或安全限制，无法存储和回放真实数据；而现有无数据方法在模型反演时存在合成数据漂移和计算开销大的问题，尤其在大型预训练模型（如CLIP）中更为严重。

Method: 提出逐层模型反演（PMI）以减少全模型反演的迭代次数，并通过高斯分布建模类别特征及对比学习确保合成数据与真实数据特征对齐，从而生成语义感知的伪图像用于回放。

Result: 该方法在多个持续学习设置下展现出优异性能和良好兼容性，能有效学习新类别同时保留旧知识。

Conclusion: 结合PMI与特征建模的无数据持续学习策略显著提升了合成数据质量与训练效率，为大规模预训练模型的持续学习提供了可行方案。

Abstract: Continual learning (CL) aims to incrementally train a model on a sequence of
tasks while retaining performance on prior ones. However, storing and replaying
data is often infeasible due to privacy or security constraints and impractical
for arbitrary pre-trained models. Data-free CL seeks to update models without
access to previous data. Beyond regularization, we employ model inversion to
synthesize data from the trained model, enabling replay without storing
samples. Yet, model inversion in predictive models faces two challenges: (1)
generating inputs solely from compressed output labels causes drift between
synthetic and real data, and replaying such data can erode prior knowledge; (2)
inversion is computationally expensive since each step backpropagates through
the full model. These issues are amplified in large pre-trained models such as
CLIP. To improve efficiency, we propose Per-layer Model Inversion (PMI),
inspired by faster convergence in single-layer optimization. PMI provides
strong initialization for full-model inversion, substantially reducing
iterations. To mitigate feature shift, we model class-wise features via
Gaussian distributions and contrastive model, ensuring alignment between
synthetic and real features. Combining PMI and feature modeling, our approach
enables continual learning of new classes by generating pseudo-images from
semantic-aware projected features, achieving strong effectiveness and
compatibility across multiple CL settings.

</details>


### [39] [On the Impact of Weight Discretization in QUBO-Based SVM Training](https://arxiv.org/abs/2510.26323)
*Sascha Mücke*

Main category: cs.LG

TL;DR: 将支持向量机（SVM）训练转化为QUBO问题后，即使使用极低精度（如1比特）的量子退火编码，也能获得与经典LIBSVM相当甚至更优的分类准确率；研究表明支持向量的选择比其权重精度更重要，展示了量子退火在SVM训练中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索将SVM训练问题转化为QUBO形式后，利用量子退火进行优化的可行性，并研究参数离散化精度（即量子比特数量）对模型预测性能的影响。

Method: 将SVM的对偶问题编码为QUBO问题，在不同比特精度下进行训练，并与经典LIBSVM求解器在多个数据集上进行性能对比。

Result: 即使使用1比特的低精度QUBO编码，也能获得具有竞争力甚至更优的分类准确率；增加比特深度虽允许更大的正则化参数，但并不总能提升分类效果。

Conclusion: 支持向量的选择可能比其权重的精确度更重要；尽管当前硬件限制了可求解QUBO问题的规模，但随着量子设备的发展，量子退火在高效SVM训练方面具有潜力。

Abstract: Training Support Vector Machines (SVMs) can be formulated as a QUBO problem,
enabling the use of quantum annealing for model optimization. In this work, we
study how the number of qubits - linked to the discretization level of dual
weights - affects predictive performance across datasets. We compare QUBO-based
SVM training to the classical LIBSVM solver and find that even low-precision
QUBO encodings (e.g., 1 bit per parameter) yield competitive, and sometimes
superior, accuracy. While increased bit-depth enables larger regularization
parameters, it does not always improve classification. Our findings suggest
that selecting the right support vectors may matter more than their precise
weighting. Although current hardware limits the size of solvable QUBOs, our
results highlight the potential of quantum annealing for efficient SVM training
as quantum devices scale.

</details>


### [40] [Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics](https://arxiv.org/abs/2510.26324)
*Zhiyang Xun,Shivam Gupta,Eric Price*

Main category: cs.LG

TL;DR: 本文研究在给定含噪线性观测和近似先验分布的情况下，如何高效地从后验分布中采样。作者提出结合扩散模型与退火朗之万动力学的方法，在仅需得分函数的$L^4$误差界条件下，实现多项式时间内的条件采样。


<details>
  <summary>Details</summary>
Motivation: 后验采样在图像修复、去模糊和MRI重建等任务中具有重要价值，但一般情况下近似后验采样计算上是困难的。现有方法如朗之万动力学对得分估计误差敏感，需要较强的误差假设（如MGF界），而无条件扩散模型仅需较弱的$L^2$界。因此，作者希望在条件采样场景下降低对得分误差的要求。

Method: 针对局部或全局对数凹的先验分布，作者将扩散模型与退火朗之万动力学相结合，提出一种新的条件采样算法。

Result: 该方法在仅需得分函数满足$L^4$误差界的情况下，可在多项式时间内实现有效的后验采样。

Conclusion: 通过结合扩散模型与退火朗之万动力学，可以在较弱的得分误差假设下高效完成条件后验采样，为实际应用提供了更鲁棒且可行的解决方案。

Abstract: Given a noisy linear measurement $y = Ax + \xi$ of a distribution $p(x)$, and
a good approximation to the prior $p(x)$, when can we sample from the posterior
$p(x \mid y)$? Posterior sampling provides an accurate and fair framework for
tasks such as inpainting, deblurring, and MRI reconstruction, and several
heuristics attempt to approximate it. Unfortunately, approximate posterior
sampling is computationally intractable in general.
  To sidestep this hardness, we focus on (local or global) log-concave
distributions $p(x)$. In this regime, Langevin dynamics yields posterior
samples when the exact scores of $p(x)$ are available, but it is brittle to
score--estimation error, requiring an MGF bound (sub-exponential error). By
contrast, in the unconditional setting, diffusion models succeed with only an
$L^2$ bound on the score error. We prove that combining diffusion models with
an annealed variant of Langevin dynamics achieves conditional sampling in
polynomial time using merely an $L^4$ bound on the score error.

</details>


### [41] [Linear Causal Discovery with Interventional Constraints](https://arxiv.org/abs/2510.26342)
*Zhigao Guo,Feng Dong*

Main category: cs.LG

TL;DR: 本文提出“干预约束”这一新概念，通过在因果效应上施加不等式约束来整合高层因果知识，从而改进因果发现模型的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法虽可施加结构约束，但无法确保学习到的因果效应方向与已知因果知识一致（如PIP3对Akt的正向作用），因此需要一种能直接约束因果效应的方法。

Method: 作者为线性因果模型定义了总因果效应的度量，并将干预约束形式化为一个约束优化问题，采用两阶段约束优化方法求解。

Result: 在真实数据集上的实验表明，引入干预约束不仅提升了模型准确性、保证了与已有发现的一致性，还帮助发现了新的因果关系。

Conclusion: 干预约束有效弥补了传统结构约束的不足，使因果模型更符合领域知识、更具可解释性，并有助于低成本发现新因果关系。

Abstract: Incorporating causal knowledge and mechanisms is essential for refining
causal models and improving downstream tasks such as designing new treatments.
In this paper, we introduce a novel concept in causal discovery, termed
interventional constraints, which differs fundamentally from interventional
data. While interventional data require direct perturbations of variables,
interventional constraints encode high-level causal knowledge in the form of
inequality constraints on causal effects. For instance, in the Sachs dataset
(Sachs et al.\ 2005), Akt has been shown to be activated by PIP3, meaning PIP3
exerts a positive causal effect on Akt. Existing causal discovery methods allow
enforcing structural constraints (for example, requiring a causal path from
PIP3 to Akt), but they may still produce incorrect causal conclusions such as
learning that "PIP3 inhibits Akt". Interventional constraints bridge this gap
by explicitly constraining the total causal effect between variable pairs,
ensuring learned models respect known causal influences. To formalize
interventional constraints, we propose a metric to quantify total causal
effects for linear causal models and formulate the problem as a constrained
optimization task, solved using a two-stage constrained optimization method. We
evaluate our approach on real-world datasets and demonstrate that integrating
interventional constraints not only improves model accuracy and ensures
consistency with established findings, making models more explainable, but also
facilitates the discovery of new causal relationships that would otherwise be
costly to identify.

</details>


### [42] [Reinforcement Learning for Pollution Detection in a Randomized, Sparse and Nonstationary Environment with an Autonomous Underwater Vehicle](https://arxiv.org/abs/2510.26347)
*Sebastian Zieglmeier,Niklas Erdmann,Narada D. Warakagoda*

Main category: cs.LG

TL;DR: 本文通过改进经典强化学习方法，使其在稀疏奖励、随机且非平稳的环境中表现更优，尤其在水下污染云搜索任务中，改进的蒙特卡洛方法显著优于传统Q学习和穷举搜索策略。


<details>
  <summary>Details</summary>
Motivation: 强化学习在随机、非平稳及奖励稀疏的环境中表现受限，尤其在如AUV搜索水下污染云等实际应用中，亟需提升算法在这些复杂条件下的适应能力。

Method: 系统研究多种改进策略，包括分层算法调整、多目标学习，以及引入位置记忆作为外部输出滤波器以避免重复访问状态，并采用改进的蒙特卡洛方法进行实验验证。

Result: 改进后的蒙特卡洛方法在稀疏、随机和非平稳环境中显著优于传统Q-learning和两种穷举搜索模式。

Conclusion: 强化学习方法可通过适当改进有效适应随机、非平稳和奖励稀疏的复杂环境。

Abstract: Reinforcement learning (RL) algorithms are designed to optimize
problem-solving by learning actions that maximize rewards, a task that becomes
particularly challenging in random and nonstationary environments. Even
advanced RL algorithms are often limited in their ability to solve problems in
these conditions. In applications such as searching for underwater pollution
clouds with autonomous underwater vehicles (AUVs), RL algorithms must navigate
reward-sparse environments, where actions frequently result in a zero reward.
This paper aims to address these challenges by revisiting and modifying
classical RL approaches to efficiently operate in sparse, randomized, and
nonstationary environments. We systematically study a large number of
modifications, including hierarchical algorithm changes, multigoal learning,
and the integration of a location memory as an external output filter to
prevent state revisits. Our results demonstrate that a modified Monte
Carlo-based approach significantly outperforms traditional Q-learning and two
exhaustive search patterns, illustrating its potential in adapting RL to
complex environments. These findings suggest that reinforcement learning
approaches can be effectively adapted for use in random, nonstationary, and
reward-sparse environments.

</details>


### [43] [CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse](https://arxiv.org/abs/2510.26369)
*Kazuma Kano,Yuki Mori,Shin Katayama,Kenta Urano,Takuro Yonezawa,Nobuo Kawaguchi*

Main category: cs.LG

TL;DR: 本文提出了一种名为CorVS的新方法，通过深度学习模型预测视觉轨迹与可穿戴传感器数据之间的对应关系，并利用该关系在真实仓库环境中实现鲁棒的人员识别。


<details>
  <summary>Details</summary>
Motivation: 在工业物流仓库中，仅依靠视觉数据难以可靠地识别人员身份；现有结合轨迹与传感器的方法在真实场景中易失效，因此需要一种更鲁棒的数据驱动方法。

Method: 提出CorVS方法：首先使用深度学习模型预测每对视觉轨迹与传感器测量之间的对应概率和可靠性，然后基于这些预测结果在时间维度上进行匹配。

Result: 在真实仓库操作数据集上验证了所提方法的有效性，表明其适用于实际应用场景。

Conclusion: CorVS通过融合视觉轨迹与传感器数据，实现了在复杂真实环境下的可靠人员识别，为工业场所的人员定位提供了有效解决方案。

Abstract: Worker location data is key to higher productivity in industrial sites.
Cameras are a promising tool for localization in logistics warehouses since
they also offer valuable environmental contexts such as package status.
However, identifying individuals with only visual data is often impractical.
Accordingly, several prior studies identified people in videos by comparing
their trajectories and wearable sensor measurements. While this approach has
advantages such as independence from appearance, the existing methods may break
down under real-world conditions. To overcome this challenge, we propose CorVS,
a novel data-driven person identification method based on correspondence
between visual tracking trajectories and sensor measurements. Firstly, our deep
learning model predicts correspondence probabilities and reliabilities for
every pair of a trajectory and sensor measurements. Secondly, our algorithm
matches the trajectories and sensor measurements over time using the predicted
probabilities and reliabilities. We developed a dataset with actual warehouse
operations and demonstrated the method's effectiveness for real-world
applications.

</details>


### [44] [Efficient Generative AI Boosts Probabilistic Forecasting of Sudden Stratospheric Warmings](https://arxiv.org/abs/2510.26376)
*Ningning Tao,Fei Xie,Baoxiang Pan,Hongyu Wang,Han Huang,Zhongpu Qiu,Ke Gui,Jiali Luo,Xiaosong Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于流匹配的生成式AI模型（FM-Cast），用于高效、准确地对平流层环流的时空演变进行概率预报，尤其针对突发性平流层增温（SSW）事件。该模型在18次重大SSW事件中成功预测了其中10次的发生时间、强度和形态，提前期达20天，集合预报准确率超50%，性能媲美或优于主流数值天气预报系统，且计算成本极低（消费级GPU上两分钟完成50成员30天预报）。此外，通过理想化实验揭示了SSW可预报性与其物理驱动机制之间的联系。


<details>
  <summary>Details</summary>
Motivation: 突发性平流层增温（SSW）是次季节可预报性的关键来源，也是冬季极端天气的重要驱动因素。然而，由于数值天气预报系统在物理表征、初始场设定及集合预报巨大计算开销等方面的局限，SSW的准确高效预报仍是长期挑战。同时，数据驱动方法在SSW复杂三维动力学尤其是概率预报方面的应用尚未充分探索。

Method: 开发了一种基于流匹配（Flow Matching）的生成式人工智能模型FM-Cast，用于对平流层环流的时空演变进行概率预报。该模型在1998–2024年间的18次重大SSW事件上进行了评估，并通过理想化实验分析SSW可预报性与其物理驱动机制的关系。

Result: FM-Cast能提前最多20天对10次SSW事件的发生时间、强度和形态做出有效预测，集合预报准确率超过50%；其预报性能与主流数值天气预报系统相当甚至更优，且仅需消费级GPU两分钟即可完成50成员、30天的预报。理想化实验进一步表明，SSW的可预报性与其物理驱动机制密切相关，可区分来自对流层强迫和源于平流层内部动力过程的事件。

Conclusion: 本研究建立了一种计算高效的平流层异常概率预报新范式，展示了生成式AI在提升大气-气候动力学物理理解方面的潜力。

Abstract: Sudden Stratospheric Warmings (SSWs) are key sources of subseasonal
predictability and major drivers of extreme winter weather. Yet, their accurate
and efficient forecast remains a persistent challenge for numerical weather
prediction (NWP) systems due to limitations in physical representation,
initialization, and the immense computational demands of ensemble forecasts.
While data-driven forecasting is rapidly evolving, its application to the
complex, three-dimensional dynamics of SSWs, particularly for probabilistic
forecast, remains underexplored. Here, we bridge this gap by developing a Flow
Matching-based generative AI model (FM-Cast) for efficient and skillful
probabilistic forecasting of the spatiotemporal evolution of stratospheric
circulation. Evaluated across 18 major SSW events (1998-2024), FM-Cast
skillfully forecasts the onset, intensity, and morphology of 10 events up to 20
days in advance, achieving ensemble accuracies above 50%. Its performance is
comparable to or exceeds leading NWP systems while requiring only two minutes
for a 50-member, 30-day forecast on a consumer GPU. Furthermore, leveraging
FM-Cast as a scientific tool, we demonstrate through idealized experiments that
SSW predictability is fundamentally linked to its underlying physical drivers,
distinguishing between events forced from the troposphere and those driven by
internal stratospheric dynamics. Our work thus establishes a computationally
efficient paradigm for probabilistic forecasting stratospheric anomalies and
showcases generative AI's potential to deepen the physical understanding of
atmosphere-climate dynamics.

</details>


### [45] [Multi-Task Learning Based on Support Vector Machines and Twin Support Vector Machines: A Comprehensive Survey](https://arxiv.org/abs/2510.26392)
*Fatemeh Bazikar,Hossein Moosaei,Atefeh Hemmati,Panos M. Pardalos*

Main category: cs.LG

TL;DR: 本文综述了基于支持向量机（SVM）和孪生支持向量机（TWSVM）的多任务学习方法，探讨其共享表示、任务正则化与结构耦合策略，比较理论性质、优化方法与实证表现，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 多任务学习在数据稀缺或高维场景中具有优势，而SVM和TWSVM因其可解释性、理论严谨性及在小数据集上的有效性仍具研究价值，但相关多任务框架尚不完善。

Method: 综述现有基于SVM和TWSVM的多任务学习方法，分析其共享表示机制、任务正则化技术与结构耦合策略，并对TWSVM在多任务设置中的新兴扩展进行重点讨论。

Result: 系统比较了各类模型在理论性质、优化策略和实证性能上的差异，总结了其在计算机视觉、自然语言处理和生物信息学等领域的应用。

Conclusion: 当前SVM和TWSVM的多任务学习研究仍有空白，未来应聚焦于构建可扩展、可解释且可靠的基于间隔的多任务学习框架。

Abstract: Multi-task learning (MTL) enables simultaneous training across related tasks,
leveraging shared information to improve generalization, efficiency, and
robustness, especially in data-scarce or high-dimensional scenarios. While deep
learning dominates recent MTL research, Support Vector Machines (SVMs) and Twin
SVMs (TWSVMs) remain relevant due to their interpretability, theoretical rigor,
and effectiveness with small datasets.
  This chapter surveys MTL approaches based on SVM and TWSVM, highlighting
shared representations, task regularization, and structural coupling
strategies. Special attention is given to emerging TWSVM extensions for
multi-task settings, which show promise but remain underexplored. We compare
these models in terms of theoretical properties, optimization strategies, and
empirical performance, and discuss applications in fields such as computer
vision, natural language processing, and bioinformatics.
  Finally, we identify research gaps and outline future directions for building
scalable, interpretable, and reliable margin-based MTL frameworks. This work
provides a comprehensive resource for researchers and practitioners interested
in SVM- and TWSVM-based multi-task learning.

</details>


### [46] [Co-Evolving Latent Action World Models](https://arxiv.org/abs/2510.26433)
*Yucen Wang,Fengming Zhang,De-Chuan Zhan,Li Zhao,Kaixin Wang,Jiang Bian*

Main category: cs.LG

TL;DR: CoLA-World introduces a joint training framework that integrates a latent action model (LAM) with a pre-trained world model, overcoming representational collapse via a warm-up phase and achieving superior performance in video simulation and visual planning compared to prior two-stage methods.


<details>
  <summary>Details</summary>
Motivation: Existing two-stage approaches for adapting pre-trained video generation models into controllable world models suffer from redundant training and limited co-adaptation. Directly combining the LAM and world model is appealing but faces challenges like representational collapse.

Method: The authors propose CoLA-World, which jointly trains the LAM and a pre-trained world model by first aligning their representations through a critical warm-up phase, enabling stable co-evolution where each component enhances the other.

Result: CoLA-World matches or outperforms previous two-stage methods in both video simulation quality and downstream visual planning tasks.

Conclusion: CoLA-World establishes a robust and efficient new paradigm for building generalist controllable world models by enabling successful joint learning of latent action models and pre-trained world models.

Abstract: Adapting pre-trained video generation models into controllable world models
via latent actions is a promising step towards creating generalist world
models. The dominant paradigm adopts a two-stage approach that trains latent
action model (LAM) and the world model separately, resulting in redundant
training and limiting their potential for co-adaptation. A conceptually simple
and appealing idea is to directly replace the forward dynamic model in LAM with
a powerful world model and training them jointly, but it is non-trivial and
prone to representational collapse. In this work, we propose CoLA-World, which
for the first time successfully realizes this synergistic paradigm, resolving
the core challenge in joint learning through a critical warm-up phase that
effectively aligns the representations of the from-scratch LAM with the
pre-trained world model. This unlocks a co-evolution cycle: the world model
acts as a knowledgeable tutor, providing gradients to shape a high-quality LAM,
while the LAM offers a more precise and adaptable control interface to the
world model. Empirically, CoLA-World matches or outperforms prior two-stage
methods in both video simulation quality and downstream visual planning,
establishing a robust and efficient new paradigm for the field.

</details>


### [47] [Robust Graph Condensation via Classification Complexity Mitigation](https://arxiv.org/abs/2510.26451)
*Jiayi Luo,Qingyun Sun,Beining Yang,Haonan Yuan,Xingcheng Fu,Yanbiao Ma,Jianxin Li,Philip S. Yu*

Main category: cs.LG

TL;DR: 本文提出了一种名为MRGC的新框架，通过引入图数据流形学习模块，使压缩图位于平滑低维流形上，从而在保持图压缩性能的同时提升其对对抗攻击的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图压缩方法在原始图被污染时性能显著下降，且现有鲁棒图学习技术效果有限；作者发现图压缩本质上是降低内在维度的过程，虽有助于分类复杂度降低，但易受对抗扰动影响。

Method: 从图数据流形的几何视角出发，提出Manifold-constrained Robust Graph Condensation（MRGC）框架，引入三个图数据流形学习模块，引导压缩图位于平滑、低维且类间模糊性最小的流形上。

Result: 大量实验表明，MRGC在多种对抗攻击场景下均表现出较强的鲁棒性。

Conclusion: MRGC在保留图压缩分类复杂度降低能力的同时，有效提升了其在对抗扰动下的鲁棒性。

Abstract: Graph condensation (GC) has gained significant attention for its ability to
synthesize smaller yet informative graphs. However, existing studies often
overlook the robustness of GC in scenarios where the original graph is
corrupted. In such cases, we observe that the performance of GC deteriorates
significantly, while existing robust graph learning technologies offer only
limited effectiveness. Through both empirical investigation and theoretical
analysis, we reveal that GC is inherently an intrinsic-dimension-reducing
process, synthesizing a condensed graph with lower classification complexity.
Although this property is critical for effective GC performance, it remains
highly vulnerable to adversarial perturbations. To tackle this vulnerability
and improve GC robustness, we adopt the geometry perspective of graph data
manifold and propose a novel Manifold-constrained Robust Graph Condensation
framework named MRGC. Specifically, we introduce three graph data manifold
learning modules that guide the condensed graph to lie within a smooth,
low-dimensional manifold with minimal class ambiguity, thereby preserving the
classification complexity reduction capability of GC and ensuring robust
performance under universal adversarial attacks. Extensive experiments
demonstrate the robustness of \ModelName\ across diverse attack scenarios.

</details>


### [48] [Data-Efficient RLVR via Off-Policy Influence Guidance](https://arxiv.org/abs/2510.26491)
*Erle Zhu,Dazhi Jiang,Yuan Wang,Xujun Li,Jiale Cheng,Yuxian Gu,Yilin Niu,Aohan Zeng,Jie Tang,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: 本文提出了一种基于影响函数的理论驱动数据选择方法CROPI，用于强化学习与可验证奖励（RLVR），通过离策略影响估计和稀疏随机投影技术，在减少90%数据使用量的同时显著加速大语言模型的训练。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR中的数据选择方法多为启发式，缺乏理论保证和泛化能力，难以高效提升大语言模型的推理能力。

Method: 利用影响函数评估每个数据点对学习目标的贡献；提出离策略影响估计方法以避免在线策略回滚的高昂计算开销；采用稀疏随机投影降低LLM高维梯度的维度；构建多阶段课程强化学习框架CROPI，迭代选择对当前策略最具影响力的数据。

Result: 在高达7B参数的模型上验证了CROPI的有效性；在1.5B模型上仅使用每阶段10%的数据，实现了2.66倍的训练步长加速。

Conclusion: 基于影响函数的数据选择方法在RLVR中具有巨大潜力，能显著提升训练效率并减少数据依赖。

Abstract: Data selection is a critical aspect of Reinforcement Learning with Verifiable
Rewards (RLVR) for enhancing the reasoning capabilities of large language
models (LLMs). Current data selection methods are largely heuristic-based,
lacking theoretical guarantees and generalizability. This work proposes a
theoretically-grounded approach using influence functions to estimate the
contribution of each data point to the learning objective. To overcome the
prohibitive computational cost of policy rollouts required for online influence
estimation, we introduce an off-policy influence estimation method that
efficiently approximates data influence using pre-collected offline
trajectories. Furthermore, to manage the high-dimensional gradients of LLMs, we
employ sparse random projection to reduce dimensionality and improve storage
and computation efficiency. Leveraging these techniques, we develop
\textbf{C}urriculum \textbf{R}L with \textbf{O}ff-\textbf{P}olicy
\text{I}nfluence guidance (\textbf{CROPI}), a multi-stage RL framework that
iteratively selects the most influential data for the current policy.
Experiments on models up to 7B parameters demonstrate that CROPI significantly
accelerates training. On a 1.5B model, it achieves a 2.66x step-level
acceleration while using only 10\% of the data per stage compared to
full-dataset training. Our results highlight the substantial potential of
influence-based data selection for efficient RLVR.

</details>


### [49] [Deep sequence models tend to memorize geometrically; it is unclear why](https://arxiv.org/abs/2510.26745)
*Shahriar Noroozizadeh,Vaishnavh Nagarajan,Elan Rosenfeld,Sanjiv Kumar*

Main category: cs.LG

TL;DR: 该论文提出了一种关于Transformer模型中参数化记忆的几何视角，指出模型不仅存储局部共现关系，还能自发构建实体间的全局几何结构，从而简化复杂推理任务。这种几何结构的出现并非源于常规架构或优化压力，而是与一种自然产生的谱偏置有关。


<details>
  <summary>Details</summary>
Motivation: 传统上，序列建模中的参数化记忆被视为对实体共现关系的暴力查找。作者旨在挑战这一关联性观点，探索记忆是否以几何方式存储，并解释为何模型能处理训练中未显式出现的实体间关系。

Method: 通过分析一个清晰可解的Transformer推理实例，结合与Node2Vec的联系，揭示模型如何在仅优化局部关联的情况下自发形成嵌入几何结构，并探讨其背后的谱偏置机制。

Result: 发现Transformer能够合成原子事实的几何表示，将复杂的ℓ步推理任务简化为一步几何操作；该几何结构并非由典型架构或优化压力驱动，而源于一种自然出现的谱偏置。

Conclusion: 研究揭示了神经嵌入几何的非直观起源，表明即使几何表示并不比暴力查找更简洁，模型仍会学习它。这为改进Transformer记忆的几何特性提供了实践方向，并呼吁重新审视知识获取与记忆机制的研究直觉。

Abstract: In sequence modeling, the parametric memory of atomic facts has been
predominantly abstracted as a brute-force lookup of co-occurrences between
entities. We contrast this associative view against a geometric view of how
memory is stored. We begin by isolating a clean and analyzable instance of
Transformer reasoning that is incompatible with memory as strictly a storage of
the local co-occurrences specified during training. Instead, the model must
have somehow synthesized its own geometry of atomic facts, encoding global
relationships between all entities, including non-co-occurring ones. This in
turn has simplified a hard reasoning task involving an $\ell$-fold composition
into an easy-to-learn 1-step geometric task.
  From this phenomenon, we extract fundamental aspects of neural embedding
geometries that are hard to explain. We argue that the rise of such a geometry,
despite optimizing over mere local associations, cannot be straightforwardly
attributed to typical architectural or optimizational pressures.
Counterintuitively, an elegant geometry is learned even when it is not more
succinct than a brute-force lookup of associations.
  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry
stems from a spectral bias that -- in contrast to prevailing theories -- indeed
arises naturally despite the lack of various pressures. This analysis also
points to practitioners a visible headroom to make Transformer memory more
strongly geometric. We hope the geometric view of parametric memory encourages
revisiting the default intuitions that guide researchers in areas like
knowledge acquisition, capacity, discovery and unlearning.

</details>


### [50] [Enhancing ECG Classification Robustness with Lightweight Unsupervised Anomaly Detection Filters](https://arxiv.org/abs/2510.26501)
*Mustafa Fuad Rifet Ibrahim,Maurice Meijer,Alexander Schlaefer,Peer Stelldinger*

Main category: cs.LG

TL;DR: 本文研究在资源受限的可穿戴设备上，通过优化无监督异常检测（UAD）方法作为上游过滤器，提升心电图（ECG）深度学习模型对分布外（OOD）数据的鲁棒性。在多种UAD方法中，经神经架构搜索优化的Deep SVDD在检测性能与效率之间取得最佳平衡，与诊断分类器结合后可将准确率最多提升21个百分点。


<details>
  <summary>Details</summary>
Motivation: 在可穿戴设备上部署用于自动ECG分析的深度学习模型时，常因分布外（OOD）数据（如未知病理或噪声干扰信号）导致高置信度但错误的预测，威胁患者安全。现有OOD检测方法或忽略计算资源限制，或分别处理噪声与未知类别，缺乏统一高效的解决方案。

Method: 作者将无监督异常检测（UAD）作为独立的上游过滤机制，在严格资源限制（最多512k参数）下，利用神经架构搜索（NAS）优化六种UAD方法（包括Deep SVDD、基于重建的模型、掩码异常检测、标准化流和扩散模型），并在PTB-XL和BUT QDB数据集上评估其对OOD心血管疾病类别及噪声信号的检测能力。

Result: 实验结果表明，Deep SVDD在检测性能与计算效率之间表现最优。在模拟真实部署场景中，将优化后的Deep SVDD过滤器与诊断分类器结合，相比仅使用分类器的基线，准确率最多提升21个百分点。

Conclusion: 优化后的无监督异常检测过滤器（特别是Deep SVDD）能有效提升可穿戴设备上自动ECG分析系统的鲁棒性与安全性，为连续心血管健康监测提供可靠支持。

Abstract: Continuous electrocardiogram (ECG) monitoring via wearables offers
significant potential for early cardiovascular disease (CVD) detection.
However, deploying deep learning models for automated analysis in
resource-constrained environments faces reliability challenges due to
inevitable Out-of-Distribution (OOD) data. OOD inputs, such as unseen
pathologies or noisecorrupted signals, often cause erroneous, high-confidence
predictions by standard classifiers, compromising patient safety. Existing OOD
detection methods either neglect computational constraints or address noise and
unseen classes separately. This paper explores Unsupervised Anomaly Detection
(UAD) as an independent, upstream filtering mechanism to improve robustness. We
benchmark six UAD approaches, including Deep SVDD, reconstruction-based models,
Masked Anomaly Detection, normalizing flows, and diffusion models, optimized
via Neural Architecture Search (NAS) under strict resource constraints (at most
512k parameters). Evaluation on PTB-XL and BUT QDB datasets assessed detection
of OOD CVD classes and signals unsuitable for analysis due to noise. Results
show Deep SVDD consistently achieves the best trade-off between detection and
efficiency. In a realistic deployment simulation, integrating the optimized
Deep SVDD filter with a diagnostic classifier improved accuracy by up to 21
percentage points over a classifier-only baseline. This study demonstrates that
optimized UAD filters can safeguard automated ECG analysis, enabling safer,
more reliable continuous cardiovascular monitoring on wearables.

</details>


### [51] [LLMs as In-Context Meta-Learners for Model and Hyperparameter Selection](https://arxiv.org/abs/2510.26510)
*Youssef Attia El Hili,Albert Thomas,Malik Tiomoko,Abdelhakim Benechehab,Corentin Léger,Corinne Ancourt,Balázs Kégl*

Main category: cs.LG

TL;DR: 本文探索利用大语言模型（LLMs）作为上下文元学习器，通过数据集元数据推荐模型和超参数，无需昂贵搜索即可取得有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 模型与超参数选择在机器学习中至关重要但困难，通常依赖专家经验或昂贵的自动搜索，作者希望探索LLMs是否能作为轻量级通用助手解决这一问题。

Method: 将数据集转化为可解释的元数据，通过两种提示策略（零样本和元信息增强）引导LLM推荐模型族和超参数，并在合成与真实基准上评估其性能。

Result: LLMs能有效利用元数据推荐具有竞争力的模型和超参数；引入元信息提示后性能进一步提升，表明其具备上下文元学习能力。

Conclusion: LLMs可作为轻量、通用的模型选择与超参数优化助手，展现出在该任务中的新潜力。

Abstract: Model and hyperparameter selection are critical but challenging in machine
learning, typically requiring expert intuition or expensive automated search.
We investigate whether large language models (LLMs) can act as in-context
meta-learners for this task. By converting each dataset into interpretable
metadata, we prompt an LLM to recommend both model families and
hyperparameters. We study two prompting strategies: (1) a zero-shot mode
relying solely on pretrained knowledge, and (2) a meta-informed mode augmented
with examples of models and their performance on past tasks. Across synthetic
and real-world benchmarks, we show that LLMs can exploit dataset metadata to
recommend competitive models and hyperparameters without search, and that
improvements from meta-informed prompting demonstrate their capacity for
in-context meta-learning. These results highlight a promising new role for LLMs
as lightweight, general-purpose assistants for model selection and
hyperparameter optimization.

</details>


### [52] [Defeating the Training-Inference Mismatch via FP16](https://arxiv.org/abs/2510.26788)
*Penghui Qi,Zichen Liu,Xiangxin Zhou,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 本文指出在大语言模型强化学习微调中，BF16精度引入的舍入误差是训练与推理策略不一致的根本原因，并证明简单切换回FP16即可显著提升稳定性、收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调大语言模型时，训练与推理策略之间存在数值不匹配问题，导致训练不稳定。现有方法多从算法或工程角度缓解，但作者认为问题根源在于浮点精度选择。

Method: 将广泛使用的BF16精度替换为FP16，在不修改模型架构或学习算法的前提下，仅通过几行代码变更实现训练与推理的一致性。

Result: 在多种任务、算法和框架下，统一使用FP16带来了更稳定的优化过程、更快的收敛速度和更强的性能表现。

Conclusion: FP16相比BF16更适合用于大语言模型的强化学习微调，建议重新审视强化学习微调中的精度权衡问题。

Abstract: Reinforcement learning (RL) fine-tuning of large language models (LLMs) often
suffers from instability due to the numerical mismatch between the training and
inference policies. While prior work has attempted to mitigate this issue
through algorithmic corrections or engineering alignments, we show that its
root cause lies in the floating point precision itself. The widely adopted
BF16, despite its large dynamic range, introduces large rounding errors that
breaks the consistency between training and inference. In this work, we
demonstrate that simply reverting to \textbf{FP16} effectively eliminates this
mismatch. The change is simple, fully supported by modern frameworks with only
a few lines of code change, and requires no modification to the model
architecture or learning algorithm. Our results suggest that using FP16
uniformly yields more stable optimization, faster convergence, and stronger
performance across diverse tasks, algorithms and frameworks. We hope these
findings motivate a broader reconsideration of precision trade-offs in RL
fine-tuning.

</details>


### [53] [Think Outside the Policy: In-Context Steered Policy Optimization](https://arxiv.org/abs/2510.26519)
*Hsiu-Yuan Huang,Chenming Tang,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: 本文提出了一种名为ICPO的新框架，通过利用大推理模型（LRM）的上下文学习能力，在不依赖更强专家模型生成轨迹的前提下，提升强化学习从可验证奖励（RLVR）中的探索效率与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法（如GRPO）因仅依赖当前策略的on-policy rollout，导致探索受限、轨迹多样性不足；而引入更强专家模型虽可扩展策略覆盖，但计算成本高且专家模型常不可得。

Method: 提出In-Context Steered Policy Optimization（ICPO）框架，包含Mixed-Policy GRPO with Implicit Expert Forcing以扩展探索，Expert Region Reject Sampling过滤不可靠的off-policy轨迹，以及Annealed Expert-Bonus Reward Shaping在训练初期提供专家引导、后期转向自主优化。

Result: 在数学推理基准上，ICPO显著提升了强化学习性能与训练稳定性。

Conclusion: ICPO为大推理模型提供了一种高效、可扩展且无需依赖高级专家模型的RLVR新范式。

Abstract: Existing Reinforcement Learning from Verifiable Rewards (RLVR) methods, such
as Group Relative Policy Optimization (GRPO), have achieved remarkable progress
in improving the reasoning capabilities of Large Reasoning Models (LRMs).
However, they exhibit limited exploration due to reliance on on-policy rollouts
where confined to the current policy's distribution, resulting in narrow
trajectory diversity. Recent approaches attempt to expand policy coverage by
incorporating trajectories generated from stronger expert models, yet this
reliance increases computational cost and such advaned models are often
inaccessible. To address these issues, we propose In-Context Steered Policy
Optimization (ICPO), a unified framework that leverages the inherent in-context
learning capability of LRMs to provide expert guidance using existing datasets.
ICPO introduces Mixed-Policy GRPO with Implicit Expert Forcing, which expands
exploration beyond the current policy distribution without requiring advanced
LRM trajectories. To further stabilize optimization, ICPO integrates Expert
Region Reject Sampling to filter unreliable off-policy trajectories and
Annealed Expert-Bonus Reward Shaping to balance early expert guidance with
later autonomous improvement. Results demonstrate that ICPO consistently
enhances reinforcement learning performance and training stability on
mathematical reasoning benchmarks, revealing a scalable and effective RLVR
paradigm for LRMs.

</details>


### [54] [Polybasic Speculative Decoding Through a Theoretical Perspective](https://arxiv.org/abs/2510.26527)
*Ruilin Wang,Huixia Li,Yuexiao Ma,Xiawu Zheng,Fei Chao,Xuefeng Xiao,Rongrong Ji*

Main category: cs.LG

TL;DR: 本文提出了一种新的多基（polybasic）推测解码框架，通过理论分析优化多模型协同生成过程，在保持输出分布不变的前提下显著加速大语言模型推理，实验显示在多个主流模型上实现3.31×至4.43×的加速比。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法多采用二元草稿-验证框架，缺乏坚实的理论基础，难以充分发挥多模型协同潜力，限制了推理加速效果。

Method: 提出多基推测解码框架，建立理论模型，证明多模型推测解码系统的最优推理时间定理，并据此优化模型能力、接受长度与计算开销之间的权衡。

Result: 在LLaMA2-Chat 7B、LLaMA3-8B、Vicuna-7B和Qwen2-7B等多个模型上实现了3.31×至4.43×的推理加速，同时完全保持原始输出分布。

Conclusion: 多基推测解码框架通过理论指导有效突破了传统二元框架的局限，在不牺牲生成质量的前提下显著提升大语言模型推理效率，为未来高效推理系统设计提供了新方向。

Abstract: Inference latency stands as a critical bottleneck in the large-scale
deployment of Large Language Models (LLMs). Speculative decoding methods have
recently shown promise in accelerating inference without compromising the
output distribution. However, existing work typically relies on a dualistic
draft-verify framework and lacks rigorous theoretical grounding. In this paper,
we introduce a novel \emph{polybasic} speculative decoding framework,
underpinned by a comprehensive theoretical analysis. Specifically, we prove a
fundamental theorem that characterizes the optimal inference time for
multi-model speculative decoding systems, shedding light on how to extend
beyond the dualistic approach to a more general polybasic paradigm. Through our
theoretical investigation of multi-model token generation, we expose and
optimize the interplay between model capabilities, acceptance lengths, and
overall computational cost. Our framework supports both standalone
implementation and integration with existing speculative techniques, leading to
accelerated performance in practice. Experimental results across multiple model
families demonstrate that our approach yields speedup ratios ranging from
$3.31\times$ to $4.01\times$ for LLaMA2-Chat 7B, up to $3.87 \times$ for
LLaMA3-8B, up to $4.43 \times$ for Vicuna-7B and up to $3.85 \times$ for
Qwen2-7B -- all while preserving the original output distribution. We release
our theoretical proofs and implementation code to facilitate further
investigation into polybasic speculative decoding.

</details>


### [55] [Higher-Order Regularization Learning on Hypergraphs](https://arxiv.org/abs/2510.26533)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: 本文扩展了高阶超图学习（HOHL）的理论基础，证明了其截断版本的一致性，并推导了在全监督学习中作为正则化项时的显式收敛速率，同时展示了其在主动学习和无几何结构数据集中的优异性能。


<details>
  <summary>Details</summary>
Motivation: 先前工作通过几何设定下的渐近一致性分析建立了HOHL的适定性与不适定性，但缺乏对截断版本一致性的证明及显式收敛速率的推导；本文旨在填补这一理论空白并验证其在更广泛学习场景中的实用性。

Method: 通过理论分析证明截断版HOHL的一致性，并在全监督学习框架下推导其作为正则化项时的显式收敛速率；同时在主动学习任务和无底层几何结构的数据集上进行实证评估。

Result: 理论方面成功证明了截断HOHL的一致性并获得收敛速率；实验方面展示了HOHL在主动学习和非几何结构数据上的强大性能。

Conclusion: HOHL不仅具备坚实的理论基础，还在多样化的学习场景中表现出良好的通用性与鲁棒性，验证了其作为超图正则化方法的有效性。

Abstract: Higher-Order Hypergraph Learning (HOHL) was recently introduced as a
principled alternative to classical hypergraph regularization, enforcing
higher-order smoothness via powers of multiscale Laplacians induced by the
hypergraph structure. Prior work established the well- and ill-posedness of
HOHL through an asymptotic consistency analysis in geometric settings. We
extend this theoretical foundation by proving the consistency of a truncated
version of HOHL and deriving explicit convergence rates when HOHL is used as a
regularizer in fully supervised learning. We further demonstrate its strong
empirical performance in active learning and in datasets lacking an underlying
geometric structure, highlighting HOHL's versatility and robustness across
diverse learning settings.

</details>


### [56] [Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices](https://arxiv.org/abs/2510.26557)
*Jan Stenkamp,Nina Herrmann,Benjamin Karic,Stefan Oehmcke,Fabian Gieseke*

Main category: cs.LG

TL;DR: 本文提出了一种针对提升决策树的压缩方案，通过在训练过程中鼓励特征和阈值的复用，显著减小模型内存占用，在保持性能不变的前提下实现4-16倍的压缩比，适用于资源受限的物联网设备。


<details>
  <summary>Details</summary>
Motivation: 在计算资源受限的物联网设备上部署机器学习模型日益重要，但现有模型（如LightGBM）内存占用较大，难以满足轻量化需求，因此需要一种高效的压缩方法。

Method: 提出一种新的提升决策树训练方法，通过在训练过程中奖励特征和阈值的复用，并结合替代性内存布局，实现模型压缩。

Result: 实验表明，所提方法在保持与LightGBM相当性能的同时，实现了4-16倍的模型压缩比。

Conclusion: 该压缩方案使物联网设备能在低功耗、无持续通信条件下自主运行，适用于远程监控、边缘分析和实时决策等场景。

Abstract: Deploying machine learning models on compute-constrained devices has become a
key building block of modern IoT applications. In this work, we present a
compression scheme for boosted decision trees, addressing the growing need for
lightweight machine learning models. Specifically, we provide techniques for
training compact boosted decision tree ensembles that exhibit a reduced memory
footprint by rewarding, among other things, the reuse of features and
thresholds during training. Our experimental evaluation shows that models
achieved the same performance with a compression ratio of 4-16x compared to
LightGBM models using an adapted training process and an alternative memory
layout. Once deployed, the corresponding IoT devices can operate independently
of constant communication or external energy supply, and, thus, autonomously,
requiring only minimal computing power and energy. This capability opens the
door to a wide range of IoT applications, including remote monitoring, edge
analytics, and real-time decision making in isolated or power-limited
environments.

</details>


### [57] [Wasserstein Regression as a Variational Approximation of Probabilistic Trajectories through the Bernstein Basis](https://arxiv.org/abs/2510.26607)
*Maksim Maslov,Alexander Kugaevskikh,Matthew Ivanov*

Main category: cs.LG

TL;DR: 本文提出了一种结合Bernstein基参数化与Wasserstein距离最小化的分布回归新方法，能有效建模输入相关的平滑概率轨迹，在保持几何准确性、计算效率和可解释性的同时，在非线性场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有分布回归方法常忽略概率空间的几何结构或计算成本过高，亟需一种兼顾几何准确性、计算可行性和模型可解释性的新方法。

Method: 将条件分布建模为由Bernstein多项式构造的高斯分量加权和，其均值与协方差为输入变量的函数；通过最小化预测分布与经验数据之间的平均平方Wasserstein距离进行训练，并采用自动微分优化。

Result: 在合成数据集上的实验表明，该方法在Wasserstein距离、Energy Distance和RMSE等指标上具有竞争力，尤其在强非线性情况下表现突出；同时展现出良好的轨迹平滑性、对数据结构变化的鲁棒性及高可解释性。

Conclusion: 所提方法在几何准确性、计算实用性和可解释性之间取得了良好平衡，未来可拓展至非高斯分布、引入熵正则化加速计算，并应用于高维数据以逼近更复杂结构。

Abstract: This paper considers the problem of regression over distributions, which is
becoming increasingly important in machine learning. Existing approaches often
ignore the geometry of the probability space or are computationally expensive.
To overcome these limitations, a new method is proposed that combines the
parameterization of probability trajectories using a Bernstein basis and the
minimization of the Wasserstein distance between distributions. The key idea is
to model a conditional distribution as a smooth probability trajectory defined
by a weighted sum of Gaussian components whose parameters -- the mean and
covariance -- are functions of the input variable constructed using Bernstein
polynomials. The loss function is the averaged squared Wasserstein distance
between the predicted Gaussian distributions and the empirical data, which
takes into account the geometry of the distributions. An autodiff-based
optimization method is used to train the model. Experiments on synthetic
datasets that include complex trajectories demonstrated that the proposed
method provides competitive approximation quality in terms of the Wasserstein
distance, Energy Distance, and RMSE metrics, especially in cases of pronounced
nonlinearity. The model demonstrates trajectory smoothness that is better than
or comparable to alternatives and robustness to changes in data structure,
while maintaining high interpretability due to explicit parameterization via
control points. The developed approach represents a balanced solution that
combines geometric accuracy, computational practicality, and interpretability.
Prospects for further research include extending the method to non-Gaussian
distributions, applying entropy regularization to speed up computations, and
adapting the approach to working with high-dimensional data for approximating
surfaces and more complex structures.

</details>


### [58] [Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization](https://arxiv.org/abs/2510.26633)
*Colin Doumont,Victor Picheny,Viacheslav Borovitskiy,Henry Moss*

Main category: cs.LG

TL;DR: 本文提出了一种基于热核的统一框架，系统地推导并表达了多种组合核，并证明许多成功的组合核与热核相关或等价。实验表明，基于热核的简单高效流程能达到甚至超越现有复杂算法的性能，且对最优解位置不敏感。


<details>
  <summary>Details</summary>
Motivation: 组合优化任务（如材料科学和神经架构搜索）需要专门的核函数来建模，但现有组合核之间的关系尚不清楚。为填补这一空白，作者提出一个统一框架以系统理解这些核。

Method: 基于热核构建统一框架，推导出简洁的闭式表达式，并通过理论分析与实验验证多种组合核与热核的关系。

Result: 实验证实许多组合核与热核等价或相关；热核对最优解位置不敏感；基于热核的简单流程在性能上达到或超过现有复杂算法。

Conclusion: 热核提供了一个强大且通用的框架，可用于组合贝叶斯优化，兼具理论清晰性与实际高效性。

Abstract: Bayesian Optimization (BO) has the potential to solve various combinatorial
tasks, ranging from materials science to neural architecture search. However,
BO requires specialized kernels to effectively model combinatorial domains.
Recent efforts have introduced several combinatorial kernels, but the
relationships among them are not well understood. To bridge this gap, we
develop a unifying framework based on heat kernels, which we derive in a
systematic way and express as simple closed-form expressions. Using this
framework, we prove that many successful combinatorial kernels are either
related or equivalent to heat kernels, and validate this theoretical claim in
our experiments. Moreover, our analysis confirms and extends the results
presented in Bounce: certain algorithms' performance decreases substantially
when the unknown optima of the function do not have a certain structure. In
contrast, heat kernels are not sensitive to the location of the optima. Lastly,
we show that a fast and simple pipeline, relying on heat kernels, is able to
achieve state-of-the-art results, matching or even outperforming certain slow
or complex algorithms.

</details>


### [59] [MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection](https://arxiv.org/abs/2510.26643)
*Emmanouil Sylligardos,John Paparrizos,Themis Palpanas,Pierre Senellart,Paul Boniol*

Main category: cs.LG

TL;DR: 本文研究了将时间序列分类方法用于异常检测中的模型选择，通过在1980多个时间序列上评估234种模型配置，发现模型选择方法优于单一异常检测方法，且计算开销相当，为AutoML中的模型选择提供了有效基线。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法在高度异构的时间序列数据集上表现不一，缺乏统一最优解，而现有AutoML方案无法直接适用于时间序列异常检测，因此需要一种基于时间序列特征自动选择最优异常检测方法的策略。

Method: 利用时间序列分类方法作为模型选择器，评估16种基础分类器衍生出的234种配置，在1980多个时间序列上进行实验，比较其作为异常检测模型选择工具的性能。

Result: 模型选择方法在准确率上优于所有单一异常检测方法，且执行时间在同一数量级；该研究首次系统验证了时间序列分类算法在异常检测模型选择中的有效性。

Conclusion: 时间序列分类可作为高效准确的模型选择策略用于异常检测任务，为构建通用AutoML流程中的模型选择模块提供了坚实基线。

Abstract: Anomaly detection is a fundamental task for time series analytics with
important implications for the downstream performance of many applications.
Despite increasing academic interest and the large number of methods proposed
in the literature, recent benchmarks and evaluation studies demonstrated that
no overall best anomaly detection methods exist when applied to very
heterogeneous time series datasets. Therefore, the only scalable and viable
solution to solve anomaly detection over very different time series collected
from diverse domains is to propose a model selection method that will select,
based on time series characteristics, the best anomaly detection methods to
run. Existing AutoML solutions are, unfortunately, not directly applicable to
time series anomaly detection, and no evaluation of time series-based
approaches for model selection exists. Towards that direction, this paper
studies the performance of time series classification methods used as model
selection for anomaly detection. In total, we evaluate 234 model configurations
derived from 16 base classifiers across more than 1980 time series, and we
propose the first extensive experimental evaluation of time series
classification as model selection for anomaly detection. Our results
demonstrate that model selection methods outperform every single anomaly
detection method while being in the same order of magnitude regarding execution
time. This evaluation is the first step to demonstrate the accuracy and
efficiency of time series classification algorithms for anomaly detection, and
represents a strong baseline that can then be used to guide the model selection
step in general AutoML pipelines. Preprint version of an article accepted at
the VLDB Journal.

</details>


### [60] [Curly Flow Matching for Learning Non-gradient Field Dynamics](https://arxiv.org/abs/2510.26645)
*Katarina Petrović,Lazar Atanackovic,Viggo Moro,Kacper Kapuśniak,İsmail İlkan Ceylan,Michael Bronstein,Avishek Joey Bose,Alexander Tong*

Main category: cs.LG

TL;DR: 本文提出了一种名为Curly Flow Matching（Curly-FM）的新方法，通过引入具有非零漂移参考过程的薛定谔桥问题，能够学习非梯度场动力学，从而有效建模具有周期性行为的真实系统（如单细胞RNA中的细胞周期），优于现有方法在轨迹推断任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于最小作用量原理的建模方法假设系统动力学为梯度场，无法捕捉如单细胞RNA中细胞周期等真实系统中存在的非梯度、周期性行为，因此需要一种能处理非梯度动力学的新方法。

Method: 作者设计并求解了一个具有非零漂移参考过程的薛定谔桥问题，该参考过程利用推断出的速度和群体快照数据构建，从而实现对非梯度场动力学的学习。

Result: Curly-FM在单细胞轨迹推断、计算流体动力学和洋流建模等任务中展现出优越性能，能够更准确地匹配参考过程和群体边缘分布。

Conclusion: Curly-FM突破了传统流匹配模型仅适用于梯度动力学的限制，为建模具有已知周期性行为的物理系统提供了新途径。

Abstract: Modeling the transport dynamics of natural processes from population-level
observations is a ubiquitous problem in the natural sciences. Such models rely
on key assumptions about the underlying process in order to enable faithful
learning of governing dynamics that mimic the actual system behavior. The de
facto assumption in current approaches relies on the principle of least action
that results in gradient field dynamics and leads to trajectories minimizing an
energy functional between two probability measures. However, many real-world
systems, such as cell cycles in single-cell RNA, are known to exhibit
non-gradient, periodic behavior, which fundamentally cannot be captured by
current state-of-the-art methods such as flow and bridge matching. In this
paper, we introduce Curly Flow Matching (Curly-FM), a novel approach that is
capable of learning non-gradient field dynamics by designing and solving a
Schr\"odinger bridge problem with a non-zero drift reference process -- in
stark contrast to typical zero-drift reference processes -- which is
constructed using inferred velocities in addition to population snapshot data.
We showcase Curly-FM by solving the trajectory inference problems for single
cells, computational fluid dynamics, and ocean currents with approximate
velocities. We demonstrate that Curly-FM can learn trajectories that better
match both the reference process and population marginals. Curly-FM expands
flow matching models beyond the modeling of populations and towards the
modeling of known periodic behavior in physical systems. Our code repository is
accessible at: https://github.com/kpetrovicc/curly-flow-matching.git

</details>


### [61] [Tight Differentially Private PCA via Matrix Coherence](https://arxiv.org/abs/2510.26679)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: 本文提出了一种基于奇异值分解和标准扰动机制的高效差分隐私算法，用于计算矩阵前r个奇异向量张成的空间。该算法的误差仅依赖于这些奇异向量的rank-r相干性和谱间隙，并在稠密设定下达到了与最优非隐私算法相同的性能。此外，文章证明了高斯扰动不会增加相干性，并将相干性概念应用于图问题，如Max-Cut。


<details>
  <summary>Details</summary>
Motivation: 解决Hardt和Roth提出的问题，即如何在差分隐私下高效且准确地计算矩阵前r个奇异向量的张成空间，同时克服现有隐私算法在某些场景（如稠密设定下的单尖峰PCA）中性能不足的问题。

Method: 采用奇异值分解（SVD）结合标准扰动机制（如高斯机制）设计差分隐私算法；分析算法误差与rank-r相干性及谱间隙的关系；证明高斯扰动不增加相干性；并将相干性假设应用于图上的约束满足问题（如Max-Cut）。

Result: 所提算法在稠密设定下对Wishart模型中的单尖峰PCA达到了与最优非隐私算法相同的保证，显著优于现有隐私算法；证明了高斯扰动保持输入的相干性；并基于低相干性假设提出了用于Max-Cut等问题的差分隐私算法。

Conclusion: 该工作通过引入rank-r相干性作为关键参数，改进了差分隐私下的低秩近似方法，在理论和实践上均优于先前方法，并为图问题中的隐私保护提供了新思路。

Abstract: We revisit the task of computing the span of the top $r$ singular vectors
$u_1, \ldots, u_r$ of a matrix under differential privacy. We show that a
simple and efficient algorithm -- based on singular value decomposition and
standard perturbation mechanisms -- returns a private rank-$r$ approximation
whose error depends only on the \emph{rank-$r$ coherence} of $u_1, \ldots, u_r$
and the spectral gap $\sigma_r - \sigma_{r+1}$. This resolves a question posed
by Hardt and Roth~\cite{hardt2013beyond}. Our estimator outperforms the state
of the art -- significantly so in some regimes. In particular, we show that in
the dense setting, it achieves the same guarantees for single-spike PCA in the
Wishart model as those attained by optimal non-private algorithms, whereas
prior private algorithms failed to do so.
  In addition, we prove that (rank-$r$) coherence does not increase under
Gaussian perturbations. This implies that any estimator based on the Gaussian
mechanism -- including ours -- preserves the coherence of the input. We
conjecture that similar behavior holds for other structured models, including
planted problems in graphs.
  We also explore applications of coherence to graph problems. In particular,
we present a differentially private algorithm for Max-Cut and other constraint
satisfaction problems under low coherence assumptions.

</details>


### [62] [LoRAQuant: Mixed-Precision Quantization of LoRA to Ultra-Low Bits](https://arxiv.org/abs/2510.26690)
*Amir Reza Mirzaei,Yuqiao Wen,Yanshuai Cao,Lili Mou*

Main category: cs.LG

TL;DR: 本文提出LoRAQuant，一种针对低秩适配器（LoRA）的混合精度后训练量化方法，通过奇异值分解（SVD）重参数化LoRA，在显著降低存储开销的同时保持甚至提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，多个LoRA适配器常需同时加载以支持个性化或多样化任务，尽管单个适配器轻量，但总体资源消耗巨大，亟需高效压缩方法。

Method: LoRAQuant利用奇异值分解（SVD）对每个LoRA适配器进行重参数化，将关键信息集中到特定行和列，并对重要部分使用高精度量化，其余部分采用超低位宽量化。

Result: 在LLaMA 2-7B、LLaMA 2-13B和Mistral 7B模型上的数学推理、代码生成和摘要任务实验表明，LoRAQuant比特数显著低于其他量化方法，同时性能相当甚至更优。

Conclusion: LoRAQuant是一种高效、实用的LoRA量化方案，能在大幅压缩适配器体积的同时维持甚至提升模型表现，适用于大规模多适配器部署场景。

Abstract: Low-Rank Adaptation (LoRA) has become a popular technique for
parameter-efficient fine-tuning of large language models (LLMs). In many
real-world scenarios, multiple adapters are loaded simultaneously to enable LLM
customization for personalized user experiences or to support a diverse range
of tasks. Although each adapter is lightweight in isolation, their aggregate
cost becomes substantial at scale. To address this, we propose LoRAQuant, a
mixed-precision post-training quantization method tailored to LoRA.
Specifically, LoRAQuant reparameterizes each adapter by singular value
decomposition (SVD) to concentrate the most important information into specific
rows and columns. This makes it possible to quantize the important components
to higher precision, while quantizing the rest to ultra-low bitwidth. We
conduct comprehensive experiments with LLaMA 2-7B, LLaMA 2-13B, and Mistral 7B
models on mathematical reasoning, coding, and summarization tasks. Results show
that our LoRAQuant uses significantly lower bits than other quantization
methods, but achieves comparable or even higher performance.

</details>


### [63] [Budgeted Multiple-Expert Deferral](https://arxiv.org/abs/2510.26706)
*Giulia DeSalvo,Clara Mohri,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 本文提出了一种预算受限的延迟决策框架，在训练过程中通过选择性查询专家来降低专家调用成本，同时保持预测性能，适用于多专家场景，并提供了理论保证和实证验证。


<details>
  <summary>Details</summary>
Motivation: 标准的延迟决策算法在训练时需对每个样本查询所有专家，导致高昂的专家查询成本，违背了延迟决策旨在减少不必要专家使用的初衷。

Method: 提出了适用于两阶段和单阶段多专家延迟决策的新算法，每训练样本仅选择性查询部分专家；该方法受主动学习启发，但核心任务是在已知标签的前提下优化专家查询策略以平衡成本与性能。

Result: 在多个领域的实验表明，所提算法在显著降低训练成本的同时，未牺牲预测准确性。

Conclusion: 本文提出的预算感知延迟决策算法在保证性能的前提下有效减少了训练阶段的专家查询开销，具有实际应用价值，并附有泛化界和标签复杂度等理论保障。

Abstract: Learning to defer uncertain predictions to costly experts offers a powerful
strategy for improving the accuracy and efficiency of machine learning systems.
However, standard training procedures for deferral algorithms typically require
querying all experts for every training instance, an approach that becomes
prohibitively expensive when expert queries incur significant computational or
resource costs. This undermines the core goal of deferral: to limit unnecessary
expert usage. To overcome this challenge, we introduce the budgeted deferral
framework, which aims to train effective deferral algorithms while minimizing
expert query costs during training. We propose new algorithms for both
two-stage and single-stage multiple-expert deferral settings that selectively
query only a subset of experts per training example. While inspired by active
learning, our setting is fundamentally different: labels are already known, and
the core challenge is to decide which experts to query in order to balance cost
and predictive performance. We establish theoretical guarantees for both of our
algorithms, including generalization bounds and label complexity analyses.
Empirical results across several domains show that our algorithms substantially
reduce training costs without sacrificing prediction accuracy, demonstrating
the practical value of our budget-aware deferral algorithms.

</details>


### [64] [On the limitation of evaluating machine unlearning using only a single training seed](https://arxiv.org/abs/2510.26714)
*Jamie Lanyon,Axel Finke,Petros Andreou,Georgina Cosma*

Main category: cs.LG

TL;DR: 本文指出，当前机器遗忘（MU）算法的评估常忽略模型训练随机种子带来的影响，仅从同一训练模型多次运行MU算法可能导致非代表性结果，建议在评估中纳入不同训练种子的变异性。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘算法多为近似方法，其性能依赖经验评估，而当前评估实践中常忽略模型训练阶段随机种子对遗忘效果的影响，可能导致评估结果不具代表性。

Method: 通过实验分析不同随机种子下MU方法的表现差异，揭示仅基于同一训练模型多次运行MU算法的局限性。

Result: 发现某些MU方法对训练阶段的随机种子高度敏感，导致在相同架构和数据集下遗忘效果差异显著。

Conclusion: 建议在MU算法的经验比较中，应考虑并反映不同模型训练种子所带来的性能变异性，以获得更具代表性的评估结果。

Abstract: Machine unlearning (MU) aims to remove the influence of certain data points
from a trained model without costly retraining. Most practical MU algorithms
are only approximate and their performance can only be assessed empirically.
Care must therefore be taken to make empirical comparisons as representative as
possible. A common practice is to run the MU algorithm multiple times
independently starting from the same trained model. In this work, we
demonstrate that this practice can give highly non-representative results
because -- even for the same architecture and same dataset -- some MU methods
can be highly sensitive to the choice of random number seed used for model
training. We therefore recommend that empirical
comphttps://info.arxiv.org/help/prep#commentsarisons of MU algorithms should
also reflect the variability across different model training seeds.

</details>


### [65] [On Purely Private Covariance Estimation](https://arxiv.org/abs/2510.26717)
*Tommaso d'Orsi,Gleb Novikov*

Main category: cs.LG

TL;DR: 本文提出了一种用于在纯差分隐私下发布$d$维协方差矩阵的简单扰动机制，在大数据集（$n \geq d^2/\varepsilon$）下实现了Frobenius范数误差的理论最优性，并在所有$p$-Schatten范数（$p \in [1,\infty]$）上达到当前最佳误差，尤其首次在谱范数下实现最优误差。对于小数据集（$n < d^2/\varepsilon$），通过将输出投影到适当半径的核范数球上，该算法在Frobenius范数误差上优于已有结果。


<details>
  <summary>Details</summary>
Motivation: 在差分隐私约束下准确估计协方差矩阵是一个核心问题，现有方法在不同范数下的误差性能存在局限，尤其缺乏在谱范数下达到信息论最优的纯差分隐私机制。

Method: 提出一种简单的扰动机制用于发布协方差矩阵，并在小数据集情形下结合核范数球上的投影操作以优化误差。

Result: 在大数据集下，该机制在Frobenius范数上达到已知最优误差，并在所有$p$-Schatten范数（特别是谱范数）上实现当前最佳或信息论最优性能；在小数据集下，Frobenius范数误差优于先前工作。

Conclusion: 该机制是首个在纯差分隐私下实现谱范数最优误差的协方差估计方法，并在各种数据规模和范数下展现出优越或最优的误差性能。

Abstract: We present a simple perturbation mechanism for the release of $d$-dimensional
covariance matrices $\Sigma$ under pure differential privacy. For large
datasets with at least $n\geq d^2/\varepsilon$ elements, our mechanism recovers
the provably optimal Frobenius norm error guarantees of
\cite{nikolov2023private}, while simultaneously achieving best known error for
all other $p$-Schatten norms, with $p\in [1,\infty]$. Our error is
information-theoretically optimal for all $p\ge 2$, in particular, our
mechanism is the first purely private covariance estimator that achieves
optimal error in spectral norm.
  For small datasets $n< d^2/\varepsilon$, we further show that by projecting
the output onto the nuclear norm ball of appropriate radius, our algorithm
achieves the optimal Frobenius norm error $O(\sqrt{d\;\text{Tr}(\Sigma) /n})$,
improving over the known bounds of $O(\sqrt{d/n})$ of \cite{nikolov2023private}
and ${O}\big(d^{3/4}\sqrt{\text{Tr}(\Sigma)/n}\big)$ of
\cite{dong2022differentially}.

</details>


### [66] [STaMP: Sequence Transformation and Mixed Precision for Low-Precision Activation Quantization](https://arxiv.org/abs/2510.26771)
*Marco Federici,Riccardo Del Chiaro,Boris van Breugel,Paul Whatmough,Markus Nagel*

Main category: cs.LG

TL;DR: 本文提出了一种名为STaMP的新量化策略，通过在序列维度上应用线性变换并结合混合精度，有效提升了低比特激活量化的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统量化方法在将激活值压缩至8比特以下时会导致模型精度显著下降，现有研究虽通过可逆线性变换（如旋转）在通道维度改善量化效果，但在序列维度上尚未充分挖掘数据局部相关性以提升量化性能。

Method: 提出STaMP（Sequence Transformation and Mixed Precision）量化方法，在序列维度上应用线性变换，并对每个中间激活中少量token保留较高精度，从而在降低平均激活比特宽度的同时保持模型精度。

Result: 在最新的LVM和LLM架构上的实验表明，STaMP显著改善了低比特激活量化效果，并能与现有的激活/权重量化方法（包括近期的特征变换技术）有效互补。

Conclusion: STaMP通过利用语言和视觉数据在序列维度上的强局部相关性，为低比特量化提供了一种有效的新策略，有助于在保持模型精度的同时降低计算资源消耗。

Abstract: Quantization is the key method for reducing inference latency, power and
memory footprint of generative AI models. However, accuracy often degrades
sharply when activations are quantized below eight bits. Recent work suggests
that invertible linear transformations (e.g. rotations) can aid quantization,
by reparameterizing feature channels and weights. In this paper, we propose
\textit{Sequence Transformation and Mixed Precision} (STaMP) quantization, a
novel strategy that applies linear transformations along the \textit{sequence}
dimension to exploit the strong local correlation in language and visual data.
By keeping a small number of tokens in each intermediate activation at higher
precision, we can maintain model accuracy at lower (average) activations
bit-widths. We evaluate STaMP on recent LVM and LLM architectures,
demonstrating that it significantly improves low bit width activation
quantization and complements established activation and weight quantization
methods including recent feature transformations.

</details>


### [67] [Faithful and Fast Influence Function via Advanced Sampling](https://arxiv.org/abs/2510.26776)
*Jungyeon Koh,Hyeonsu Lyu,Jonggyu Jang,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 本文提出基于特征和 logits 的两种先进采样技术，以提升影响函数（IFs）估计的准确性和效率，相比基线方法显著降低计算和内存开销，或提升 F1 分数。


<details>
  <summary>Details</summary>
Motivation: 传统影响函数计算需对整个数据集计算 Hessian 矩阵，资源消耗大；而随机采样子集虽节省资源，但因样本配置方差高，导致 IF 估计不稳定。

Method: 提出两种采样策略：基于特征分布和 logits 分布，从训练集中选择小而具代表性的子集，用于更准确地估计影响函数。

Result: 在类别移除实验中，所提方法相比基线减少30.1%计算时间、42.2%内存使用，或提升2.5%的F1分数。

Conclusion: 所提出的采样方法在保持模型推理一致性的前提下，有效提高了影响函数估计的效率与准确性。

Abstract: How can we explain the influence of training data on black-box models?
Influence functions (IFs) offer a post-hoc solution by utilizing gradients and
Hessians. However, computing the Hessian for an entire dataset is
resource-intensive, necessitating a feasible alternative. A common approach
involves randomly sampling a small subset of the training data, but this method
often results in highly inconsistent IF estimates due to the high variance in
sample configurations. To address this, we propose two advanced sampling
techniques based on features and logits. These samplers select a small yet
representative subset of the entire dataset by considering the stochastic
distribution of features or logits, thereby enhancing the accuracy of IF
estimations. We validate our approach through class removal experiments, a
typical application of IFs, using the F1-score to measure how effectively the
model forgets the removed class while maintaining inference consistency on the
remaining classes. Our method reduces computation time by 30.1% and memory
usage by 42.2%, or improves the F1-score by 2.5% compared to the baseline.

</details>


### [68] [Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification](https://arxiv.org/abs/2510.26777)
*Andreas Auer,Daniel Klotz,Sebastinan Böck,Sepp Hochreiter*

Main category: cs.LG

TL;DR: 该研究发现，冻结的预训练时间序列预测模型可提供有效的分类表示，其性能媲美甚至超越专为分类预训练的模型，表明预测任务可能足以构建通用时间序列基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型研究多集中于预测任务，尚不清楚其学习到的表示是否具有良好的泛化能力，尤其在分类任务上的适用性。

Method: 作者评估了冻结的预训练预测模型在分类任务中的表现，比较了不同的表示提取策略，并提出了两种与模型无关的嵌入增强方法。

Result: 最佳预测模型在分类任务上的准确率与甚至超过专为分类预训练的最先进模型，并且预测性能与分类性能呈正相关。

Conclusion: 任务特定的预训练并非必要，学习预测可能是一种构建通用时间序列基础模型的有效途径。

Abstract: Recent research on time series foundation models has primarily focused on
forecasting, leaving it unclear how generalizable their learned representations
are. In this study, we examine whether frozen pre-trained forecasting models
can provide effective representations for classification. To this end, we
compare different representation extraction strategies and introduce two
model-agnostic embedding augmentations. Our experiments show that the best
forecasting models achieve classification accuracy that matches or even
surpasses that of state-of-the-art models pre-trained specifically for
classification. Moreover, we observe a positive correlation between forecasting
and classification performance. These findings challenge the assumption that
task-specific pre-training is necessary, and suggest that learning to forecast
may provide a powerful route toward constructing general-purpose time series
foundation models.

</details>


### [69] [Clone Deterministic 3D Worlds with Geometrically-Regularized World Models](https://arxiv.org/abs/2510.26782)
*Zaishuo Xia,Yukuan Lu,Xinyi Li,Yifan Xu,Yubei Chen*

Main category: cs.LG

TL;DR: 本文提出几何正则化世界模型（GRWM），通过改进表征学习来提升世界模型在确定性3D环境中的长期预测准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型在长期预测中表现脆弱，主要原因是表征质量差：高维外感受输入（如图像）导致潜在表征有损或纠缠，使动态学习困难。

Method: 提出GRWM方法，在潜在空间中强制自然感知轨迹上的连续点保持邻近，从而学习与环境真实拓扑结构对齐的几何结构更优的潜在流形。

Result: 在确定性3D环境和长期预测任务中，GRWM显著提高了rollout的保真度和稳定性，且无需扩大动态模块。

Conclusion: 改进表征学习是构建鲁棒世界模型的有效路径，能显著提升长期预测性能。

Abstract: A world model is an internal model that simulates how the world evolves.
Given past observations and actions, it predicts the future of both the
embodied agent and its environment. Accurate world models are essential for
enabling agents to think, plan, and reason effectively in complex, dynamic
settings. Despite rapid progress, current world models remain brittle and
degrade over long horizons. We argue that a central cause is representation
quality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy or
entangled latents make dynamics learning unnecessarily hard. We therefore ask
whether improving representation learning alone can substantially improve
world-model performance. In this work, we take a step toward building a truly
accurate world model by addressing a fundamental yet open problem: constructing
a model that can fully clone and overfit to a deterministic 3D world. We
propose Geometrically-Regularized World Models (GRWM), which enforces that
consecutive points along a natural sensory trajectory remain close in latent
representation space. This approach yields significantly improved latent
representations that align closely with the true topology of the environment.
GRWM is plug-and-play, requires only minimal architectural modification, scales
with trajectory length, and is compatible with diverse latent generative
backbones. Across deterministic 3D settings and long-horizon prediction tasks,
GRWM significantly increases rollout fidelity and stability. Analyses show that
its benefits stem from learning a latent manifold with superior geometric
structure. These findings support a clear takeaway: improving representation
learning is a direct and useful path to robust world models, delivering
reliable long-horizon predictions without enlarging the dynamics module.

</details>


### [70] [Learning Pseudorandom Numbers with Transformers: Permuted Congruential Generators, Curricula, and Interpretability](https://arxiv.org/abs/2510.26792)
*Tao Tao,Maissam Barkeshli*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型在学习由置换同余生成器（PCG）生成的伪随机序列方面的能力，发现即使在输出被截断为单个比特的情况下，模型仍能可靠预测，并揭示了模数与所需上下文长度之间的缩放规律以及嵌入层中的新型聚类现象。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型是否能够学习并预测由复杂伪随机数生成器（如PCG）生成的序列，这类序列比传统线性同余生成器（LCG）更具挑战性，因其引入了位移、异或、旋转和截断等操作。

Method: 使用不同规模的Transformer模型（最多5000万参数）在大规模数据集（最多50亿token）上训练，对多种PCG变体进行上下文内预测任务；采用课程学习策略，从小模数数据逐步过渡到大模数（最高达$2^{22}$）；分析嵌入层结构以理解模型内部表示。

Result: Transformer模型能够成功预测未见过的PCG序列，即使输出仅保留单比特；多个PRNG可被联合学习；预测所需上下文长度随模数$m$的平方根增长；训练大模数时需依赖小模数数据进行课程学习；嵌入层自发形成对位旋转不变的整数聚类。

Conclusion: Transformer模型展现出强大的序列建模能力，能捕捉PCG这类复杂伪随机序列的隐藏结构；课程学习对训练大模数模型至关重要；嵌入层中发现的聚类现象揭示了模型在不同模数间迁移表示的机制。

Abstract: We study the ability of Transformer models to learn sequences generated by
Permuted Congruential Generators (PCGs), a widely used family of pseudo-random
number generators (PRNGs). PCGs introduce substantial additional difficulty
over linear congruential generators (LCGs) by applying a series of bit-wise
shifts, XORs, rotations and truncations to the hidden state. We show that
Transformers can nevertheless successfully perform in-context prediction on
unseen sequences from diverse PCG variants, in tasks that are beyond published
classical attacks. In our experiments we scale moduli up to $2^{22}$ using up
to $50$ million model parameters and datasets with up to $5$ billion tokens.
Surprisingly, we find even when the output is truncated to a single bit, it can
be reliably predicted by the model. When multiple distinct PRNGs are presented
together during training, the model can jointly learn them, identifying
structures from different permutations. We demonstrate a scaling law with
modulus $m$: the number of in-context sequence elements required for
near-perfect prediction grows as $\sqrt{m}$. For larger moduli, optimization
enters extended stagnation phases; in our experiments, learning moduli $m \geq
2^{20}$ requires incorporating training data from smaller moduli, demonstrating
a critical necessity for curriculum learning. Finally, we analyze embedding
layers and uncover a novel clustering phenomenon: the model spontaneously
groups the integer inputs into bitwise rotationally-invariant clusters,
revealing how representations can transfer from smaller to larger moduli.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [71] [ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for Efficient MoE Inference](https://arxiv.org/abs/2510.26730)
*Zixu Shen,Kexin Chu,Yifan Zhang,Dawei Xiang,Runxin Wu,Wei Zhang*

Main category: cs.DC

TL;DR: ExpertFlow 是一种用于 Mixture-of-Experts（MoE）推理的运行时系统，通过自适应专家预取和缓存感知路由，显著降低因专家参数频繁交换引起的延迟，在内存受限条件下将模型停顿时间减少至基线的 0.1% 以下。


<details>
  <summary>Details</summary>
Motivation: 现代 GPU 内存容量限制了大语言模型的扩展，而现有 MoE 推理方法在各层独立选择专家会导致频繁的参数传输，引入显著延迟；同时，现有跨层预测策略缺乏对不同硬件和工作负载的适应性。

Method: ExpertFlow 结合自适应专家预取与缓存感知路由，利用运行时统计信息（如传输带宽、参数维度和模型反馈）动态调整专家激活的预测范围，并采用融合预门控信息与中间计算状态的混合跨层预测方案。

Result: 实验表明，ExpertFlow 将模型停顿时间降低至基线的 0.1% 以下，有效减少了缓存未命中和专家换入带来的延迟。

Conclusion: ExpertFlow 能在严格内存限制下高效优化 MoE 推理过程，提升推理速度与系统鲁棒性。

Abstract: The expansion of large language models is increasingly limited by the
constrained memory capacity of modern GPUs. To mitigate this,
Mixture-of-Experts (MoE) architectures activate only a small portion of
parameters during inference, significantly lowering both memory demand and
computational overhead. However, conventional MoE inference approaches, which
select active experts independently at each layer, often introduce considerable
latency because of frequent parameter transfers between host and GPU memory. In
addition, current cross-layer prediction strategies, which are typically based
on fixed steps, lack adaptability across different hardware platforms and
workloads, thereby reducing their robustness and effectiveness.
  To address these challenges, we present ExpertFlow, a runtime system for MoE
inference that combines adaptive expert prefetching and cache-aware routing.
ExpertFlow continuously adjusts its prediction horizon for expert activation by
leveraging runtime statistics such as transfer bandwidth, parameter
dimensionality, and model feedback signals. Furthermore, it incorporates a
hybrid cross-layer prediction scheme that fuses pregating information with
intermediate computational states to anticipate future expert needs. By
adaptively refining prefetching decisions and aligning them with actual usage
behavior, ExpertFlow effectively decreases cache misses and removes latency
caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces
model stall time to less than 0.1% of the baseline, highlighting its capability
to optimize MoE inference under stringent memory constraints.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [72] [Approximating Heavy-Tailed Distributions with a Mixture of Bernstein Phase-Type and Hyperexponential Models](https://arxiv.org/abs/2510.26524)
*Abdelhakim Ziani,András Horváth,Paolo Ballarini*

Main category: cs.PF

TL;DR: 本文提出了一种结合Bernstein相位型（BPH）与超指数（HE）分布的新型混合模型，以更准确地拟合重尾分布的整体（包括主体和尾部），并通过优化HE部分的初始参数提升模型鲁棒性与精度。


<details>
  <summary>Details</summary>
Motivation: 重尾分布在金融、通信、排队论和自然语言处理等领域广泛存在，但现有模型如BPH难以准确捕捉其尾部行为，而HE模型虽适应重尾但对主体拟合不佳且对初始参数敏感，因此需要一种兼顾两者优势的新方法。

Method: 提出一种BPH与HE分布的混合模型，利用优化方法设定HE组件的初始参数，以提升其鲁棒性并避免无效模型，从而同时准确拟合分布的主体和尾部。

Result: 实验表明，该混合模型在拟合重尾分布的均值和变异系数等关键参数上显著优于单独使用BPH或HE模型，并在排队论实验中验证了其实际应用的准确性与有效性。

Conclusion: 所提出的混合模型有效结合了BPH和HE的优点，能够同时精确拟合重尾分布的主体与尾部，具有更强的鲁棒性和实际应用价值。

Abstract: Heavy-tailed distributions, prevalent in a lot of real-world applications
such as finance, telecommunications, queuing theory, and natural language
processing, are challenging to model accurately owing to their slow tail decay.
Bernstein phase-type (BPH) distributions, through their analytical tractability
and good approximations in the non-tail region, can present a good solution,
but they suffer from an inability to reproduce these heavy-tailed behaviors
exactly, thus leading to inadequate performance in important tail areas. On the
contrary, while highly adaptable to heavy-tailed distributions,
hyperexponential (HE) models struggle in the body part of the distribution.
Additionally, they are highly sensitive to initial parameter selection,
significantly affecting their precision.
  To solve these issues, we propose a novel hybrid model of BPH and HE
distributions, borrowing the most desirable features from each for enhanced
approximation quality. Specifically, we leverage an optimization to set initial
parameters for the HE component, significantly enhancing its robustness and
reducing the possibility that the associated procedure results in an invalid HE
model. Experimental validation demonstrates that the novel hybrid approach is
more performant than individual application of BPH or HE models. More
precisely, it can capture both the body and the tail of heavy-tailed
distributions, with a considerable enhancement in matching parameters such as
mean and coefficient of variation. Additional validation through experiments
utilizing queuing theory proves the practical usefulness, accuracy, and
precision of our hybrid approach.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [73] [MIREDO: MIP-Driven Resource-Efficient Dataflow Optimization for Computing-in-Memory Accelerator](https://arxiv.org/abs/2510.26463)
*Xiaolin He,Cenlin Duan,Yingjie Qi,Xiao Ma,Jianlei Yang*

Main category: cs.AR

TL;DR: MIREDO is a framework that formulates dataflow optimization for Computing-in-Memory (CIM) accelerators as a Mixed-Integer Programming problem, achieving up to 3.2× performance improvement by accurately modeling CIM-specific constraints and data movement behaviors.


<details>
  <summary>Details</summary>
Motivation: Existing dataflow optimization methods for CIM architectures fail to fully exploit hardware potential due to a large design space and strict architectural constraints, resulting in a significant gap between theoretical and actual efficiency.

Method: MIREDO models dataflow optimization as a Mixed-Integer Programming (MIP) problem, using a hierarchical hardware abstraction and an analytical latency model that captures CIM-specific data transfer behaviors, jointly considering workload, dataflow strategies, and hardware constraints.

Result: MIREDO achieves up to 3.2× performance improvement across various DNN models and CIM hardware configurations compared to existing approaches.

Conclusion: MIREDO effectively bridges the efficiency gap in CIM-based DNN acceleration by enabling systematic and accurate dataflow optimization under realistic architectural constraints.

Abstract: Computing-in-Memory (CIM) architectures have emerged as a promising solution
for accelerating Deep Neural Networks (DNNs) by mitigating data movement
bottlenecks. However, realizing the potential of CIM requires specialized
dataflow optimizations, which are challenged by an expansive design space and
strict architectural constraints. Existing optimization approaches often fail
to fully exploit CIM accelerators, leading to noticeable gaps between
theoretical and actual system-level efficiency. To address these limitations,
we propose the MIREDO framework, which formulates dataflow optimization as a
Mixed-Integer Programming (MIP) problem. MIREDO introduces a hierarchical
hardware abstraction coupled with an analytical latency model designed to
accurately reflect the complex data transfer behaviors within CIM systems. By
jointly modeling workload characteristics, dataflow strategies, and
CIM-specific constraints, MIREDO systematically navigates the vast design space
to determine the optimal dataflow configurations. Evaluation results
demonstrate that MIREDO significantly enhances performance, achieving up to
$3.2\times$ improvement across various DNN models and hardware setups.

</details>
