{"id": "2512.03416", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03416", "abs": "https://arxiv.org/abs/2512.03416", "authors": ["Ruiqi Lai", "Hongrui Liu", "Chengzhi Lu", "Zonghao Liu", "Siyu Cao", "Siyang Shao", "Yixin Zhang", "Luo Mai", "Dmitrii Ustiugov"], "title": "TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity", "comment": null, "summary": "The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.", "AI": {"tldr": "TokenScale \u662f\u4e00\u79cd\u9762\u5411\u9884\u586b\u5145/\u89e3\u7801\uff08PD\uff09\u5206\u79bb\u67b6\u6784\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u670d\u52a1\u81ea\u52a8\u6269\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165 Token Velocity \u9884\u6d4b\u6307\u6807\u548c\u53ef\u8f6c\u6362\u89e3\u7801\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347 SLO \u8fbe\u6210\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e GPU \u5229\u7528\u7387\u6216\u7c97\u7c92\u5ea6\u8bf7\u6c42\u8ba1\u6570\u7684\u81ea\u52a8\u6269\u7f29\u7b56\u7565\u5728\u5e94\u5bf9\u7a81\u53d1\u8d1f\u8f7d\u65f6\u53cd\u5e94\u8fdf\u7f13\uff0c\u5bfc\u81f4 TTFT \u548c TPOT \u670d\u52a1\u7b49\u7ea7\u76ee\u6807\uff08SLO\uff09\u4e25\u91cd\u8fdd\u53cd\uff0c\u5e76\u9020\u6210\u8d44\u6e90\u8fc7\u5ea6\u914d\u7f6e\u3002", "method": "\u63d0\u51fa Token Velocity \u6307\u6807\u4f5c\u4e3a\u7cfb\u7edf\u80cc\u538b\u7684\u9886\u5148\u6307\u6807\u4ee5\u5b9e\u73b0\u4e3b\u52a8\u6269\u7f29\uff0c\u5e76\u8bbe\u8ba1 Convertible Decoders \u673a\u5236\uff0c\u4f7f\u89e3\u7801 GPU \u80fd\u5728\u6d41\u91cf\u9ad8\u5cf0\u65f6\u52a8\u6001\u6267\u884c\u9884\u586b\u5145\u4efb\u52a1\uff0c\u5feb\u901f\u5438\u6536\u7a81\u53d1\u8bf7\u6c42\u3002", "result": "\u5728\u771f\u5b9e\u751f\u4ea7\u8d1f\u8f7d\u4e0b\u7684 GPU \u96c6\u7fa4\u8bc4\u4f30\u8868\u660e\uff0cTokenScale \u5c06 SLO \u8fbe\u6210\u7387\u4ece 50\u201388% \u63d0\u5347\u81f3 80\u201396%\uff0c\u76f8\u6bd4 DistServe\u3001BlitzScale \u548c AIBrix \u7b49\u5148\u8fdb\u7cfb\u7edf\uff0c\u6210\u672c\u964d\u4f4e 4\u201314%\u3002", "conclusion": "TokenScale \u901a\u8fc7\u7ed3\u5408\u9884\u6d4b\u6027\u6307\u6807\u4e0e\u7075\u6d3b\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u6709\u6548\u63d0\u5347\u4e86 PD \u5206\u79bb\u5f0f LLM \u670d\u52a1\u7cfb\u7edf\u7684\u6027\u80fd\u4e0e\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2512.03421", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.03421", "abs": "https://arxiv.org/abs/2512.03421", "authors": ["Hexiang Xu", "Hengyuan Liu", "Yonghao Wu", "Xiaolan Kang", "Xiang Chen", "Yong Liu"], "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization", "comment": "The paper has been accepted for publication in The Journal of Systems & Software", "summary": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e8613\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7a0b\u5e8f\u9519\u8bef\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u5148\u8fdb\u6a21\u578b\uff08\u5982OpenAI o3\u548cDeepSeekR1\uff09\u5728\u65e0\u9700\u590d\u6742\u63d0\u793a\u5de5\u7a0b\u7684\u60c5\u51b5\u4e0b\u5373\u53ef\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u65b0\u6784\u5efa\u7684BugT\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u7a33\u5065\uff1b\u5c3d\u7ba1LLM\u5bf9\u521d\u5b66\u8005\u8c03\u8bd5\u5177\u6709\u663e\u8457\u8f85\u52a9\u4ef7\u503c\uff0c\u4f46\u5176\u5728\u96be\u9898\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u3001\u8fc7\u5ea6\u63a8\u7406\u53ca\u9ad8\u8ba1\u7b97\u6210\u672c\u4ecd\u662f\u5b9e\u9645\u5e94\u7528\u7684\u4e3b\u8981\u6311\u6218\u3002", "motivation": "\u521d\u5b66\u8005\u7a0b\u5e8f\u5458\u56e0\u7ecf\u9a8c\u4e0d\u8db3\u96be\u4ee5\u8fdb\u884c\u6709\u6548\u7684\u9519\u8bef\u5b9a\u4f4d\uff0c\u4f20\u7edf\u65b9\u6cd5\uff08\u5982SBFL\u548cMBFL\uff09\u7f3a\u4e4f\u5bf9\u4ee3\u7801\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6709\u671b\u901a\u8fc7\u7406\u89e3\u7a0b\u5e8f\u8bed\u6cd5\u4e0e\u8bed\u4e49\u63d0\u4f9b\u66f4\u51c6\u786e\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u9519\u8bef\u5b9a\u4f4d\u652f\u6301\u3002", "method": "\u7814\u7a76\u5728Codeflaws\u3001Condefects\u548c\u65b0\u6784\u5efa\u7684BugT\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e866\u4e2a\u95ed\u6e90\u548c7\u4e2a\u5f00\u6e90LLM\u7684\u9519\u8bef\u5b9a\u4f4d\u80fd\u529b\uff0c\u5206\u6790\u4e0d\u540c\u6a21\u578b\u5728\u6709\u65e0\u63a8\u7406\u80fd\u529b\u3001\u63d0\u793a\u5de5\u7a0b\u4f9d\u8d56\u6027\u3001\u95ee\u9898\u96be\u5ea6\u53d8\u5316\u7b49\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u8003\u5bdf\u5176\u89e3\u91ca\u5bf9\u521d\u5b66\u8005\u7684\u5e2e\u52a9\u7a0b\u5ea6\u3002", "result": "\u5177\u5907\u63a8\u7406\u80fd\u529b\u7684\u6a21\u578b\uff08\u5982OpenAI o3\u3001DeepSeekR1\uff09\u5728\u9519\u8bef\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u66f4\u9ad8\u4e14\u5bf9\u63d0\u793a\u5de5\u7a0b\u4f9d\u8d56\u8f83\u5c0f\uff1bLLM\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u96be\u9898\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u5176\u4e2d\u9876\u5c16\u6a21\u578b\u5728BugT\u6570\u636e\u96c6\u4e0a\u4ecd\u4fdd\u6301\u7a33\u5065\uff1b\u5b58\u5728\u8fc7\u5ea6\u63a8\u7406\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff1bLLM\u751f\u6210\u7684\u89e3\u91ca\u88ab\u521d\u5b66\u8005\u9ad8\u5ea6\u8bc4\u4ef7\u3002", "conclusion": "LLM\u5728\u63d0\u5347\u521d\u5b66\u8005\u8c03\u8bd5\u6548\u7387\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u63a8\u7406\u80fd\u529b\u3001\u51cf\u5c11\u8fc7\u5ea6\u89e3\u91ca\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u9645\u90e8\u7f72\u548c\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2512.03565", "categories": ["cs.DC", "cs.CE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.03565", "abs": "https://arxiv.org/abs/2512.03565", "authors": ["Luis Gall", "Samuel James Newcome", "Fabio Alexander Gratl", "Markus M\u00fchlh\u00e4u\u00dfer", "Manish Kumar Mishra", "Hans-Joachim Bungartz"], "title": "Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas", "comment": "20 pages, 8 figures. Submitted to the 5th International Conference on Computational Engineering (ICCE 2024). No changes were made after the peer review process", "summary": "Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.\n  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.\n  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u901a\u8fc7SIMD\u5411\u91cf\u5316\u4f18\u5316\u7c92\u5b50\u95f4\u4f5c\u7528\u529b\u8ba1\u7b97\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5728\u4e8e\u5411\u91cf\u5bc4\u5b58\u5668\u4e2d\u7c92\u5b50\u6570\u636e\u7684\u52a0\u8f7d\u987a\u5e8f\uff0c\u5e76\u7ed3\u5408AutoPas\u5e93\u7684\u52a8\u6001\u8c03\u4f18\u673a\u5236\uff0c\u5728\u8fd0\u884c\u65f6\u6839\u636e\u7c92\u5b50\u5bc6\u5ea6\u548c\u90bb\u57df\u8bc6\u522b\u7b97\u6cd5\u7b49\u56e0\u7d20\u9009\u62e9\u6700\u4f18\u5411\u91cf\u5316\u987a\u5e8f\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\u6a21\u62df\u5728\u539f\u5b50\u5c3a\u5ea6\u4e0a\u63d0\u4f9b\u91cd\u8981\u7269\u7406\u8fc7\u7a0b\u6d1e\u5bdf\uff0c\u4f46\u5176\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u5df2\u6709\u7814\u7a76\u8868\u660e\uff0c\u6700\u4f18MD\u7b97\u6cd5\u53ef\u80fd\u5728\u8fd0\u884c\u65f6\u53d1\u751f\u53d8\u5316\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u63a2\u7d22\u80fd\u52a8\u6001\u9002\u5e94\u4e0d\u540c\u6a21\u62df\u6761\u4ef6\uff08\u5982\u7c92\u5b50\u5bc6\u5ea6\u3001\u90bb\u57df\u8bc6\u522b\u7b97\u6cd5\uff09\u7684\u5411\u91cf\u5316\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u6267\u884c\u65f6\u95f4\u6216\u80fd\u8017\u3002", "method": "\u672c\u6587\u5728AutoPas\u7c92\u5b50\u6a21\u62df\u5e93\u4e2d\u5b9e\u73b0\u5e76\u8bc4\u4f30\u591a\u79cdSIMD\u5411\u91cf\u5316\u6280\u672f\uff0c\u91cd\u70b9\u5173\u6ce8\u7c92\u5b50\u503c\u52a0\u8f7d\u5230\u5411\u91cf\u5bc4\u5b58\u5668\u7684\u987a\u5e8f\uff1b\u540c\u65f6\u6269\u5c55AutoPas\u7684\u52a8\u6001\u8c03\u4f18\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u5728\u8fd0\u884c\u65f6\u6839\u636e\u6a21\u62df\u53c2\u6570\uff08\u5982\u7c92\u5b50\u5bc6\u5ea6\u548c\u90bb\u57df\u8bc6\u522b\u7b97\u6cd5\uff09\u81ea\u52a8\u9009\u62e9\u6700\u4f18\u7684\u5411\u91cf\u5316\u987a\u5e8f\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u76f8\u8f83\u4e8eAutoPas\u4e4b\u524d\u7684\u65b9\u6cd5\uff0c\u5728\u8fd0\u884c\u65f6\u8003\u8651\u4e0d\u540c\u7684\u7c92\u5b50\u76f8\u4e92\u4f5c\u7528\u987a\u5e8f\u53ef\u663e\u8457\u63d0\u5347\u529b\u8ba1\u7b97\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5728\u8fd0\u884c\u65f6\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684SIMD\u5411\u91cf\u5316\u987a\u5e8f\uff0c\u7ed3\u5408\u7c92\u5b50\u5bc6\u5ea6\u548c\u90bb\u57df\u7b97\u6cd5\u7b49\u6a21\u62df\u7279\u5b9a\u53c2\u6570\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u529b\u8ba1\u7b97\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.03815", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.03815", "abs": "https://arxiv.org/abs/2512.03815", "authors": ["Shayan Ghasemnezhad", "Samarth KaPatel", "Sofia Nikiforova", "Giacinto Paolo Saggese", "Paul Smith", "Heanh Sok"], "title": "Runnable Directories: The Solution to the Monorepo vs. Multi-repo Debate", "comment": null, "summary": "Modern software systems increasingly strain traditional codebase organization strategies. Monorepos offer consistency but often suffer from scalability issues and tooling complexity, while multi-repos provide modularity at the cost of coordination and dependency management challenges. As an answer to this trade-off, we present the Causify Dev system, a hybrid approach that integrates key benefits of both. Its central concept is the runnable directory -- a self-contained, independently executable unit with its own development, testing, and deployment lifecycles. Backed by a unified thin environment, shared helper utilities, and containerized Docker-based workflows, runnable directories enable consistent setups, isolated dependencies, and efficient CI/CD processes. The Causify Dev approach provides a practical middle ground between monorepo and multi-repo strategies, improving reliability and maintainability for growing, complex codebases.", "AI": {"tldr": "Causify Dev \u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u4ee3\u7801\u5e93\u7ec4\u7ec7\u65b9\u6cd5\uff0c\u901a\u8fc7\u201c\u53ef\u8fd0\u884c\u76ee\u5f55\u201d\u7ed3\u5408\u5355\u4f53\u4ed3\u5e93\u4e0e\u591a\u4ed3\u5e93\u7684\u4f18\u70b9\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u590d\u6742\u4ee3\u7801\u5e93\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u7ef4\u62a4\u6027\u3002", "motivation": "\u4f20\u7edf\u5355\u4f53\u4ed3\u5e93\u5b58\u5728\u6269\u5c55\u6027\u548c\u5de5\u5177\u590d\u6742\u6027\u95ee\u9898\uff0c\u800c\u591a\u4ed3\u5e93\u5219\u9762\u4e34\u534f\u8c03\u548c\u4f9d\u8d56\u7ba1\u7406\u6311\u6218\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6743\u8861\uff0c\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u517c\u987e\u4e24\u8005\u4f18\u52bf\u7684\u65b0\u65b9\u6848\u3002", "method": "\u5f15\u5165\u201c\u53ef\u8fd0\u884c\u76ee\u5f55\u201d\u6982\u5ff5\uff0c\u6bcf\u4e2a\u76ee\u5f55\u5177\u5907\u72ec\u7acb\u7684\u5f00\u53d1\u3001\u6d4b\u8bd5\u548c\u90e8\u7f72\u751f\u547d\u5468\u671f\uff0c\u5e76\u4f9d\u6258\u7edf\u4e00\u7684\u8f7b\u91cf\u73af\u5883\u3001\u5171\u4eab\u5de5\u5177\u53ca\u57fa\u4e8e Docker \u7684\u5bb9\u5668\u5316\u5de5\u4f5c\u6d41\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u914d\u7f6e\u4e00\u81f4\u6027\u3001\u4f9d\u8d56\u9694\u79bb\u4ee5\u53ca\u9ad8\u6548\u7684 CI/CD \u6d41\u7a0b\uff0c\u5728\u5b9e\u8df5\u4e2d\u4e3a\u4ee3\u7801\u5e93\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u4e2d\u95f4\u8def\u7ebf\u3002", "conclusion": "Causify Dev \u5728\u5355\u4f53\u4e0e\u591a\u4ed3\u5e93\u7b56\u7565\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u6709\u6548\u63d0\u5347\u4e86\u590d\u6742\u7cfb\u7edf\u4ee3\u7801\u5e93\u7684\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u3002"}}
{"id": "2512.03278", "categories": ["cs.DB", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.03278", "abs": "https://arxiv.org/abs/2512.03278", "authors": ["Michael Theologitis", "Dan Suciu"], "title": "Thucy: An LLM-based Multi-Agent System for Claim Verification across Relational Databases", "comment": "Accepted at AAAI 2026 Workshop on LLM-based Multi-Agent Systems (LaMAS)", "summary": "In today's age, it is becoming increasingly difficult to decipher truth from lies. Every day, politicians, media outlets, and public figures make conflicting claims$\\unicode{x2014}$often about topics that can, in principle, be verified against structured data. For instance, statements about crime rates, economic growth or healthcare can all be verified against official public records and structured datasets. Building a system that can automatically do that would have sounded like science fiction just a few years ago. Yet, with the extraordinary progress in LLMs and agentic AI, this is now within reach. Still, there remains a striking gap between what is technically possible and what is being demonstrated by recent work. Most existing verification systems operate only on small, single-table databases$\\unicode{x2014}$typically a few hundred rows$\\unicode{x2014}$that conveniently fit within an LLM's context window.\n  In this paper we report our progress on Thucy, the first cross-database, cross-table multi-agent claim verification system that also provides concrete evidence for each verification verdict. Thucy remains completely agnostic to the underlying data sources before deployment and must therefore autonomously discover, inspect, and reason over all available relational databases to verify claims. Importantly, Thucy also reports the exact SQL queries that support its verdict (whether the claim is accurate or not) offering full transparency to expert users familiar with SQL. When evaluated on the TabFact dataset$\\unicode{x2014}$the standard benchmark for fact verification over structured data$\\unicode{x2014}$Thucy surpasses the previous state of the art by 5.6 percentage points in accuracy (94.3% vs. 88.7%).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Thucy\uff0c\u9996\u4e2a\u8de8\u6570\u636e\u5e93\u3001\u8de8\u8868\u7684\u591a\u667a\u80fd\u4f53\u58f0\u660e\u9a8c\u8bc1\u7cfb\u7edf\uff0c\u80fd\u81ea\u52a8\u53d1\u73b0\u5e76\u63a8\u7406\u591a\u4e2a\u5173\u7cfb\u6570\u636e\u5e93\u4ee5\u9a8c\u8bc1\u58f0\u660e\uff0c\u5e76\u63d0\u4f9b\u652f\u6301\u5176\u5224\u65ad\u7684\u5177\u4f53SQL\u67e5\u8be2\u4f5c\u4e3a\u8bc1\u636e\uff0c\u5728TabFact\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u8d85\u8d8a\u6b64\u524d\u6700\u4f18\u65b9\u6cd55.6\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u5f53\u524d\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5927\u591a\u5c40\u9650\u4e8e\u5c0f\u578b\u5355\u8868\u6570\u636e\u5e93\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u4e2d\u590d\u6742\u7684\u8de8\u6570\u636e\u5e93\u3001\u8de8\u8868\u9a8c\u8bc1\u4efb\u52a1\uff1b\u800c\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u548c\u667a\u80fd\u4f53\u6280\u672f\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u73b0\u6709\u5de5\u4f5c\u5c1a\u672a\u5145\u5206\u5c55\u73b0\u5176\u5728\u7ed3\u6784\u5316\u6570\u636e\u9a8c\u8bc1\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "Thucy\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u80fd\u5728\u90e8\u7f72\u524d\u5bf9\u5e95\u5c42\u6570\u636e\u6e90\u5b8c\u5168\u65e0\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u4e3b\u53d1\u73b0\u3001\u68c0\u67e5\u5e76\u63a8\u7406\u6240\u6709\u53ef\u7528\u7684\u5173\u7cfb\u6570\u636e\u5e93\u4ee5\u9a8c\u8bc1\u58f0\u660e\uff0c\u5e76\u8f93\u51fa\u652f\u6301\u5176\u5224\u65ad\u7684\u7cbe\u786eSQL\u67e5\u8be2\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6TabFact\u6570\u636e\u96c6\u4e0a\uff0cThucy\u7684\u51c6\u786e\u7387\u8fbe\u523094.3%\uff0c\u6bd4\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0888.7%\uff09\u9ad8\u51fa5.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "Thucy\u5c55\u793a\u4e86\u5229\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548cLLM\u5b9e\u73b0\u590d\u6742\u7ed3\u6784\u5316\u6570\u636e\u4e8b\u5b9e\u6838\u67e5\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684SQL\u8bc1\u636e\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2512.03594", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03594", "abs": "https://arxiv.org/abs/2512.03594", "authors": ["Afsara Khan", "Austin Rovinski"], "title": "Accelerating Detailed Routing Convergence through Offline Reinforcement Learning", "comment": "To be published in the Design, Automation and Test in Europe (DATE) 2026 Conference", "summary": "Detailed routing remains one of the most complex and time-consuming steps in modern physical design due to the challenges posed by shrinking feature sizes and stricter design rules. Prior detailed routers achieve state-of-the-art results by leveraging iterative pathfinding algorithms to route each net. However, runtimes are a major issue in detailed routers, as converging to a solution with zero design rule violations (DRVs) can be prohibitively expensive.\n  In this paper, we propose leveraging reinforcement learning (RL) to enable rapid convergence in detailed routing by learning from previous designs. We make the key observation that prior detailed routers statically schedule the cost weights used in their routing algorithms, meaning they do not change in response to the design or technology. By training a conservative Q-learning (CQL) model to dynamically select the routing cost weights which minimize the number of algorithm iterations, we find that our work completes the ISPD19 benchmarks with 1.56x average and up to 3.01x faster runtime than the baseline router while maintaining or improving the DRV count in all cases. We also find that this learning shows signs of generalization across technologies, meaning that learning designs in one technology can translate to improved outcomes in other technologies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u5f3a\u5316\u5b66\u4e60\uff08\u7279\u522b\u662f\u4fdd\u5b88Q\u5b66\u4e60\uff0cCQL\uff09\u52a8\u6001\u8c03\u6574\u8be6\u7ec6\u5e03\u7ebf\u4e2d\u7684\u6210\u672c\u6743\u91cd\uff0c\u4ece\u800c\u663e\u8457\u52a0\u901f\u6536\u655b\u8fc7\u7a0b\u3002\u76f8\u6bd4\u57fa\u7ebf\u5e03\u7ebf\u5668\uff0c\u8be5\u65b9\u6cd5\u5728ISPD19\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u901f1.56\u500d\u3001\u6700\u9ad8\u8fbe3.01\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u51cf\u5c11\u4e86\u8bbe\u8ba1\u89c4\u5219\u8fdd\u89c4\uff08DRV\uff09\u6570\u91cf\uff0c\u5e76\u5c55\u73b0\u51fa\u8de8\u5de5\u827a\u8282\u70b9\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u7279\u5f81\u5c3a\u5bf8\u7f29\u5c0f\u548c\u8bbe\u8ba1\u89c4\u5219\u8d8b\u4e25\uff0c\u8be6\u7ec6\u5e03\u7ebf\u6210\u4e3a\u7269\u7406\u8bbe\u8ba1\u4e2d\u6700\u590d\u6742\u4e14\u8017\u65f6\u7684\u6b65\u9aa4\u4e4b\u4e00\u3002\u73b0\u6709\u8be6\u7ec6\u5e03\u7ebf\u5668\u4f9d\u8d56\u8fed\u4ee3\u8def\u5f84\u641c\u7d22\u7b97\u6cd5\uff0c\u4f46\u5176\u9759\u6001\u8bbe\u5b9a\u7684\u6210\u672c\u6743\u91cd\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u8bbe\u8ba1\u6216\u5de5\u827a\uff0c\u5bfc\u81f4\u6536\u655b\u81f3\u96f6DRV\u89e3\u6240\u9700\u65f6\u95f4\u8fc7\u957f\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u4fdd\u5b88Q\u5b66\u4e60\uff08CQL\uff09\u8bad\u7ec3\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u6839\u636e\u5386\u53f2\u8bbe\u8ba1\u7ecf\u9a8c\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684\u5e03\u7ebf\u6210\u672c\u6743\u91cd\uff0c\u4ee5\u6700\u5c0f\u5316\u7b97\u6cd5\u8fed\u4ee3\u6b21\u6570\uff0c\u4ece\u800c\u52a0\u901f\u8be6\u7ec6\u5e03\u7ebf\u8fc7\u7a0b\u3002", "result": "\u5728ISPD19\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u5e73\u5747\u8fd0\u884c\u901f\u5ea6\u63d0\u53471.56\u500d\uff0c\u6700\u9ad8\u8fbe3.01\u500d\uff0c\u540c\u65f6\u6240\u6709\u6d4b\u8bd5\u6848\u4f8b\u7684DRV\u6570\u91cf\u5747\u672a\u589e\u52a0\uff0c\u90e8\u5206\u751a\u81f3\u6709\u6240\u6539\u5584\uff1b\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5de5\u827a\u8282\u70b9\u95f4\u8868\u73b0\u51fa\u4e00\u5b9a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u6210\u672c\u6743\u91cd\u53ef\u6709\u6548\u63d0\u5347\u8be6\u7ec6\u5e03\u7ebf\u6548\u7387\uff0c\u5728\u4fdd\u8bc1\u5e03\u7ebf\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u7f29\u77ed\u8fd0\u884c\u65f6\u95f4\uff0c\u5e76\u5177\u5907\u8de8\u6280\u672f\u8282\u70b9\u8fc1\u79fb\u5b66\u4e60\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.03868", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.03868", "abs": "https://arxiv.org/abs/2512.03868", "authors": ["Shree Hari Bittugondanahalli Indra Kumar", "Lilia Rodrigues Sampaio", "Andr\u00e9 Martin", "Andrey Brito", "Christof Fetzer"], "title": "A Comprehensive Study on the Impact of Vulnerable Dependencies on Open-Source Software", "comment": null, "summary": "Open-source libraries are widely used by software developers to speed up the development of products, however, they can introduce security vulnerabilities, leading to incidents like Log4Shell. With the expanding usage of open-source libraries, it becomes even more imperative to comprehend and address these dependency vulnerabilities. The use of Software Composition Analysis (SCA) tools does greatly help here as they provide a deep insight on what dependencies are used in a project, enhancing the security and integrity in the software supply chain. In order to learn how wide spread vulnerabilities are and how quickly they are being fixed, we conducted a study on over 1k open-source software projects with about 50k releases comprising several languages such as Java, Python, Rust, Go, Ruby, PHP, and JavaScript. Our objective is to investigate the severity, persistence, and distribution of these vulnerabilities, as well as their correlation with project metrics such as team and contributors size, activity and release cycles. In order to perform such analysis, we crawled over 1k projects from github including their version history ranging from 2013 to 2023 using VODA, our SCA tool. Using our approach, we can provide information such as library versions, dependency depth, and known vulnerabilities, and how they evolved over the software development cycle. Being larger and more diverse than datasets used in earlier works and studies, ours provides better insights and generalizability of the gained results. The data collected answers several research questions about the dependency depth and the average time a vulnerability persists. Among other findings, we observed that for most programming languages, vulnerable dependencies are transitive, and a critical vulnerability persists in average for over a year before being fixed.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e861000\u591a\u4e2a\u5f00\u6e90\u9879\u76ee\u3001\u7ea65\u4e07\u6b21\u53d1\u5e03\u4e2d\u4f9d\u8d56\u6f0f\u6d1e\u7684\u4e25\u91cd\u6027\u3001\u6301\u7eed\u65f6\u95f4\u548c\u5206\u5e03\u60c5\u51b5\uff0c\u53d1\u73b0\u5927\u591a\u6570\u8bed\u8a00\u4e2d\u7684\u6f0f\u6d1e\u4f9d\u8d56\u662f\u4f20\u9012\u6027\u7684\uff0c\u4e14\u5173\u952e\u6f0f\u6d1e\u5e73\u5747\u9700\u4e00\u5e74\u4ee5\u4e0a\u624d\u80fd\u4fee\u590d\u3002", "motivation": "\u968f\u7740\u5f00\u6e90\u5e93\u7684\u5e7f\u6cdb\u4f7f\u7528\uff0c\u5176\u5f15\u5165\u7684\u5b89\u5168\u6f0f\u6d1e\uff08\u5982Log4Shell\uff09\u5bf9\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u6784\u6210\u5a01\u80c1\uff0c\u56e0\u6b64\u4e9f\u9700\u6df1\u5165\u7406\u89e3\u5e76\u89e3\u51b3\u4f9d\u8d56\u6f0f\u6d1e\u95ee\u9898\u3002", "method": "\u5229\u7528\u81ea\u7814\u7684SCA\u5de5\u5177VODA\uff0c\u4eceGitHub\u722c\u53d62013\u81f32023\u5e74\u95f41000\u591a\u4e2a\u591a\u8bed\u8a00\u5f00\u6e90\u9879\u76ee\u7684\u7248\u672c\u5386\u53f2\uff0c\u5206\u6790\u5176\u4f9d\u8d56\u7248\u672c\u3001\u4f9d\u8d56\u6df1\u5ea6\u3001\u5df2\u77e5\u6f0f\u6d1e\u53ca\u5176\u5728\u5f00\u53d1\u5468\u671f\u4e2d\u7684\u6f14\u53d8\uff0c\u5e76\u5173\u8054\u9879\u76ee\u6307\u6807\uff08\u5982\u56e2\u961f\u89c4\u6a21\u3001\u6d3b\u8dc3\u5ea6\u7b49\uff09\u8fdb\u884c\u7efc\u5408\u7814\u7a76\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u591a\u6570\u7f16\u7a0b\u8bed\u8a00\u4e2d\u7684\u6f0f\u6d1e\u4f9d\u8d56\u4e3a\u4f20\u9012\u6027\u4f9d\u8d56\uff1b\u5173\u952e\u6f0f\u6d1e\u5e73\u5747\u6301\u7eed\u5b58\u5728\u8d85\u8fc7\u4e00\u5e74\u624d\u88ab\u4fee\u590d\uff1b\u8be5\u6570\u636e\u96c6\u6bd4\u4ee5\u5f80\u7814\u7a76\u66f4\u5e7f\u6cdb\u591a\u6837\uff0c\u63d0\u5347\u4e86\u7ed3\u679c\u7684\u4ee3\u8868\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5f00\u6e90\u9879\u76ee\u4e2d\u4f9d\u8d56\u6f0f\u6d1e\u666e\u904d\u5b58\u5728\u4e14\u4fee\u590d\u7f13\u6162\uff0c\u5c24\u5176\u5728\u4f20\u9012\u4f9d\u8d56\u4e2d\u66f4\u4e3a\u663e\u8457\uff0c\u9700\u52a0\u5f3aSCA\u5de5\u5177\u7684\u5e94\u7528\u4e0e\u6f0f\u6d1e\u54cd\u5e94\u673a\u5236\u4ee5\u63d0\u5347\u8f6f\u4ef6\u4f9b\u5e94\u94fe\u5b89\u5168\u3002"}}
{"id": "2512.03389", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.03389", "abs": "https://arxiv.org/abs/2512.03389", "authors": ["Shu Chen", "Deepti Raghavan", "U\u011fur \u00c7etintemel"], "title": "Continuous Prompts: LLM-Augmented Pipeline Processing over Unstructured Streams", "comment": null, "summary": "Monitoring unstructured streams increasingly requires persistent, semantics-aware computation, yet today's LLM frameworks remain stateless and one-shot, limiting their usefulness for long-running analytics. We introduce Continuous Prompts (CPs), the first framework that brings LLM reasoning into continuous stream processing. CPs extend RAG to streaming settings, define continuous semantic operators, and provide multiple implementations, primarily focusing on LLM-based approaches but also reporting one embedding-based variants. Furthermore, we study two LLM-centric optimizations, tuple batching and operator fusion, to significantly improve efficiency while managing accuracy loss.\n  Because these optimizations inherently trade accuracy for speed, we present a dynamic optimization framework that uses lightweight shadow executions and cost-aware multi-objective Bayesian optimization (MOBO) to learn throughput-accuracy frontiers and adapt plans under probing budgets.\n  We implement CPs in the VectraFlow stream processing system. Using operator-level microbenchmarks and streaming pipelines on real datasets, we show that VectraFlow can adapt to workload dynamics, navigate accuracy-efficiency trade-offs, and sustain persistent semantic queries over evolving unstructured streams.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Continuous Prompts\uff08CPs\uff09\u6846\u67b6\uff0c\u9996\u6b21\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u5f15\u5165\u6301\u7eed\u6d41\u5904\u7406\u4e2d\uff0c\u901a\u8fc7\u6269\u5c55RAG\u3001\u5b9a\u4e49\u8bed\u4e49\u7b97\u5b50\u5e76\u7ed3\u5408LLM\u4e0e\u5d4c\u5165\u65b9\u6cd5\uff0c\u652f\u6301\u5bf9\u975e\u7ed3\u6784\u5316\u6570\u636e\u6d41\u8fdb\u884c\u6301\u4e45\u8bed\u4e49\u67e5\u8be2\uff1b\u540c\u65f6\u63d0\u51fa\u52a8\u6001\u4f18\u5316\u673a\u5236\uff0c\u5728\u7cbe\u5ea6\u4e0e\u541e\u5410\u4e4b\u95f4\u5b9e\u73b0\u81ea\u9002\u5e94\u6743\u8861\u3002", "motivation": "\u5f53\u524dLLM\u6846\u67b6\u591a\u4e3a\u65e0\u72b6\u6001\u7684\u4e00\u6b21\u6027\u8c03\u7528\uff0c\u96be\u4ee5\u652f\u6301\u5bf9\u975e\u7ed3\u6784\u5316\u6570\u636e\u6d41\u8fdb\u884c\u6301\u7eed\u3001\u8bed\u4e49\u611f\u77e5\u7684\u957f\u671f\u5206\u6790\uff0c\u9650\u5236\u4e86\u5176\u5728\u6d41\u5f0f\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faContinuous Prompts\u6846\u67b6\uff0c\u5c06RAG\u6269\u5c55\u81f3\u6d41\u5f0f\u73af\u5883\uff0c\u5b9a\u4e49\u8fde\u7eed\u8bed\u4e49\u7b97\u5b50\uff0c\u5e76\u5b9e\u73b0\u57fa\u4e8eLLM\u548c\u5d4c\u5165\u7684\u591a\u79cd\u7b97\u5b50\uff1b\u5f15\u5165\u5143\u7ec4\u6279\u5904\u7406\u4e0e\u7b97\u5b50\u878d\u5408\u4e24\u79cd\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5f71\u5b50\u6267\u884c\u4e0e\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff08MOBO\uff09\u6784\u5efa\u52a8\u6001\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u5728\u7cbe\u5ea6\u4e0e\u6548\u7387\u95f4\u81ea\u9002\u5e94\u8c03\u6574\u3002", "result": "\u5728VectraFlow\u7cfb\u7edf\u4e2d\u5b9e\u73b0CPs\uff0c\u901a\u8fc7\u5fae\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6d41\u5904\u7406\u7ba1\u9053\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u80fd\u6709\u6548\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\uff0c\u5728\u4fdd\u8bc1\u8bed\u4e49\u67e5\u8be2\u6301\u7eed\u6027\u7684\u540c\u65f6\uff0c\u7075\u6d3b\u6743\u8861\u7cbe\u5ea6\u4e0e\u541e\u5410\u6027\u80fd\u3002", "conclusion": "Continuous Prompts\u4e3aLLM\u5728\u6301\u7eed\u6d41\u5904\u7406\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u673a\u5236\u5b9e\u73b0\u4e86\u5bf9\u975e\u7ed3\u6784\u5316\u6570\u636e\u6d41\u7684\u9ad8\u6548\u3001\u6301\u4e45\u8bed\u4e49\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2512.03608", "categories": ["cs.AR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.03608", "abs": "https://arxiv.org/abs/2512.03608", "authors": ["Lishuo Deng", "Shaojie Xu", "Jinwu Chen", "Changwei Yan", "Jiajie Wang", "Zhe Jiang", "Weiwei Shan"], "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing", "comment": null, "summary": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.", "AI": {"tldr": "KVNAND \u662f\u4e00\u79cd\u57fa\u4e8e\u8ba1\u7b97\u578b 3D NAND \u95ea\u5b58\u7684\u65e0 DRAM \u67b6\u6784\uff0c\u9996\u6b21\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6743\u91cd\u548c\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u5168\u90e8\u5b58\u50a8\u5728\u95ea\u5b58\u4e2d\uff0c\u901a\u8fc7\u5229\u7528\u7247\u4e0a\u8ba1\u7b97\u3001\u5934\u7ec4\u5e76\u884c\u6027\u548c\u9875\u7ea7 KV \u6620\u5c04\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u6027\u80fd\u5e76\u907f\u514d\u5185\u5b58\u6ea2\u51fa\u95ee\u9898\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u5185\u5b58\u5e26\u5bbd\u4e0e\u5bb9\u91cf\u74f6\u9888\uff1a\u4f20\u7edf\u65b9\u6848\u4f9d\u8d56 DRAM \u5b58\u50a8 KV \u7f13\u5b58\uff0c\u800c\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\uff0cKV \u7f13\u5b58\u4f53\u79ef\u751a\u81f3\u8d85\u8fc7\u6a21\u578b\u6743\u91cd\uff0c\u5bfc\u81f4 DRAM \u6210\u672c\u548c\u5bb9\u91cf\u9700\u6c42\u8fc7\u9ad8\uff1b\u82e5\u5c06 KV \u7f13\u5b58\u5378\u8f7d\u81f3\u95ea\u5b58\uff0c\u5219\u4f1a\u5e26\u6765\u4e25\u91cd\u6027\u80fd\u635f\u5931\u3002", "method": "\u63d0\u51fa KVNAND \u67b6\u6784\uff0c\u5c06\u6a21\u578b\u6743\u91cd\u4e0e KV \u7f13\u5b58\u5168\u90e8\u7f6e\u4e8e\u652f\u6301\u8ba1\u7b97\u7684 3D NAND \u95ea\u5b58\u4e2d\uff1b\u91c7\u7528\u7247\u4e0a\u8ba1\u7b97\u51cf\u5c11\u6570\u636e\u642c\u8fd0\u3001\u5f15\u5165\u5934\u7ec4\u5e76\u884c\u6027\u63d0\u9ad8\u541e\u5410\u91cf\u3001\u8bbe\u8ba1\u9875\u7ea7 KV \u7f13\u5b58\u6620\u5c04\u4ee5\u5339\u914d\u95ea\u5b58\u8bbf\u95ee\u7279\u6027\uff0c\u5e76\u6784\u5efa\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u6846\u67b6\u81ea\u52a8\u4f18\u5316\u6743\u91cd\u4e0e KV \u7f13\u5b58\u5e03\u5c40\u3002", "result": "\u5728 MHA 7B \u548c GQA 70B \u6a21\u578b\u4e0a\u8bc4\u4f30\u8868\u660e\uff0cKVNAND \u5728 128/1K/10K token \u4e0a\u4e0b\u6587\u4e2d\u76f8\u6bd4\u5e26 DRAM \u7684 IFC \u65b9\u6848\u5206\u522b\u5b9e\u73b0 1.98\u00d7/1.94\u00d7/2.05\u00d7 \u7684\u51e0\u4f55\u5e73\u5747\u52a0\u901f\uff0c\u5e76\u6210\u529f\u652f\u6301 100K token \u4e0a\u4e0b\u6587\u800c\u4e0d\u4f1a\u53d1\u751f\u5185\u5b58\u6ea2\u51fa\u3002", "conclusion": "KVNAND \u6709\u6548\u7f13\u89e3\u4e86\u95ea\u5b58\u5728\u5bc6\u96c6 KV \u7f13\u5b58\u8bbf\u95ee\u4e0b\u7684\u5ef6\u8fdf\u3001\u80fd\u8017\u4e0e\u53ef\u9760\u6027\u95ee\u9898\uff0c\u4f7f\u95ea\u5b58\u6210\u4e3a\u957f\u4e0a\u4e0b\u6587 LLM \u63a8\u7406\u4e2d\u5b9e\u7528\u7684\u5b58\u50a8\u4ecb\u8d28\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u9ad8\u6548 LLM \u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2512.03049", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.03049", "abs": "https://arxiv.org/abs/2512.03049", "authors": ["Kirk Roffi"], "title": "State Transition Block Diagram of the Generalized Maxwell Slip Friction Model", "comment": "6 pages, 4 figures", "summary": "Dynamic friction models (DFMs) encode essential information for the simulation and control of systems with friction. Traditionally, DFMs have been published with conceptual block diagrams, promoting clarity and reproducibility in simulation. However, modern DFMs have grown increasingly complex and block diagrams are now rarely presented, limiting accessibility. This letter presents a block diagram representation of the Generalized Maxwell Slip (GMS) friction model, an advanced multi-state DFM capable of simulating a wide range of nonlinear friction phenomena. The diagram can be implemented in the MATLAB-Simulink environment using a Stateflow chart or embedded if-else logic to represent the state transition criteria, but it is not limited to this platform. Closed-loop and open-loop simulations were conducted to verify that the block diagram reproduces non-drifting behavior and stick-slip friction, including benchmarking against the LuGre model. The proposed diagram improves accessibility to advanced dynamic friction models and provides the engineering community with a practical tool for the simulation and control of systems with friction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u9ea6\u514b\u65af\u97e6\u6ed1\u79fb\uff08GMS\uff09\u6469\u64e6\u6a21\u578b\u7684\u6846\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u590d\u6742\u52a8\u6001\u6469\u64e6\u6a21\u578b\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u5b9e\u73b0\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u4ee3\u52a8\u6001\u6469\u64e6\u6a21\u578b\u65e5\u76ca\u590d\u6742\uff0c\u7f3a\u4e4f\u4f20\u7edf\u7684\u6846\u56fe\u8868\u793a\uff0c\u9650\u5236\u4e86\u5176\u53ef\u7406\u89e3\u6027\u4e0e\u53ef\u590d\u73b0\u6027\u3002", "method": "\u6784\u5efaGMS\u6469\u64e6\u6a21\u578b\u7684\u6846\u56fe\uff0c\u5e76\u5728MATLAB-Simulink\u4e2d\u901a\u8fc7Stateflow\u6216\u5d4c\u5165\u5f0fif-else\u903b\u8f91\u5b9e\u73b0\u72b6\u6001\u8f6c\u79fb\u903b\u8f91\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u6846\u56fe\u80fd\u51c6\u786e\u518d\u73b0\u975e\u6f02\u79fb\u884c\u4e3a\u548c\u7c98\u6ed1\u6469\u64e6\u73b0\u8c61\uff0c\u5e76\u4e0eLuGre\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u5bf9\u6bd4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u56fe\u4e3a\u5de5\u7a0b\u754c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u9ad8\u7ea7\u52a8\u6001\u6469\u64e6\u6a21\u578b\u5728\u4eff\u771f\u4e0e\u63a7\u5236\u4e2d\u7684\u53ef\u53ca\u6027\u4e0e\u5e94\u7528\u6027\u3002"}}
{"id": "2512.03697", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.03697", "abs": "https://arxiv.org/abs/2512.03697", "authors": ["Rafael Ravedutti Lucio Machado", "Jan Eitzinger", "Georg Hager", "Gerhard Wellein"], "title": "On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs", "comment": "8 pages, 4 figures, conference", "summary": "This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728Fritz\u548cAlex\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u4e0a\u5206\u6790\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548cGromacs\u8f6f\u4ef6\u5305\u80fd\u6548\u65f6\u6240\u9762\u4e34\u7684\u6311\u6218\uff0c\u4f7f\u7528Intel Ice Lake\u3001Sapphire Rapids CPU\u53caNvidia A40\u3001A100 GPU\u8fdb\u884cMPI\u5e76\u884c\u5b9e\u9a8c\uff0c\u5e76\u57fa\u4e8eLikwid\u4e0eNvidia\u6027\u80fd\u5206\u6790\u5de5\u5177\u63d0\u51fa\u6d4b\u91cf\u7ed3\u679c\u3001\u95ee\u9898\u5256\u6790\u53ca\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae\u3002", "motivation": "\u5f53\u524d\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u80fd\u6548\u7684\u8bc4\u4f30\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\uff0c\u4e14\u5728\u5b9e\u9645\u6d4b\u91cf\u4e2d\u5b58\u5728\u8bf8\u591a\u6280\u672f\u6311\u6218\u548c\u9677\u9631\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u8fd9\u4e9b\u95ee\u9898\u5e76\u63d0\u51fa\u53ef\u884c\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "method": "\u5728Fritz\u548cAlex HPC\u96c6\u7fa4\u4e0a\uff0c\u5229\u7528MPI\u5728Intel Ice Lake\u548cSapphire Rapids CPU\u5168\u63d2\u69fd\u4ee5\u53caNvidia A40\u548cA100 GPU\u4e0a\u8fd0\u884c\u5408\u6210\u57fa\u51c6\u548cGromacs\uff1b\u4f7f\u7528Likwid\u548cNvidia\u6027\u80fd\u5206\u6790\u5de5\u5177\u6536\u96c6\u80fd\u6548\u76f8\u5173\u6307\u6807\uff0c\u5e76\u5bf9\u5b9e\u9a8c\u8fc7\u7a0b\u4e2d\u7684\u95ee\u9898\u8fdb\u884c\u5206\u6790\u3002", "result": "\u83b7\u5f97\u4e86\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u4e0b\u7684\u80fd\u6548\u6d4b\u91cf\u6570\u636e\uff0c\u63ed\u793a\u4e86\u5728\u80fd\u6548\u5206\u6790\u8fc7\u7a0b\u4e2d\u5e38\u89c1\u7684\u6311\u6218\u4e0e\u8bef\u533a\uff0c\u5982\u5de5\u5177\u517c\u5bb9\u6027\u3001\u6d4b\u91cf\u7c92\u5ea6\u548c\u7cfb\u7edf\u5e72\u6270\u7b49\u95ee\u9898\u3002", "conclusion": "\u4e3a\u672a\u6765\u9ad8\u6027\u80fd\u8ba1\u7b97\u80fd\u6548\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6700\u4f73\u5b9e\u8df5\u6307\u5357\uff0c\u5f3a\u8c03\u9700\u8c28\u614e\u9009\u62e9\u6d4b\u91cf\u5de5\u5177\u3001\u63a7\u5236\u5b9e\u9a8c\u73af\u5883\uff0c\u5e76\u5145\u5206\u7406\u89e3\u786c\u4ef6\u4e0e\u8f6f\u4ef6\u4ea4\u4e92\u5bf9\u80fd\u6548\u7ed3\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2512.03401", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.03401", "abs": "https://arxiv.org/abs/2512.03401", "authors": ["Ryoto Miyamoto", "Akira Kasuga"], "title": "Enterprise Data Science Platform: A Unified Architecture for Federated Data Access", "comment": "10 pages, 2 figures, 3 tables, WS-D2ET @ IEEE BigData 2025", "summary": "Organizations struggle to share data across departments that have adopted different data analytics platforms. If n datasets must serve m environments, up to n*m replicas can emerge, increasing inconsistency and cost. Traditional warehouses copy data into vendor-specific stores; cross-platform access is hard. This study proposes the Enterprise Data Science Platform (EDSP), which builds on data lakehouse architecture and follows a Write-Once, Read-Anywhere principle. EDSP enables federated data access for multi-query engine environments, targeting data science workloads with periodic data updates and query response times ranging from seconds to minutes. By providing centralized data management with federated access from multiple query engines to the same data sources, EDSP eliminates data duplication and vendor lock-in inherent in traditional data warehouses. The platform employs a four-layer architecture: Data Preparation, Data Store, Access Interface, and Query Engines. This design enforces separation of concerns and reduces the need for data migration when integrating additional analytical environments. Experimental results demonstrate that major cloud data warehouses and programming environments can directly query EDSP-managed datasets. We implemented and deployed EDSP in production, confirming interoperability across multiple query engines. For data sharing across different analytical environments, EDSP achieves a 33-44% reduction in operational steps compared with conventional approaches requiring data migration. Although query latency may increase by up to a factor of 2.6 compared with native tables, end-to-end completion times remain on the order of seconds, maintaining practical performance for analytical use cases. Based on our production experience, EDSP provides practical design guidelines for addressing the data-silo problem in multi-query engine environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4f01\u4e1a\u6570\u636e\u79d1\u5b66\u5e73\u53f0\uff08EDSP\uff09\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u57fa\u4e8e\u6e56\u4ed3\u4e00\u4f53\u67b6\u6784\uff0c\u91c7\u7528\u201c\u4e00\u6b21\u5199\u5165\u3001\u968f\u5904\u8bfb\u53d6\u201d\u539f\u5219\uff0c\u901a\u8fc7\u56db\u5c42\u67b6\u6784\u5b9e\u73b0\u8de8\u591a\u4e2a\u67e5\u8be2\u5f15\u64ce\u7684\u8054\u90a6\u6570\u636e\u8bbf\u95ee\uff0c\u6709\u6548\u51cf\u5c11\u6570\u636e\u5197\u4f59\u548c\u5382\u5546\u9501\u5b9a\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u663e\u8457\u964d\u4f4e\u64cd\u4f5c\u6b65\u9aa4\uff0c\u867d\u7565\u6709\u589e\u52a0\u67e5\u8be2\u5ef6\u8fdf\uff0c\u4f46\u6574\u4f53\u4ecd\u6ee1\u8db3\u5206\u6790\u578b\u573a\u666f\u7684\u6027\u80fd\u9700\u6c42\u3002", "motivation": "\u7ec4\u7ec7\u5728\u4e0d\u540c\u90e8\u95e8\u4f7f\u7528\u5f02\u6784\u6570\u636e\u5206\u6790\u5e73\u53f0\u65f6\uff0c\u9762\u4e34\u6570\u636e\u5b64\u5c9b\u3001\u91cd\u590d\u5b58\u50a8\u3001\u4e0d\u4e00\u81f4\u6027\u548c\u9ad8\u6602\u6210\u672c\u7b49\u95ee\u9898\uff1b\u4f20\u7edf\u6570\u636e\u4ed3\u5e93\u56e0\u4f9d\u8d56\u5382\u5546\u7279\u5b9a\u5b58\u50a8\uff0c\u96be\u4ee5\u652f\u6301\u8de8\u5e73\u53f0\u8bbf\u95ee\u3002", "method": "\u63d0\u51faEDSP\u5e73\u53f0\uff0c\u57fa\u4e8e\u6e56\u4ed3\u4e00\u4f53\u67b6\u6784\uff0c\u91c7\u7528\u56db\u5c42\u8bbe\u8ba1\uff08\u6570\u636e\u51c6\u5907\u3001\u6570\u636e\u5b58\u50a8\u3001\u8bbf\u95ee\u63a5\u53e3\u3001\u67e5\u8be2\u5f15\u64ce\uff09\uff0c\u5b9e\u73b0\u96c6\u4e2d\u5f0f\u6570\u636e\u7ba1\u7406\u4e0e\u591a\u67e5\u8be2\u5f15\u64ce\u7684\u8054\u90a6\u8bbf\u95ee\uff0c\u907f\u514d\u6570\u636e\u590d\u5236\u548c\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u548c\u751f\u4ea7\u90e8\u7f72\u8868\u660e\uff0c\u4e3b\u6d41\u4e91\u6570\u4ed3\u548c\u7f16\u7a0b\u73af\u5883\u53ef\u76f4\u63a5\u67e5\u8be2EDSP\u7ba1\u7406\u7684\u6570\u636e\uff1b\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u64cd\u4f5c\u6b65\u9aa4\u51cf\u5c1133\u201344%\uff1b\u67e5\u8be2\u5ef6\u8fdf\u6700\u591a\u589e\u52a02.6\u500d\uff0c\u4f46\u7aef\u5230\u7aef\u54cd\u5e94\u4ecd\u5728\u79d2\u7ea7\uff0c\u6ee1\u8db3\u5b9e\u9645\u5206\u6790\u9700\u6c42\u3002", "conclusion": "EDSP\u4e3a\u591a\u67e5\u8be2\u5f15\u64ce\u73af\u5883\u4e0b\u7684\u6570\u636e\u5b64\u5c9b\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u4e14\u5b9e\u7528\u7684\u67b6\u6784\u65b9\u6848\uff0c\u517c\u987e\u4e92\u64cd\u4f5c\u6027\u3001\u6210\u672c\u6548\u76ca\u4e0e\u6027\u80fd\uff0c\u5177\u6709\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2512.03616", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03616", "abs": "https://arxiv.org/abs/2512.03616", "authors": ["Christian Ewert", "Amrit Sharma Poudel", "Mouadh Ayache", "Andrija Neskovic", "Rainer Buchty", "Mladen Berekovic", "Sebastian Berndt", "Saleh Mulhem"], "title": "Lightweight Unified Sha-3/Shake Architecture with a Fault-Resilient State", "comment": "-", "summary": "Hash functions have become a key part of standard Post-quantum cryptography (PQC) schemes, especially Sha-3 and Shake, calling arXiv:submit/7045552 [cs.AR] 3 Dec 2025 for lightweight implementation. A fault-resilient design is always desirable to make the whole PQC system reliable. We, therefore, propose a) a unified hash engine supporting Sha-3 and Shake that follows a byte-wise in-place partitioning mechanism of the so-called Keccak state, and b) an according fault detection for Keccak state protection exploiting its cube structure by deploying two-dimensional parity checks. It outperforms the state-of-the-art (SoA) regarding area requirements at competitive register-level fault detection by achieving 100% detection of three and still near 100% of higher numbers of Keccak state faults. Unlike SoA solutions, the proposed unified hash engine covers all standard hash configurations. Moreover, the introduced multidimensional cross-parity check mechanism achieves a 3.7x improvement in area overhead, with an overall 4.5x smaller fault-resilient engine design as demonstrated in ASIC and FPGA implementations. Integrated into a RISC-V environment, the unified hash engine with the integrated fault-resilient mechanism introduced less than 8% area overhead. Our approach thus provides a robust and lightweight fault-detection solution for protecting hash functions deployed in resource-constrained PQC applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301SHA-3\u548cSHAKE\u7684\u7edf\u4e00\u54c8\u5e0c\u5f15\u64ce\uff0c\u91c7\u7528\u5b57\u8282\u7ea7\u539f\u4f4d\u5212\u5206Keccak\u72b6\u6001\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u4e8c\u7ef4\u5947\u5076\u6821\u9a8c\u7684\u6545\u969c\u68c0\u6d4b\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u9ad8\u6545\u969c\u8986\u76d6\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u9762\u79ef\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u5e94\u7528\u3002", "motivation": "\u5728\u540e\u91cf\u5b50\u5bc6\u7801\uff08PQC\uff09\u7cfb\u7edf\u4e2d\uff0c\u54c8\u5e0c\u51fd\u6570\uff08\u5982SHA-3\u548cSHAKE\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u786c\u4ef6\u5b9e\u73b0\u9700\u517c\u987e\u8f7b\u91cf\u6027\u4e0e\u53ef\u9760\u6027\u3002\u73b0\u6709\u65b9\u6848\u5728\u652f\u6301\u6807\u51c6\u914d\u7f6e\u3001\u6545\u969c\u68c0\u6d4b\u80fd\u529b\u6216\u9762\u79ef\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u56e0\u6b64\u4e9f\u9700\u4e00\u79cd\u65e2\u80fd\u8986\u76d6\u6240\u6709\u6807\u51c6\u54c8\u5e0c\u914d\u7f6e\u53c8\u5177\u5907\u9ad8\u6548\u5bb9\u9519\u80fd\u529b\u7684\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\uff1aa) \u4e00\u79cd\u7edf\u4e00\u7684\u54c8\u5e0c\u5f15\u64ce\uff0c\u901a\u8fc7\u5b57\u8282\u7ea7\u539f\u4f4d\u5212\u5206\u673a\u5236\u5904\u7406Keccak\u72b6\u6001\uff0c\u540c\u65f6\u652f\u6301SHA-3\u548cSHAKE\uff1bb) \u4e00\u79cd\u5229\u7528Keccak\u72b6\u6001\u7acb\u65b9\u7ed3\u6784\u7684\u4e8c\u7ef4\u4ea4\u53c9\u5947\u5076\u6821\u9a8c\u673a\u5236\uff0c\u7528\u4e8e\u68c0\u6d4b\u5bc4\u5b58\u5668\u7ea7\u6545\u969c\u3002", "result": "\u8be5\u8bbe\u8ba1\u5728ASIC\u548cFPGA\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u5c0f4.5\u500d\u7684\u6574\u4f53\u5bb9\u9519\u5f15\u64ce\uff0c\u9762\u79ef\u5f00\u9500\u964d\u4f4e3.7\u500d\uff1b\u5bf9Keccak\u72b6\u6001\u4e2d\u4e09\u4e2a\u6545\u969c\u5b9e\u73b0100%\u68c0\u6d4b\uff0c\u5bf9\u66f4\u591a\u6545\u969c\u4ecd\u63a5\u8fd1100%\u68c0\u6d4b\u7387\uff1b\u96c6\u6210\u5230RISC-V\u73af\u5883\u4e2d\u4ec5\u5e26\u6765\u4e0d\u52308%\u7684\u9762\u79ef\u5f00\u9500\uff0c\u5e76\u5168\u9762\u652f\u6301\u6240\u6709\u6807\u51c6\u54c8\u5e0c\u914d\u7f6e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7edf\u4e00\u54c8\u5e0c\u5f15\u64ce\u7ed3\u5408\u591a\u7ef4\u4ea4\u53c9\u5947\u5076\u6821\u9a8c\u673a\u5236\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u540e\u91cf\u5b50\u5bc6\u7801\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u4e14\u9ad8\u53ef\u9760\u6027\u7684\u54c8\u5e0c\u51fd\u6570\u5bb9\u9519\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.03279", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2512.03279", "abs": "https://arxiv.org/abs/2512.03279", "authors": ["Kaiwei Tu", "Kan Wu", "Andrea C. Arpaci-Dusseau", "Remzi H. Arpaci-Dusseau"], "title": "Getting the MOST out of your Storage Hierarchy with Mirror-Optimized Storage Tiering", "comment": "18 pages, to be published in 24th USENIX Conference on File and Storage Technologies (FAST '26)", "summary": "We present Mirror-Optimized Storage Tiering (MOST), a novel tiering-based approach optimized for modern storage hierarchies. The key idea of MOST is to combine the load balancing advantages of mirroring with the space-efficiency advantages of tiering. Specifically, MOST dynamically mirrors a small amount of hot data across storage tiers to efficiently balance load, avoiding costly migrations. As a result, MOST is as space-efficient as classic tiering while achieving better bandwidth utilization under I/O-intensive workloads. We implement MOST in Cerberus, a user-level storage management layer based on CacheLib. We show the efficacy of Cerberus through a comprehensive empirical study: across a range of static and dynamic workloads, Cerberus achieves better throughput than competing approaches on modern storage hierarchies especially under I/O-intensive and dynamic workloads.", "AI": {"tldr": "MOST \u662f\u4e00\u79cd\u7ed3\u5408\u955c\u50cf\u4e0e\u5206\u5c42\u5b58\u50a8\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7a7a\u95f4\u6548\u7387\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u52a8\u6001\u955c\u50cf\u70ed\u70b9\u6570\u636e\u63d0\u5347 I/O \u5bc6\u96c6\u578b\u8d1f\u8f7d\u4e0b\u7684\u5e26\u5bbd\u5229\u7528\u7387\u3002", "motivation": "\u4f20\u7edf\u5b58\u50a8\u5206\u5c42\u5728 I/O \u5bc6\u96c6\u578b\u6216\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u96be\u4ee5\u9ad8\u6548\u5e73\u8861\u8d1f\u8f7d\uff0c\u800c\u955c\u50cf\u867d\u80fd\u8d1f\u8f7d\u5747\u8861\u4f46\u7a7a\u95f4\u5f00\u9500\u5927\uff1bMOST \u65e8\u5728\u878d\u5408\u4e24\u8005\u4f18\u70b9\uff0c\u517c\u987e\u7a7a\u95f4\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "MOST \u52a8\u6001\u5730\u5728\u5b58\u50a8\u5c42\u7ea7\u95f4\u5bf9\u5c11\u91cf\u70ed\u70b9\u6570\u636e\u8fdb\u884c\u955c\u50cf\uff0c\u907f\u514d\u6602\u8d35\u7684\u6570\u636e\u8fc1\u79fb\uff0c\u5e76\u5728 CacheLib \u57fa\u7840\u4e0a\u5b9e\u73b0\u4e3a\u7528\u6237\u6001\u5b58\u50a8\u7ba1\u7406\u5c42 Cerberus\u3002", "result": "\u5728\u591a\u79cd\u9759\u6001\u548c\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0cCerberus\uff08MOST \u7684\u5b9e\u73b0\uff09\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u73b0\u4ee3\u5b58\u50a8\u5c42\u6b21\u7ed3\u6784\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u541e\u5410\u91cf\uff0c\u5c24\u5176\u5728 I/O \u5bc6\u96c6\u578b\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MOST \u6210\u529f\u7ed3\u5408\u4e86\u955c\u50cf\u7684\u8d1f\u8f7d\u5747\u8861\u4f18\u52bf\u4e0e\u5206\u5c42\u7684\u7a7a\u95f4\u6548\u7387\uff0c\u5728\u4e0d\u589e\u52a0\u663e\u8457\u5b58\u50a8\u5f00\u9500\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86 I/O \u6027\u80fd\u3002"}}
{"id": "2512.03825", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03825", "abs": "https://arxiv.org/abs/2512.03825", "authors": ["Aingeru Ramos", "Jose A Pascual", "Javier Navaridas", "Ivan Coluzza"], "title": "Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods", "comment": "14 pages, 7 figures (5 of them composed by 2 subfigures)", "summary": "Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOpenMP\u548cCUDA\u7684\u5e76\u884c\u5316Metropolis-Hastings\u4e0eParallel Tempering\u7b97\u6cd5\u5b9e\u73b0\uff0c\u5728CPU\u548cGPU\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86\u6700\u9ad852\u500d\u548c986\u500d\u7684\u52a0\u901f\uff0c\u5e76\u4e3a\u672a\u6765\u91cf\u5b50\u5b9e\u73b0\u63d0\u4f9b\u57fa\u51c6\u3002", "motivation": "\u4f20\u7edf\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\uff08MCMC\uff09\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6784\u578b\u7a7a\u95f4\u65f6\u91c7\u6837\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u63d0\u5347\u7cbe\u5ea6\u7684\u6280\u672f\uff08\u5982Parallel Tempering\uff09\u901a\u5e38\u5e26\u6765\u9ad8\u6602\u8ba1\u7b97\u5f00\u9500\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u7684\u5e76\u884c\u5316\u7b56\u7565\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "method": "\u91c7\u7528OpenMP\u5728\u591a\u6838CPU\u4e0a\u3001CUDA\u5728GPU\u4e0a\u5bf9Metropolis-Hastings\u7b97\u6cd5\u7ed3\u5408Parallel Tempering\u8fdb\u884c\u5e76\u884c\u5b9e\u73b0\u3002", "result": "\u572848\u6838CPU\u4e0a\u4f7f\u7528OpenMP\u83b7\u5f97\u6700\u9ad852\u500d\u52a0\u901f\uff0c\u5728GPU\u4e0a\u4f7f\u7528CUDA\u83b7\u5f97\u6700\u9ad8986\u500d\u52a0\u901f\uff1b\u7ed3\u679c\u53ef\u4f5c\u4e3a\u672a\u6765\u91cf\u5b50\u5b9e\u73b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u901a\u8fc7\u73b0\u4ee3\u786c\u4ef6\u7684\u5e76\u884c\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86MCMC\u4e0eParallel Tempering\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4f7f\u5f97\u66f4\u5927\u89c4\u6a21\u7269\u7406/\u5316\u5b66\u6a21\u578b\u7684\u6a21\u62df\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u4e3a\u540e\u7eed\u91cf\u5b50\u7b97\u6cd5\u6bd4\u8f83\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2512.03790", "categories": ["cs.DB", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.03790", "abs": "https://arxiv.org/abs/2512.03790", "authors": ["Iris Beerepoot", "Vinicius Stein Dani", "Xixi Lu"], "title": "ExOAR: Expert-Guided Object and Activity Recognition from Textual Data", "comment": "Accepted manuscript (on August 22, 2025) to the 2nd International Workshop on Generative AI for Process Mining (GenAI4PM 2025), held in conjunction with the 7th International Conference on Process Mining (ICPM 2025)", "summary": "Object-centric process mining requires structured data, but extracting it from unstructured text remains a challenge. We introduce ExOAR (Expert-Guided Object and Activity Recognition), an interactive method that combines large language models (LLMs) with human verification to identify objects and activities from textual data. ExOAR guides users through consecutive stages in which an LLM generates candidate object types, activities, and object instances based on contextual input, such as a user's profession, and textual data. Users review and refine these suggestions before proceeding to the next stage. Implemented as a practical tool, ExOAR is initially validated through a demonstration and then evaluated with real-world Active Window Tracking data from five users. Our results show that ExOAR can effectively bridge the gap between unstructured textual data and the structured log with clear semantics needed for object-centric process analysis, while it maintains flexibility and human oversight.", "AI": {"tldr": "ExOAR is an interactive method that combines large language models with human verification to extract structured object-centric process mining data from unstructured text.", "motivation": "Object-centric process mining needs structured data, yet extracting such data from unstructured textual sources is difficult; existing automated methods often lack semantic clarity or human interpretability.", "method": "ExOAR uses an LLM guided by user context (e.g., profession) to propose object types, activities, and instances from text; users iteratively review and refine these suggestions in a staged workflow.", "result": "Evaluation with real-world Active Window Tracking data from five users shows ExOAR successfully produces structured logs with clear semantics suitable for object-centric process mining.", "conclusion": "ExOAR effectively bridges unstructured text and structured object-centric logs while preserving flexibility and enabling essential human oversight."}}
{"id": "2512.03781", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03781", "abs": "https://arxiv.org/abs/2512.03781", "authors": ["Joscha Ilmberger", "Johannes Schemmel"], "title": "The BrainScaleS-2 multi-chip system: Interconnecting continuous-time neuromorphic compute substrates", "comment": null, "summary": "The BrainScaleS-2 SoC integrates analog neuron and synapse circuits with digital periphery, including two CPUs with SIMD extensions. Each ASIC is connected to a Node-FPGA, providing experiment control and Ethernet connectivity. This work details the scaling of the compute substrate through FPGA-based interconnection via an additional Aggregator unit. The Aggregator provides up to 12 transceiver links to a backplane of Node-FPGAs, as well as 4 transceiver lanes for further extension. Two such interconnected backplanes are integrated into a standard 19in rack case with 4U height together with an Ethernet switch, system controller and power supplies. For all spike rates, chip-to-chip latencies -- consisting of four hops across three FPGAs -- below 1.3$\u03bc$s are achieved within each backplane.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BrainScaleS-2 SoC\u7cfb\u7edf\u901a\u8fc7FPGA\u4e92\u8fde\u67b6\u6784\u7684\u6269\u5c55\u65b9\u6848\uff0c\u5229\u7528Aggregator\u5355\u5143\u5b9e\u73b0\u591a\u82af\u7247\u4f4e\u5ef6\u8fdf\u901a\u4fe1\uff0c\u5e76\u96c6\u6210\u5230\u6807\u51c6\u673a\u67b6\u4e2d\u3002", "motivation": "\u4e3a\u652f\u6301\u5927\u89c4\u6a21\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\uff0c\u9700\u6269\u5c55BrainScaleS-2 SoC\u7684\u8ba1\u7b97\u57fa\u5e95\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u82af\u7247\u95f4\u901a\u4fe1\u3002", "method": "\u91c7\u7528\u57fa\u4e8eFPGA\u7684\u4e92\u8fde\u67b6\u6784\uff0c\u5f15\u5165Aggregator\u5355\u5143\u8fde\u63a5\u591a\u4e2aNode-FPGA\uff0c\u6784\u5efa\u5305\u542b12\u6761\u6536\u53d1\u5668\u94fe\u8def\u7684\u80cc\u677f\u7cfb\u7edf\uff0c\u5e76\u96c6\u6210\u81f319\u82f1\u5bf84U\u673a\u67b6\u4e2d\u3002", "result": "\u5728\u6bcf\u4e2a\u80cc\u677f\u5185\uff0c\u8de8\u4e09\u4e2aFPGA\u7684\u56db\u8df3\u901a\u4fe1\u5b9e\u73b0\u4e86\u4f4e\u4e8e1.3\u5fae\u79d2\u7684\u82af\u7247\u95f4\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8e\u6240\u6709\u8109\u51b2\u53d1\u653e\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e92\u8fde\u67b6\u6784\u6709\u6548\u652f\u6301\u4e86BrainScaleS-2\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4fdd\u6301\u4e86\u6781\u4f4e\u7684\u901a\u4fe1\u5ef6\u8fdf\uff0c\u4e3a\u5927\u89c4\u6a21\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.03569", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.03569", "abs": "https://arxiv.org/abs/2512.03569", "authors": ["Gianluca Cena", "Pietro Chiavassa", "Stefano Scanzio"], "title": "Performance Evaluation of Parallel Wi-Fi Redundancy with Deferral Techniques", "comment": "preprint accepted, 8 pages, 2025", "summary": "Wireless communication is increasingly used in industrial environments, since it supports mobility of interconnected devices. Among the transmission technologies operating in unlicensed bands available to this purpose, Wi-Fi is certainly one of the most interesting, because of its high performance and the relatively low deployment costs. Unfortunately, its dependability is often deemed unsuitable for real-time control systems. In this paper, the use of parallel redundancy is evaluated from a quantitative viewpoint, by considering a number of performance indices that are relevant for soft real-time applications. Analysis is carried out on a large dataset acquired from a real setup, to provide realistic insights on the advantages this kind of approaches can provide. As will be seen, deferred parallel redundancy provides clear advantages in terms of the worst-case transmission latency, at limited costs concerning the amount of consumed spectrum. Hence, it can be practically exploited every time a wireless connection is included in a control loop.", "AI": {"tldr": "\u672c\u6587\u4ece\u5b9a\u91cf\u89d2\u5ea6\u8bc4\u4f30\u4e86\u5728\u5de5\u4e1a\u65e0\u7ebf\u901a\u4fe1\u4e2d\u91c7\u7528\u5e76\u884c\u5197\u4f59\uff08\u7279\u522b\u662f\u5ef6\u8fdf\u5e76\u884c\u5197\u4f59\uff09\u5bf9\u8f6f\u5b9e\u65f6\u5e94\u7528\u6027\u80fd\u7684\u63d0\u5347\uff0c\u57fa\u4e8e\u771f\u5b9e\u5b9e\u9a8c\u6570\u636e\u8868\u660e\u5176\u80fd\u663e\u8457\u6539\u5584\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u4f20\u8f93\u5ef6\u8fdf\uff0c\u4e14\u9891\u8c31\u5f00\u9500\u6709\u9650\u3002", "motivation": "Wi-Fi\u867d\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u56e0\u9ad8\u6027\u80fd\u548c\u4f4e\u6210\u672c\u800c\u5177\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5176\u53ef\u9760\u6027\u5e38\u88ab\u8ba4\u4e3a\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u63a7\u5236\u7cfb\u7edf\uff0c\u56e0\u6b64\u9700\u63a2\u7d22\u63d0\u5347\u5176\u53ef\u9760\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u771f\u5b9e\u5b9e\u9a8c\u5e73\u53f0\u91c7\u96c6\u5927\u91cf\u6570\u636e\uff0c\u4ece\u5b9a\u91cf\u89d2\u5ea6\u8bc4\u4f30\u5e76\u884c\u5197\u4f59\u7b56\u7565\u5728\u591a\u4e2a\u4e0e\u8f6f\u5b9e\u65f6\u5e94\u7528\u76f8\u5173\u7684\u6027\u80fd\u6307\u6807\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5ef6\u8fdf\u5e76\u884c\u5197\u4f59\u5728\u6700\u574f\u60c5\u51b5\u4f20\u8f93\u5ef6\u8fdf\u65b9\u9762\u5e26\u6765\u660e\u663e\u4f18\u52bf\uff0c\u540c\u65f6\u4ec5\u6d88\u8017\u6709\u9650\u7684\u9891\u8c31\u8d44\u6e90\u3002", "conclusion": "\u5ef6\u8fdf\u5e76\u884c\u5197\u4f59\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u65e0\u7ebf\u8fde\u63a5\u88ab\u7eb3\u5165\u63a7\u5236\u56de\u8def\u65f6\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2512.03927", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03927", "abs": "https://arxiv.org/abs/2512.03927", "authors": ["Liujianfu Wang", "Yuyang Du", "Yuchen Pan", "Soung Chang Liew", "Jiacheng Liu", "Kexin Chen"], "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faOD-MoE\uff0c\u4e00\u79cd\u65e0\u9700\u4e13\u5bb6\u7f13\u5b58\u7684\u5206\u5e03\u5f0fMoE\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6309\u9700\u52a0\u8f7d\u4e13\u5bb6\u548c\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u673a\u5236\uff0c\u5728\u4ec51/3 GPU\u5185\u5b58\u4e0b\u5b9e\u73b0\u63a5\u8fd1\u5168\u7f13\u5b58\u90e8\u7f7275%\u7684\u89e3\u7801\u901f\u5ea6\uff0c\u5e76\u652f\u6301\u4f4e\u4e8e1GB\u663e\u5b58\u7684\u8fb9\u7f18\u8bbe\u5907\u8fd0\u884cMoE\u6a21\u578b\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4f4e\u6210\u672c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72Mixture-of-Experts\uff08MoE\uff09\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34GPU\u5185\u5b58\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u73b0\u6709\u4e13\u5bb6\u5378\u8f7d\u65b9\u6cd5\u867d\u80fd\u7f13\u89e3\u5185\u5b58\u538b\u529b\uff0c\u4f46\u4e13\u5bb6\u7f13\u5b58\u5229\u7528\u7387\u4f4e\uff0c\u9650\u5236\u4e86\u6027\u80fd\u4e0e\u90e8\u7f72\u7075\u6d3b\u6027\u3002", "method": "OD-MoE\u91c7\u7528\u4e24\u79cd\u6838\u5fc3\u6280\u672f\uff1a1\uff09\u5728\u5206\u5e03\u5f0f\u8fb9\u7f18\u8282\u70b9\u4e0a\u5e76\u884c\u6267\u884c\u4e13\u5bb6\u52a0\u8f7d\u4e0e\u8ba1\u7b97\uff1b2\uff09\u4f7f\u7528\u9ad8\u7cbe\u5ea6\u6a21\u62df\u9884\u6d4b\u5668\uff0c\u5728\u5f53\u524d\u5c42\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u63d0\u524d\u591a\u5c42\u9884\u6d4b\u540e\u7eed\u4e13\u5bb6\u6fc0\u6d3b\u60c5\u51b5\uff0c\u4ece\u800c\u5b9e\u73b0\u6309\u9700\u5373\u65f6\u52a0\u8f7d\u4e0e\u91ca\u653e\u4e13\u5bb6\u53c2\u6570\uff0c\u5b8c\u5168\u6d88\u9664\u5bf9\u4e13\u5bb6\u7f13\u5b58\u7684\u9700\u6c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOD-MoE\u7684\u4e13\u5bb6\u6fc0\u6d3b\u9884\u6d4b\u51c6\u786e\u7387\u8fbe99.94%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u5728\u4ec5\u4f7f\u75281/3 GPU\u5185\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u5168\u7f13\u5b58\u90e8\u7f72\u7ea675%\u7684\u89e3\u7801\u901f\u5ea6\uff0c\u5e76\u53ef\u5728GPU\u5185\u5b58\u5c0f\u4e8e1GB\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884cMoE\u6a21\u578b\u3002", "conclusion": "OD-MoE\u901a\u8fc7\u6d88\u9664\u4e13\u5bb6\u7f13\u5b58\u9700\u6c42\uff0c\u663e\u8457\u964d\u4f4eMoE\u6a21\u578b\u5bf9\u8fb9\u7f18\u8bbe\u5907GPU\u5185\u5b58\u7684\u8981\u6c42\uff0c\u4e3a\u5728\u4f4e\u6210\u672c\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u5b9e\u9645\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684MoE\u67b6\u6784\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.03906", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.03906", "abs": "https://arxiv.org/abs/2512.03906", "authors": ["Alberto Ronzoni", "Anina Antony", "Anjana M R", "Francesca De Leo", "Jesna Jose", "Mattia Freda", "Nandini Narayanankutty", "Rafflesia Khan", "Raji RV", "Thomas Diacci"], "title": "IBM Multilevel Process Mining vs de facto Object-Centric Process Mining approaches", "comment": null, "summary": "The academic evolution of process mining is moving toward object centric process mining, marking a significant shift in how processes are modeled and analyzed. IBM has developed its own distinctive approach called Multilevel Process Mining. This paper provides a description of the two approaches and presents a comparative analysis of their respective advantages and limitations. IBM leveraged this comparison to drive the evolution of IBM Process Mining product, creating the new Organizational Mining feature, an innovation that combines the best of the two approaches. Demonstrate the potential of this novel, innovative and distinct methodology with an example.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u5bf9\u8c61\u4e2d\u5fc3\u6d41\u7a0b\u6316\u6398\u4e0eIBM\u7684\u591a\u7ea7\u6d41\u7a0b\u6316\u6398\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u63d0\u51fa\u4e86\u7ec4\u7ec7\u6316\u6398\u8fd9\u4e00\u65b0\u529f\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u6d41\u7a0b\u6316\u6398\u5411\u5bf9\u8c61\u4e2d\u5fc3\u8303\u5f0f\u6f14\u8fdb\uff0cIBM\u5e0c\u671b\u8bc4\u4f30\u81ea\u8eab\u591a\u7ea7\u6d41\u7a0b\u6316\u6398\u65b9\u6cd5\u4e0e\u65b0\u5174\u65b9\u6cd5\u7684\u4f18\u52a3\uff0c\u4ee5\u6307\u5bfc\u4ea7\u54c1\u521b\u65b0\u548c\u529f\u80fd\u6f14\u8fdb\u3002", "method": "\u5bf9\u5bf9\u8c61\u4e2d\u5fc3\u6d41\u7a0b\u6316\u6398\u4e0eIBM\u591a\u7ea7\u6d41\u7a0b\u6316\u6398\u8fdb\u884c\u63cf\u8ff0\u4e0e\u5bf9\u6bd4\u5206\u6790\uff0c\u63d0\u70bc\u5404\u81ea\u4f18\u7f3a\u70b9\uff0c\u5e76\u878d\u5408\u4e8c\u8005\u4f18\u52bf\u5f00\u53d1\u51fa\u65b0\u7684\u201c\u7ec4\u7ec7\u6316\u6398\u201d\u529f\u80fd\u3002", "result": "\u6210\u529f\u5f00\u53d1\u5e76\u5c55\u793a\u4e86\u201c\u7ec4\u7ec7\u6316\u6398\u201d\u8fd9\u4e00\u521b\u65b0\u529f\u80fd\uff0c\u4f53\u73b0\u4e86\u7ed3\u5408\u4e24\u79cd\u65b9\u6cd5\u4f18\u52bf\u6240\u5e26\u6765\u7684\u65b0\u65b9\u6cd5\u8bba\u6f5c\u529b\u3002", "conclusion": "\u878d\u5408\u5bf9\u8c61\u4e2d\u5fc3\u6d41\u7a0b\u6316\u6398\u4e0e\u591a\u7ea7\u6d41\u7a0b\u6316\u6398\u7684\u4f18\u52bf\u53ef\u50ac\u751f\u66f4\u5f3a\u5927\u7684\u5206\u6790\u80fd\u529b\uff0cIBM\u901a\u8fc7\u7ec4\u7ec7\u6316\u6398\u529f\u80fd\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u6574\u5408\u8def\u5f84\u7684\u6709\u6548\u6027\u4e0e\u521b\u65b0\u4ef7\u503c\u3002"}}
{"id": "2512.03839", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.03839", "abs": "https://arxiv.org/abs/2512.03839", "authors": ["Weilian Li", "Jun Zhu", "Saied Pirasteh", "Qing Zhu", "Yukun Guo", "Lan Luo", "Youness Dehbi"], "title": "A 3D virtual geographic environment for flood representation towards risk communication", "comment": null, "summary": "Risk communication seeks to develop a shared understanding of disaster among stakeholders, thereby amplifying public awareness and empowering them to respond more effectively to emergencies. However, existing studies have overemphasized specialized numerical modelling, making the professional output challenging to understand and use by non-research stakeholders. In this context, this article proposes a 3D virtual geographic environment for flood representation towards risk communication, which integrates flood modelling, parallel computation, and 3D representation in a pipeline. Finally, a section of the Rhine River in Bonn, Germany, is selected for experiment analysis. The experimental results show that the proposed approach is capable of flood modelling and 3D representation within a few hours, the parallel speedup ratio reached 6.45. The intuitive flood scene with 3D city models is beneficial for promoting flood risk communication and is particularly helpful for participants without direct experience of floods to understand its spatiotemporal process. It also can be embedded in the Geospatial Infrastructure Management Ecosystem (GeoIME) cloud application for intelligent flood systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6d2a\u6c34\u98ce\u9669\u6c9f\u901a\u7684\u4e09\u7ef4\u865a\u62df\u5730\u7406\u73af\u5883\uff0c\u96c6\u6210\u4e86\u6d2a\u6c34\u5efa\u6a21\u3001\u5e76\u884c\u8ba1\u7b97\u4e0e\u4e09\u7ef4\u53ef\u89c6\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u975e\u4e13\u4e1a\u5229\u76ca\u76f8\u5173\u8005\u5bf9\u6d2a\u6c34\u8fc7\u7a0b\u7684\u7406\u89e3\u3002", "motivation": "\u73b0\u6709\u98ce\u9669\u6c9f\u901a\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u4e13\u4e1a\u6570\u503c\u6a21\u578b\uff0c\u5bfc\u81f4\u975e\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u7406\u89e3\u548c\u4f7f\u7528\uff0c\u963b\u788d\u4e86\u6709\u6548\u7684\u516c\u4f17\u53c2\u4e0e\u548c\u5e94\u6025\u54cd\u5e94\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u96c6\u6210\u6d2a\u6c34\u5efa\u6a21\u3001\u5e76\u884c\u8ba1\u7b97\u548c\u4e09\u7ef4\u8868\u793a\u7684\u6d41\u6c34\u7ebf\u5f0f3D\u865a\u62df\u5730\u7406\u73af\u5883\uff0c\u5e76\u5728\u5fb7\u56fd\u6ce2\u6069\u83b1\u8335\u6cb3\u6bb5\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u5728\u6570\u5c0f\u65f6\u5185\u5b8c\u6210\u6d2a\u6c34\u5efa\u6a21\u4e0e\u4e09\u7ef4\u53ef\u89c6\u5316\uff0c\u5e76\u884c\u52a0\u901f\u6bd4\u8fbe6.45\uff1b\u76f4\u89c2\u7684\u4e09\u7ef4\u57ce\u5e02\u6d2a\u6c34\u573a\u666f\u6709\u52a9\u4e8e\u63d0\u5347\u516c\u4f17\u5c24\u5176\u662f\u65e0\u4eb2\u5386\u7ecf\u9a8c\u8005\u5bf9\u6d2a\u6c34\u65f6\u7a7a\u8fc7\u7a0b\u7684\u7406\u89e3\uff0c\u5e76\u53ef\u5d4c\u5165GeoIME\u4e91\u5e73\u53f0\u3002", "conclusion": "\u6240\u63d0\u51fa\u76843D\u865a\u62df\u5730\u7406\u73af\u5883\u6709\u6548\u4fc3\u8fdb\u4e86\u6d2a\u6c34\u98ce\u9669\u6c9f\u901a\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u4e0e\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u6d2a\u6c34\u7cfb\u7edf\u3002"}}
