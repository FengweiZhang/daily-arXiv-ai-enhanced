{"id": "2512.16238", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2512.16238", "abs": "https://arxiv.org/abs/2512.16238", "authors": ["Yifeng Cai", "Zhida An", "Yuhan Meng", "Houqian Liu", "Pengli Wang", "Yao Guo", "Ding Li"], "title": "Trustworthy and Controllable Professional Knowledge Utilization in Large Language Models with TEE-GPU Execution", "comment": null, "summary": "Future improvements in large language model (LLM) services increasingly hinge on access to high-value professional knowledge rather than more generic web data. However, the data providers of this knowledge face a skewed tradeoff between income and risk: they receive little share of downstream value yet retain copyright and privacy liability, making them reluctant to contribute their assets to LLM services. Existing techniques do not offer a trustworthy and controllable way to use professional knowledge, because they keep providers in the dark and combine knowledge parameters with the underlying LLM backbone.\n  In this paper, we present PKUS, the Professional Knowledge Utilization System, which treats professional knowledge as a first-class, separable artifact. PKUS keeps the backbone model on GPUs and encodes each provider's contribution as a compact adapter that executes only inside an attested Trusted Execution Environment (TEE). A hardware-rooted lifecycle protocol, adapter pruning, multi-provider aggregation, and split-execution scheduling together make this design practical at serving time. On SST-2, MNLI, and SQuAD with GPT-2 Large and Llama-3.2-1B, PKUS preserves model utility, matching the accuracy and F1 of full fine-tuning and plain LoRA, while achieving the lowest per-request latency with 8.1-11.9x speedup over CPU-only TEE inference and naive CPU-GPU co-execution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PKUS\u7cfb\u7edf\uff0c\u5c06\u4e13\u4e1a\u77e5\u8bc6\u4f5c\u4e3a\u72ec\u7acb\u3001\u53ef\u5206\u79bb\u7684\u7ec4\u4ef6\uff0c\u5728\u53ef\u4fe1\u6267\u884c\u73af\u5883\uff08TEE\uff09\u4e2d\u4ee5\u7d27\u51d1\u9002\u914d\u5668\u5f62\u5f0f\u8fd0\u884c\uff0c\u4ece\u800c\u5728\u4fdd\u969c\u6570\u636e\u63d0\u4f9b\u65b9\u6743\u76ca\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u4f9d\u8d56\u9ad8\u8d28\u91cf\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4f46\u77e5\u8bc6\u63d0\u4f9b\u65b9\u56e0\u6536\u76ca\u4f4e\u3001\u98ce\u9669\u9ad8\uff08\u5982\u7248\u6743\u548c\u9690\u79c1\u8d23\u4efb\uff09\u800c\u4e0d\u613f\u8d21\u732e\u6570\u636e\uff1b\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5728\u4fdd\u62a4\u63d0\u4f9b\u65b9\u6743\u76ca\u7684\u540c\u65f6\u5b9e\u73b0\u53ef\u63a7\u3001\u53ef\u4fe1\u7684\u77e5\u8bc6\u4f7f\u7528\u3002", "method": "\u63d0\u51faPKUS\u7cfb\u7edf\uff0c\u5c06\u4e13\u4e1a\u77e5\u200b\u200b\u8bc6\u7f16\u7801\u4e3a\u7d27\u51d1\u9002\u914d\u5668\uff0c\u5728\u786c\u4ef6\u7ea7\u53ef\u4fe1\u6267\u884c\u73af\u5883\uff08TEE\uff09\u4e2d\u6267\u884c\uff1b\u7ed3\u5408\u786c\u4ef6\u6839\u751f\u547d\u5468\u671f\u534f\u8bae\u3001\u9002\u914d\u5668\u526a\u679d\u3001\u591a\u63d0\u4f9b\u65b9\u805a\u5408\u4e0e\u5206\u62c6\u6267\u884c\u8c03\u5ea6\u7b49\u6280\u672f\u3002", "result": "\u5728SST-2\u3001MNLI\u548cSQuAD\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528GPT-2 Large\u548cLlama-3.2-1B\u6a21\u578b\uff0cPKUS\u5728\u4fdd\u6301\u4e0e\u5168\u91cf\u5fae\u8c03\u548c\u666e\u901aLoRA\u76f8\u5f53\u7684\u51c6\u786e\u7387\u548cF1\u5206\u6570\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u4f4e\u5355\u8bf7\u6c42\u5ef6\u8fdf\uff0c\u6bd4\u7eafCPU TEE\u63a8\u7406\u548c\u7b80\u5355CPU-GPU\u534f\u540c\u6267\u884c\u5feb8.1\u201311.9\u500d\u3002", "conclusion": "PKUS\u901a\u8fc7\u5c06\u4e13\u4e1a\u77e5\u8bc6\u6a21\u5757\u5316\u5e76\u5728TEE\u4e2d\u5b89\u5168\u6267\u884c\uff0c\u6709\u6548\u5e73\u8861\u4e86\u77e5\u8bc6\u63d0\u4f9b\u65b9\u7684\u6743\u76ca\u4fdd\u969c\u4e0e\u6a21\u578b\u670d\u52a1\u7684\u9ad8\u6548\u6027\uff0c\u4e3a\u4e13\u4e1a\u6570\u636e\u53c2\u4e0eLLM\u751f\u6001\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2512.15827", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.15827", "abs": "https://arxiv.org/abs/2512.15827", "authors": ["FNU Vikas", "Paul Gratz", "Daniel Jim\u00e9nez"], "title": "Workload Characterization for Branch Predictability", "comment": "This manuscript is an archival version of work conducted as part of the author's 2020 Master's at Texas A\\&M University under the supervision of Professors Paul Gratz and Daniel A.~Jim\u00e9nez. No part of this work was conducted at, funded by, or related to the author's current employer", "summary": "Conditional branch prediction predicts the likely direction of a conditional branch instruction to support ILP extraction. Branch prediction is a pattern recognition problem that learns mappings between a context to the branch outcome. An accurate predictor reduces the number of instructions executed on the wrong path resulting in an improvement of performance and energy consumption. In this paper, we present a workload characterization methodology for branch prediction. We propose two new workload-driven branch prediction accuracy identifiers -- branch working set size and branch predictability. These parameters are highly correlated with misprediction rates of modern branch prediction schemes (e.g. TAGE and perceptron). We define the branch working set of a trace as a group of most frequently occurring branch contexts, i.e. the 3-part tuple of branch address, and associated global and local history. We analyze the branch working set's size and predictability on a per-trace basis to study its relationship with a modern branch predictor's accuracy. We have characterized 2,451 workload traces into seven branch working set size and nine predictability categories after analyzing their branch behavior. We present further insights into the source of prediction accuracy and favored workload categories for modern branch predictors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5206\u652f\u9884\u6d4b\u7684\u5de5\u4f5c\u8d1f\u8f7d\u8868\u5f81\u65b9\u6cd5\uff0c\u5b9a\u4e49\u4e86\u4e24\u4e2a\u65b0\u6307\u6807\u2014\u2014\u5206\u652f\u5de5\u4f5c\u96c6\u5927\u5c0f\u548c\u5206\u652f\u53ef\u9884\u6d4b\u6027\uff0c\u7528\u4e8e\u5206\u6790\u73b0\u4ee3\u5206\u652f\u9884\u6d4b\u5668\uff08\u5982TAGE\u548c\u611f\u77e5\u673a\uff09\u7684\u51c6\u786e\u7387\u4e0e\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u5206\u652f\u9884\u6d4b\u662f\u63d0\u5347\u6307\u4ee4\u7ea7\u5e76\u884c\u6027\uff08ILP\uff09\u7684\u5173\u952e\u6280\u672f\uff0c\u5176\u51c6\u786e\u6027\u76f4\u63a5\u5f71\u54cd\u6027\u80fd\u4e0e\u80fd\u8017\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u672c\u8eab\u5982\u4f55\u5f71\u54cd\u73b0\u4ee3\u5206\u652f\u9884\u6d4b\u5668\u51c6\u786e\u6027\u7684\u7cfb\u7edf\u6027\u523b\u753b\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5206\u652f\u9884\u6d4b\u8868\u5f81\u65b9\u6cd5\uff0c\u5b9a\u4e49\u201c\u5206\u652f\u5de5\u4f5c\u96c6\u201d\u4e3a\u6700\u5e38\u51fa\u73b0\u7684\u5206\u652f\u4e0a\u4e0b\u6587\uff08\u5305\u542b\u5206\u652f\u5730\u5740\u3001\u5168\u5c40\u5386\u53f2\u548c\u5c40\u90e8\u5386\u53f2\u7684\u4e09\u5143\u7ec4\uff09\uff0c\u5e76\u636e\u6b64\u8ba1\u7b97\u6bcf\u4e2atrace\u7684\u5206\u652f\u5de5\u4f5c\u96c6\u5927\u5c0f\u548c\u53ef\u9884\u6d4b\u6027\u3002\u901a\u8fc7\u5bf92,451\u4e2a\u5de5\u4f5c\u8d1f\u8f7dtrace\u8fdb\u884c\u5206\u7c7b\uff087\u7c7b\u5de5\u4f5c\u96c6\u5927\u5c0f\u30019\u7c7b\u53ef\u9884\u6d4b\u6027\uff09\uff0c\u5206\u6790\u5176\u4e0e\u73b0\u4ee3\u5206\u652f\u9884\u6d4b\u5668\u8bef\u9884\u6d4b\u7387\u7684\u76f8\u5173\u6027\u3002", "result": "\u5206\u652f\u5de5\u4f5c\u96c6\u5927\u5c0f\u548c\u53ef\u9884\u6d4b\u6027\u4e0eTAGE\u548c\u611f\u77e5\u673a\u7b49\u73b0\u4ee3\u5206\u652f\u9884\u6d4b\u5668\u7684\u8bef\u9884\u6d4b\u7387\u9ad8\u5ea6\u76f8\u5173\uff1b\u7814\u7a76\u5c06\u5927\u91cftrace\u5f52\u7c7b\uff0c\u5e76\u63ed\u793a\u4e86\u54ea\u4e9b\u5de5\u4f5c\u8d1f\u8f7d\u7c7b\u522b\u66f4\u53d7\u73b0\u4ee3\u9884\u6d4b\u5668\u9752\u7750\uff0c\u4ee5\u53ca\u9884\u6d4b\u51c6\u786e\u6027\u7684\u6765\u6e90\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e24\u4e2a\u6307\u6807\u80fd\u6709\u6548\u523b\u753b\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u5206\u652f\u9884\u6d4b\u51c6\u786e\u7387\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u5206\u652f\u9884\u6d4b\u5668\u8bbe\u8ba1\u548c\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u7ef4\u5ea6\u548c\u6d1e\u5bdf\u3002"}}
{"id": "2512.16716", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.16716", "abs": "https://arxiv.org/abs/2512.16716", "authors": ["Sanjeeb Poudel", "Sanghyun Lee", "Lin Mu"], "title": "Pressure-robust enriched Galerkin finite element methods for coupled Navier-Stokes and heat equations", "comment": null, "summary": "We propose a pressure-robust enriched Galerkin (EG) finite element method for the incompressible Navier-Stokes and heat equations in the Boussinesq regime. For the Navier-Stokes equations, the EG formulation combines continuous Lagrange elements with a discontinuous enrichment vector per element in the velocity space and a piecewise constant pressure space, and it can be implemented efficiently within standard finite element frameworks. To enforce pressure robustness, we construct velocity reconstruction operators that map the discrete EG velocity field into exactly divergence-free, H(div)-conforming fields. In particular, we develop reconstructions based on Arbogast-Correa (AC) mixed finite element spaces on quadrilateral meshes and demonstrate that the resulting schemes remain stable and accurate even on highly distorted grids. The nonlinearity of the coupled Navier-Stokes-Boussinesq system is treated with several iterative strategies, including Picard iterations and Anderson-accelerated iterations; our numerical study shows that Anderson acceleration yields robust and efficient convergence for high Rayleigh number flows within the proposed framework. The performance of the method is assessed on a set of benchmark problems and application-driven test cases. These numerical experiments highlight the potential of pressure-robust EG methods as flexible and accurate tools for coupled flow and heat transport in complex geometries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u538b\u529b\u9c81\u68d2\u7684\u5bcc\u96c6Galerkin\uff08EG\uff09\u6709\u9650\u5143\u65b9\u6cd5\uff0c\u7528\u4e8e\u6c42\u89e3Boussinesq\u8fd1\u4f3c\u4e0b\u7684\u4e0d\u53ef\u538b\u7f29Navier-Stokes\u65b9\u7a0b\u4e0e\u70ed\u4f20\u5bfc\u65b9\u7a0b\u8026\u5408\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5728\u901f\u5ea6\u7a7a\u95f4\u4e2d\u7ed3\u5408\u8fde\u7eedLagrange\u5143\u4e0e\u5355\u5143\u5185\u4e0d\u8fde\u7eed\u5411\u91cf\u589e\u5f3a\u9879\uff0c\u91c7\u7528\u5206\u7247\u5e38\u6570\u538b\u529b\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u901f\u5ea6\u91cd\u6784\u7b97\u5b50\u5b9e\u73b0\u538b\u529b\u9c81\u68d2\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8Rayleigh\u6570\u548c\u9ad8\u5ea6\u626d\u66f2\u7f51\u683c\u4e0b\u4ecd\u4fdd\u6301\u7a33\u5b9a\u3001\u51c6\u786e\u4e14\u9ad8\u6548\u3002", "motivation": "\u4f20\u7edf\u6709\u9650\u5143\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u53ef\u538b\u7f29\u6d41\u52a8\u65f6\u53ef\u80fd\u56e0\u538b\u529b-\u901f\u5ea6\u8026\u5408\u4e0d\u826f\u800c\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u590d\u6742\u51e0\u4f55\u6216\u9ad8Rayleigh\u6570\u70ed\u5bf9\u6d41\u95ee\u9898\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u517c\u5177\u7075\u6d3b\u6027\u3001\u51c6\u786e\u6027\u4e0e\u538b\u529b\u9c81\u68d2\u6027\u7684\u6570\u503c\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u538b\u529b\u9c81\u68d2\u7684\u5bcc\u96c6Galerkin\uff08EG\uff09\u6709\u9650\u5143\u65b9\u6cd5\uff1a\u901f\u5ea6\u7a7a\u95f4\u7531\u8fde\u7eedLagrange\u57fa\u51fd\u6570\u4e0e\u6bcf\u4e2a\u5355\u5143\u5185\u7684\u4e0d\u8fde\u7eed\u5411\u91cf\u589e\u5f3a\u9879\u6784\u6210\uff0c\u538b\u529b\u91c7\u7528\u5206\u7247\u5e38\u6570\u7a7a\u95f4\uff1b\u5f15\u5165\u57fa\u4e8eArbogast-Correa\u6df7\u5408\u6709\u9650\u5143\u7684\u901f\u5ea6\u91cd\u6784\u7b97\u5b50\uff0c\u5c06\u79bb\u6563\u901f\u5ea6\u573a\u6620\u5c04\u4e3a\u65e0\u6563\u4e14H(div)-\u534f\u8c03\u7684\u573a\uff1b\u975e\u7ebf\u6027\u7cfb\u7edf\u91c7\u7528Picard\u8fed\u4ee3\u53caAnderson\u52a0\u901f\u7b56\u7565\u6c42\u89e3\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u9ad8\u5ea6\u626d\u66f2\u7684\u56db\u8fb9\u5f62\u7f51\u683c\u4e0a\u4ecd\u4fdd\u6301\u7a33\u5b9a\u6027\u548c\u7cbe\u5ea6\uff1bAnderson\u52a0\u901f\u663e\u8457\u63d0\u5347\u9ad8Rayleigh\u6570\u6d41\u52a8\u7684\u6536\u655b\u6548\u7387\uff1b\u5728\u591a\u4e2a\u57fa\u51c6\u95ee\u9898\u548c\u5e94\u7528\u9a71\u52a8\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u590d\u6742\u51e0\u4f55\u4e2d\u6a21\u62df\u8026\u5408\u6d41\u52a8\u4e0e\u4f20\u70ed\u95ee\u9898\u7684\u6f5c\u529b\u3002", "conclusion": "\u538b\u529b\u9c81\u68d2\u7684EG\u65b9\u6cd5\u662f\u4e00\u79cd\u7075\u6d3b\u3001\u51c6\u786e\u4e14\u9ad8\u6548\u7684\u6570\u503c\u5de5\u5177\uff0c\u9002\u7528\u4e8eBoussinesq\u8fd1\u4f3c\u4e0b\u590d\u6742\u51e0\u4f55\u4e2d\u7684\u8026\u5408\u6d41\u52a8\u4e0e\u70ed\u4f20\u8f93\u95ee\u9898\uff0c\u5c24\u5176\u5728\u9ad8Rayleigh\u6570\u548c\u975e\u7ed3\u6784\u5316\u7f51\u683c\u6761\u4ef6\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2512.15798", "categories": ["cs.DB", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15798", "abs": "https://arxiv.org/abs/2512.15798", "authors": ["Faisal Chowdhury", "Sola Shirai", "Sarthak Dash", "Nandana Mihindukulasooriya", "Horst Samulowitz"], "title": "DP-Bench: A Benchmark for Evaluating Data Product Creation Systems", "comment": null, "summary": "A data product is created with the intention of solving a specific problem, addressing a specific business usecase or meeting a particular need, going beyond just serving data as a raw asset. Data products enable end users to gain greater insights about their data. Since it was first introduced over a decade ago, there has been considerable work, especially in industry, to create data products manually or semi-automatically. However, there exists hardly any benchmark to evaluate automatic data product creation. In this work, we present a benchmark, first of its kind, for this task. We call it DP-Bench. We describe how this benchmark was created by taking advantage of existing work in ELT (Extract-Load-Transform) and Text-to-SQL benchmarks. We also propose a number of LLM based approaches that can be considered as baselines for generating data products automatically. We make the DP-Bench and supplementary materials available in https://huggingface.co/datasets/ibm-research/dp-bench .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u6570\u636e\u4ea7\u54c1\u751f\u6210\u7684\u57fa\u51c6DP-Bench\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u521b\u5efa\u6570\u636e\u4ea7\u54c1\u7684\u57fa\u51c6\uff0c\u5c3d\u7ba1\u5de5\u4e1a\u754c\u5df2\u6709\u5927\u91cf\u624b\u52a8\u6216\u534a\u81ea\u52a8\u6784\u5efa\u6570\u636e\u4ea7\u54c1\u7684\u5b9e\u8df5\u3002", "method": "\u5229\u7528\u73b0\u6709\u7684ELT\uff08Extract-Load-Transform\uff09\u548cText-to-SQL\u57fa\u51c6\u6784\u5efaDP-Bench\uff0c\u5e76\u63d0\u51fa\u591a\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u4f5c\u4e3a\u81ea\u52a8\u6570\u636e\u4ea7\u54c1\u751f\u6210\u7684\u57fa\u7ebf\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86DP-Bench\u57fa\u51c6\uff0c\u5e76\u5728Hugging Face\u4e0a\u516c\u5f00\u53d1\u5e03\u6570\u636e\u96c6\u53ca\u8865\u5145\u6750\u6599\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u3002", "conclusion": "DP-Bench\u586b\u8865\u4e86\u81ea\u52a8\u6570\u636e\u4ea7\u54c1\u751f\u6210\u8bc4\u4f30\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u8be5\u65b9\u5411\u7684\u7814\u7a76\u548c\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2512.16038", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.16038", "abs": "https://arxiv.org/abs/2512.16038", "authors": ["Eric Simon", "Renato B. Hoffmann", "Lucas Alf", "Dalvan Griebler"], "title": "LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines", "comment": null, "summary": "This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.", "AI": {"tldr": "LOG.io \u662f\u4e00\u79cd\u9762\u5411\u65e0\u670d\u52a1\u5668\u53ef\u6269\u5c55\u67b6\u6784\u7684\u5206\u5e03\u5f0f\u6570\u636e\u7ba1\u9053\u7cfb\u7edf\uff0c\u652f\u6301\u7cbe\u786e\u56de\u6eda\u6062\u590d\u548c\u7ec6\u7c92\u5ea6\u6570\u636e\u8840\u7f18\u8ffd\u8e2a\uff0c\u5728\u5b58\u5728\u6162\u8282\u70b9\u4e14\u4e8b\u4ef6\u541e\u5410\u9002\u4e2d\u65f6\uff0c\u5176\u6062\u590d\u6027\u80fd\u4f18\u4e8e ABS \u534f\u8bae\uff1b\u540c\u65f6\uff0c\u5176\u6570\u636e\u8840\u7f18\u6355\u83b7\u5f00\u9500\u4f4e\u4e8e 1.5%\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u6570\u636e\u6d41\u7cfb\u7edf\u5728\u5bb9\u9519\u548c\u6570\u636e\u8840\u7f18\u8ffd\u8e2a\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u5728\u65e0\u670d\u52a1\u5668\u3001\u9ad8\u5ea6\u5e76\u884c\u548c\u52a8\u6001\u6269\u5c55\u573a\u666f\u4e0b\u96be\u4ee5\u517c\u987e\u6b63\u786e\u6027\u3001\u6548\u7387\u4e0e\u901a\u7528\u6027\u3002\u56e0\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa LOG.io \u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "LOG.io \u91c7\u7528\u57fa\u4e8e\u65e5\u5fd7\u7684\u56de\u6eda\u6062\u590d\u534f\u8bae\uff0c\u652f\u6301\u901a\u7528\u7f16\u7a0b\u6a21\u578b\uff08\u5305\u62ec\u975e\u786e\u5b9a\u6027\u7b97\u5b50\u3001\u5916\u90e8\u7cfb\u7edf\u4ea4\u4e92\u548c\u4efb\u610f\u81ea\u5b9a\u4e49\u4ee3\u7801\uff09\uff0c\u5b9e\u73b0\u975e\u963b\u585e\u5f0f\u6545\u969c\u6062\u590d\uff0c\u5e76\u5141\u8bb8\u8fd0\u884c\u65f6\u52a8\u6001\u6269\u7f29\u5bb9\u3002\u540c\u65f6\uff0c\u5b83\u4ee5\u4e8b\u4ef6\u7ea7\u522b\u7c92\u5ea6\u6355\u83b7\u4efb\u610f\u4e24\u4e2a\u7b97\u5b50\u95f4\u7684\u6570\u636e\u8840\u7f18\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u5728\u5b58\u5728\u6162\u8282\u70b9\u4e14\u4e8b\u4ef6\u541e\u5410\u9002\u4e2d\uff08\u5982\u6bcf100ms\u4e00\u4e2a\u4e8b\u4ef6\uff09\u65f6\uff0cLOG.io \u5728\u6b63\u5e38\u5904\u7406\u9636\u6bb5\u4e0e ABS \u6027\u80fd\u76f8\u5f53\uff0c\u5728\u6062\u590d\u9636\u6bb5\u4f18\u4e8e ABS\uff1b\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b ABS \u66f4\u4f18\uff0c\u4f46\u6b64\u65f6 LOG.io \u53ef\u901a\u8fc7\u6570\u636e\u5e76\u884c\u663e\u8457\u964d\u4f4e\u5f00\u9500\uff0c\u800c ABS \u65e0\u6cd5\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u6b64\u5916\uff0cLOG.io \u7684\u6570\u636e\u8840\u7f18\u6355\u83b7\u5f00\u9500\u59cb\u7ec8\u4f4e\u4e8e 1.5%\u3002", "conclusion": "LOG.io \u5728\u4fdd\u8bc1\u901a\u7528\u6027\u548c\u7ec6\u7c92\u5ea6\u6570\u636e\u8840\u7f18\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5bb9\u9519\u673a\u5236\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b58\u5728\u6162\u8282\u70b9\u6216\u9700\u8981\u52a8\u6001\u6269\u5c55\u7684\u65e0\u670d\u52a1\u5668\u6570\u636e\u7ba1\u9053\u573a\u666f\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u548c\u90e8\u7f72\u6f5c\u529b\u3002"}}
{"id": "2512.15804", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.15804", "abs": "https://arxiv.org/abs/2512.15804", "authors": ["Balreet Grewal", "James Graham", "Jeff Muizelaar", "Jan Honza Odvarko", "Suhaib Mujahid", "Marco Castelluccio", "Cor-Paul Bezemer"], "title": "XBIDetective: Leveraging Vision Language Models for Identifying Cross-Browser Visual Inconsistencies", "comment": null, "summary": "Browser rendering bugs can be challenging to detect for browser developers, as they may be triggered by very specific conditions that are exhibited on only a very small subset of websites. Cross-browser inconsistencies (XBIs), variations in how a website is interpreted and displayed on different browsers, can be helpful guides to detect such rendering bugs. Although visual and Document Object Model (DOM)-based analysis techniques exist for detecting XBIs, they often struggle with dynamic and interactive elements. In this study, we discuss our industry experience with using vision language models (VLMs) to identify XBIs. We present the XBIDetective tool which automatically captures screenshots of a website in Mozilla Firefox and Google Chrome, and analyzes them with a VLM for XBIs. We evaluate XBIDetective's performance with an off-the-shelf and a fine-tuned VLM on 1,052 websites. We show that XBIDetective can identify cross-browser discrepancies with 79% accuracy and detect dynamic elements and advertisements with 84% and 85% accuracy, respectively, when using the fine-tuned VLM. We discuss important lessons learned, and we present several potential practical use cases for XBIDetective, including automated regression testing, large-scale monitoring of websites, and rapid triaging of XBI bug reports.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faXBIDetective\u5de5\u5177\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u81ea\u52a8\u68c0\u6d4b\u8de8\u6d4f\u89c8\u5668\u4e0d\u4e00\u81f4\u6027\uff08XBI\uff09\uff0c\u57281,052\u4e2a\u7f51\u7ad9\u4e0a\u8bc4\u4f30\u8868\u660e\uff0c\u5fae\u8c03\u540e\u7684VLM\u53ef\u5b9e\u73b079%\u7684XBI\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u6709\u6548\u8bc6\u522b\u52a8\u6001\u5143\u7d20\u548c\u5e7f\u544a\u3002", "motivation": "\u6d4f\u89c8\u5668\u6e32\u67d3\u9519\u8bef\u901a\u5e38\u7531\u7279\u5b9a\u6761\u4ef6\u89e6\u53d1\uff0c\u4ec5\u5728\u5c11\u6570\u7f51\u7ad9\u4e0a\u663e\u73b0\uff0c\u96be\u4ee5\u88ab\u5f00\u53d1\u8005\u53d1\u73b0\u3002\u8de8\u6d4f\u89c8\u5668\u4e0d\u4e00\u81f4\u6027\uff08XBI\uff09\u53ef\u4f5c\u4e3a\u68c0\u6d4b\u6b64\u7c7b\u9519\u8bef\u7684\u91cd\u8981\u7ebf\u7d22\uff0c\u4f46\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u6216DOM\u7684\u65b9\u6cd5\u5728\u5904\u7406\u52a8\u6001\u548c\u4ea4\u4e92\u5f0f\u5185\u5bb9\u65f6\u6548\u679c\u6709\u9650\u3002", "method": "\u5f00\u53d1\u4e86XBIDetective\u5de5\u5177\uff0c\u81ea\u52a8\u5728Firefox\u548cChrome\u4e2d\u622a\u53d6\u7f51\u9875\u622a\u56fe\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5206\u6790\u8fd9\u4e9b\u622a\u56fe\u4ee5\u8bc6\u522bXBI\uff1b\u8bc4\u4f30\u4e86\u73b0\u6210\u548c\u5fae\u8c03\u540e\u7684VLM\u57281,052\u4e2a\u7f51\u7ad9\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u4f7f\u7528\u5fae\u8c03VLM\u65f6\uff0cXBIDetective\u5728XBI\u8bc6\u522b\u3001\u52a8\u6001\u5143\u7d20\u68c0\u6d4b\u548c\u5e7f\u544a\u8bc6\u522b\u65b9\u9762\u5206\u522b\u8fbe\u523079%\u300184%\u548c85%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "XBIDetective\u5c55\u793a\u4e86VLM\u5728\u81ea\u52a8\u5316\u68c0\u6d4b\u8de8\u6d4f\u89c8\u5668\u6e32\u67d3\u5dee\u5f02\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u9002\u7528\u4e8e\u56de\u5f52\u6d4b\u8bd5\u3001\u5927\u89c4\u6a21\u7f51\u7ad9\u76d1\u63a7\u548cXBI\u7f3a\u9677\u62a5\u544a\u7684\u5feb\u901f\u5206\u7c7b\u7b49\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.16045", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.16045", "abs": "https://arxiv.org/abs/2512.16045", "authors": ["Vincent T. Lee", "Tanfer Alan", "Sung Kim", "Ecenur Ustun", "Amr Suleiman", "Ajit Krisshna", "Tim Balbekov", "Armin Alaghi", "Richard Newcombe"], "title": "Full System Architecture Modeling for Wearable Egocentric Contextual AI", "comment": "13 pages, 3 tables, 6 figures, technical report", "summary": "The next generation of human-oriented computing will require always-on, spatially-aware wearable devices to capture egocentric vision and functional primitives (e.g., Where am I? What am I looking at?, etc.). These devices will sense an egocentric view of the world around us to observe all human- relevant signals across space and time to construct and maintain a user's personal context. This personal context, combined with advanced generative AI, will unlock a powerful new generation of contextual AI personal assistants and applications. However, designing a wearable system to support contextual AI is a daunting task because of the system's complexity and stringent power constraints due to weight and battery restrictions. To understand how to guide design for such systems, this work provides the first complete system architecture view of one such wearable contextual AI system (Aria2), along with the lessons we have learned through the system modeling and design space exploration process. We show that an end-to-end full system model view of such systems is vitally important, as no single component or category overwhelmingly dominates system power. This means long-range design decisions and power optimizations need to be made in the full system context to avoid running into limits caused by other system bottlenecks (i.e., Amdahl's law as applied to power) or as bottlenecks change. Finally, we reflect on lessons and insights for the road ahead, which will be important toward eventually enabling all-day, wearable, contextual AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u60c5\u5883\u611f\u77e5AI\u7684\u53ef\u7a7f\u6234\u7cfb\u7edf\uff08Aria2\uff09\u7684\u5b8c\u6574\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u7cfb\u7edf\u5efa\u6a21\u4e0e\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u63ed\u793a\u4e86\u5728\u529f\u8017\u53d7\u9650\u6761\u4ef6\u4e0b\u9700\u4ee5\u5168\u7cfb\u7edf\u89c6\u89d2\u8fdb\u884c\u534f\u540c\u4f18\u5316\u7684\u5173\u952e\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u4e0b\u4e00\u4ee3\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8ba1\u7b97\u9700\u8981\u59cb\u7ec8\u5728\u7ebf\u3001\u5177\u5907\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7684\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u4ee5\u6355\u6349\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u548c\u529f\u80fd\u539f\u8bed\uff0c\u6784\u5efa\u7528\u6237\u4e2a\u4eba\u60c5\u5883\uff0c\u5e76\u7ed3\u5408\u751f\u6210\u5f0fAI\u5b9e\u73b0\u5f3a\u5927\u7684\u60c5\u5883AI\u52a9\u624b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7cfb\u7edf\u590d\u6742\u6027\u548c\u4e25\u683c\u7684\u529f\u8017\u9650\u5236\uff0c\u8bbe\u8ba1\u6b64\u7c7b\u53ef\u7a7f\u6234\u7cfb\u7edf\u6781\u5177\u6311\u6218\u3002", "method": "\u4f5c\u8005\u5bf9\u4e00\u4e2a\u5b8c\u6574\u7684\u53ef\u7a7f\u6234\u60c5\u5883AI\u7cfb\u7edf\uff08Aria2\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u67b6\u6784\u5efa\u6a21\u548c\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\uff0c\u5206\u6790\u5404\u7ec4\u4ef6\u5bf9\u6574\u4f53\u529f\u8017\u7684\u5f71\u54cd\uff0c\u5e76\u4ece\u5168\u7cfb\u7edf\u89c6\u89d2\u8bc4\u4f30\u957f\u671f\u8bbe\u8ba1\u51b3\u7b56\u4e0e\u529f\u8017\u4f18\u5316\u7b56\u7565\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6ca1\u6709\u4efb\u4f55\u5355\u4e00\u7ec4\u4ef6\u6216\u7c7b\u522b\u5728\u7cfb\u7edf\u529f\u8017\u4e2d\u5360\u636e\u7edd\u5bf9\u4e3b\u5bfc\u5730\u4f4d\uff0c\u56e0\u6b64\u5fc5\u987b\u5728\u5168\u7cfb\u7edf\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u4ee5\u907f\u514d\u56e0\u5176\u4ed6\u74f6\u9888\uff08\u5373\u201c\u529f\u8017\u7248Amdahl\u5b9a\u5f8b\u201d\uff09\u800c\u53d7\u9650\u3002", "conclusion": "\u5b9e\u73b0\u5168\u5929\u5019\u53ef\u7a7f\u6234\u60c5\u5883AI\u7cfb\u7edf\u9700\u8981\u8de8\u7ec4\u4ef6\u534f\u540c\u4f18\u5316\uff0c\u672a\u6765\u7684\u8bbe\u8ba1\u5e94\u57fa\u4e8e\u5b8c\u6574\u7684\u7cfb\u7edf\u6a21\u578b\uff0c\u7efc\u5408\u8003\u8651\u4e0d\u65ad\u53d8\u5316\u7684\u74f6\u9888\u56e0\u7d20\uff0c\u4ee5\u6709\u6548\u5e94\u5bf9\u529f\u8017\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u6311\u6218\u3002"}}
{"id": "2512.15815", "categories": ["cs.DB", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.15815", "abs": "https://arxiv.org/abs/2512.15815", "authors": ["Valeria Granata", "Francois Liot", "Xing Wang", "Steen Lysgaard", "Ivano E. Castelli", "Tejs Vegge", "Nicola Marzari", "Giovanni Pizzi"], "title": "Implementing a Scalable, Redeployable and Multitiered Repository for FAIR and Secure Scientific Data Sharing: The BIG-MAP Archive", "comment": null, "summary": "Data sharing in large consortia, such as research collaborations or industry partnerships, requires addressing both organizational and technical challenges. A common platform is essential to promote collaboration, facilitate exchange of findings, and ensure secure access to sensitive data. Key technical challenges include creating a scalable architecture, a user-friendly interface, and robust security and access control. The BIG-MAP Archive is a cloud-based, disciplinary, private repository designed to address these challenges. Built on InvenioRDM, it leverages platform functionalities to meet consortium-specific needs, providing a tailored solution compared to general repositories. Access can be restricted to members of specific communities or open to the entire consortium, such as the BATTERY 2030+, a consortium accelerating advanced battery technologies. Uploaded data and metadata are controlled via fine grained permissions, allowing access to individual project members or the full initiative. The formalized upload process ensures data are formatted and ready for publication in open repositories when needed. This paper reviews the repository's key features, showing how the BIG-MAP Archive enables secure, controlled data sharing within large consortia. It ensures data confidentiality while supporting flexible, permissions-based access and can be easily redeployed for other consortia, including MaterialsCommons4.eu and RAISE (Resource for AI Science in Europe).", "AI": {"tldr": "BIG-MAP Archive is a cloud-based, private repository built on InvenioRDM to enable secure, scalable, and permission-controlled data sharing within large research consortia like BATTERY 2030+.", "motivation": "Large research consortia face organizational and technical challenges in data sharing, including the need for secure access, scalability, user-friendly interfaces, and fine-grained access control\u2014motivating the development of a tailored repository solution.", "method": "The authors designed and implemented the BIG-MAP Archive, a cloud-based disciplinary repository based on InvenioRDM, featuring customizable access controls, community-based permissions, and standardized data upload workflows.", "result": "The BIG-MAP Archive successfully supports secure and flexible data sharing within consortia such as BATTERY 2030+, with fine-grained permissions, structured metadata, and readiness for eventual open publication; it is also redeployable for other initiatives like MaterialsCommons4.eu and RAISE.", "conclusion": "The BIG-MAP Archive provides a robust, adaptable platform that addresses key data-sharing challenges in large consortia by combining security, usability, and scalability, while ensuring compliance with future open-data requirements."}}
{"id": "2512.16056", "categories": ["cs.DC", "cs.NI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.16056", "abs": "https://arxiv.org/abs/2512.16056", "authors": ["Lingfeng Tang", "Daoping Zhang", "Junjie Chen", "Peihao Huang", "Feng Jin", "Chengguang Xu", "Yuxin Chen", "Feiqiang Sun", "Guo Chen"], "title": "MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services", "comment": null, "summary": "The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMultipath Memory Access\uff08MMA\uff09\uff0c\u9996\u6b21\u5b9e\u73b0GPU\u4e0e\u4e3b\u673a\u5185\u5b58\u4e4b\u95f4\u7684\u9ad8\u6548\u591a\u8def\u5f84\u6570\u636e\u4f20\u8f93\uff0c\u663e\u8457\u63d0\u5347\u5e26\u5bbd\u5e76\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u4e2d\u7684\u9996token\u5ef6\u8fdf\u548c\u6a21\u578b\u5207\u6362\u5ef6\u8fdf\u3002", "motivation": "PCIe\u5e26\u5bbd\u9650\u5236\u5df2\u6210\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u74f6\u9888\uff0c\u5c24\u5176\u662f\u5728\u524d\u7f00\u7f13\u5b58\u83b7\u53d6\u548c\u6a21\u578b\u5207\u6362\u7b49\u573a\u666f\u4e2d\u3002\u5c3d\u7ba1\u670d\u52a1\u5668\u5185\u90e8GPU\u4e0e\u4e3b\u673a\u5185\u5b58\u4e4b\u95f4\u7406\u8bba\u4e0a\u53ef\u652f\u6301\u591a\u8def\u5f84\u4f20\u8f93\uff0c\u4f46\u73b0\u6709\u5f02\u6784\u534f\u8bae\uff08\u5982PCIe\u548cNVLink\uff09\u5c06\u5e26\u5bbd\u9650\u5236\u4e3a\u5355\u6761PCIe\u94fe\u8def\uff0c\u5bfc\u81f4\u670d\u52a1\u5668\u5185\u90e8\u5e26\u5bbd\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "\u63d0\u51faMultipath Memory Access\uff08MMA\uff09\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001\u5e93\u6ce8\u5165\u5b9e\u73b0\u65e0\u7f1d\u90e8\u7f72\uff0c\u65e0\u9700\u4fee\u6539\u5e94\u7528\u4ee3\u7801\u5373\u53ef\u542f\u7528\u591a\u8def\u5f84\u6570\u636e\u4f20\u8f93\uff0c\u4ece\u800c\u7a81\u7834\u5355PCIe\u94fe\u8def\u7684\u5e26\u5bbd\u9650\u5236\u3002", "result": "\u5728\u6d4b\u8bd5\u5e73\u53f0\u4e0a\uff0cMMA\u5c06GPU\u4e0e\u4e3b\u673a\u5185\u5b58\u95f4\u7684\u6570\u636e\u4f20\u8f93\u5cf0\u503c\u5e26\u5bbd\u63d0\u5347\u81f3245 GB/s\uff0c\u76f8\u8f83\u539f\u751f\u5355\u8def\u5f84\u63d0\u53474.62\u500d\uff1b\u7aef\u5230\u7aef\u8bc4\u4f30\u663e\u793a\uff0cLLM\u670d\u52a1\u7684\u9996token\u65f6\u95f4\uff08TTFT\uff09\u964d\u4f4e1.14x\u20132.38x\uff0cvLLM\u7761\u7720\u6a21\u5f0f\u4e0b\u7684\u6a21\u578b\u5207\u6362\u5ef6\u8fdf\u51cf\u5c111.12x\u20132.48x\u3002", "conclusion": "MMA\u6709\u6548\u89e3\u51b3\u4e86PCIe\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u5177\u5907\u826f\u597d\u7684\u517c\u5bb9\u6027\u548c\u6613\u7528\u6027\u3002"}}
{"id": "2512.15813", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15813", "abs": "https://arxiv.org/abs/2512.15813", "authors": ["Nishant Gaurav", "Adit Akarsh", "Tejas Ravishankar", "Manoj Bajaj"], "title": "CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory", "comment": "11 pages, 2 figures", "summary": "Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.", "AI": {"tldr": "CodeMem introduces a procedural memory architecture using code to enable deterministic, reusable agentic workflows, addressing the probabilistic instability of current tool-using AI agents.", "motivation": "Current AI agents struggle with repetitive tasks due to limited action space, context inefficiency, and probabilistic instability; existing solutions improve action space and context but fail to ensure consistent behavior across runs.", "method": "The paper proposes CodeMem, an architecture that implements procedural memory through executable code, allowing agents to store and reuse deterministic workflows for reliable task execution.", "result": "CodeMem enables the creation and execution of reusable, deterministic agentic workflows, overcoming the inconsistency caused by the probabilistic nature of LLMs.", "conclusion": "By integrating procedural memory via code, CodeMem provides a path toward reliable and efficient AI agents capable of handling repetitive tasks with deterministic consistency."}}
{"id": "2512.16083", "categories": ["cs.DB", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16083", "abs": "https://arxiv.org/abs/2512.16083", "authors": ["Thanh Dat Hoang", "Thanh Tam Nguyen", "Thanh Trung Huynh", "Hongzhi Yin", "Quoc Viet Hung Nguyen"], "title": "Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers", "comment": null, "summary": "Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \\toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \\toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\toolname\u7684\u9ad8\u6548\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5927\u89c4\u6a21\u6570\u636e\u5e93\u4e2d\u5bf9Text2SQL\u4efb\u52a1\u8fdb\u884c\u6a21\u5f0f\u8fc7\u6ee4\uff0c\u901a\u8fc7\u7ed3\u5408\u67e5\u8be2\u611f\u77e5\u7684\u5217\u6392\u5e8f\u3001\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u5217\u91cd\u6392\u5e8f\u4ee5\u53ca\u4fdd\u7559\u8fde\u901a\u6027\u7684\u5b50\u6a21\u5f0f\u9009\u62e9\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u3002", "motivation": "\u73b0\u6709Text2SQL\u65b9\u6cd5\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u7684\u5927\u89c4\u6a21\u6570\u636e\u5e93\uff08\u5982Spider 2.0\uff09\u65f6\uff0c\u56e0\u8d85\u51fa\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u9650\u5236\u800c\u5931\u6548\uff1b\u5f53\u524d\u7f13\u89e3\u7b56\u7565\u8981\u4e48\u6210\u672c\u9ad8\uff0c\u8981\u4e48\u5ffd\u7565\u5217\u95f4\u7ed3\u6784\u5173\u7cfb\u3002", "method": "\\toolname\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a(i) \u4f7f\u7528\u878d\u5408\u503c\u4e0e\u5143\u6570\u636e\u7684\u67e5\u8be2\u611f\u77e5LLM\u7f16\u7801\u5668\u5bf9\u5217\u8fdb\u884c\u6392\u5e8f\uff1b(ii) \u5229\u7528\u8f7b\u91cf\u56fe\u53d8\u6362\u5668\u57fa\u4e8e\u51fd\u6570\u4f9d\u8d56\u5bf9\u4e92\u8fde\u5217\u8fdb\u884c\u91cd\u6392\u5e8f\uff1b(iii) \u91c7\u7528\u65af\u5766\u7eb3\u6811\u542f\u53d1\u5f0f\u7b97\u6cd5\u9009\u62e9\u4fdd\u6301\u8fde\u901a\u6027\u7684\u5b50\u6a21\u5f0f\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\\toolname\u76f8\u6bd4CodeS\u3001SchemaExP\u3001Qwen\u91cd\u6392\u5668\u548c\u5d4c\u5165\u68c0\u7d22\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u53ec\u56de\u7387\u548c\u66f4\u9ad8\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u4e2d\u4f4d\u5ef6\u8fdf\u4f4e\u4e8e1\u79d2\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u5305\u542b23,000+\u5217\u7684\u6a21\u5f0f\u3002", "conclusion": "\\toolname\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u5f00\u6e90\u7684Text2SQL\u6a21\u5f0f\u8fc7\u6ee4\u6846\u67b6\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u6570\u636e\u5e93\u4e2d\u7684\u4e0a\u4e0b\u6587\u9650\u5236\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.15979", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15979", "abs": "https://arxiv.org/abs/2512.15979", "authors": ["Mia Mohammad Imran", "Tarannum Shaila Zaman"], "title": "OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \\textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \\textit{reliability, calibration, drift, consensus, aggregation}, and \\textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7528\u4e8e\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u6807\u6ce8\u4efb\u52a1\u5e94\u89c6\u4e3a\u4e00\u79cd\u6d4b\u91cf\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aOLAF\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u6b64\u7c7b\u6807\u6ce8\u7684\u53ef\u9760\u6027\u3001\u53ef\u590d\u73b0\u6027\u4e0e\u900f\u660e\u5ea6\u3002", "motivation": "\u5f53\u524d\u5728\u5b9e\u8bc1\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u4f7f\u7528LLM\u8fdb\u884c\u6807\u6ce8\u7684\u7814\u7a76\u7f3a\u4e4f\u5bf9\u53ef\u9760\u6027\u3001\u6821\u51c6\u548c\u6f02\u79fb\u7b49\u5173\u952e\u6307\u6807\u7684\u6807\u51c6\u5316\u8861\u91cf\uff0c\u4e14\u5e38\u5ffd\u7565\u5fc5\u8981\u7684\u914d\u7f6e\u7ec6\u8282\uff0c\u5bfc\u81f4\u7ed3\u679c\u96be\u4ee5\u590d\u73b0\u548c\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3a\u201c\u57fa\u4e8eLLM\u7684\u6807\u6ce8\u64cd\u4f5c\u5316\u6846\u67b6\uff08OLAF\uff09\u201d\u7684\u6982\u5ff5\u6027\u6846\u67b6\uff0c\u6574\u5408\u53ef\u9760\u6027\u3001\u6821\u51c6\u3001\u6f02\u79fb\u3001\u5171\u8bc6\u3001\u805a\u5408\u4e0e\u900f\u660e\u5ea6\u7b49\u6838\u5fc3\u8981\u7d20\u3002", "result": "\u8be5\u6846\u67b6\u5c1a\u672a\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\uff0c\u4f46\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u65b9\u6cd5\u8bba\u6307\u5bfc\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u900f\u660e\u548c\u53ef\u590d\u73b0\u7684LLM\u6807\u6ce8\u5b9e\u8df5\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u6807\u6ce8\u5e94\u88ab\u89c6\u4f5c\u4e00\u79cd\u6d4b\u91cf\u8fc7\u7a0b\uff0c\u9700\u91c7\u7528\u4e25\u8c28\u7684\u65b9\u6cd5\u8bba\uff1bOLAF\u6846\u67b6\u6709\u671b\u63a8\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u9886\u57df\u5728\u6b64\u65b9\u5411\u4e0a\u7684\u65b9\u6cd5\u8bba\u8ba8\u8bba\u4e0e\u5b9e\u8bc1\u7814\u7a76\u3002"}}
{"id": "2512.16470", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.16470", "abs": "https://arxiv.org/abs/2512.16470", "authors": ["Longfei Zhao", "Jingbo Tan", "Jintao Wang", "Ian F Akyildiz", "Zhi Sun"], "title": "Acoustic RIS for Massive Spatial Multiplexing: Unleashing Degrees of Freedom and Capacity in Underwater Communications", "comment": "This work has been accepted for publication at IEEE INFOCOM 2026", "summary": "Underwater acoustic (UWA) communications are essential for high-speed marine data transmission but remain severely constrained by limited bandwidth, significant propagation loss, and sparse multipath structures. Conventional underwater acoustic multiple-input multiple-output (MIMO) systems primarily utilize spatial diversity but suffer from limited array resolution, causing angular ambiguity and insufficient spatial degrees of freedom (DoFs). This paper addresses these limitations through acoustic Reconfigurable Intelligent Surfaces (aRIS) to actively generate orthogonally distinguishable virtual paths, significantly enhancing spatial DoFs and channel capacity. An ocean-specific DoF-channel coupling model is established, explicitly deriving conditions for spatial rank enhancement. Subsequently, the optimal geometric locus, termed the Light-Point, is analytically identified, where deploying a single aRIS maximizes DoFs by introducing two and three additional resolvable paths in deep-sea and shallow-sea environments, respectively. Furthermore, an active simultaneous transmitting and reflecting (ASTAR) aRIS architecture with independent beam control and adaptive beam-tracking mechanism integrating unmanned underwater vehicles (UUVs) and acoustic intensity gradient sensing is proposed. Extensive simulations validate the proposed joint aRIS deployment and beamforming framework, demonstrating substantial UWA channel capacity improvements-up to 265% and 170% in shallow-sea and deep-sea scenarios, respectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u58f0\u5b66\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08aRIS\uff09\u589e\u5f3a\u6c34\u4e0b\u58f0\u5b66MIMO\u7cfb\u7edf\u7684\u7a7a\u95f4\u81ea\u7531\u5ea6\u548c\u4fe1\u9053\u5bb9\u91cf\uff0c\u901a\u8fc7\u5efa\u7acb\u6d77\u6d0b\u73af\u5883\u4e0b\u7684\u81ea\u7531\u5ea6-\u4fe1\u9053\u8026\u5408\u6a21\u578b\uff0c\u786e\u5b9a\u6700\u4f18\u90e8\u7f72\u4f4d\u7f6e\uff08Light-Point\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e00\u79cd\u5177\u5907\u4e3b\u52a8\u540c\u65f6\u900f\u5c04\u4e0e\u53cd\u5c04\uff08ASTAR\uff09\u80fd\u529b\u7684aRIS\u67b6\u6784\uff0c\u7ed3\u5408\u65e0\u4eba\u6c34\u4e0b\u822a\u884c\u5668\uff08UUV\uff09\u4e0e\u58f0\u5f3a\u68af\u5ea6\u611f\u77e5\u5b9e\u73b0\u81ea\u9002\u5e94\u6ce2\u675f\u8ddf\u8e2a\uff0c\u5728\u6d45\u6d77\u548c\u6df1\u6d77\u573a\u666f\u4e2d\u5206\u522b\u63d0\u5347\u4fe1\u9053\u5bb9\u91cf\u8fbe265%\u548c170%\u3002", "motivation": "\u4f20\u7edf\u6c34\u4e0b\u58f0\u5b66MIMO\u7cfb\u7edf\u53d7\u9650\u4e8e\u9635\u5217\u5206\u8fa8\u7387\u4e0d\u8db3\uff0c\u5bfc\u81f4\u89d2\u5ea6\u6a21\u7cca\u548c\u7a7a\u95f4\u81ea\u7531\u5ea6\u6709\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u9ad8\u901f\u6d77\u6d0b\u6570\u636e\u4f20\u8f93\u9700\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u73af\u5883\u7279\u6027\u6765\u589e\u5f3a\u4fe1\u9053\u5bb9\u91cf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u58f0\u5b66\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08aRIS\uff09\u7684\u65b0\u67b6\u6784\uff1a\u9996\u5148\u5efa\u7acb\u9002\u7528\u4e8e\u6d77\u6d0b\u73af\u5883\u7684\u81ea\u7531\u5ea6-\u4fe1\u9053\u8026\u5408\u6a21\u578b\uff0c\u63a8\u5bfc\u7a7a\u95f4\u79e9\u589e\u5f3a\u6761\u4ef6\uff1b\u5176\u6b21\u89e3\u6790\u786e\u5b9a\u6700\u4f18\u90e8\u7f72\u4f4d\u7f6e\u201cLight-Point\u201d\uff1b\u6700\u540e\u8bbe\u8ba1\u5177\u5907\u72ec\u7acb\u6ce2\u675f\u63a7\u5236\u80fd\u529b\u7684ASTAR aRIS\uff0c\u5e76\u96c6\u6210UUV\u4e0e\u58f0\u5f3a\u68af\u5ea6\u4f20\u611f\u5b9e\u73b0\u81ea\u9002\u5e94\u6ce2\u675f\u8ddf\u8e2a\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u8054\u5408aRIS\u90e8\u7f72\u4e0e\u6ce2\u675f\u6210\u5f62\u6846\u67b6\u5728\u6d45\u6d77\u548c\u6df1\u6d77\u73af\u5883\u4e2d\u5206\u522b\u5b9e\u73b0\u9ad8\u8fbe265%\u548c170%\u7684\u6c34\u4e0b\u58f0\u5b66\u4fe1\u9053\u5bb9\u91cf\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165aRIS\u5e76\u4f18\u5316\u5176\u90e8\u7f72\u4e0e\u63a7\u5236\u7b56\u7565\uff0c\u53ef\u663e\u8457\u63d0\u5347\u6c34\u4e0b\u58f0\u5b66\u901a\u4fe1\u7cfb\u7edf\u7684\u7a7a\u95f4\u81ea\u7531\u5ea6\u4e0e\u4fe1\u9053\u5bb9\u91cf\uff0c\u4e3a\u9ad8\u541e\u5410\u91cf\u6d77\u6d0b\u901a\u4fe1\u63d0\u4f9b\u65b0\u8def\u5f84\u3002"}}
{"id": "2512.16106", "categories": ["cs.DB", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16106", "abs": "https://arxiv.org/abs/2512.16106", "authors": ["Zhengyuan Dong", "Victor Zhong", "Ren\u00e9e J. Miller"], "title": "ModelTables: A Corpus of Tables about Models", "comment": "14 pages, 8 figures and 8 tables", "summary": "We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.", "AI": {"tldr": "ModelTables \u662f\u4e00\u4e2a\u5305\u542b90K\u5f20\u8868\u683c\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6e90\u81eaHugging Face\u6a21\u578b\u5361\u3001GitHub README\u548c\u76f8\u5173\u8bba\u6587\uff0c\u65e8\u5728\u652f\u6301\u5bf9AI\u6a21\u578b\u6027\u80fd\u4e0e\u914d\u7f6e\u8868\u683c\u7684\u8bed\u4e49\u68c0\u7d22\u7814\u7a76\u3002\u8be5\u57fa\u51c6\u901a\u8fc7\u4e09\u79cd\u4e92\u8865\u4fe1\u53f7\u6784\u5efa\u591a\u6e90\u771f\u503c\uff0c\u5e76\u5728\u8868\u683c\u641c\u7d22\u4efb\u52a1\u4e2d\u8bc4\u4f30\u591a\u79cd\u68c0\u7d22\u65b9\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u4ecd\u6709\u8f83\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u68c0\u7d22\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6a21\u578b\u6587\u6863\u4e2d\u7ed3\u6784\u5316\u8868\u683c\u6240\u8574\u542b\u7684\u8bed\u4e49\u4fe1\u606f\uff1b\u540c\u65f6\u7f3a\u4e4f\u9488\u5bf9AI\u6a21\u578b\u76f8\u5173\u8868\u683c\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u7528\u4e8e\u5f00\u53d1\u548c\u8bc4\u4f30\u8bed\u4e49\u68c0\u7d22\u3001\u7ed3\u6784\u5316\u6bd4\u8f83\u7b49\u6280\u672f\u3002", "method": "\u4eceHugging Face\u6a21\u578b\u5361\u3001GitHub README\u548c\u5f15\u7528\u8bba\u6587\u4e2d\u63d0\u53d6\u8868\u683c\u5e76\u5173\u8054\u5176\u4e0a\u4e0b\u6587\uff0c\u6784\u5efa\u5305\u542b60K\u6a21\u578b\u548c90K\u8868\u683c\u7684\u6570\u636e\u96c6\uff1b\u5229\u7528\u8bba\u6587\u5f15\u7528\u3001\u6a21\u578b\u5361\u94fe\u63a5/\u7ee7\u627f\u5173\u7cfb\u3001\u5171\u4eab\u8bad\u7ec3\u6570\u636e\u96c6\u4e09\u79cd\u4fe1\u53f7\u6784\u5efa\u591a\u6e90\u771f\u503c\uff1b\u5728\u8868\u683c\u641c\u7d22\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4f20\u7edf\u6570\u636e\u6e56\u64cd\u4f5c\u7b26\uff08\u53ef\u8054\u5408\u3001\u53ef\u8fde\u63a5\u3001\u5173\u952e\u8bcd\uff09\u548c\u4fe1\u606f\u68c0\u7d22\u57fa\u7ebf\uff08\u7a20\u5bc6\u3001\u7a00\u758f\u3001\u6df7\u5408\u68c0\u7d22\uff09\u3002", "result": "\u57fa\u4e8e\u8054\u5408\u7684\u8bed\u4e49\u8868\u683c\u68c0\u7d22\u6574\u4f53P@1\u4e3a54.8%\uff1b\u57fa\u4e8e\u8868\u683c\u7684\u7a20\u5bc6\u68c0\u7d22\u8fbe\u523066.5% P@1\uff1b\u5143\u6570\u636e\u6df7\u5408\u68c0\u7d22\u4e3a54.1%\u3002\u7ed3\u679c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u5728\u6355\u6349\u8868\u683c\u95f4\u8bed\u4e49\u5173\u8054\u65b9\u9762\u4ecd\u6709\u660e\u663e\u4e0d\u8db3\u3002", "conclusion": "ModelTables \u662f\u9996\u4e2a\u9762\u5411AI\u6a21\u578b\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u5927\u89c4\u6a21\u8868\u683c\u57fa\u51c6\uff0c\u4e3a\u8bed\u4e49\u68c0\u7d22\u3001\u7ed3\u6784\u5316\u6bd4\u8f83\u548c\u77e5\u8bc6\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u4e0e\u5b9e\u8bc1\u4f9d\u636e\uff0c\u63a8\u52a8\u66f4\u7cbe\u51c6\u7684\u6a21\u578b\u8868\u683c\u53d1\u73b0\u4e0e\u5229\u7528\u3002"}}
{"id": "2512.16066", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.16066", "abs": "https://arxiv.org/abs/2512.16066", "authors": ["Syed Salauddin Mohammad Tariq", "Foyzul Hassan", "Amiangshu Bosu", "Probir Roy"], "title": "Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study", "comment": null, "summary": "Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u679081\u4e2a\u5f00\u6e90\u65e0\u670d\u52a1\u5668\u7cfb\u7edf\u7684\u95ee\u9898\u62a5\u544a\uff0c\u63d0\u51fa\u4e86\u51b7\u542f\u52a8\u95ee\u9898\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86SCABENCH\u57fa\u51c6\u548cINITSCOPE\u5206\u6790\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b7\u542f\u52a8\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u4e2d\u7684\u51b7\u542f\u52a8\u5ef6\u8fdf\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5c06\u5176\u89c6\u4e3a\u9ed1\u76d2\u4f18\u5316\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u5f00\u53d1\u8005\u53ef\u89c1\u7684\u8bbe\u8ba1\u5c42\u9762\u7684\u7406\u89e3\u4e0e\u652f\u6301\u3002", "method": "\u4f5c\u8005\u4ece81\u4e2a\u7ecf\u8fc7\u88c1\u5b9a\u7684\u5f00\u6e90\u65e0\u670d\u52a1\u5668\u7cfb\u7edf\u95ee\u9898\u62a5\u544a\u4e2d\u5f52\u7eb3\u51fa\u521d\u59cb\u5316\u53cd\u6a21\u5f0f\u3001\u4fee\u590d\u7b56\u7565\u548c\u8bca\u65ad\u6311\u6218\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u51c6SCABENCH\u548c\u8f7b\u91cf\u7ea7\u5206\u6790\u6846\u67b6INITSCOPE\uff0c\u7528\u4e8e\u5173\u8054\u52a0\u8f7d\u4ee3\u7801\u4e0e\u6267\u884c\u4ee3\u7801\u3002", "result": "\u5728SCABENCH\u4e0a\uff0cINITSCOPE\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u5c06\u5b9a\u4f4d\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534740%\uff0c\u8bca\u65ad\u5de5\u4f5c\u91cf\u51cf\u5c1164%\uff1b\u5f00\u53d1\u8005\u7814\u7a76\u8868\u660e\u5176\u80fd\u63d0\u9ad8\u4efb\u52a1\u51c6\u786e\u7387\u5e76\u52a0\u5feb\u8bca\u65ad\u901f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u57fa\u4e8e\u5b9e\u8bc1\u3001\u5173\u6ce8\u6027\u80fd\u7684\u65e0\u670d\u52a1\u5668\u51b7\u542f\u52a8\u7f13\u89e3\u8bbe\u8ba1\u5b9e\u8df5\uff0c\u5e76\u516c\u5f00\u4e86\u7814\u7a76\u5de5\u5177\u4ee5\u4fc3\u8fdb\u540e\u7eed\u5de5\u4f5c\u3002"}}
{"id": "2512.15980", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15980", "abs": "https://arxiv.org/abs/2512.15980", "authors": ["Yirui He", "Yuqi Huai", "Xingyu Chen", "Joshua Garcia"], "title": "Embedding Software Intent: Lightweight Java Module Recovery", "comment": null, "summary": "As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faClassLAR\uff0c\u4e00\u79cd\u57fa\u4e8e\u7c7b\u540d\u548c\u8bed\u8a00\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784\u6062\u590d\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u5355\u4f53Java\u9879\u76ee\u9ad8\u6548\u3001\u51c6\u786e\u5730\u91cd\u6784\u4e3aJPMS\u6a21\u5757\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u7cfb\u7edf\u89c4\u6a21\u6269\u5927\uff0c\u4ec5\u4f9d\u8d56\u4ee3\u7801\u7ea7\u62bd\u8c61\u5df2\u4e0d\u73b0\u5b9e\uff1b\u800c\u73b0\u6709\u67b6\u6784\u6062\u590d\u6280\u672f\u5728\u5c06\u5355\u4f53Java\u9879\u76ee\u8fc1\u79fb\u5230Java\u5e73\u53f0\u6a21\u5757\u7cfb\u7edf\uff08JPMS\uff09\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u96be\u4ee5\u6709\u6548\u6062\u590d\u6a21\u5757\u7ed3\u6784\u3002", "method": "ClassLAR\u5229\u7528\u5168\u9650\u5b9a\u7c7b\u540d\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u4ece\u5305\u540d\u548c\u7c7b\u540d\u4e2d\u63d0\u53d6\u8bed\u4e49\u4fe1\u606f\uff0c\u540c\u65f6\u6355\u6349\u7ed3\u6784\u4e0e\u529f\u80fd\u610f\u56fe\uff0c\u4ece\u800c\u5b9e\u73b0\u8f7b\u91cf\u9ad8\u6548\u7684Java\u6a21\u5757\u6062\u590d\u3002", "result": "\u572820\u4e2a\u4e3b\u6d41Java\u9879\u76ee\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cClassLAR\u5728\u67b6\u6784\u76f8\u4f3c\u6027\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u6267\u884c\u901f\u5ea6\u63d0\u53473.99\u81f310.50\u500d\u3002", "conclusion": "ClassLAR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u67b6\u6784\u6062\u590d\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u652f\u6301\u5c06\u5355\u4f53Java\u7cfb\u7edf\u91cd\u6784\u4e3a\u7b26\u5408JPMS\u89c4\u8303\u7684\u6a21\u5757\u5316\u67b6\u6784\u3002"}}
{"id": "2512.16255", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.16255", "abs": "https://arxiv.org/abs/2512.16255", "authors": ["Chrysanthi Kosyfaki", "Nikos Mamoulis", "Reynold Cheng", "Ben Kai"], "title": "Multi-granularity Spatiotemporal Flow Patterns", "comment": null, "summary": "Analyzing flow of objects or data at different granularities of space and time can unveil interesting insights or trends. For example, transportation companies, by aggregating passenger travel data (e.g., counting passengers traveling from one region to another), can analyze movement behavior. In this paper, we study the problem of finding important trends in passenger movements between regions at different granularities. We define Origin (O), Destination (D), and Time (T ) patterns (ODT patterns) and propose a bottom-up algorithm that enumerates them. We suggest and employ optimizations that greatly reduce the search space and the computational cost of pattern enumeration. We also propose pattern variants (constrained patterns and top-k patterns) that could be useful to differ- ent applications scenarios. Finally, we propose an approximate solution that fast identifies ODT patterns of specific sizes, following a generate-and-test approach. We evaluate the efficiency and effectiveness of our methods on three real datasets and showcase interesting ODT flow patterns in them.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u5728\u4e0d\u540c\u65f6\u7a7a\u7c92\u5ea6\u4e0b\u53d1\u73b0\u4e58\u5ba2\u79fb\u52a8\u4e2d\u7684\u91cd\u8981\u8d8b\u52bf\uff0c\u63d0\u51faODT\uff08Origin-Destination-Time\uff09\u6a21\u5f0f\u53ca\u5176\u9ad8\u6548\u6316\u6398\u7b97\u6cd5\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u3001\u53d8\u4f53\u6a21\u5f0f\u548c\u8fd1\u4f3c\u65b9\u6cd5\u63d0\u5347\u6548\u7387\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u4ea4\u901a\u7b49\u9886\u57df\uff0c\u901a\u8fc7\u805a\u5408\u4e58\u5ba2\u51fa\u884c\u6570\u636e\uff08\u5982\u533a\u57df\u95f4\u6d41\u52a8\u4eba\u6570\uff09\u53ef\u63ed\u793a\u79fb\u52a8\u884c\u4e3a\u8d8b\u52bf\uff1b\u7136\u800c\uff0c\u5982\u4f55\u5728\u591a\u7c92\u5ea6\u65f6\u7a7a\u4e0b\u9ad8\u6548\u8bc6\u522b\u6709\u610f\u4e49\u7684\u6d41\u52a8\u6a21\u5f0f\u4ecd\u5177\u6311\u6218\u3002", "method": "\u63d0\u51faODT\u6a21\u5f0f\u5b9a\u4e49\uff0c\u8bbe\u8ba1\u81ea\u5e95\u5411\u4e0a\u7684\u679a\u4e3e\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u591a\u79cd\u4f18\u5316\u7b56\u7565\u4ee5\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\u548c\u8ba1\u7b97\u5f00\u9500\uff1b\u540c\u65f6\u63d0\u51fa\u7ea6\u675f\u6a21\u5f0f\u3001Top-k\u6a21\u5f0f\u7b49\u53d8\u4f53\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u751f\u6210-\u6d4b\u8bd5\u7684\u8fd1\u4f3c\u65b9\u6cd5\u4ee5\u5feb\u901f\u8bc6\u522b\u7279\u5b9a\u89c4\u6a21\u7684ODT\u6a21\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6709\u6548\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u5e76\u6210\u529f\u6316\u6398\u51fa\u5177\u6709\u5b9e\u9645\u610f\u4e49\u7684ODT\u6d41\u52a8\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u591a\u7c92\u5ea6\u65f6\u7a7a\u6d41\u52a8\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u80fd\u6709\u6548\u652f\u6301\u4ea4\u901a\u7b49\u9886\u57df\u4e2d\u7684\u8d8b\u52bf\u53d1\u73b0\u4e0e\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2512.16099", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.16099", "abs": "https://arxiv.org/abs/2512.16099", "authors": ["Hsu-Tzu Ting", "Jerry Chou", "Ming-Hung Chen", "I-Hsin Chung"], "title": "An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs", "comment": "10 pages, 10 figures", "summary": "Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411NVIDIA MIG\uff08\u591a\u5b9e\u4f8bGPU\uff09\u7684\u5728\u7ebf\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u8d1f\u8f7d\u5747\u8861\u3001\u52a8\u6001\u5206\u533a\u548c\u4f5c\u4e1a\u8fc1\u79fb\uff0c\u6709\u6548\u7f13\u89e3\u4e86MIG\u73af\u5883\u4e0b\u7684\u8d44\u6e90\u4e89\u7528\u4e0e\u788e\u7247\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6548\u7387\uff0c\u6700\u591a\u53ef\u5c06\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff08makespan\uff09\u7f29\u77ed35%\u3002", "motivation": "MIG\u867d\u63d0\u4f9b\u786c\u4ef6\u7ea7\u5f3a\u9694\u79bb\u7684GPU\u5171\u4eab\u80fd\u529b\uff0c\u4f46\u4ecd\u9762\u4e34PCIe\u5e26\u5bbd\u7b49\u5171\u4eab\u8d44\u6e90\u5f15\u8d77\u7684\u4e89\u7528\u95ee\u9898\uff0c\u4ee5\u53ca\u7531MIG\u914d\u7f6e\u9650\u5236\u548c\u521a\u6027\u653e\u7f6e\u7b56\u7565\u5bfc\u81f4\u7684\u72ec\u7279\u788e\u7247\u5316\u95ee\u9898\uff0c\u5c24\u5176\u5728\u4f5c\u4e1a\u52a8\u6001\u5230\u8fbe\u4e0e\u9000\u51fa\u65f6\u66f4\u4e3a\u4e25\u91cd\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5728\u7ebf\u8c03\u5ea6\u6846\u67b6\uff0c\u6574\u5408\u6761\u4ef6\u8d1f\u8f7d\u5747\u8861\u3001\u52a8\u6001\u5206\u533a\u8c03\u6574\u548c\u4f5c\u4e1a\u8fc1\u79fb\u673a\u5236\uff0c\u4ee5\u52a8\u6001\u4f18\u5316\u4f5c\u4e1a\u653e\u7f6e\u3001\u51cf\u5c11\u8d44\u6e90\u4e89\u7528\uff0c\u5e76\u91cd\u7ec4GPU\u5206\u914d\u4ee5\u7f13\u89e3\u5185\u90e8\u4e0e\u5916\u90e8\u788e\u7247\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6548\u7387\uff0c\u5728\u7efc\u5408\u5e94\u7528\u6240\u6709\u6280\u672f\u65f6\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u6700\u591a\u53ef\u51cf\u5c1135%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6709\u6548\u89e3\u51b3\u4e86MIG\u73af\u5883\u4e0b\u8d44\u6e90\u4e89\u7528\u4e0e\u788e\u7247\u5316\u96be\u9898\uff0c\u4e3a\u9ad8\u6548\u5229\u7528MIG\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8c03\u5ea6\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2512.16070", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.16070", "abs": "https://arxiv.org/abs/2512.16070", "authors": ["Xin Wang", "Zhenhao Li", "Zishuo Ding"], "title": "LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)", "comment": null, "summary": "The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86LLM4Perf\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4f5c\u4e3a\u9ad8\u6548\u7684\u591a\u76ee\u6807\u6027\u80fd\u5efa\u6a21\u91c7\u6837\u5668\uff0c\u5728\u591a\u6570\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u5176\u6210\u529f\u6e90\u4e8e\u914d\u7f6e\u7a7a\u95f4\u526a\u679d\u4e0e\u53cd\u9988\u9a71\u52a8\u7b56\u7565\u4f18\u5316\u7684\u53cc\u91cd\u80fd\u529b\u3002", "motivation": "\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u914d\u7f6e\u9009\u9879\uff0c\u800c\u73b0\u6709\u91c7\u6837\u65b9\u6cd5\u5728\u591a\u76ee\u6807\u4f18\u5316\u548c\u5229\u7528\u6587\u6863\u8bed\u4e49\u4fe1\u606f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u53d7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fd1\u671f\u6210\u529f\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u63a2\u7d22LLMs\u662f\u5426\u80fd\u6709\u6548\u7528\u4e8e\u591a\u76ee\u6807\u6027\u80fd\u5efa\u6a21\u4e2d\u7684\u91c7\u6837\u4efb\u52a1\u3002", "method": "\u4f5c\u8005\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u540d\u4e3aLLM4Perf\u7684\u57fa\u4e8e\u53cd\u9988\u7684\u6846\u67b6\uff0c\u5bf9LLM\u5f15\u5bfc\u7684\u91c7\u6837\u8fc7\u7a0b\u5728\u56db\u4e2a\u771f\u5b9e\u9ad8\u53ef\u914d\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u5206\u6790\u5176\u5728\u4e0d\u540c\u7ec4\u4ef6\u9009\u62e9\u548c\u8d85\u53c2\u6570\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u5bf9\u6bd4\u4f20\u7edf\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM4Perf\u5728112\u4e2a\u8bc4\u4f30\u573a\u666f\u4e2d\u7ea668.8%\uff0877\u4e2a\uff09\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff1b\u5176\u914d\u7f6e\u7a7a\u95f4\u526a\u679d\u80fd\u529b\u8fd8\u80fd\u63d0\u5347\u57fa\u7ebf\u65b9\u6cd5\u5728448\u4e2a\u6848\u4f8b\u4e2d\u7ea691.5%\uff08410\u4e2a\uff09\u7684\u8868\u73b0\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aLLMs\u5728\u6027\u80fd\u5de5\u7a0b\u4e2d\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u6709\u529b\u8bc1\u636e\uff0c\u5e76\u6df1\u5165\u63ed\u793a\u4e86\u5176\u6210\u529f\u673a\u5236\uff0c\u5305\u62ec\u914d\u7f6e\u7a7a\u95f4\u526a\u679d\u4e0e\u53cd\u9988\u9a71\u52a8\u7b56\u7565\u4f18\u5316\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\u3002"}}
{"id": "2512.16321", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.16321", "abs": "https://arxiv.org/abs/2512.16321", "authors": ["Aryan Esmailpour", "Xiao Hu", "Jinchao Huang", "Stavros Sintos"], "title": "Subset Sampling over Joins", "comment": null, "summary": "Subset sampling (also known as Poisson sampling), where the decision to include any specific element in the sample is made independently of all others, is a fundamental primitive in data analytics, enabling efficient approximation by processing representative subsets rather than massive datasets. While sampling from explicit lists is well-understood, modern applications -- such as machine learning over relational data -- often require sampling from a set defined implicitly by a relational join. In this paper, we study the problem of \\emph{subset sampling over joins}: drawing a random subset from the join results, where each join result is included independently with some probability. We address the general setting where the probability is derived from input tuple weights via decomposable functions (e.g., product, sum, min, max). Since the join size can be exponentially larger than the input, the naive approach of materializing all join results to perform subset sampling is computationally infeasible. We propose the first efficient algorithms for subset sampling over acyclic joins: (1) a \\emph{static index} for generating multiple (independent) subset samples over joins; (2) a \\emph{one-shot} algorithm for generating a single subset sample over joins; (3) a \\emph{dynamic index} that can support tuple insertions, while maintaining a one-shot sample or generating multiple (independent) samples. Our techniques achieve near-optimal time and space complexity with respect to the input size and the expected sample size.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u5173\u7cfb\u8fde\u63a5\u7ed3\u679c\u4e0a\u8fdb\u884c\u5b50\u96c6\u91c7\u6837\uff08\u5373\u6bcf\u4e2a\u8fde\u63a5\u7ed3\u679c\u4ee5\u4e00\u5b9a\u6982\u7387\u72ec\u7acb\u88ab\u9009\u4e2d\uff09\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9488\u5bf9\u65e0\u73af\u8fde\u63a5\u7684\u9ad8\u6548\u9759\u6001\u7d22\u5f15\u3001\u4e00\u6b21\u6027\u91c7\u6837\u7b97\u6cd5\u548c\u52a8\u6001\u7d22\u5f15\u65b9\u6cd5\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u751f\u6210\u6240\u6709\u8fde\u63a5\u7ed3\u679c\uff0c\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e0a\u63a5\u8fd1\u6700\u4f18\u3002", "motivation": "\u73b0\u4ee3\u5e94\u7528\uff08\u5982\u5173\u7cfb\u6570\u636e\u4e0a\u7684\u673a\u5668\u5b66\u4e60\uff09\u5e38\u9700\u5bf9\u7531\u5173\u7cfb\u8fde\u63a5\u9690\u5f0f\u5b9a\u4e49\u7684\u96c6\u5408\u8fdb\u884c\u5b50\u96c6\u91c7\u6837\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u56e0\u8fde\u63a5\u7ed3\u679c\u53ef\u80fd\u6307\u6570\u7ea7\u5927\u4e8e\u8f93\u5165\u6570\u636e\u800c\u4e0d\u9002\u7528\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u91c7\u6837\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u65b0\u7b97\u6cd5\uff1a(1) \u7528\u4e8e\u591a\u6b21\u72ec\u7acb\u91c7\u6837\u7684\u9759\u6001\u7d22\u5f15\uff1b(2) \u7528\u4e8e\u5355\u6b21\u91c7\u6837\u7684\u4e00\u6b21\u6027\u7b97\u6cd5\uff1b(3) \u652f\u6301\u5143\u7ec4\u63d2\u5165\u5e76\u7ef4\u6301\u91c7\u6837\u80fd\u529b\u7684\u52a8\u6001\u7d22\u5f15\u3002\u8fd9\u4e9b\u65b9\u6cd5\u9002\u7528\u4e8e\u7531\u53ef\u5206\u89e3\u51fd\u6570\uff08\u5982\u4e58\u79ef\u3001\u6c42\u548c\u7b49\uff09\u4ece\u8f93\u5165\u5143\u7ec4\u6743\u91cd\u5bfc\u51fa\u91c7\u6837\u6982\u7387\u7684\u901a\u7528\u573a\u666f\u3002", "result": "\u6240\u63d0\u7b97\u6cd5\u5728\u65e0\u73af\u8fde\u63a5\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u65f6\u95f4\u4e0e\u7a7a\u95f4\u590d\u6742\u5ea6\uff0c\u663e\u8457\u4f18\u4e8e\u9700\u7269\u5316\u5168\u90e8\u8fde\u63a5\u7ed3\u679c\u7684\u6734\u7d20\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9996\u6b21\u4e3a\u8fde\u63a5\u4e0a\u7684\u5b50\u96c6\u91c7\u6837\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5173\u7cfb\u6570\u636e\u5206\u6790\u4e2d\u7684\u8fd1\u4f3c\u8ba1\u7b97\u9700\u6c42\u3002"}}
{"id": "2512.16134", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16134", "abs": "https://arxiv.org/abs/2512.16134", "authors": ["Jian Tian", "Shuailong Li", "Yang Cao", "Wenbo Cui", "Minghan Zhu", "Wenkang Wu", "Jianming Zhang", "Yanpeng Wang", "Zhiwen Xiao", "Zhenyu Hou", "Dou Shen"], "title": "Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference", "comment": null, "summary": "The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.", "AI": {"tldr": "\u9488\u5bf9\u5927\u89c4\u6a21DP+EP\u67b6\u6784\u4e0bLLM\u670d\u52a1\u4e2d\u56e0\u5373\u65f6\u8c03\u5ea6\u5bfc\u81f4\u7684\u5185\u90e8\u6392\u961f\u548c\u5e76\u884c\u6c14\u6ce1\u95ee\u9898\uff0c\u63d0\u51fa\u4ea4\u9519\u6279\u5904\u7406\u8c03\u5ea6\uff08SBS\uff09\u4e0e\u8d1f\u8f7d\u611f\u77e5\u5168\u5c40\u5206\u914d\u7b56\u7565\uff0c\u5728H800\u96c6\u7fa4\u4e0a\u90e8\u7f72Deepseek-V3\u65f6\u663e\u8457\u964d\u4f4eTTFT 30%-40%\u5e76\u63d0\u5347\u541e\u5410\u91cf15%-20%\u3002", "motivation": "\u5728P/D\u5206\u79bb\u3001\u5927\u89c4\u6a21DP+EP\u67b6\u6784\u7684LLM\u670d\u52a1\u4e2d\uff0c\u4f20\u7edf\u5373\u65f6\u8bf7\u6c42\u5206\u53d1\u4f1a\u5f15\u53d1\u4e25\u91cd\u7684\u5f15\u64ce\u5185\u6392\u961f\u548c\u5e76\u884c\u5316\u6c14\u6ce1\uff0c\u4ece\u800c\u663e\u8457\u6076\u5316\u9996Token\u751f\u6210\u65f6\u95f4\uff08TTFT\uff09\u3002", "method": "\u63d0\u51fa\u4ea4\u9519\u6279\u5904\u7406\u8c03\u5ea6\uff08SBS\uff09\uff0c\u901a\u8fc7\u6709\u610f\u7f13\u51b2\u8bf7\u6c42\u4ee5\u5f62\u6210\u6700\u4f18\u6267\u884c\u6279\u6b21\uff0c\u6d88\u9664\u5185\u90e8\u6392\u961f\u6c14\u6ce1\uff1b\u540c\u65f6\u5229\u7528\u7f13\u51b2\u5e26\u6765\u7684\u8c03\u5ea6\u7a97\u53e3\uff0c\u8bbe\u8ba1\u8d1f\u8f7d\u611f\u77e5\u7684\u5168\u5c40\u5206\u914d\u7b56\u7565\uff0c\u5728Prefill\u548cDecode\u9636\u6bb5\u5747\u8861DP\u5355\u5143\u95f4\u7684\u8ba1\u7b97\u8d1f\u8f7d\u3002", "result": "\u5728\u751f\u4ea7\u7ea7H800\u96c6\u7fa4\u4e0a\u90e8\u7f72Deepseek-V3\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5373\u65f6\u8c03\u5ea6\u65b9\u6cd5\uff0cTTFT\u964d\u4f4e30%-40%\uff0c\u541e\u5410\u91cf\u63d0\u534715%-20%\u3002", "conclusion": "\u4ea4\u9519\u6279\u5904\u7406\u8c03\u5ea6\u4e0e\u8d1f\u8f7d\u611f\u77e5\u5206\u914d\u80fd\u6709\u6548\u89e3\u51b3DP+EP\u67b6\u6784\u4e0b\u7684\u8c03\u5ea6\u74f6\u9888\uff0c\u5728\u4e0d\u727a\u7272\u541e\u5410\u7684\u524d\u63d0\u4e0b\u663e\u8457\u4f18\u5316\u5ef6\u8fdf\u6307\u6807\u3002"}}
{"id": "2512.16146", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.16146", "abs": "https://arxiv.org/abs/2512.16146", "authors": ["Muzeeb Mohammad"], "title": "Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems", "comment": "Accepted for publication. Camera-ready version presented at an international IEEE conference. Final version to appear in 2026", "summary": "Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e862015\u81f32025\u5e74\u95f442\u7bc7\u5173\u4e8eApache Kafka\u7684\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\uff0c\u63d0\u70bc\u51fa\u4e5d\u79cd\u5e38\u89c1\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5e76\u5206\u6790\u5176\u4f7f\u7528\u8d8b\u52bf\u3001\u9886\u57df\u90e8\u7f72\u53ca\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u5728\u914d\u7f6e\u62ab\u9732\u4e0e\u53ef\u590d\u73b0\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u7edf\u4e00\u5206\u7c7b\u6cd5\u548c\u51b3\u7b56\u542f\u53d1\u5f0f\u4ee5\u6307\u5bfc\u5b9e\u8df5\u3002", "motivation": "\u5c3d\u7ba1Apache Kafka\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5173\u4e8e\u5176\u53ef\u590d\u7528\u67b6\u6784\u8bbe\u8ba1\u6a21\u5f0f\u548c\u53ef\u590d\u73b0\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u7684\u7814\u7a76\u5206\u6563\u4e8e\u5b66\u672f\u4e0e\u5de5\u4e1a\u6587\u732e\u4e2d\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6574\u5408\uff0c\u9650\u5236\u4e86\u8de8\u7814\u7a76\u6bd4\u8f83\u4e0e\u5b9e\u9645\u590d\u73b0\u3002", "method": "\u5bf92015\u81f32025\u5e74\u95f4\u53d1\u8868\u768442\u7bc7\u540c\u884c\u8bc4\u5ba1\u6587\u732e\u8fdb\u884c\u7ed3\u6784\u5316\u7efc\u8ff0\uff0c\u8bc6\u522b\u5e76\u5206\u7c7bKafka\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u5206\u6790\u5176\u5171\u73b0\u8d8b\u52bf\u3001\u9886\u57df\u5e94\u7528\u53ca\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u8df5\uff08\u5305\u62ecTPCx-Kafka\u3001Yahoo Streaming Benchmark\u7b49\uff09\uff0c\u5e76\u8bc4\u4f30\u5176\u914d\u7f6e\u900f\u660e\u5ea6\u4e0e\u5b9e\u9a8c\u4e25\u8c28\u6027\u3002", "result": "\u8bc6\u522b\u51fa\u4e5d\u79cd\u9ad8\u9891Kafka\u8bbe\u8ba1\u6a21\u5f0f\uff08\u5982\u65e5\u5fd7\u538b\u7f29\u3001CQRS\u603b\u7ebf\u3001\u7cbe\u786e\u4e00\u6b21\u7ba1\u9053\u7b49\uff09\uff0c\u53d1\u73b0\u73b0\u6709\u7814\u7a76\u5728\u914d\u7f6e\u62ab\u9732\u3001\u8bc4\u4f30\u4e25\u8c28\u6027\u548c\u53ef\u590d\u73b0\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u4e00\u81f4\uff0c\u963b\u788d\u4e86\u6210\u679c\u7684\u6a2a\u5411\u6bd4\u8f83\u4e0e\u5de5\u7a0b\u843d\u5730\u3002", "conclusion": "\u901a\u8fc7\u6784\u5efa\u7edf\u4e00\u7684\u6a21\u5f0f\u5206\u7c7b\u4f53\u7cfb\u3001\u57fa\u51c6\u77e9\u9635\u548c\u51b3\u7b56\u542f\u53d1\u89c4\u5219\uff0c\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u9ad8\u6027\u80fd\u3001\u5bb9\u9519\u4e14\u53ef\u590d\u73b0\u7684Kafka\u4e8b\u4ef6\u6d41\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u547c\u5401\u793e\u533a\u52a0\u5f3a\u5b9e\u9a8c\u900f\u660e\u5ea6\u4e0e\u6807\u51c6\u5316\u3002"}}
{"id": "2512.16136", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.16136", "abs": "https://arxiv.org/abs/2512.16136", "authors": ["Zhisheng Hu", "Pengfei Zuo", "Junliang Hu", "Yizou Chen", "Yingjia Wang", "Ming-Chang Yang"], "title": "Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks", "comment": null, "summary": "Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.\n  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.", "AI": {"tldr": "Lotus \u662f\u4e00\u79cd\u9762\u5411\u89e3\u8026\u5185\u5b58\uff08DM\uff09\u67b6\u6784\u7684\u53ef\u6269\u5c55\u5206\u5e03\u5f0f\u4e8b\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u9501\u4e0e\u6570\u636e\u5206\u79bb\u5e76\u5c06\u9501\u64cd\u4f5c\u5378\u8f7d\u5230\u8ba1\u7b97\u8282\u70b9\uff08CNs\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u4e2d\u5185\u5b58\u8282\u70b9 RDMA \u7f51\u5361\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u7684\u95ee\u9898\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e DM \u7684\u5206\u5e03\u5f0f\u4e8b\u52a1\u7cfb\u7edf\u5728\u5185\u5b58\u8282\u70b9\uff08MNs\uff09\u4e0a\u4f7f\u7528\u5927\u91cf\u5355\u8fb9 RDMA \u539f\u5b50\u64cd\u4f5c\u5b9e\u73b0\u9501\u673a\u5236\uff0c\u5bfc\u81f4 MN \u7684 RDMA \u7f51\u5361\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u3002", "method": "Lotus \u5c06\u9501\u4e0e\u6570\u636e\u89e3\u8026\uff0c\u6240\u6709\u9501\u64cd\u4f5c\u5728\u8ba1\u7b97\u8282\u70b9\uff08CNs\uff09\u4e0a\u6267\u884c\uff1b\u91c7\u7528\u5e94\u7528\u611f\u77e5\u7684\u9501\u7ba1\u7406\u673a\u5236\uff0c\u5229\u7528 OLTP \u5de5\u4f5c\u8d1f\u8f7d\u7684\u5c40\u90e8\u6027\u5bf9\u9501\u8fdb\u884c\u5206\u7247\u5e76\u4fdd\u6301\u8d1f\u8f7d\u5747\u8861\uff1b\u5f15\u5165\u201c\u5148\u9501\u540e\u4e8b\u52a1\u201d\u534f\u8bae\uff0c\u5728\u4e8b\u52a1\u6267\u884c\u521d\u671f\u5b8c\u6210\u52a0\u9501\u5e76\u63d0\u524d\u4e2d\u6b62\u51b2\u7a81\u4e8b\u52a1\uff1b\u5e76\u901a\u8fc7\u65e0\u9501\u91cd\u5efa\u7684\u6062\u590d\u673a\u5236\u5904\u7406 CN \u6545\u969c\uff0c\u5c06\u9501\u89c6\u4e3a\u4e34\u65f6\u72b6\u6001\u4ee5\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u6062\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLotus \u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684 DM \u4e8b\u52a1\u7cfb\u7edf\uff0c\u4e8b\u52a1\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u5347 2.1 \u500d\uff0c\u5ef6\u8fdf\u6700\u591a\u964d\u4f4e 49.4%\u3002", "conclusion": "\u901a\u8fc7\u9501\u89e3\u8026\u3001\u5e94\u7528\u611f\u77e5\u9501\u7ba1\u7406\u548c\u8f7b\u91cf\u7ea7\u6545\u969c\u6062\u590d\u673a\u5236\uff0cLotus \u6709\u6548\u6d88\u9664\u4e86 DM \u67b6\u6784\u4e0b\u5185\u5b58\u8282\u70b9\u7684 RDMA \u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5f0f\u4e8b\u52a1\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2512.16272", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16272", "abs": "https://arxiv.org/abs/2512.16272", "authors": ["Ora Nova Fandina", "Eitan Farchi", "Shmulik Froimovich", "Raviv Gal", "Wesam Ibraheem", "Rami Katan", "Alice Podolsky"], "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls", "comment": null, "summary": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.\n  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.\n  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7801\u751f\u6210\u8bc4\u5224\u8005\uff08LaaJ\uff09\u5728COBOL\u73b0\u4ee3\u5316\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5176\u5e38\u5ffd\u7565\u9886\u57df\u5173\u952e\u9519\u8bef\uff1b\u4e3a\u6b64\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5206\u6790\u68c0\u67e5\u5668\uff0c\u901a\u8fc7\u5411LaaJ\u63d0\u793a\u4e2d\u52a8\u6001\u6ce8\u5165\u5206\u6790\u63d0\u793a\uff08hints\uff09\uff0c\u663e\u8457\u63d0\u5347\u9519\u8bef\u68c0\u6d4b\u7387\uff08\u4ece45%\u63d0\u5347\u81f394%\uff09\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4ee3\u7801\u8bc4\u5224\u8005\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982COBOL\u4ee3\u7801\u73b0\u4ee3\u5316\uff09\u4e2d\u5e38\u5ffd\u7565\u5173\u952e\u9519\u8bef\uff0c\u5f71\u54cd\u5176\u5728\u5173\u952e\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7406\u89e3\u5e76\u5f25\u8865\u5176\u8bc4\u4f30\u76f2\u70b9\u3002", "method": "\u4f5c\u8005\u57fa\u4e8e\u4e13\u5bb6\u77e5\u8bc6\u6784\u5efa\u4e86\u4e00\u4e2aCOBOL\u9886\u57df\u9519\u8bef\u7684\u521d\u6b65\u5206\u7c7b\u6cd5\uff0c\u5e76\u636e\u6b64\u5f00\u53d1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5206\u6790\u68c0\u67e5\u5668\uff0c\u7528\u4e8e\u68c0\u6d4b30\u591a\u79cd\u9886\u57df\u7279\u5b9a\u95ee\u9898\uff1b\u968f\u540e\u5c06\u68c0\u67e5\u5668\u7684\u8f93\u51fa\u4f5c\u4e3a\u201c\u5206\u6790\u63d0\u793a\u201d\u52a8\u6001\u6ce8\u5165\u5230LaaJ\u7684\u63d0\u793a\u4e2d\uff0c\u5f62\u6210LaaJ+Hints\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u5728\u5305\u542b100\u4e2a\u7a0b\u5e8f\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5355\u72ec\u4f7f\u7528LaaJ\u4ec5\u80fd\u68c0\u6d4b\u7ea645%\u7684\u9519\u8bef\uff0c\u800c\u7ed3\u5408\u5206\u6790\u63d0\u793a\u540e\uff0c\u6700\u4f73\u914d\u7f6e\u4e0b\u9519\u8bef\u68c0\u6d4b\u8986\u76d6\u7387\u63d0\u5347\u81f394%\uff0c\u4e14\u751f\u6210\u7684\u89e3\u91ca\u66f4\u4e30\u5bcc\u51c6\u786e\u3002", "conclusion": "\u5c06\u5206\u6790\u5de5\u5177\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u5f62\u6210\u7684\u6df7\u5408\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u5728\u5b9e\u9645\u90e8\u7f72\u7ba1\u9053\u4e2d\u5bf9\u9886\u57df\u7279\u5b9a\u4ee3\u7801\u9519\u8bef\u7684\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u8bc1\u660e\u4e86\u5206\u6790-LLM\u534f\u540c\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.16335", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.16335", "abs": "https://arxiv.org/abs/2512.16335", "authors": ["Yibiao Yang", "Qingyang Li", "Maolin Sun", "Jiangchang Wu", "Yuming Zhou"], "title": "Using a Sledgehammer to Crack a Nut? Revisiting Automated Compiler Fault Isolation", "comment": "Accepted at ICSE'26", "summary": "Background: Compilers are fundamental to software development, translating high-level source code into executable software systems. Faults in compilers can have severe consequences and thus effective localization and resolution of compiler bugs are crucial. Problem: In practice, developers often examine version history to identify and investigate bug-inducing commit (BIC) for fixing bugs. However, while numerous sophisticated Spectrum-Based Fault Localization (SBFL) techniques have been proposed for compiler fault isolation, their effectiveness has not been evaluated against the BIC-based strategies widely adopted in practice. Objective: This study aims to bridge this gap by directly comparing a BIC-based strategy, Basic, with representative SBFL techniques in the context of compiler fault localization. The BIC-based strategy closely aligns with common developer practices, as it directly identifies the BIC and treats the files modified in that commit as faulty candidates. Method: The Basic identifies the most recent good release and earliest bad release, and then employs a binary search to pinpoint the bug-inducing commit. All files modified in the identified commit are flagged as potentially faulty. We rigorously compare Basic against SBFL-based techniques using a benchmark consisting of 60 GCC bugs and 60 LLVM bugs. Result: Our analysis reveals that Basic performs comparably to, and in many cases outperforms, state-of-the-art SBFL-based techniques, particularly on the critical Top-1 and Top-5 ranking metrics. Conclusion: This study provides new insights into the practical effectiveness of SBFL-based techniques in real-world compiler debugging scenarios. We recommend that future research adopt Basic as a baseline when developing and evaluating new compiler fault isolation methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u57fa\u4e8eBug\u5f15\u5165\u63d0\u4ea4\uff08BIC\uff09\u7684\u7b56\u7565\uff08Basic\uff09\u4e0e\u4e3b\u6d41\u9891\u8c31\u6545\u969c\u5b9a\u4f4d\uff08SBFL\uff09\u6280\u672f\u5728\u7f16\u8bd1\u5668\u6545\u969c\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u8fdb\u884c\u5bf9\u6bd4\uff0c\u53d1\u73b0Basic\u5728Top-1\u548cTop-5\u6307\u6807\u4e0a\u8868\u73b0\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709SBFL\u6280\u672f\u5728\u7f16\u8bd1\u5668\u6545\u969c\u5b9a\u4f4d\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u4e0e\u5b9e\u8df5\u4e2d\u5e7f\u6cdb\u91c7\u7528\u7684BIC\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\uff0c\u5b58\u5728\u7814\u7a76\u4e0e\u5b9e\u8df5\u8131\u8282\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u540d\u4e3aBasic\u7684BIC\u7b56\u7565\uff1a\u901a\u8fc7\u4e8c\u5206\u67e5\u627e\u5b9a\u4f4d\u5f15\u5165\u6545\u969c\u7684\u63d0\u4ea4\uff0c\u5e76\u5c06\u8be5\u63d0\u4ea4\u4e2d\u4fee\u6539\u7684\u6240\u6709\u6587\u4ef6\u89c6\u4e3a\u53ef\u7591\u6545\u969c\u6587\u4ef6\uff1b\u5728\u5305\u542b60\u4e2aGCC\u548c60\u4e2aLLVM\u7f3a\u9677\u7684\u57fa\u51c6\u4e0a\u4e0eSBFL\u65b9\u6cd5\u8fdb\u884c\u4e25\u683c\u5bf9\u6bd4\u3002", "result": "Basic\u5728\u5173\u952e\u7684Top-1\u548cTop-5\u6392\u540d\u6307\u6807\u4e0a\u8868\u73b0\u4e0e\u5148\u8fdbSBFL\u6280\u672f\u76f8\u5f53\uff0c\u751a\u81f3\u66f4\u4f18\u3002", "conclusion": "\u5efa\u8bae\u672a\u6765\u7f16\u8bd1\u5668\u6545\u969c\u9694\u79bb\u7814\u7a76\u5c06Basic\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4ee5\u66f4\u8d34\u8fd1\u5b9e\u9645\u5f00\u53d1\u573a\u666f\u8bc4\u4f30\u65b0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.16455", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16455", "abs": "https://arxiv.org/abs/2512.16455", "authors": ["Ignacio Heredia", "\u00c1lvaro L\u00f3pez Garc\u00eda", "Germ\u00e1n Molt\u00f3", "Amanda Calatrava", "Valentin Kozlov", "Alessandro Costantini", "Viet Tran", "Mario David", "Daniel San Mart\u00edn", "Marcin P\u0142\u00f3ciennik", "Marta Obreg\u00f3n Ruiz", "Sa\u00fal Fernandez", "Judith S\u00e1inz-Pardo D\u00edaz", "Miguel Caballer", "Caterina Alarc\u00f3n Mar\u00edn", "Stefan Dlugolinsky", "Martin \u0160eleng", "Lisana Berberi", "Khadijeh Alibabaei", "Borja Esteban Sanchis", "Pedro Castro", "Giacinto Donvito", "Diego Aguirre", "Sergio Langarita", "Vicente Rodriguez", "Leonhard Duda", "Andr\u00e9s Heredia Canales", "Susana Rebolledo Ruiz", "Jo\u00e3o Machado", "Giang Nguyen", "Fernando Aguilar G\u00f3mez", "Jaime D\u00edez"], "title": "AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research", "comment": null, "summary": "In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u9762\u5411\u79d1\u7814\u4eba\u5de5\u667a\u80fd\u5de5\u4f5c\u8d1f\u8f7d\u7684\u8054\u90a6\u8ba1\u7b97\u5e73\u53f0\uff0c\u652f\u6301\u673a\u5668\u5b66\u4e60\u5168\u751f\u547d\u5468\u671f\uff0c\u5e76\u5f3a\u8c03\u53ef\u590d\u73b0\u6027\u3001\u900f\u660e\u8bbf\u95ee\u548c\u6613\u7528\u6027\u3002", "motivation": "\u4e3a\u79d1\u7814\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e00\u4e2a\u53ef\u590d\u73b0\u3001\u900f\u660e\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u8054\u90a6\u8ba1\u7b97\u5e73\u53f0\uff0c\u4ee5\u6574\u5408\u5206\u6563\u7684\u7535\u5b50\u57fa\u7840\u8bbe\u65bd\u8d44\u6e90\u5e76\u964d\u4f4e\u5916\u90e8\u793e\u533a\u7684\u91c7\u7528\u95e8\u69db\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u8054\u90a6\u8ba1\u7b97\u5e73\u53f0\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u670d\u52a1\u76ee\u5f55\u63d0\u4f9b\u6db5\u76d6\u6a21\u578b\u5f00\u53d1\u3001\u8bad\u7ec3\u548c\u90e8\u7f72\u7684\u5b8c\u6574\u673a\u5668\u5b66\u4e60\u751f\u547d\u5468\u671f\u652f\u6301\uff0c\u5e76\u96c6\u6210GPU\u8d44\u6e90\u3001\u6807\u6ce8\u5de5\u5177\u3001\u5b9e\u9a8c\u8ffd\u8e2a\u3001\u8054\u90a6\u5b66\u4e60\u3001\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u7ba1\u7406\u7b49\u529f\u80fd\u3002", "result": "\u8be5\u5e73\u53f0\u5b9e\u73b0\u4e86\u5bf9\u5206\u5e03\u5f0fe-\u57fa\u7840\u8bbe\u65bd\u7684\u4e00\u81f4\u900f\u660e\u8bbf\u95ee\uff0c\u652f\u6301\u591a\u79cd\u90e8\u7f72\u6a21\u5f0f\uff0c\u5177\u5907\u826f\u597d\u7684\u53ef\u5b9a\u5236\u6027\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5e73\u53f0\u6709\u6548\u652f\u6301\u79d1\u7814AI\u5de5\u4f5c\u6d41\uff0c\u63d0\u5347\u4e86\u53ef\u8ffd\u6eaf\u6027\u3001\u53ef\u590d\u73b0\u6027\u53ca\u8de8\u793e\u533a\u534f\u4f5c\u80fd\u529b\u3002"}}
{"id": "2512.16741", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.16741", "abs": "https://arxiv.org/abs/2512.16741", "authors": ["Zaheed Ahmed", "Philip Makedonski", "Jens Grabowski"], "title": "An Empirical Study of the Realism of Mutants in Deep Learning", "comment": null, "summary": "Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.\n  This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.\n  Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5bf9\u6df1\u5ea6\u5b66\u4e60\u4e2d\u9884\u8bad\u7ec3\u548c\u540e\u8bad\u7ec3\u7a81\u53d8\u65b9\u6cd5\u7684\u73b0\u5b9e\u6027\u8fdb\u884c\u4e86\u5b9e\u8bc1\u6bd4\u8f83\uff0c\u53d1\u73b0\u9884\u8bad\u7ec3\u7a81\u53d8\u66f4\u63a5\u8fd1\u771f\u5b9e\u6545\u969c\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u6539\u8fdb\u540e\u8bad\u7ec3\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4ef6\u4e2d\u7684\u7a81\u53d8\u5206\u6790\u5df2\u88ab\u5e7f\u6cdb\u7528\u4e8e\u8bc4\u4f30\u6d4b\u8bd5\u8d28\u91cf\uff0c\u5176\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u867d\u5df2\u6269\u5c55\u81f3\u6545\u969c\u5b9a\u4f4d\u3001\u4fee\u590d\u3001\u6570\u636e\u751f\u6210\u548c\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u7b49\u591a\u4e2a\u4efb\u52a1\uff0c\u4f46\u201c\u7a81\u53d8\u4f53\u884c\u4e3a\u4e0e\u771f\u5b9e\u6545\u969c\u76f8\u4f3c\u201d\u8fd9\u4e00\u6838\u5fc3\u5047\u8bbe\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u5c1a\u672a\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u8ba1\u6846\u67b6\uff0c\u5229\u7528CleanML\u3001DeepFD\u3001DeepLocalize\u548cdefect4ML\u7b49\u516c\u5f00\u7684\u771f\u5b9e\u7f3a\u9677\u6570\u636e\u96c6\uff0c\u91cf\u5316\u9884\u8bad\u7ec3\u4e0e\u540e\u8bad\u7ec3\u7a81\u53d8\u65b9\u6cd5\u4e0e\u771f\u5b9e\u6545\u969c\u4e4b\u95f4\u7684\u8026\u5408\u5f3a\u5ea6\u548c\u884c\u4e3a\u76f8\u4f3c\u6027\uff1b\u4f7f\u7528\u4ee3\u8868\u4e24\u7c7b\u65b9\u6cd5\u7684\u524d\u6cbf\u5de5\u5177\u751f\u6210\u7a81\u53d8\u4f53\u3002", "result": "\u9884\u8bad\u7ec3\u7a81\u53d8\u4f53\u5728\u8026\u5408\u5f3a\u5ea6\u548c\u884c\u4e3a\u76f8\u4f3c\u6027\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u540e\u8bad\u7ec3\u7a81\u53d8\u4f53\uff0c\u8868\u660e\u5176\u5177\u6709\u66f4\u9ad8\u7684\u73b0\u5b9e\u6027\uff1b\u4f46\u9884\u8bad\u7ec3\u7a81\u53d8\u7684\u9ad8\u6602\u8ba1\u7b97\u6210\u672c\u51f8\u663e\u4e86\u5f00\u53d1\u66f4\u9ad8\u6548\u540e\u8bad\u7ec3\u7b97\u5b50\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u9884\u8bad\u7ec3\u7a81\u53d8\u5728\u6a21\u62df\u771f\u5b9e\u6545\u969c\u65b9\u9762\u66f4\u5177\u73b0\u5b9e\u6027\uff0c\u4f46\u56e0\u5176\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u672a\u6765\u5e94\u81f4\u529b\u4e8e\u8bbe\u8ba1\u80fd\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u5176\u73b0\u5b9e\u6027\u7684\u9ad8\u6548\u540e\u8bad\u7ec3\u7a81\u53d8\u7b97\u5b50\u3002"}}
{"id": "2512.16473", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.16473", "abs": "https://arxiv.org/abs/2512.16473", "authors": ["En-Ming Huang", "Li-Shang Lin", "Chun-Yi Lee"], "title": "Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems", "comment": "7 pages, 6 figures, to be published in ASP-DAC 2026", "summary": "Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684CPU-GPU\u534f\u540c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5728GPU\u4e0a\u5f15\u5165\u4e13\u5bb6\u7f13\u5b58\u673a\u5236\uff0c\u51cf\u5c11MoE\u6a21\u578b\u63a8\u7406\u65f6\u7684\u6570\u636e\u4f20\u8f93\u5f00\u9500\uff0c\u5e76\u5229\u7528CPU\u591a\u7ebf\u7a0b\u4f18\u5316\u5904\u7406\u7f13\u5b58\u672a\u547d\u4e2d\u60c5\u51b5\uff0c\u4ece\u800c\u63d0\u5347\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u7684\u5355\u8bf7\u6c42\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u96be\u4ee5\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u90e8\u7f72\uff1b\u5c3d\u7ba1\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u901a\u8fc7\u9009\u62e9\u6027\u6fc0\u6d3b\u53c2\u6570\u5b50\u96c6\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4f46\u5176\u5185\u5b58\u9700\u6c42\u4ecd\u8d85\u51fa\u5178\u578b\u6d88\u8d39\u7ea7GPU\u5bb9\u91cf\u3002\u4f20\u7edfCPU-GPU\u6743\u91cd\u5378\u8f7d\u65b9\u6cd5\u56e0\u9891\u7e41\u6570\u636e\u4f20\u8f93\u5e26\u6765\u9ad8\u5ef6\u8fdf\uff0c\u9650\u5236\u4e86\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cdCPU-GPU\u534f\u540c\u63a8\u7406\u6846\u67b6\uff1a\u5728GPU\u4e0a\u7f13\u5b58\u5e38\u7528\u4e13\u5bb6\u4ee5\u51cf\u5c11\u6570\u636e\u4f20\u8f93\uff0c\u5229\u7528\u7f13\u5b58\u547d\u4e2d\u52a0\u901f\u63a8\u7406\uff1b\u5bf9\u4e8e\u7f13\u5b58\u672a\u547d\u4e2d\u7684\u4e13\u5bb6\u8ba1\u7b97\uff0c\u5219\u5378\u8f7d\u81f3CPU\u6267\u884c\uff0c\u5e76\u501f\u52a9CPU\u591a\u7ebf\u7a0b\u4f18\u5316\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6d88\u8d39\u7ea7\u7cfb\u7edf\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5355\u8bf7\u6c42\u63a8\u7406\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u6570\u636e\u4f20\u8f93\u5e76\u5145\u5206\u5229\u7528\u4e86CPU\u4e0eGPU\u7684\u534f\u540c\u8ba1\u7b97\u80fd\u529b\u3002", "conclusion": "CPU-GPU\u534f\u540c\u63a8\u7406\u7ed3\u5408\u4e13\u5bb6\u7f13\u5b58\u673a\u5236\u662f\u4e00\u79cd\u9ad8\u6548\u53ef\u884c\u7684\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347MoE\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u6d88\u8d39\u7ea7\u786c\u4ef6\u90e8\u7f72\u5927\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.16790", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.16790", "abs": "https://arxiv.org/abs/2512.16790", "authors": ["Aaron Imani", "Mohammad Moshirpour", "Iftekhar Ahmed"], "title": "Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse", "comment": "Accepted in the 48th IEEE/ACM International Conference on Software Engineering (ICSE)", "summary": "While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4ece\u6982\u5ff5\u5c42\u9762\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5bf9\u4ee3\u7801\u6ce8\u91ca\u7684\u5185\u90e8\u8868\u5f81\uff0c\u53d1\u73b0\u6ce8\u91ca\u88ab\u6a21\u578b\u89c6\u4e3a\u53ef\u533a\u5206\u7684\u6f5c\u5728\u6982\u5ff5\uff0c\u5e76\u4e14\u6fc0\u6d3b\u6216\u6291\u5236\u8fd9\u4e9b\u6982\u5ff5\u4f1a\u663e\u8457\u5f71\u54cd\u4e0d\u540c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6ce8\u91ca\u662f\u975e\u529f\u80fd\u6027\u4ee3\u7801\u5143\u7d20\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u5e38\u4f9d\u8d56\u5b83\u4eec\u6267\u884c\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\uff1b\u7136\u800c\uff0c\u8fd9\u79cd\u4f9d\u8d56\u5728\u6a21\u578b\u5185\u90e8\u5982\u4f55\u4f53\u73b0\u53ca\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u4f7f\u7528\u6982\u5ff5\u6fc0\u6d3b\u5411\u91cf\uff08CAV\uff09\u5206\u6790LLM\u5728\u4ee3\u7801\u8865\u5168\u3001\u7ffb\u8bd1\u548c\u4f18\u5316\u4e09\u4e2a\u4efb\u52a1\u4e2d\u5bf9\u4e0d\u540c\u7c7b\u578b\u6ce8\u91ca\uff08\u5982Javadoc\u3001\u884c\u5185\u3001\u591a\u884c\uff09\u7684\u5185\u90e8\u8868\u5f81\uff0c\u5e76\u901a\u8fc7\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7cfb\u7edf\u6027\u6fc0\u6d3b/\u53bb\u6fc0\u6d3b\u8fd9\u4e9b\u6982\u5ff5\u6765\u8bc4\u4f30\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff1b\u6b64\u5916\uff0c\u8fd8\u5bf910\u4e2a\u4e0d\u540cSE\u4efb\u52a1\u4e2d\u6ce8\u91ca\u6982\u5ff5\u7684\u6fc0\u6d3b\u7a0b\u5ea6\u8fdb\u884c\u4e86\u5bf9\u7167\u5b9e\u9a8c\u3002", "result": "LLM\u80fd\u5c06\u6ce8\u91ca\u5185\u5316\u4e3a\u53ef\u533a\u5206\u7684\u6f5c\u5728\u6982\u5ff5\uff1b\u6fc0\u6d3b/\u53bb\u6fc0\u6d3b\u8fd9\u4e9b\u6982\u5ff5\u4f1a\u5bfc\u81f4\u4efb\u52a1\u6027\u80fd\u53d1\u751f\u663e\u8457\u53d8\u5316\uff08-90%\u81f3+67%\uff09\uff1b\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0c\u4ee3\u7801\u6458\u8981\u6700\u5f3a\u70c8\u5730\u6fc0\u6d3b\u6ce8\u91ca\u6982\u5ff5\uff0c\u800c\u4ee3\u7801\u8865\u5168\u5bf9\u5176\u6700\u4e0d\u654f\u611f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4efb\u52a1\u4e2d\u5bf9\u6ce8\u91ca\u7684\u5185\u90e8\u673a\u5236\uff0c\u4e3a\u672a\u6765\u6784\u5efa\u57fa\u4e8e\u5185\u90e8\u6982\u5ff5\u64cd\u4f5c\u800c\u975e\u4ec5\u4f9d\u8d56\u8868\u9762\u8f93\u5165\u7684SE\u5de5\u5177\u548c\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.16816", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.16816", "abs": "https://arxiv.org/abs/2512.16816", "authors": ["Alessandra Parziale", "Gianmario Voria", "Valeria Pontillo", "Gemma Catolino", "Andrea De Lucia", "Fabio Palomba"], "title": "Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework", "comment": null, "summary": "Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCAFFE\u6846\u67b6\uff0c\u7528\u4e8e\u66f4\u6709\u6548\u5730\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u53cd\u4e8b\u5b9e\u516c\u5e73\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u66f4\u5168\u9762\u3001\u53ef\u9760\u5730\u68c0\u6d4b\u4e0d\u516c\u5e73\u884c\u4e3a\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u516c\u5e73\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff1b\u73b0\u6709\u57fa\u4e8e\u53d8\u5f62\u6d4b\u8bd5\u7684\u65b9\u6cd5\u5728\u68c0\u6d4b\u516c\u5e73\u6027\u7f3a\u9677\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7ed3\u6784\u5316\u3001\u610f\u56fe\u611f\u77e5\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCAFFE\uff08Counterfactual Assessment Framework for Fairness Evaluation\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u660e\u786e\u5b9a\u4e49\u63d0\u793a\u610f\u56fe\u3001\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u3001\u8f93\u5165\u53d8\u4f53\u3001\u516c\u5e73\u6027\u9608\u503c\u548c\u6d4b\u8bd5\u73af\u5883\u7b49\u7ec4\u4ef6\u6784\u5efa\u516c\u5e73\u6027\u6d4b\u8bd5\u7528\u4f8b\uff0c\u81ea\u52a8\u751f\u200b\u200b\u6210\u9488\u5bf9\u6027\u6d4b\u8bd5\u6570\u636e\uff0c\u5e76\u5229\u7528\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u3002", "result": "\u5728\u4e09\u79cd\u4e0d\u540c\u67b6\u6784\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAFFE\u6bd4\u73b0\u6709\u53d8\u5f62\u6d4b\u8bd5\u65b9\u6cd5\u5177\u6709\u66f4\u5e7f\u7684\u504f\u89c1\u8986\u76d6\u8303\u56f4\u548c\u66f4\u9ad8\u7684\u4e0d\u516c\u5e73\u884c\u4e3a\u68c0\u6d4b\u53ef\u9760\u6027\u3002", "conclusion": "CAFFE\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u3001\u53ef\u64cd\u4f5c\u7684\u53cd\u4e8b\u5b9e\u516c\u5e73\u6027\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u53d8\u5f62\u6d4b\u8bd5\uff0c\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u516c\u5e73\u6027\u8bc4\u4f30\u65b9\u9762\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
