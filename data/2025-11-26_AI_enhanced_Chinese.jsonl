{"id": "2511.19740", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19740", "abs": "https://arxiv.org/abs/2511.19740", "authors": ["Tergel Molom-Ochir", "Benjamin F. Morris", "Mark Horton", "Chiyue Wei", "Cong Guo", "Brady Taylor", "Peter Liu", "Shan X. Wang", "Deliang Fan", "Hai Helen Li", "Yiran Chen"], "title": "CAMformer: Associative Memory is All You Need", "comment": "7 pages, 10 figures", "summary": "Transformers face scalability challenges due to the quadratic cost of attention, which involves dense similarity computations between queries and keys. We propose CAMformer, a novel accelerator that reinterprets attention as an associative memory operation and computes attention scores using a voltage-domain Binary Attention Content Addressable Memory (BA-CAM). This enables constant-time similarity search through analog charge sharing, replacing digital arithmetic with physical similarity sensing. CAMformer integrates hierarchical two-stage top-k filtering, pipelined execution, and high-precision contextualization to achieve both algorithmic accuracy and architectural efficiency. Evaluated on BERT and Vision Transformer workloads, CAMformer achieves over 10x energy efficiency, up to 4x higher throughput, and 6-8x lower area compared to state-of-the-art accelerators--while maintaining near-lossless accuracy.", "AI": {"tldr": "CAMformer \u662f\u4e00\u79cd\u65b0\u578b\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u673a\u5236\u91cd\u6784\u4e3a\u8054\u60f3\u8bb0\u5fc6\u64cd\u4f5c\uff0c\u5e76\u5229\u7528\u7535\u538b\u57df\u4e8c\u503c\u6ce8\u610f\u529b\u5185\u5bb9\u53ef\u5bfb\u5740\u5b58\u50a8\u5668\uff08BA-CAM\uff09\u5b9e\u73b0\u5e38\u6570\u65f6\u95f4\u7684\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u80fd\u6548\u3001\u541e\u5410\u91cf\u5e76\u51cf\u5c0f\u9762\u79ef\uff0c\u540c\u65f6\u4fdd\u6301\u51e0\u4e4e\u65e0\u635f\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf Transformer \u4e2d\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u4e3a\u4e8c\u6b21\u65b9\uff0c\u5bfc\u81f4\u5728\u6269\u5c55\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u3002", "method": "\u63d0\u51fa CAMformer \u67b6\u6784\uff0c\u5c06\u6ce8\u610f\u529b\u8ba1\u7b97\u8f6c\u5316\u4e3a\u8054\u60f3\u8bb0\u5fc6\u64cd\u4f5c\uff0c\u91c7\u7528 BA-CAM \u5728\u6a21\u62df\u7535\u538b\u57df\u4e2d\u901a\u8fc7\u7535\u8377\u5171\u4eab\u5b9e\u73b0\u5e38\u6570\u65f6\u95f4\u76f8\u4f3c\u6027\u641c\u7d22\uff0c\u5e76\u7ed3\u5408\u4e24\u9636\u6bb5 top-k \u8fc7\u6ee4\u3001\u6d41\u6c34\u7ebf\u6267\u884c\u548c\u9ad8\u7cbe\u5ea6\u4e0a\u4e0b\u6587\u5efa\u6a21\u3002", "result": "\u5728 BERT \u548c Vision Transformer \u4efb\u52a1\u4e0a\uff0cCAMformer \u76f8\u6bd4\u73b0\u6709\u5148\u8fdb\u52a0\u901f\u5668\u5b9e\u73b0\u4e86\u8d85\u8fc7 10 \u500d\u7684\u80fd\u6548\u63d0\u5347\u3001\u6700\u9ad8 4 \u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u4ee5\u53ca 6\u20138 \u500d\u7684\u9762\u79ef\u7f29\u51cf\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u65e0\u635f\u7684\u6a21\u578b\u51c6\u786e\u7387\u3002", "conclusion": "CAMformer \u901a\u8fc7\u786c\u4ef6-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6ce8\u610f\u529b\u673a\u5236\u7684\u53ef\u6269\u5c55\u6027\u74f6\u9888\uff0c\u5728\u6027\u80fd\u3001\u80fd\u6548\u4e0e\u9762\u79ef\u65b9\u9762\u5747\u53d6\u5f97\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u5927\u89c4\u6a21 Transformer \u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2511.19973", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.19973", "abs": "https://arxiv.org/abs/2511.19973", "authors": ["Hoa Nguyen", "Pongstorn Maidee", "Jason Lowe-Power", "Alireza Kaviani"], "title": "Pickle Prefetcher: Programmable and Scalable Last-Level Cache Prefetcher", "comment": "13 pages, 13 figures", "summary": "Modern high-performance architectures employ large last-level caches (LLCs). While large LLCs can reduce average memory access latency for workloads with a high degree of locality, they can also increase latency for workloads with irregular memory access patterns. Prefetchers are widely used to reduce memory latency by prefetching data into the cache hierarchy before it is accessed by the core. However, existing prediction-based prefetchers often struggle with irregular memory access patterns, which are especially prevalent in modern applications. This paper introduces the Pickle Prefetcher, a programmable and scalable LLC prefetcher designed to handle independent irregular memory access patterns effectively. Instead of relying on static heuristics or complex prediction algorithms, Pickle Prefetcher allows software to define its own prefetching strategies using a simple programming interface without expanding the instruction set architecture (ISA). By trading the logic complexity of hardware prediction for software programmability, Pickle Prefetcher can adapt to a wide range of access patterns without requiring extensive hardware resources for prediction. This allows the prefetcher to dedicate its resources to scheduling and issuing timely prefetch requests. Graph applications are an example where the memory access pattern is irregular but easily predictable by software. Through extensive evaluations of the Pickle Prefetcher on gem5 full-system simulations, we demonstrate tha Pickle Prefetcher significantly outperforms traditional prefetching techniques. Our results show that Pickle Prefetcher achieves speedups of up to 1.74x on the GAPBS breadth-first search (BFS) implementation over a baseline system. When combined with private cache prefetchers, Pickle Prefetcher provides up to a 1.40x speedup over systems using only private cache prefetchers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Pickle Prefetcher\uff0c\u4e00\u79cd\u53ef\u7f16\u7a0b\u4e14\u53ef\u6269\u5c55\u7684\u672b\u7ea7\u7f13\u5b58\uff08LLC\uff09\u9884\u53d6\u5668\uff0c\u901a\u8fc7\u8f6f\u4ef6\u5b9a\u4e49\u9884\u53d6\u7b56\u7565\u6765\u9ad8\u6548\u5904\u7406\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u5728\u56fe\u5e94\u7528\u7b49\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u9884\u53d6\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9884\u6d4b\u7684\u9884\u53d6\u5668\u96be\u4ee5\u6709\u6548\u5904\u7406\u73b0\u4ee3\u5e94\u7528\u4e2d\u666e\u904d\u5b58\u5728\u7684\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u800c\u5927\u578b\u672b\u7ea7\u7f13\u5b58\u5bf9\u8fd9\u7c7b\u5de5\u4f5c\u8d1f\u8f7d\u53cd\u800c\u53ef\u80fd\u589e\u52a0\u5ef6\u8fdf\u3002", "method": "Pickle Prefetcher\u653e\u5f03\u590d\u6742\u7684\u786c\u4ef6\u9884\u6d4b\u903b\u8f91\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u7684\u8f6f\u4ef6\u7f16\u7a0b\u63a5\u53e3\uff0c\u5141\u8bb8\u8f6f\u4ef6\u81ea\u5b9a\u4e49\u9884\u53d6\u7b56\u7565\uff0c\u65e0\u9700\u6269\u5c55\u6307\u4ee4\u96c6\u67b6\u6784\uff08ISA\uff09\uff0c\u4ece\u800c\u5c06\u786c\u4ef6\u8d44\u6e90\u96c6\u4e2d\u4e8e\u53ca\u65f6\u8c03\u5ea6\u548c\u53d1\u51fa\u9884\u53d6\u8bf7\u6c42\u3002", "result": "\u5728gem5\u5168\u7cfb\u7edf\u6a21\u62df\u4e2d\uff0cPickle Prefetcher\u5728GAPBS\u7684BFS\u5b9e\u73b0\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u7cfb\u7edf\u6700\u9ad8\u63d0\u901f1.74\u500d\uff1b\u4e0e\u79c1\u6709\u7f13\u5b58\u9884\u53d6\u5668\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u79c1\u6709\u7f13\u5b58\u9884\u53d6\u5668\u7684\u7cfb\u7edf\u6700\u9ad8\u63d0\u901f1.40\u500d\u3002", "conclusion": "Pickle Prefetcher\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u4ee5\u8f6f\u4ef6\u53ef\u7f16\u7a0b\u6027\u66ff\u4ee3\u590d\u6742\u786c\u4ef6\u9884\u6d4b\uff0c\u5728\u4e0d\u663e\u8457\u589e\u52a0\u786c\u4ef6\u5f00\u9500\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4e0b\u7684\u6027\u80fd\u3002"}}
{"id": "2511.20090", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20090", "abs": "https://arxiv.org/abs/2511.20090", "authors": ["Zizhang Luo", "Fan Cui", "Kexing Zhou", "Runlin Guo", "Mile Xia", "Hongyuan Hou", "Yun Lian"], "title": "R3A: Reliable RTL Repair Framework with Multi-Agent Fault Localization and Stochastic Tree-of-Thoughts Patch Generation", "comment": null, "summary": "Repairing RTL bugs is crucial for hardware design and verification. Traditional automatic program repair (APR) methods define dedicated search spaces to locate and fix bugs with program synthesis. However, they heavily rely on fixed templates and can only deal with limited bugs. As an alternative, Large Language Models with the ability to understand code semantics can be explored for RTL repair. However, they suffer from unreliable outcomes due to inherent randomness and long input contexts of RTL code and waveform. To address these challenges, we propose R3A, an LLM-based automatic RTL program repair framework upon the basic model to improve reliability. R3A proposes the stochastic Tree-Of-Thoughts method to control a patch generation agent to explore a validated solution for the bug. The algorithm samples search states according to a heuristic function to balance between exploration and exploitation for a reliable outcome. Besides, R3A proposes a multi-agent fault localization method to find fault candidates as the starting points for the patch generation agent, further increasing the reliability. Experiments show R3A can fix 90.6% of bugs in the RTL-repair dataset within a given time limit, which covers 45% more bugs than traditional methods and other LLM-based approaches, while achieving an 86.7% pass@5 rate on average, showing a high reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faR3A\uff0c\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u52a8RTL\u7a0b\u5e8f\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u601d\u7ef4\u6811\u65b9\u6cd5\u548c\u591a\u667a\u80fd\u4f53\u6545\u969c\u5b9a\u4f4d\u673a\u5236\u63d0\u5347\u4fee\u590d\u53ef\u9760\u6027\uff0c\u5728RTL-repair\u6570\u636e\u96c6\u4e0a\u4fee\u590d\u4e8690.6%\u7684bug\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u5176\u4ed6LLM\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u7a0b\u5e8f\u4fee\u590d\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6a21\u677f\uff0c\u9002\u7528\u8303\u56f4\u6709\u9650\uff1b\u800c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728RTL\u4fee\u590d\u4e2d\u56e0\u8f93\u5165\u4e0a\u4e0b\u6587\u957f\u3001\u7ed3\u679c\u968f\u673a\u6027\u9ad8\u800c\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "method": "R3A\u5f15\u5165\u968f\u673a\u601d\u7ef4\u6811\uff08stochastic Tree-Of-Thoughts\uff09\u65b9\u6cd5\u63a7\u5236\u8865\u4e01\u751f\u6210\u667a\u80fd\u4f53\uff0c\u5e76\u7ed3\u5408\u542f\u53d1\u5f0f\u51fd\u6570\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff1b\u540c\u65f6\u91c7\u7528\u591a\u667a\u80fd\u4f53\u6545\u969c\u5b9a\u4f4d\u65b9\u6cd5\u786e\u5b9a\u4fee\u590d\u8d77\u70b9\u3002", "result": "\u5728RTL-repair\u6570\u636e\u96c6\u4e0a\uff0cR3A\u5728\u65f6\u9650\u5185\u4fee\u590d\u4e8690.6%\u7684bug\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u53ca\u5176\u4ed6LLM\u65b9\u6cd5\u591a\u8986\u76d645%\u7684bug\uff0c\u5e73\u5747pass@5\u7387\u8fbe86.7%\u3002", "conclusion": "R3A\u663e\u8457\u63d0\u5347\u4e86\u57fa\u4e8eLLM\u7684RTL\u81ea\u52a8\u4fee\u590d\u7684\u53ef\u9760\u6027\u4e0e\u4fee\u590d\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u786c\u4ef6\u8bbe\u8ba1\u4e0e\u9a8c\u8bc1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.19991", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2511.19991", "abs": "https://arxiv.org/abs/2511.19991", "authors": ["Meng-Chia Lee", "Wen Sheng Lim", "Yuan-Hao Chang", "Tei-Wei Kuo"], "title": "SARA: A Stall-Aware Memory Allocation Strategy for Mixed-Criticality Systems", "comment": null, "summary": "The memory capacity in edge devices is often limited due to constraints on cost, size, and power. Consequently, memory competition leads to inevitable page swapping in memory-constrained mixed-criticality edge devices, causing slow storage I/O and thus performance degradation. In such scenarios, inefficient memory allocation disrupts the balance between application performance, causing soft real-time (soft RT) tasks to miss deadlines or preventing non-real-time (non-RT) applications from optimizing throughput. Meanwhile, we observe unpredictable, long system-level stalls (called long stalls) under high memory and I/O pressure, which further degrade performance. In this work, we propose a Stall-Aware Real-Time Memory Allocator (SARA), which discovers opportunities for performance balance by allocating just enough memory to soft RT tasks to meet deadlines and, at the same time, optimizing the remaining memory for non-RT applications. To minimize the memory usage of soft RT tasks while meeting real-time requirements, SARA leverages our insight into how latency, caused by memory insufficiency and measured by our proposed PSI-based metric, affects the execution time of each soft RT job, where a job runs per period and a soft RT task consists of multiple periods. Moreover, SARA detects long stalls using our definition and proactively drops affected jobs, minimizing stalls in task execution. Experiments show that SARA achieves an average of 97.13% deadline hit ratio for soft RT tasks and improves non-RT application throughput by up to 22.32x over existing approaches, even with memory capacity limited to 60% of peak demand.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSARA\u7684\u5185\u5b58\u5206\u914d\u5668\uff0c\u7528\u4e8e\u5728\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e2d\u5e73\u8861\u8f6f\u5b9e\u65f6\u4efb\u52a1\u4e0e\u975e\u5b9e\u65f6\u5e94\u7528\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u7cbe\u51c6\u5206\u914d\u5185\u5b58\u4ee5\u6ee1\u8db3\u8f6f\u5b9e\u65f6\u4efb\u52a1\u622a\u6b62\u671f\u9650\uff0c\u5e76\u4f18\u5316\u5269\u4f59\u5185\u5b58\u63d0\u5347\u975e\u5b9e\u65f6\u5e94\u7528\u541e\u5410\u91cf\u3002", "motivation": "\u8fb9\u7f18\u8bbe\u5907\u56e0\u6210\u672c\u3001\u5c3a\u5bf8\u548c\u529f\u8017\u9650\u5236\u5bfc\u81f4\u5185\u5b58\u5bb9\u91cf\u6709\u9650\uff0c\u5f15\u53d1\u9891\u7e41\u9875\u9762\u4ea4\u6362\u548c\u7cfb\u7edf\u7ea7\u957f\u505c\u987f\uff08long stalls\uff09\uff0c\u7834\u574f\u8f6f\u5b9e\u65f6\u4efb\u52a1\u7684\u622a\u6b62\u671f\u9650\u4fdd\u969c\u5e76\u964d\u4f4e\u975e\u5b9e\u65f6\u5e94\u7528\u541e\u5410\u91cf\u3002", "method": "SARA\u57fa\u4e8ePSI\u6307\u6807\u91cf\u5316\u5185\u5b58\u4e0d\u8db3\u5f15\u8d77\u7684\u5ef6\u8fdf\u5bf9\u8f6f\u5b9e\u65f6\u4efb\u52a1\u5404\u5468\u671f\u4f5c\u4e1a\u6267\u884c\u65f6\u95f4\u7684\u5f71\u54cd\uff0c\u52a8\u6001\u5206\u914d\u521a\u597d\u6ee1\u8db3\u622a\u6b62\u671f\u9650\u7684\u5185\u5b58\uff1b\u540c\u65f6\u5b9a\u4e49\u5e76\u68c0\u6d4b\u957f\u505c\u987f\uff0c\u4e3b\u52a8\u4e22\u5f03\u53d7\u5f71\u54cd\u4f5c\u4e1a\u4ee5\u51cf\u5c11\u6267\u884c\u4e2d\u65ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5185\u5b58\u4ec5\u4e3a\u5cf0\u503c\u9700\u6c4260%\u7684\u60c5\u51b5\u4e0b\uff0cSARA\u4f7f\u8f6f\u5b9e\u65f6\u4efb\u52a1\u5e73\u5747\u622a\u6b62\u547d\u4e2d\u7387\u8fbe97.13%\uff0c\u975e\u5b9e\u65f6\u5e94\u7528\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u534722.32\u500d\u3002", "conclusion": "SARA\u6709\u6548\u89e3\u51b3\u4e86\u5185\u5b58\u53d7\u9650\u6df7\u5408\u5173\u952e\u6027\u8fb9\u7f18\u8bbe\u5907\u4e2d\u7684\u5185\u5b58\u5206\u914d\u95ee\u9898\uff0c\u5728\u4fdd\u969c\u8f6f\u5b9e\u65f6\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u975e\u5b9e\u65f6\u5e94\u7528\u6548\u7387\u3002"}}
{"id": "2511.19445", "categories": ["cs.DC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2511.19445", "abs": "https://arxiv.org/abs/2511.19445", "authors": ["Luca Accorsi", "Demetrio Lagan\u00e0", "Federico Michelotto", "Roberto Musmanno", "Daniele Vigo"], "title": "Asynchronous Cooperative Optimization of a Capacitated Vehicle Routing Problem Solution", "comment": null, "summary": "We propose a parallel shared-memory schema to cooperatively optimize the solution of a Capacitated Vehicle Routing Problem instance with minimal synchronization effort and without the need for an explicit decomposition. To this end, we design FILO2$^x$ as a single-trajectory parallel adaptation of the FILO2 algorithm originally proposed for extremely large-scale instances and described in Accorsi and Vigo (2024). Using the locality of the FILO2 optimization applications, in FILO2$^x$ several possibly unrelated solution areas are concurrently asynchronously optimized. The overall search trajectory emerges as an iteration-based parallelism obtained by the simultaneous optimization of the same underlying solution performed by several solvers. Despite the high efficiency exhibited by the single-threaded FILO2 algorithm, the computational results show that, by better exploiting the available computing resources, FILO2$^x$ can greatly enhance the resolution time compared to the original approach, still maintaining a similar final solution quality for instances ranging from hundreds to hundreds of thousands customers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5e76\u884c\u5171\u4eab\u5185\u5b58\u6846\u67b6 FILO2$^x$\uff0c\u7528\u4e8e\u9ad8\u6548\u6c42\u89e3\u5e26\u5bb9\u91cf\u7ea6\u675f\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08CVRP\uff09\uff0c\u5728\u51e0\u4e4e\u65e0\u9700\u540c\u6b65\u4e14\u4e0d\u663e\u5f0f\u5206\u89e3\u95ee\u9898\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u591a\u7ebf\u7a0b\u5e76\u53d1\u4f18\u5316\u540c\u4e00\u89e3\u7684\u4e0d\u540c\u5c40\u90e8\u533a\u57df\uff0c\u663e\u8457\u7f29\u77ed\u6c42\u89e3\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u5355\u7ebf\u7a0b\u7b97\u6cd5 FILO2 \u76f8\u5f53\u7684\u89e3\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5355\u7ebf\u7a0b\u7b97\u6cd5 FILO2 \u867d\u9ad8\u6548\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u73b0\u4ee3\u591a\u6838\u8ba1\u7b97\u8d44\u6e90\uff1b\u4f5c\u8005\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u4f4e\u540c\u6b65\u5f00\u9500\u3001\u65e0\u9700\u663e\u5f0f\u95ee\u9898\u5206\u89e3\u7684\u5e76\u884c\u7b56\u7565\uff0c\u4ee5\u52a0\u901f CVRP \u6c42\u89e3\u8fc7\u7a0b\u3002", "method": "\u57fa\u4e8e FILO2 \u7b97\u6cd5\u7684\u5c40\u90e8\u4f18\u5316\u7279\u6027\uff0c\u8bbe\u8ba1\u5176\u5e76\u884c\u7248\u672c FILO2$^x$\uff1a\u591a\u4e2a\u6c42\u89e3\u5668\u5f02\u6b65\u3001\u5e76\u53d1\u5730\u5bf9\u540c\u4e00\u5e95\u5c42\u89e3\u7684\u4e0d\u540c\u5c40\u90e8\u533a\u57df\u8fdb\u884c\u4f18\u5316\uff0c\u5f62\u6210\u57fa\u4e8e\u8fed\u4ee3\u7684\u5e76\u884c\u641c\u7d22\u8f68\u8ff9\uff0c\u5229\u7528\u5171\u4eab\u5185\u5b58\u5b9e\u73b0\u534f\u4f5c\u4f18\u5316\u3002", "result": "\u5728\u6570\u767e\u81f3\u6570\u5341\u4e07\u5ba2\u6237\u89c4\u6a21\u7684 CVRP \u5b9e\u4f8b\u4e0a\uff0cFILO2$^x$ \u663e\u8457\u4f18\u4e8e\u539f\u59cb FILO2 \u7684\u6c42\u89e3\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u8fd1\u7684\u6700\u7ec8\u89e3\u8d28\u91cf\u3002", "conclusion": "FILO2$^x$ \u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5e76\u884c CVRP \u4f18\u5316\uff0c\u5728\u51e0\u4e4e\u4e0d\u727a\u7272\u89e3\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u5927\u5e45\u7f29\u77ed\u8ba1\u7b97\u65f6\u95f4\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u5c40\u90e8\u6027\u4e0e\u5f02\u6b65\u534f\u4f5c\u7684\u5e76\u884c\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.19450", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19450", "abs": "https://arxiv.org/abs/2511.19450", "authors": ["M. Zeeshan Haider", "Tayyaba Noreen", "M. D. Assuncao", "Kaiwen Zhang"], "title": "AI-driven Predictive Shard Allocation for Scalable Next Generation Blockchains", "comment": null, "summary": "Sharding has emerged as a key technique to address blockchain scalability by partitioning the ledger into multiple shards that process transactions in parallel. Although this approach improves throughput, static or heuristic shard allocation often leads to workload skew, congestion, and excessive cross-shard communication diminishing the scalability benefits of sharding. To overcome these challenges, we propose the Predictive Shard Allocation Protocol (PSAP), a dynamic and intelligent allocation framework that proactively assigns accounts and transactions to shards based on workload forecasts. PSAP integrates a Temporal Workload Forecasting (TWF) model with a safety-constrained reinforcement learning (Safe-PPO) controller, jointly enabling multi-block-ahead prediction and adaptive shard reconfiguration. The protocol enforces deterministic inference across validators through a synchronized quantized runtime and a safety gate that limits stake concentration, migration gas, and utilization thresholds. By anticipating hotspot formation and executing bounded, atomic migrations, PSAP achieves stable load balance while preserving Byzantine safety. Experimental evaluation on heterogeneous datasets, including Ethereum, NEAR, and Hyperledger Fabric mapped via address-clustering heuristics, demonstrates up to 2x throughput improvement, 35\\% lower latency, and 20\\% reduced cross-shard overhead compared to existing dynamic sharding baselines. These results confirm that predictive, deterministic, and security-aware shard allocation is a promising direction for next-generation scalable blockchain systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9884\u6d4b\u6027\u5206\u7247\u5206\u914d\u534f\u8bae\uff08PSAP\uff09\u7684\u52a8\u6001\u667a\u80fd\u5206\u7247\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u65f6\u5e8f\u5de5\u4f5c\u8d1f\u8f7d\u9884\u6d4b\u6a21\u578b\u4e0e\u5b89\u5168\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u524d\u77bb\u6027\u8d26\u6237\u4e0e\u4ea4\u6613\u5206\u914d\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u533a\u5757\u94fe\u7cfb\u7edf\u7684\u541e\u5410\u91cf\u3001\u964d\u4f4e\u5ef6\u8fdf\u5e76\u51cf\u5c11\u8de8\u5206\u7247\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u533a\u5757\u94fe\u5206\u7247\u6280\u672f\u5e38\u91c7\u7528\u9759\u6001\u6216\u542f\u53d1\u5f0f\u5206\u7247\u7b56\u7565\uff0c\u6613\u5bfc\u81f4\u8d1f\u8f7d\u4e0d\u5747\u3001\u62e5\u585e\u53ca\u8fc7\u591a\u8de8\u5206\u7247\u901a\u4fe1\uff0c\u524a\u5f31\u4e86\u5206\u7247\u5e26\u6765\u7684\u53ef\u6269\u5c55\u6027\u4f18\u52bf\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u52a8\u6001\u3001\u667a\u80fd\u5730\u5206\u914d\u8d1f\u8f7d\u5e76\u4fdd\u969c\u5b89\u5168\u6027\u7684\u5206\u7247\u673a\u5236\u3002", "method": "PSAP\u534f\u8bae\u6574\u5408\u4e86\u65f6\u5e8f\u5de5\u4f5c\u8d1f\u8f7d\u9884\u6d4b\uff08TWF\uff09\u6a21\u578b\u4e0e\u5b89\u5168\u7ea6\u675f\u7684\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08Safe-PPO\uff09\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u5668\uff0c\u652f\u6301\u591a\u533a\u5757\u524d\u77bb\u9884\u6d4b\u4e0e\u81ea\u9002\u5e94\u5206\u7247\u91cd\u914d\u7f6e\uff1b\u5e76\u901a\u8fc7\u540c\u6b65\u91cf\u5316\u8fd0\u884c\u65f6\u548c\u5b89\u5168\u95e8\u9650\u673a\u5236\uff08\u9650\u5236\u8d28\u62bc\u96c6\u4e2d\u5ea6\u3001\u8fc1\u79fbGas\u4e0e\u5229\u7528\u7387\uff09\u786e\u4fdd\u9a8c\u8bc1\u8005\u95f4\u786e\u5b9a\u6027\u63a8\u7406\u4e0e\u62dc\u5360\u5ead\u5b89\u5168\u6027\u3002", "result": "\u5728Ethereum\u3001NEAR\u548cHyperledger Fabric\u7b49\u5f02\u6784\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u73b0\u6709\u52a8\u6001\u5206\u7247\u65b9\u6848\uff0cPSAP\u53ef\u5b9e\u73b0\u6700\u9ad82\u500d\u541e\u5410\u91cf\u63d0\u5347\u300135%\u5ef6\u8fdf\u964d\u4f4e\u4ee5\u53ca20%\u8de8\u5206\u7247\u5f00\u9500\u51cf\u5c11\u3002", "conclusion": "\u9884\u6d4b\u6027\u3001\u786e\u5b9a\u6027\u4e14\u5b89\u5168\u611f\u77e5\u7684\u5206\u7247\u5206\u914d\u673a\u5236\u662f\u6784\u5efa\u4e0b\u4e00\u4ee3\u9ad8\u53ef\u6269\u5c55\u533a\u5757\u94fe\u7cfb\u7edf\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2511.19453", "categories": ["cs.DC", "cs.DB", "cs.OS", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19453", "abs": "https://arxiv.org/abs/2511.19453", "authors": ["Yuxin Wang", "Yuankai He", "Weisong Shi"], "title": "AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles", "comment": null, "summary": "Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAVS\uff0c\u4e00\u79cd\u9762\u5411\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u65b0\u578b\u8f66\u8f7d\u5b58\u50a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u8ba1\u7b97\u4e0e\u5b58\u50a8\u534f\u540c\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6570\u636e\u5199\u5165\u3001\u5feb\u901f\u9009\u62e9\u6027\u68c0\u7d22\u548c\u663e\u8457\u7684\u5b58\u50a8\u7a7a\u95f4\u538b\u7f29\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4ea7\u751f\u7684\u6d77\u91cf\u5f02\u6784\u6570\u636e\uff08\u5982\u6bcf\u592914TB\uff09\u7f3a\u4e4f\u9ad8\u6548\u7684\u901a\u7528\u5b58\u50a8\u4e0e\u67e5\u8be2\u7cfb\u7edf\uff0c\u73b0\u6709\u8f66\u8f7d\u6570\u636e\u8bb0\u5f55\u5668\u548c\u5b58\u50a8\u6808\u65e0\u6cd5\u6ee1\u8db3\u65b0\u5174\u7b2c\u4e09\u65b9\u5e94\u7528\u5bf9\u9ad8\u6548\u5b58\u53d6\u7684\u9700\u6c42\u3002", "method": "AVS\u91c7\u7528\u5206\u5c42\u5e03\u5c40\u534f\u540c\u8ba1\u7b97\uff1a\u5305\u62ec\u6a21\u6001\u611f\u77e5\u7684\u6570\u636e\u7f29\u51cf\u4e0e\u538b\u7f29\u3001\u57fa\u4e8e\u51b7\u70ed\u6570\u636e\u5206\u5c42\u7684\u6bcf\u65e5\u5f52\u6863\u673a\u5236\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u5143\u6570\u636e\u7d22\u5f15\u5c42\uff1b\u5e76\u901a\u8fc7\u7cfb\u7edf\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9eL4\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u5728\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u8bbe\u8ba1\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u4e86\u53ef\u9884\u6d4b\u7684\u5b9e\u65f6\u6570\u636e\u5199\u5165\u3001\u5feb\u901f\u7684\u9009\u62e9\u6027\u6570\u636e\u68c0\u7d22\u548c\u663e\u8457\u7684\u5b58\u50a8\u5360\u7528\u51cf\u5c11\u3002", "conclusion": "AVS\u5c55\u793a\u4e86\u5c06\u5b58\u50a8\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e00\u7ea7\u7ec4\u4ef6\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u66f4\u53ef\u6269\u5c55\u3001\u66f4\u957f\u671f\u90e8\u7f72\u7684\u8f66\u8f7d\u5b58\u50a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c2\u5bdf\u4e0e\u65b9\u5411\u3002"}}
{"id": "2511.19477", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19477", "abs": "https://arxiv.org/abs/2511.19477", "authors": ["Aram Vardanyan"], "title": "Building Browser Agents: Architecture, Security, and Practical Solutions", "comment": "30 pages, 22 figures. Production architecture and benchmark evaluation of browser agents", "summary": "Browser agents enable autonomous web interaction but face critical reliability and security challenges in production. This paper presents findings from building and operating a production browser agent. The analysis examines where current approaches fail and what prevents safe autonomous operation. The fundamental insight: model capability does not limit agent performance; architectural decisions determine success or failure. Security analysis of real-world incidents reveals prompt injection attacks make general-purpose autonomous operation fundamentally unsafe. The paper argues against developing general browsing intelligence in favor of specialized tools with programmatic constraints, where safety boundaries are enforced through code instead of large language model (LLM) reasoning. Through hybrid context management combining accessibility tree snapshots with selective vision, comprehensive browser tooling matching human interaction capabilities, and intelligent prompt engineering, the agent achieved approximately 85% success rate on the WebGames benchmark across 53 diverse challenges (compared to approximately 50% reported for prior browser agents and 95.7% human baseline).", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51fa\uff0c\u6d4f\u89c8\u5668\u667a\u80fd\u4f53\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u9762\u4e34\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u6311\u6218\uff0c\u5176\u6027\u80fd\u74f6\u9888\u5e76\u975e\u6a21\u578b\u80fd\u529b\uff0c\u800c\u662f\u67b6\u6784\u8bbe\u8ba1\uff1b\u4f5c\u8005\u901a\u8fc7\u6df7\u5408\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u5168\u9762\u7684\u6d4f\u89c8\u5668\u5de5\u5177\u548c\u667a\u80fd\u63d0\u793a\u5de5\u7a0b\uff0c\u5728WebGames\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u7ea685%\u7684\u6210\u529f\u7387\uff0c\u5e76\u4e3b\u5f20\u91c7\u7528\u53d7\u7a0b\u5e8f\u7ea6\u675f\u7684\u4e13\u7528\u5de5\u5177\u66ff\u4ee3\u901a\u7528\u81ea\u4e3b\u6d4f\u89c8\u3002", "motivation": "\u5f53\u524d\u6d4f\u89c8\u5668\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u4e0e\u5b89\u5168\u98ce\u9669\uff0c\u5c24\u5176\u662f\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u4f7f\u901a\u7528\u81ea\u4e3b\u64cd\u4f5c\u672c\u8d28\u4e0a\u4e0d\u5b89\u5168\uff0c\u4e9f\u9700\u91cd\u65b0\u601d\u8003\u5176\u67b6\u6784\u4e0e\u8bbe\u8ba1\u539f\u5219\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e0a\u4e0b\u6587\u7ba1\u7406\uff08\u7ed3\u5408\u65e0\u969c\u788d\u6811\u5feb\u7167\u4e0e\u9009\u62e9\u6027\u89c6\u89c9\uff09\u3001\u5339\u914d\u4eba\u7c7b\u4ea4\u4e92\u80fd\u529b\u7684\u5b8c\u6574\u6d4f\u89c8\u5668\u5de5\u5177\u96c6\uff0c\u4ee5\u53ca\u667a\u80fd\u63d0\u793a\u5de5\u7a0b\uff1b\u540c\u65f6\u5f3a\u8c03\u901a\u8fc7\u4ee3\u7801\u800c\u975e\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6765\u5b9e\u65bd\u5b89\u5168\u8fb9\u754c\uff0c\u8f6c\u5411\u4e13\u7528\u5de5\u5177\u800c\u975e\u901a\u7528\u667a\u80fd\u3002", "result": "\u5728\u5305\u542b53\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u7684WebGames\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u667a\u80fd\u4f53\u8fbe\u5230\u7ea685%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u6b64\u524d\u7ea650%\u7684\u6d4f\u89c8\u5668\u667a\u80fd\u4f53\u8868\u73b0\uff0c\u63a5\u8fd195.7%\u7684\u4eba\u7c7b\u57fa\u7ebf\u3002", "conclusion": "\u6a21\u578b\u80fd\u529b\u5e76\u975e\u9650\u5236\u6d4f\u89c8\u5668\u667a\u80fd\u4f53\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u67b6\u6784\u8bbe\u8ba1\u624d\u662f\u51b3\u5b9a\u6210\u8d25\u7684\u6838\u5fc3\uff1b\u4e3a\u786e\u4fdd\u5b89\u5168\u6027\u4e0e\u53ef\u9760\u6027\uff0c\u5e94\u653e\u5f03\u901a\u7528\u6d4f\u89c8\u667a\u80fd\uff0c\u8f6c\u800c\u53d1\u5c55\u5177\u6709\u7a0b\u5e8f\u5316\u7ea6\u675f\u7684\u4e13\u7528\u5de5\u5177\u3002"}}
{"id": "2511.19456", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.19456", "abs": "https://arxiv.org/abs/2511.19456", "authors": ["Anton Reinhard", "Simeon Ehrig", "Ren\u00e9 Widera", "Michael Bussmann", "Uwe Hernandez Acosta"], "title": "Optimizations on Graph-Level for Domain Specific Computations in Julia and Application to QED", "comment": null, "summary": "Complex computational problems in science often consist of smaller parts that can have largely distinct compute requirements from one another. For optimal efficiency, analyzing each subtask and scheduling it on the best-suited hardware would be necessary. Other considerations must be taken into account, too, such as parallelism, dependencies between different subtasks, and data transfer speeds between devices. To achieve this, directed acyclic graphs are often employed to represent these problems and enable utilizing as much hardware as possible on a given machine. In this paper, we present a software framework written in Julia capable of automatically and dynamically producing statically scheduled and compiled code. We lay theoretical foundations and add domain-specific information about the computation to the existing concepts of DAG scheduling, enabling optimizations that would otherwise be impossible. To illustrate the theory we implement an example application: the computation of matrix elements for scattering processes with many external particles in quantum electrodynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eJulia\u8bed\u8a00\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u8c03\u5ea6\u4e0e\u9886\u57df\u7279\u5b9a\u4fe1\u606f\uff0c\u5b9e\u73b0\u5bf9\u590d\u6742\u79d1\u5b66\u8ba1\u7b97\u4efb\u52a1\u7684\u81ea\u52a8\u9759\u6001\u8c03\u5ea6\u4e0e\u7f16\u8bd1\u4f18\u5316\u3002", "motivation": "\u79d1\u5b66\u8ba1\u7b97\u4e2d\u7684\u590d\u6742\u95ee\u9898\u901a\u5e38\u7531\u591a\u4e2a\u5177\u6709\u4e0d\u540c\u8ba1\u7b97\u9700\u6c42\u7684\u5b50\u4efb\u52a1\u7ec4\u6210\uff0c\u4e3a\u63d0\u5347\u6548\u7387\uff0c\u9700\u5c06\u5404\u5b50\u4efb\u52a1\u5206\u914d\u81f3\u6700\u9002\u5408\u7684\u786c\u4ef6\u4e0a\u6267\u884c\uff0c\u5e76\u517c\u987e\u5e76\u884c\u6027\u3001\u4efb\u52a1\u4f9d\u8d56\u548c\u6570\u636e\u4f20\u8f93\u7b49\u56e0\u7d20\u3002", "method": "\u5229\u7528\u6709\u5411\u65e0\u73af\u56fe\u8868\u793a\u8ba1\u7b97\u4efb\u52a1\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u5bf9DAG\u8c03\u5ea6\u8fdb\u884c\u6269\u5c55\uff0c\u5728Julia\u4e2d\u6784\u5efa\u4e00\u4e2a\u80fd\u81ea\u52a8\u52a8\u6001\u751f\u6210\u9759\u6001\u8c03\u5ea6\u4e0e\u7f16\u8bd1\u4ee3\u7801\u7684\u8f6f\u4ef6\u6846\u67b6\u3002", "result": "\u5b9e\u73b0\u4e86\u9488\u5bf9\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u4e2d\u591a\u5916\u7c92\u5b50\u6563\u5c04\u8fc7\u7a0b\u77e9\u9635\u5143\u8ba1\u7b97\u7684\u793a\u4f8b\u5e94\u7528\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u5316\u6f5c\u529b\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u9886\u57df\u7279\u5b9a\u4fe1\u606f\u4e0eDAG\u8c03\u5ea6\u7406\u8bba\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u786c\u4ef6\u5229\u7528\u548c\u6027\u80fd\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7684\u79d1\u5b66\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "2511.20049", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20049", "abs": "https://arxiv.org/abs/2511.20049", "authors": ["Yushuai Ji", "Sheng Wang", "Zhiyu Chen", "Yuan Sun", "Zhiyong Peng"], "title": "Updatable Balanced Index for Fast On-device Search with Auto-selection Model", "comment": "Accepted for publication in the 42nd IEEE International Conference on Data Engineering (ICDE 2026). To appear", "summary": "Diverse types of edge data, such as 2D geo-locations and 3D point clouds, are collected by sensors like lidar and GPS receivers on edge devices. On-device searches, such as k-nearest neighbor (kNN) search and radius search, are commonly used to enable fast analytics and learning technologies, such as k-means dataset simplification using kNN. To maintain high search efficiency, a representative approach is to utilize a balanced multi-way KD-tree (BMKD-tree). However, the index has shown limited gains, mainly due to substantial construction overhead, inflexibility to real-time insertion, and inconsistent query performance. In this paper, we propose UnIS to address the above limitations. We first accelerate the construction process of the BMKD-tree by utilizing the dataset distribution to predict the splitting hyperplanes. To make the continuously generated data searchable, we propose a selective sub-tree rebuilding scheme to accelerate rebalancing during insertion by reducing the number of data points involved. We then propose an auto-selection model to improve query performance by automatically selecting the optimal search strategy among multiple strategies for an arbitrary query task. Experimental results show that UnIS achieves average speedups of 17.96x in index construction, 1.60x in insertion, 7.15x in kNN search, and 1.09x in radius search compared to the BMKD-tree. We further verify its effectiveness in accelerating dataset simplification on edge devices, achieving a speedup of 217x over Lloyd's algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUnIS\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u6d4b\u5206\u5272\u8d85\u5e73\u9762\u52a0\u901fBMKD-tree\u6784\u5efa\u3001\u91c7\u7528\u9009\u62e9\u6027\u5b50\u6811\u91cd\u5efa\u652f\u6301\u9ad8\u6548\u63d2\u5165\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u641c\u7d22\u7b56\u7565\u9009\u62e9\u673a\u5236\u63d0\u5347\u67e5\u8be2\u6027\u80fd\uff0c\u5728\u591a\u79cd\u8fb9\u7f18\u6570\u636e\u641c\u7d22\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edfBMKD-tree\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eBMKD-tree\u7684\u8fb9\u7f18\u8bbe\u5907\u7d22\u5f15\u65b9\u6cd5\u5b58\u5728\u6784\u5efa\u5f00\u9500\u5927\u3001\u4e0d\u652f\u6301\u5b9e\u65f6\u63d2\u5165\u53ca\u67e5\u8be2\u6027\u80fd\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e0b\u7684\u5b9e\u7528\u6027\u3002", "method": "UnIS\u901a\u8fc7\u4e09\u65b9\u9762\u6539\u8fdbBMKD-tree\uff1a1\uff09\u5229\u7528\u6570\u636e\u5206\u5e03\u9884\u6d4b\u5206\u5272\u8d85\u5e73\u9762\u4ee5\u52a0\u901f\u6784\u5efa\uff1b2\uff09\u8bbe\u8ba1\u9009\u62e9\u6027\u5b50\u6811\u91cd\u5efa\u673a\u5236\u4ee5\u9ad8\u6548\u5904\u7406\u52a8\u6001\u63d2\u5165\uff1b3\uff09\u6784\u5efa\u81ea\u52a8\u9009\u62e9\u6a21\u578b\u4e3a\u4e0d\u540c\u67e5\u8be2\u4efb\u52a1\u9009\u53d6\u6700\u4f18\u641c\u7d22\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUnIS\u76f8\u8f83BMKD-tree\u5728\u7d22\u5f15\u6784\u5efa\u3001\u63d2\u5165\u3001kNN\u641c\u7d22\u548c\u534a\u5f84\u641c\u7d22\u4e0a\u5206\u522b\u63d0\u901f17.96\u500d\u30011.60\u500d\u30017.15\u500d\u548c1.09\u500d\uff0c\u5e76\u5728\u8fb9\u7f18\u8bbe\u5907\u6570\u636e\u96c6\u7b80\u5316\u4efb\u52a1\u4e2d\u6bd4Lloyd\u7b97\u6cd5\u5feb217\u500d\u3002", "conclusion": "UnIS\u6709\u6548\u89e3\u51b3\u4e86BMKD-tree\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u6548\u7387\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u6570\u636e\u7d22\u5f15\u4e0e\u67e5\u8be2\u7684\u6574\u4f53\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u8fb9\u7f18\u6570\u636e\u5206\u6790\u4e0e\u5b66\u4e60\u4efb\u52a1\u3002"}}
{"id": "2511.19483", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19483", "abs": "https://arxiv.org/abs/2511.19483", "authors": ["Qingsong He", "Jing Nan", "Jiayu Jiao", "Liangjie Tang", "Xiaodong Xu", "Mengmeng Sun", "Qingyao Wang", "Minghui Yan"], "title": "Z-Space: A Multi-Agent Tool Orchestration Framework for Enterprise-Grade LLM Automation", "comment": null, "summary": "Large Language Models can break through knowledge and timeliness limitations by invoking external tools within the Model Context Protocol framework to achieve automated execution of complex tasks. However, with the rapid growth of enterprise-scale MCP services, efficiently and accurately matching target functionalities among thousands of heterogeneous tools has become a core challenge restricting system practicality. Existing approaches generally rely on full-prompt injection or static semantic retrieval, facing issues including semantic disconnection between user queries and tool descriptions, context inflation in LLM input, and high inference latency. To address these challenges, this paper proposes Z-Space, a data-generation-oriented multi-agent collaborative tool invocation framework Z-Space. The Z-Space framework establishes a multi-agent collaborative architecture and tool filtering algorithm: (1) A structured semantic understanding of user queries is achieved through an intent parsing model; (2) A tool filtering module (FSWW) based on fused subspace weighted algorithm realizes fine-grained semantic alignment between intents and tools without parameter tuning; (3) An inference execution agent is constructed to support dynamic planning and fault-tolerant execution for multi-step tasks. This framework has been deployed in the Eleme platform's technical division, serving large-scale test data generation scenarios across multiple business units including Taotian, Gaode, and Hema. Production data demonstrates that the system reduces average token consumption in tool inference by 96.26\\% while achieving a 92\\% tool invocation accuracy rate, significantly enhancing the efficiency and reliability of intelligent test data generation systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faZ-Space\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u540c\u4e0e\u878d\u5408\u5b50\u7a7a\u95f4\u52a0\u6743\u7b97\u6cd5\uff08FSWW\uff09\uff0c\u5728\u4f01\u4e1a\u7ea7\u5927\u89c4\u6a21\u5de5\u5177\u8c03\u7528\u573a\u666f\u4e2d\u663e\u8457\u964d\u4f4eLLM\u7684token\u6d88\u8017\u5e76\u63d0\u5347\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u4f01\u4e1a\u7ea7MCP\u670d\u52a1\u5feb\u901f\u589e\u957f\uff0c\u5728\u6570\u5343\u4e2a\u5f02\u6784\u5de5\u5177\u4e2d\u9ad8\u6548\u51c6\u786e\u5730\u5339\u914d\u76ee\u6807\u529f\u80fd\u6210\u4e3a\u9650\u5236\u7cfb\u7edf\u5b9e\u7528\u6027\u7684\u6838\u5fc3\u6311\u6218\uff1b\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u8131\u8282\u3001\u4e0a\u4e0b\u6587\u81a8\u80c0\u548c\u63a8\u7406\u5ef6\u8fdf\u9ad8\u7b49\u95ee\u9898\u3002", "method": "Z-Space\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1) \u57fa\u4e8e\u610f\u56fe\u89e3\u6790\u6a21\u578b\u5bf9\u7528\u6237\u67e5\u8be2\u8fdb\u884c\u7ed3\u6784\u5316\u8bed\u4e49\u7406\u89e3\uff1b(2) \u57fa\u4e8e\u878d\u5408\u5b50\u7a7a\u95f4\u52a0\u6743\u7b97\u6cd5\uff08FSWW\uff09\u7684\u5de5\u5177\u8fc7\u6ee4\u6a21\u5757\uff0c\u5b9e\u73b0\u65e0\u9700\u53c2\u6570\u8c03\u4f18\u7684\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\uff1b(3) \u652f\u6301\u52a8\u6001\u89c4\u5212\u4e0e\u5bb9\u9519\u6267\u884c\u7684\u63a8\u7406\u6267\u884c\u667a\u80fd\u4f53\u3002", "result": "\u5728\u997f\u4e86\u4e48\u5e73\u53f0\u6280\u672f\u90e8\u95e8\u90e8\u7f72\u540e\uff0c\u7cfb\u7edf\u5728\u6dd8\u5929\u3001\u9ad8\u5fb7\u3001\u76d2\u9a6c\u7b49\u591a\u4e2a\u4e1a\u52a1\u5355\u5143\u7684\u5927\u89c4\u6a21\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u573a\u666f\u4e2d\uff0c\u5c06\u5de5\u5177\u63a8\u7406\u5e73\u5747token\u6d88\u8017\u964d\u4f4e96.26%\uff0c\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\u8fbe92%\u3002", "conclusion": "Z-Space\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u667a\u80fd\u6d4b\u8bd5\u6570\u636e\u751f\u6210\u7cfb\u7edf\u7684\u6548\u7387\u4e0e\u53ef\u9760\u6027\uff0c\u4e3a\u4f01\u4e1a\u7ea7\u590d\u6742\u4efb\u52a1\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20084", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20084", "abs": "https://arxiv.org/abs/2511.20084", "authors": ["Mariana M. Garcez Duarte", "Dwi P. A. Nugroho", "Georges Tod", "Evert Bevernage", "Pieter Moelans", "Emine Tas", "Esteban Zimanyi", "Mahmoud Sakr", "Steffen Zeuch", "Volker Markl"], "title": "Mobility Stream Processing on NebulaStream and MEOS", "comment": null, "summary": "The increasing use of Internet-of-Things (IoT) sensors in moving objects has resulted in vast amounts of spatiotemporal streaming data. To analyze this data in situ, real-time spatiotemporal processing is needed. However, current stream processing systems designed for IoT environments often lack spatiotemporal processing capabilities, and existing spatiotemporal libraries primarily focus on analyzing historical data. This gap makes performing real-time spatiotemporal analytics challenging. In this demonstration, we present NebulaMEOS, which combines MEOS (Mobility Engine Open Source), a spatiotemporal processing library, with NebulaStream, a scalable data management system for IoT applications. By integrating MEOS into NebulaStream, NebulaMEOS utilizes spatiotemporal functionalities to process and analyze streaming data in real-time. We demonstrate NebulaMEOS by querying data streamed from edge devices on trains by the Soci\u00e9t\u00e9 Nationale des Chemins de fer Belges (SNCB). Visitors can experience demonstrations of geofencing and geospatial complex event processing, visualizing real-time train operations and environmental impacts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86NebulaMEOS\u7cfb\u7edf\uff0c\u5c06\u65f6\u7a7a\u5904\u7406\u5e93MEOS\u96c6\u6210\u5230\u7269\u8054\u7f51\u6d41\u6570\u636e\u7ba1\u7406\u7cfb\u7edfNebulaStream\u4e2d\uff0c\u4ee5\u652f\u6301\u5bf9\u79fb\u52a8\u7269\u4f53\uff08\u5982\u706b\u8f66\uff09\u4ea7\u751f\u7684\u65f6\u7a7a\u6d41\u6570\u636e\u8fdb\u884c\u5b9e\u65f6\u5206\u6790\uff0c\u5e76\u901a\u8fc7SNCB\u5217\u8f66\u6570\u636e\u5c55\u793a\u4e86\u5730\u7406\u56f4\u680f\u548c\u590d\u6742\u4e8b\u4ef6\u5904\u7406\u7b49\u5e94\u7528\u3002", "motivation": "\u5f53\u524d\u9762\u5411\u7269\u8054\u7f51\u7684\u6d41\u5904\u7406\u7cfb\u7edf\u666e\u904d\u7f3a\u4e4f\u65f6\u7a7a\u5904\u7406\u80fd\u529b\uff0c\u800c\u73b0\u6709\u65f6\u7a7a\u5e93\u4e3b\u8981\u9488\u5bf9\u5386\u53f2\u6570\u636e\uff0c\u96be\u4ee5\u6ee1\u8db3\u5bf9\u79fb\u52a8\u5bf9\u8c61\u4ea7\u751f\u7684\u65f6\u7a7a\u6d41\u6570\u636e\u8fdb\u884c\u5b9e\u65f6\u5206\u6790\u7684\u9700\u6c42\u3002", "method": "\u5c06\u5f00\u6e90\u65f6\u7a7a\u5904\u7406\u5e93MEOS\u4e0e\u53ef\u6269\u5c55\u7684\u7269\u8054\u7f51\u6570\u636e\u7ba1\u7406\u7cfb\u7edfNebulaStream\u96c6\u6210\uff0c\u6784\u5efa\u540d\u4e3aNebulaMEOS\u7684\u65b0\u7cfb\u7edf\uff0c\u4ee5\u652f\u6301\u5b9e\u65f6\u65f6\u7a7a\u6d41\u6570\u636e\u5904\u7406\u3002", "result": "\u901a\u8fc7SNCB\u5217\u8f66\u8fb9\u7f18\u8bbe\u5907\u4f20\u6765\u7684\u6570\u636e\uff0c\u6210\u529f\u6f14\u793a\u4e86NebulaMEOS\u5728\u5730\u7406\u56f4\u680f\u548c\u5730\u7406\u7a7a\u95f4\u590d\u6742\u4e8b\u4ef6\u5904\u7406\u65b9\u9762\u7684\u5b9e\u65f6\u5206\u6790\u4e0e\u53ef\u89c6\u5316\u80fd\u529b\u3002", "conclusion": "NebulaMEOS\u6709\u6548\u586b\u8865\u4e86\u7269\u8054\u7f51\u6d41\u5904\u7406\u7cfb\u7edf\u5728\u5b9e\u65f6\u65f6\u7a7a\u5206\u6790\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u79fb\u52a8\u5bf9\u8c61\u7684\u5b9e\u65f6\u76d1\u63a7\u4e0e\u73af\u5883\u5f71\u54cd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.19484", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19484", "abs": "https://arxiv.org/abs/2511.19484", "authors": ["Randall Balestriero", "Hugues Van Assel", "Sami BuGhanem", "Lucas Maes"], "title": "stable-pretraining-v1: Foundation Model Research Made Simple", "comment": null, "summary": "Foundation models and self-supervised learning (SSL) have become central to modern AI, yet research in this area remains hindered by complex codebases, redundant re-implementations, and the heavy engineering burden of scaling experiments. We present stable-pretraining, a modular, extensible, and performance-optimized library built on top of PyTorch, Lightning, Hugging Face, and TorchMetrics. Unlike prior toolkits focused narrowly on reproducing state-of-the-art results, stable-pretraining is designed for flexibility and iteration speed: it unifies essential SSL utilities--including probes, collapse detection metrics, augmentation pipelines, and extensible evaluation routines--within a coherent and reliable framework. A central design principle is logging everything, enabling fine-grained visibility into training dynamics that makes debugging, monitoring, and reproducibility seamless. We validate the library by demonstrating its ability to generate new research insights with minimal overhead, including depthwise representation probing and the analysis of CLIP degradation under synthetic data finetuning. By lowering barriers to entry while remaining scalable to large experiments, stable-pretraining aims to accelerate discovery and expand the possibilities of foundation model research.", "AI": {"tldr": "stable-pretraining \u662f\u4e00\u4e2a\u57fa\u4e8e PyTorch \u7b49\u6846\u67b6\u6784\u5efa\u7684\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u6027\u80fd\u4f18\u5316\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5e93\uff0c\u65e8\u5728\u7b80\u5316\u57fa\u7840\u6a21\u578b\u7814\u7a76\uff0c\u63d0\u5347\u5b9e\u9a8c\u8fed\u4ee3\u901f\u5ea6\u4e0e\u53ef\u590d\u73b0\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u7840\u6a21\u578b\u4e0e\u81ea\u76d1\u7763\u5b66\u4e60\u7814\u7a76\u53d7\u9650\u4e8e\u590d\u6742\u4ee3\u7801\u5e93\u3001\u91cd\u590d\u5b9e\u73b0\u548c\u7e41\u91cd\u7684\u5de5\u7a0b\u8d1f\u62c5\uff0c\u4e9f\u9700\u4e00\u4e2a\u7075\u6d3b\u9ad8\u6548\u7684\u7814\u7a76\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86 stable-pretraining \u5e93\uff0c\u6574\u5408\u63a2\u9488\u3001\u5d29\u6e83\u68c0\u6d4b\u6307\u6807\u3001\u589e\u5f3a\u6d41\u6c34\u7ebf\u548c\u53ef\u6269\u5c55\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5e76\u5f3a\u8c03\u5168\u9762\u65e5\u5fd7\u8bb0\u5f55\u4ee5\u652f\u6301\u8c03\u8bd5\u4e0e\u590d\u73b0\u3002", "result": "\u8be5\u5e93\u80fd\u4ee5\u8f83\u4f4e\u5f00\u9500\u751f\u6210\u65b0\u7814\u7a76\u6d1e\u89c1\uff0c\u4f8b\u5982\u6df1\u5ea6\u8868\u793a\u63a2\u9488\u548c CLIP \u5728\u5408\u6210\u6570\u636e\u5fae\u8c03\u4e0b\u7684\u9000\u5316\u5206\u6790\u3002", "conclusion": "stable-pretraining \u901a\u8fc7\u964d\u4f4e\u7814\u7a76\u95e8\u69db\u5e76\u652f\u6301\u5927\u89c4\u6a21\u5b9e\u9a8c\uff0c\u6709\u671b\u52a0\u901f\u57fa\u7840\u6a21\u578b\u9886\u57df\u7684\u53d1\u73b0\u4e0e\u521b\u65b0\u3002"}}
{"id": "2511.19460", "categories": ["cs.DC", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19460", "abs": "https://arxiv.org/abs/2511.19460", "authors": ["Sofiane Ben Amor", "Guillaume Guerard", "Loup-No\u00e9 Levy"], "title": "Systemic approach for modeling a generic smart grid", "comment": null, "summary": "Smart grid technological advances present a recent class of complex interdisciplinary modeling and increasingly difficult simulation problems to solve using traditional computational methods. To simulate a smart grid requires a systemic approach to integrated modeling of power systems, energy markets, demand-side management, and much other resources and assets that are becoming part of the current paradigm of the power grid. This paper presents a backbone model of a smart grid to test alternative scenarios for the grid. This tool simulates disparate systems to validate assumptions before the human scale model. Thanks to a distributed optimization of subsystems, the production and consumption scheduling is achieved while maintaining flexibility and scalability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u667a\u80fd\u7535\u7f51\u7684\u9aa8\u5e72\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u4f18\u5316\u5b9e\u73b0\u591a\u7cfb\u7edf\u534f\u540c\u4eff\u771f\uff0c\u4ee5\u652f\u6301\u5728\u5b9e\u9645\u90e8\u7f72\u524d\u9a8c\u8bc1\u4e0d\u540c\u573a\u666f\u5047\u8bbe\u3002", "motivation": "\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u667a\u80fd\u7535\u7f51\u65e5\u76ca\u590d\u6742\u7684\u8de8\u5b66\u79d1\u5efa\u6a21\u4e0e\u4eff\u771f\u6311\u6218\uff0c\u4e9f\u9700\u4e00\u79cd\u7cfb\u7edf\u6027\u96c6\u6210\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u667a\u80fd\u7535\u7f51\u9aa8\u5e72\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5e03\u5f0f\u5b50\u7cfb\u7edf\u4f18\u5316\u65b9\u6cd5\uff0c\u5bf9\u7535\u529b\u7cfb\u7edf\u3001\u80fd\u6e90\u5e02\u573a\u3001\u9700\u6c42\u4fa7\u7ba1\u7406\u7b49\u8fdb\u884c\u96c6\u6210\u4eff\u771f\u3002", "result": "\u5b9e\u73b0\u4e86\u751f\u4ea7\u4e0e\u6d88\u8d39\u8c03\u5ea6\uff0c\u5728\u4fdd\u6301\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u540c\u65f6\uff0c\u80fd\u591f\u6d4b\u8bd5\u591a\u79cd\u7535\u7f51\u8fd0\u884c\u573a\u666f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4e3a\u667a\u80fd\u7535\u7f51\u7684\u7cfb\u7edf\u7ea7\u4eff\u771f\u548c\u65b9\u6848\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u5728\u5927\u89c4\u6a21\u90e8\u7f72\u524d\u8bc4\u4f30\u4e0d\u540c\u7b56\u7565\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.20125", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20125", "abs": "https://arxiv.org/abs/2511.20125", "authors": ["Yihua Hu", "Hao Ding", "Wei Dong"], "title": "N2E: A General Framework to Reduce Node-Differential Privacy to Edge-Differential Privacy for Graph Analytics", "comment": null, "summary": "Differential privacy (DP) has been widely adopted to protect sensitive information in graph analytics. While edge-DP, which protects privacy at the edge level, has been extensively studied, node-DP, offering stronger protection for entire nodes and their incident edges, remains largely underexplored due to its technical challenges. A natural way to bridge this gap is to develop a general framework for reducing node-DP graph analytical tasks to edge-DP ones, enabling the reuse of existing edge-DP mechanisms. A straightforward solution based on group privacy divides the privacy budget by a given degree upper bound, but this leads to poor utility when the bound is set conservatively large to accommodate worst-case inputs. To address this, we propose node-to-edge (N2E), a general framework that reduces any node-DP graph analytical task to an edge-DP one, with the error dependency on the graph's true maximum degree. N2E introduces two novel techniques: a distance-preserving clipping mechanism that bounds edge distance between neighboring graphs after clipping, and the first node-DP mechanism for maximum degree approximation, enabling tight, privacy-preserving clipping thresholds. By instantiating N2E with existing edge-DP mechanisms, we obtain the first node-DP solutions for tasks such as maximum degree estimation. For edge counting, our method theoretically matches the error of the state-of-the-art, which is provably optimal, and significantly outperforms existing approaches for degree distribution estimation. Experimental results demonstrate that our framework achieves up to a 2.5x reduction in error for edge counting and up to an 80x reduction for degree distribution estimation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aN2E\u7684\u901a\u7528\u6846\u67b6\uff0c\u5c06\u8282\u70b9\u5dee\u5206\u9690\u79c1\uff08node-DP\uff09\u56fe\u5206\u6790\u4efb\u52a1\u8f6c\u5316\u4e3a\u8fb9\u5dee\u5206\u9690\u79c1\uff08edge-DP\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u5f15\u5165\u4fdd\u8ddd\u88c1\u526a\u673a\u5236\u548c\u9996\u4e2a\u8282\u70b9DP\u6700\u5927\u5ea6\u8fd1\u4f3c\u673a\u5236\uff0c\u5728\u8fb9\u7f18\u8ba1\u6570\u548c\u5ea6\u5206\u5e03\u4f30\u8ba1\u7b49\u4efb\u52a1\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u8bef\u5dee\u3002", "motivation": "\u8282\u70b9\u5dee\u5206\u9690\u79c1\uff08node-DP\uff09\u76f8\u6bd4\u8fb9\u5dee\u5206\u9690\u79c1\uff08edge-DP\uff09\u63d0\u4f9b\u66f4\u5f3a\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u4f46\u7531\u4e8e\u6280\u672f\u6311\u6218\uff0c\u7814\u7a76\u8f83\u5c11\u3002\u73b0\u6709\u57fa\u4e8e\u7fa4\u9690\u79c1\u7684\u7b80\u5355\u65b9\u6cd5\u5728\u8bbe\u5b9a\u4fdd\u5b88\u7684\u5927\u5ea6\u6570\u4e0a\u9650\u65f6\u4f1a\u5bfc\u81f4\u6548\u7528\u4f4e\u4e0b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u901a\u7528\u8f6c\u6362\u6846\u67b6\u3002", "method": "\u63d0\u51faN2E\u6846\u67b6\uff0c\u5305\u542b\u4e24\u9879\u5173\u952e\u6280\u672f\uff1a\u4e00\u662f\u4fdd\u8ddd\u88c1\u526a\u673a\u5236\uff0c\u63a7\u5236\u88c1\u526a\u540e\u76f8\u90bb\u56fe\u4e4b\u95f4\u7684\u8fb9\u8ddd\u79bb\uff1b\u4e8c\u662f\u9996\u4e2a\u6ee1\u8db3\u8282\u70b9DP\u7684\u6700\u5927\u5ea6\u8fd1\u4f3c\u673a\u5236\uff0c\u7528\u4e8e\u751f\u6210\u7d27\u81f4\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u88c1\u526a\u9608\u503c\u3002\u8be5\u6846\u67b6\u53ef\u4e0e\u73b0\u6709edge-DP\u673a\u5236\u7ed3\u5408\u4f7f\u7528\u3002", "result": "\u5728\u8fb9\u7f18\u8ba1\u6570\u4efb\u52a1\u4e2d\uff0c\u7406\u8bba\u8bef\u5dee\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\uff1b\u5728\u5ea6\u5206\u5e03\u4f30\u8ba1\u4efb\u52a1\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8fb9\u7f18\u8ba1\u6570\u8bef\u5dee\u6700\u591a\u964d\u4f4e2.5\u500d\uff0c\u5ea6\u5206\u5e03\u4f30\u8ba1\u8bef\u5dee\u6700\u591a\u964d\u4f4e80\u500d\u3002", "conclusion": "N2E\u6846\u67b6\u6709\u6548\u5f25\u5408\u4e86node-DP\u4e0eedge-DP\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u591a\u79cd\u56fe\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u9996\u4e2a\u5b9e\u7528\u4e14\u9ad8\u7cbe\u5ea6\u7684node-DP\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\u3002"}}
{"id": "2511.19489", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19489", "abs": "https://arxiv.org/abs/2511.19489", "authors": ["Zhe Zhao", "Yuheng Yang", "Haibin Wen", "Xiaojie Qiu", "Zaixi Zhang", "Qingfu Zhang"], "title": "Evolution without an Oracle: Driving Effective Evolution with LLM Judges", "comment": "14 pages, 5 figures", "summary": "The integration of Large Language Models (LLMs) with Evolutionary Computation (EC) has unlocked new frontiers in scientific discovery but remains shackled by a fundamental constraint: the reliance on an Oracle--an objective, machine-computable fitness function. This paper breaks this barrier by asking: Can evolution thrive in a purely subjective landscape governed solely by LLM judges? We introduce MADE (Multi-Agent Decomposed Evolution), a framework that tames the inherent noise of subjective evaluation through \"Problem Specification.\" By decomposing vague instructions into specific, verifiable sub-requirements, MADE transforms high-variance LLM feedback into stable, precise selection pressure. The results are transformative: across complex benchmarks like DevAI and InfoBench, MADE outperforms strong baselines by over 50% in software requirement satisfaction (39.9% to 61.9%) and achieves a 95% perfect pass rate on complex instruction following. This work validates a fundamental paradigm shift: moving from optimizing \"computable metrics\" to \"describable qualities,\" thereby unlocking evolutionary optimization for the vast open-ended domains where no ground truth exists.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMADE\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6a21\u7cca\u6307\u4ee4\u5206\u89e3\u4e3a\u53ef\u9a8c\u8bc1\u7684\u5b50\u9700\u6c42\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4f5c\u4e3a\u4e3b\u89c2\u8bc4\u5224\u8005\u9a71\u52a8\u8fdb\u5316\u8ba1\u7b97\uff0c\u5728\u65e0\u5ba2\u89c2\u9002\u5e94\u5ea6\u51fd\u6570\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u8fdb\u5316\u8ba1\u7b97\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u53ef\u8ba1\u7b97\u7684\u5ba2\u89c2\u9002\u5e94\u5ea6\u51fd\u6570\uff08Oracle\uff09\uff0c\u9650\u5236\u4e86\u5176\u5728\u5f00\u653e\u57df\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff1b\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4ec5\u4f9d\u9760LLM\u4e3b\u89c2\u8bc4\u5224\u8fdb\u884c\u8fdb\u5316\u7684\u53ef\u884c\u6027\u3002", "method": "\u63d0\u51faMADE\uff08Multi-Agent Decomposed Evolution\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u201c\u95ee\u9898\u89c4\u8303\u201d\u5c06\u6a21\u7cca\u76ee\u6807\u5206\u89e3\u4e3a\u5177\u4f53\u3001\u53ef\u9a8c\u8bc1\u7684\u5b50\u9700\u6c42\uff0c\u4ece\u800c\u964d\u4f4eLLM\u4e3b\u89c2\u8bc4\u4f30\u7684\u566a\u58f0\uff0c\u5f62\u6210\u7a33\u5b9a\u7684\u9009\u62e9\u538b\u529b\u3002", "result": "\u5728DevAI\u548cInfoBench\u7b49\u590d\u6742\u57fa\u51c6\u4e0a\uff0cMADE\u5728\u8f6f\u4ef6\u9700\u6c42\u6ee1\u8db3\u7387\u4e0a\u4ece39.9%\u63d0\u5347\u81f361.9%\uff0c\u5e76\u5728\u590d\u6742\u6307\u4ee4\u9075\u5faa\u4efb\u52a1\u4e2d\u8fbe\u523095%\u7684\u5b8c\u7f8e\u901a\u8fc7\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u4ece\u4f18\u5316\u201c\u53ef\u8ba1\u7b97\u6307\u6807\u201d\u8f6c\u5411\u4f18\u5316\u201c\u53ef\u63cf\u8ff0\u54c1\u8d28\u201d\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u7f3a\u4e4f\u771f\u5b9e\u6807\u7b7e\u7684\u5f00\u653e\u57df\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8fdb\u5316\u4f18\u5316\u8def\u5f84\u3002"}}
{"id": "2511.19463", "categories": ["cs.DC", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2511.19463", "abs": "https://arxiv.org/abs/2511.19463", "authors": ["Aldo Canfora", "Eleonora Bergamaschi", "Riccardo Mioli", "Federico Battini", "Mirko Degli Esposti", "Giorgio Pedrazzi", "Chiara Dellacasa"], "title": "Urban Buildings Energy Consumption Estimation Using HPC: A Case Study of Bologna", "comment": "Preprint submitted for publication", "summary": "Urban Building Energy Modeling (UBEM) plays a central role in understanding and forecasting energy consumption at the city scale. In this work, we present a UBEM pipeline that integrates EnergyPlus simulations, high-performance computing (HPC), and open geospatial datasets to estimate the energy demand of buildings in Bologna, Italy. Geometric information including building footprints and heights was obtained from the Bologna Open Data portal and enhanced with aerial LiDAR measurements. Non-geometric attributes such as construction materials, insulation characteristics, and window performance were derived from regional building regulations and the European TABULA database. The computation was carried out on Leonardo, the Cineca-hosted supercomputer, enabling the simulation of approximately 25,000 buildings in under 30 minutes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408EnergyPlus\u6a21\u62df\u3001\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u548c\u5f00\u653e\u5730\u7406\u7a7a\u95f4\u6570\u636e\u7684\u57ce\u5e02\u5efa\u7b51\u80fd\u8017\u5efa\u6a21\uff08UBEM\uff09\u6d41\u7a0b\uff0c\u7528\u4e8e\u4f30\u7b97\u610f\u5927\u5229\u535a\u6d1b\u5c3c\u4e9a\u7ea625,000\u680b\u5efa\u7b51\u7684\u80fd\u8017\uff0c\u6574\u4e2a\u6a21\u62df\u5728Cineca\u8d85\u7ea7\u8ba1\u7b97\u673aLeonardo\u4e0a\u4e0d\u523030\u5206\u949f\u5b8c\u6210\u3002", "motivation": "\u57ce\u5e02\u5c3a\u5ea6\u7684\u5efa\u7b51\u80fd\u8017\u5efa\u6a21\u5bf9\u7406\u89e3\u548c\u9884\u6d4b\u80fd\u6e90\u6d88\u8017\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u6570\u636e\u83b7\u53d6\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684UBEM\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u6574\u5408\u4e86EnergyPlus\u6a21\u62df\u3001\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u548c\u5f00\u653e\u5730\u7406\u7a7a\u95f4\u6570\u636e\u96c6\u3002\u5efa\u7b51\u51e0\u4f55\u4fe1\u606f\u6765\u81ea\u535a\u6d1b\u5c3c\u4e9a\u5f00\u653e\u6570\u636e\u95e8\u6237\u5e76\u8f85\u4ee5LiDAR\u6d4b\u91cf\uff1b\u975e\u51e0\u4f55\u5c5e\u6027\uff08\u5982\u5efa\u7b51\u6750\u6599\u3001\u4fdd\u6e29\u6027\u80fd\u548c\u7a97\u6237\u6027\u80fd\uff09\u5219\u4f9d\u636e\u5730\u533a\u5efa\u7b51\u6cd5\u89c4\u548c\u6b27\u6d32TABULA\u6570\u636e\u5e93\u83b7\u53d6\u3002\u8ba1\u7b97\u5728Cineca\u7684Leonardo\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u6267\u884c\u3002", "result": "\u6210\u529f\u5728\u4e0d\u523030\u5206\u949f\u5185\u5b8c\u6210\u4e86\u5bf9\u7ea625,000\u680b\u5efa\u7b51\u7684\u80fd\u8017\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0UBEM\u6d41\u7a0b\u7684\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u7ed3\u5408\u5f00\u653e\u6570\u636e\u3001\u7269\u7406\u6a21\u62df\u4e0e\u9ad8\u6027\u80fd\u8ba1\u7b97\u7684\u57ce\u5e02\u5efa\u7b51\u80fd\u8017\u5efa\u6a21\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u3001\u51c6\u786e\u5730\u652f\u6301\u57ce\u5e02\u7ea7\u80fd\u6e90\u89c4\u5212\u4e0e\u653f\u7b56\u5236\u5b9a\u3002"}}
{"id": "2511.20139", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20139", "abs": "https://arxiv.org/abs/2511.20139", "authors": ["Mariana M Garcez Duarte", "Mahmoud Sakr"], "title": "An experimental study of existing tools for outlier detection and cleaning in trajectories", "comment": null, "summary": "Outlier detection and cleaning are essential steps in data preprocessing to ensure the integrity and validity of data analyses. This paper focuses on outlier points within individual trajectories, i.e., points that deviate significantly inside a single trajectory. We experiment with ten open-source libraries to comprehensively evaluate available tools, comparing their efficiency and accuracy in identifying and cleaning outliers. This experiment considers the libraries as they are offered to end users, with real-world applicability. We compare existing outlier detection libraries, introduce a method for establishing ground-truth, and aim to guide users in choosing the most appropriate tool for their specific outlier detection needs. Furthermore, we survey the state-of-the-art algorithms for outlier detection and classify them into five types: Statistic-based methods, Sliding window algorithms, Clustering-based methods, Graph-based methods, and Heuristic-based methods. Our research provides insights into these libraries' performance and contributes to developing data preprocessing and outlier detection methodologies.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5341\u4e2a\u5f00\u6e90\u5e93\u5728\u5355\u8f68\u8ff9\u5185\u5f02\u5e38\u70b9\u68c0\u6d4b\u4e0e\u6e05\u6d17\u65b9\u9762\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86\u5efa\u7acb\u771f\u5b9e\u6807\u7b7e\u7684\u65b9\u6cd5\uff0c\u5e76\u5bf9\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u7efc\u8ff0\uff0c\u4ee5\u6307\u5bfc\u7528\u6237\u9009\u62e9\u5408\u9002\u5de5\u5177\u3002", "motivation": "\u786e\u4fdd\u6570\u636e\u5206\u6790\u7684\u5b8c\u6574\u6027\u4e0e\u6709\u6548\u6027\uff0c\u9700\u8981\u5728\u6570\u636e\u9884\u5904\u7406\u9636\u6bb5\u6709\u6548\u8bc6\u522b\u548c\u6e05\u7406\u8f68\u8ff9\u4e2d\u7684\u5f02\u5e38\u70b9\uff1b\u7136\u800c\u76ee\u524d\u7f3a\u4e4f\u5bf9\u73b0\u6709\u5f00\u6e90\u5de5\u5177\u5728\u771f\u5b9e\u4f7f\u7528\u573a\u666f\u4e0b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u548c\u6307\u5bfc\u3002", "method": "\u5b9e\u9a8c\u8bc4\u4f30\u5341\u4e2a\u5f00\u6e90\u5f02\u5e38\u68c0\u6d4b\u5e93\u5728\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e00\u79cd\u6784\u5efa\u771f\u5b9e\u6807\u7b7e\uff08ground-truth\uff09\u7684\u65b9\u6cd5\uff0c\u5e76\u5bf9\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\u8fdb\u884c\u5206\u7c7b\uff1a\u57fa\u4e8e\u7edf\u8ba1\u3001\u6ed1\u52a8\u7a97\u53e3\u3001\u57fa\u4e8e\u805a\u7c7b\u3001\u57fa\u4e8e\u56fe\u548c\u57fa\u4e8e\u542f\u53d1\u5f0f\u4e94\u7c7b\u3002", "result": "\u5bf9\u5404\u5f00\u6e90\u5e93\u5728\u6548\u7387\u4e0e\u51c6\u786e\u6027\u65b9\u9762\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u63d0\u4f9b\u4e86\u5176\u6027\u80fd\u6d1e\u5bdf\uff0c\u5e76\u4e3a\u7528\u6237\u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u5408\u9002\u5de5\u5177\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u672c\u7814\u7a76\u4e0d\u4ec5\u6709\u52a9\u4e8e\u7406\u89e3\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u5e93\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u8fd8\u63a8\u52a8\u4e86\u6570\u636e\u9884\u5904\u7406\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u7684\u53d1\u5c55\uff0c\u4e3a\u8f68\u8ff9\u6570\u636e\u6e05\u6d17\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u8003\u3002"}}
{"id": "2511.19510", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.19510", "abs": "https://arxiv.org/abs/2511.19510", "authors": ["Asif Zaman", "Kallol Naha", "Khalid Belhajjame", "Hasan M. Jamil"], "title": "CodeR3: A GenAI-Powered Workflow Repair and Revival Ecosystem", "comment": "9 pages, 4 figures", "summary": "Scientific workflows encode valuable domain expertise and computational methodologies. Yet studies consistently show that a significant proportion of published workflows suffer from decay over time. This problem is particularly acute for legacy workflow systems like Taverna, where discontinued services, obsolete dependencies, and system retirement render previously functional workflows unusable. We present a novel legacy workflow migration system, called CodeR$^3$ (stands for Code Repair, Revival and Reuse), that leverages generative AI to analyze the characteristics of decayed workflows, reproduce them into modern workflow technologies like Snakemake and VisFlow. Our system additionally integrates stepwise workflow analysis visualization, automated service substitution, and human-in-the-loop validation. Through several case studies of Taverna workflow revival, we demonstrate the feasibility of this approach while identifying key challenges that require human oversight. Our findings reveal that automation significantly reduces manual effort in workflow parsing and service identification. However, critical tasks such as service substitution and data validation still require domain expertise. Our result will be a crowdsourcing platform that enables the community to collaboratively revive decayed workflows and validate the functionality and correctness of revived workflows. This work contributes a framework for workflow revival that balances automation efficiency with necessary human judgment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCodeR\u00b3\u7684\u65b0\u578b\u9057\u7559\u5de5\u4f5c\u6d41\u8fc1\u79fb\u7cfb\u7edf\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u5c06\u5df2\u5931\u6548\u7684Taverna\u5de5\u4f5c\u6d41\u81ea\u52a8\u8f6c\u6362\u4e3a\u73b0\u4ee3\u5de5\u4f5c\u6d41\u6280\u672f\uff08\u5982Snakemake\u548cVisFlow\uff09\uff0c\u5e76\u7ed3\u5408\u53ef\u89c6\u5316\u5206\u6790\u3001\u670d\u52a1\u81ea\u52a8\u66ff\u6362\u4e0e\u4eba\u5de5\u9a8c\u8bc1\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u89e3\u6790\u8d1f\u62c5\uff0c\u540c\u65f6\u5f3a\u8c03\u5173\u952e\u6b65\u9aa4\u4ecd\u9700\u9886\u57df\u4e13\u5bb6\u53c2\u4e0e\u3002", "motivation": "\u79d1\u5b66\u5de5\u4f5c\u6d41\u8574\u542b\u5b9d\u8d35\u7684\u9886\u57df\u77e5\u8bc6\u548c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4f46\u5927\u91cf\u5df2\u53d1\u8868\u7684\u5de5\u4f5c\u6d41\u968f\u65f6\u95f4\u63a8\u79fb\u800c\u5931\u6548\uff0c\u5c24\u5176\u5728Taverna\u7b49\u5df2\u505c\u7528\u7684\u65e7\u7cfb\u7edf\u4e2d\u66f4\u4e3a\u4e25\u91cd\uff0c\u4e9f\u9700\u6709\u6548\u624b\u6bb5\u6062\u590d\u5176\u53ef\u7528\u6027\u3002", "method": "\u5f00\u53d1CodeR\u00b3\u7cfb\u7edf\uff0c\u7ed3\u5408\u751f\u6210\u5f0fAI\u5206\u6790\u5931\u6548\u5de5\u4f5c\u6d41\u7279\u5f81\uff0c\u5c06\u5176\u81ea\u52a8\u8fc1\u79fb\u81f3\u73b0\u4ee3\u5de5\u4f5c\u6d41\u5e73\u53f0\uff0c\u5e76\u96c6\u6210\u9010\u6b65\u5206\u6790\u53ef\u89c6\u5316\u3001\u81ea\u52a8\u5316\u670d\u52a1\u66ff\u6362\u53ca\u4eba\u673a\u534f\u540c\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u901a\u8fc7\u591a\u4e2aTaverna\u5de5\u4f5c\u6d41\u590d\u5174\u6848\u4f8b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff1b\u81ea\u52a8\u5316\u663e\u8457\u964d\u4f4e\u4e86\u5de5\u4f5c\u6d41\u89e3\u6790\u4e0e\u670d\u52a1\u8bc6\u522b\u7684\u4eba\u5de5\u6210\u672c\uff0c\u4f46\u670d\u52a1\u66ff\u6362\u4e0e\u6570\u636e\u9a8c\u8bc1\u4ecd\u4f9d\u8d56\u4eba\u7c7b\u4e13\u5bb6\u3002\u8ba1\u5212\u6784\u5efa\u4f17\u5305\u5e73\u53f0\u652f\u6301\u793e\u533a\u534f\u4f5c\u590d\u5174\u4e0e\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u517c\u987e\u81ea\u52a8\u5316\u6548\u7387\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5de5\u4f5c\u6d41\u590d\u5174\u6846\u67b6\uff0c\u4e3a\u89e3\u51b3\u79d1\u5b66\u5de5\u4f5c\u6d41\u8870\u51cf\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.19464", "categories": ["cs.DC", "cs.AI", "cs.CR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.19464", "abs": "https://arxiv.org/abs/2511.19464", "authors": ["Marcio Pohlmann", "Alex Severo", "Geft\u00e9 Almeida", "Diego Kreutz", "Tiago Heinrich", "Louren\u00e7o Pereira"], "title": "Temperature in SLMs: Impact on Incident Categorization in On-Premises Environments", "comment": "5 pages, 3 figures, 2 tables, submitted to ERRC/WRSeg 2025", "summary": "SOCs and CSIRTs face increasing pressure to automate incident categorization, yet the use of cloud-based LLMs introduces costs, latency, and confidentiality risks. We investigate whether locally executed SLMs can meet this challenge. We evaluated 21 models ranging from 1B to 20B parameters, varying the temperature hyperparameter and measuring execution time and precision across two distinct architectures. The results indicate that temperature has little influence on performance, whereas the number of parameters and GPU capacity are decisive factors.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u672c\u5730\u90e8\u7f72\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u662f\u5426\u53ef\u7528\u4e8e\u5b89\u5168\u8fd0\u8425\u4e2d\u5fc3\uff08SOC\uff09\u548c\u8ba1\u7b97\u673a\u5b89\u5168\u4e8b\u4ef6\u54cd\u5e94\u56e2\u961f\uff08CSIRT\uff09\u7684\u81ea\u52a8\u5316\u4e8b\u4ef6\u5206\u7c7b\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e8621\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684\u6a21\u578b\uff0c\u53d1\u73b0\u6a21\u578b\u53c2\u6570\u91cf\u548cGPU\u6027\u80fd\u662f\u5173\u952e\u56e0\u7d20\uff0c\u800c\u6e29\u5ea6\u8d85\u53c2\u6570\u5f71\u54cd\u751a\u5fae\u3002", "motivation": "\u7531\u4e8e\u4f7f\u7528\u4e91\u7aef\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u5b89\u5168\u4e8b\u4ef6\u5206\u7c7b\u5b58\u5728\u6210\u672c\u3001\u5ef6\u8fdf\u548c\u4fdd\u5bc6\u6027\u98ce\u9669\uff0c\u56e0\u6b64\u63a2\u7d22\u5728\u672c\u5730\u8fd0\u884c\u7684\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u662f\u5426\u80fd\u6709\u6548\u66ff\u4ee3\u3002", "method": "\u8bc4\u4f30\u4e8621\u4e2a\u53c2\u6570\u89c4\u6a21\u4ece1B\u523020B\u4e0d\u7b49\u7684\u672c\u5730SLM\uff0c\u5728\u4e24\u79cd\u4e0d\u540c\u67b6\u6784\u4e0b\u8c03\u6574\u6e29\u5ea6\u8d85\u53c2\u6570\uff0c\u6d4b\u91cf\u5176\u6267\u884c\u65f6\u95f4\u548c\u5206\u7c7b\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6e29\u5ea6\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u5f88\u5c0f\uff0c\u800c\u6a21\u578b\u53c2\u6570\u6570\u91cf\u548cGPU\u5bb9\u91cf\u662f\u51b3\u5b9a\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u672c\u5730\u90e8\u7f72\u7684SLM\u5728\u9002\u5f53\u53c2\u6570\u89c4\u6a21\u548c\u786c\u4ef6\u652f\u6301\u4e0b\uff0c\u6709\u671b\u6ee1\u8db3SOC\u548cCSIRT\u5bf9\u81ea\u52a8\u5316\u4e8b\u4ef6\u5206\u7c7b\u7684\u9700\u6c42\uff0c\u540c\u65f6\u89c4\u907f\u4e91LLM\u5e26\u6765\u7684\u98ce\u9669\u3002"}}
{"id": "2511.20293", "categories": ["cs.DB", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.20293", "abs": "https://arxiv.org/abs/2511.20293", "authors": ["Chaowei He", "Yuanjun Liu", "Qingzhi Ma", "Shenyuan Ren", "Xizhao Luo", "Lei Zhao", "An Liu"], "title": "Forgetting by Pruning: Data Deletion in Join Cardinality Estimation", "comment": "AAAI26", "summary": "Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9762\u5411\u591a\u8868\u5b66\u4e60\u578b\u57fa\u6570\u4f30\u8ba1\u7cfb\u7edf\u7684\u673a\u5668\u9057\u5fd8\u6846\u67b6CEP\uff0c\u901a\u8fc7\u5206\u5e03\u654f\u611f\u526a\u679d\u548c\u57df\u526a\u679d\u6709\u6548\u5e94\u5bf9\u6570\u636e\u5220\u9664\u5e26\u6765\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u6781\u4f4e\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u663e\u8457\u4f18\u4e8e\u5b8c\u5168\u91cd\u8bad\u7ec3\u3002", "motivation": "\u5728\u5b66\u4e60\u578b\u57fa\u6570\u4f30\u8ba1\u7cfb\u7edf\u4e2d\uff0c\u7531\u4e8e\u591a\u8868\u5173\u7cfb\u6570\u636e\u5b58\u5728\u590d\u6742\u7684\u5206\u5e03\u4f9d\u8d56\u6027\uff0c\u673a\u5668\u9057\u5fd8\uff08\u5c24\u5176\u662f\u6570\u636e\u5220\u9664\uff09\u9762\u4e34\u5c5e\u6027\u7ea7\u654f\u611f\u6027\u3001\u8de8\u8868\u4f20\u64ad\u4ee5\u53ca\u503c\u57df\u6d88\u5931\u5bfc\u81f4\u7684\u591a\u8868\u8fde\u63a5\u4e25\u91cd\u9ad8\u4f30\u4e09\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faCardinality Estimation Pruning\uff08CEP\uff09\u6846\u67b6\uff0c\u5305\u542b\u5206\u5e03\u654f\u611f\u526a\u679d\uff08\u6784\u5efa\u534a\u8fde\u63a5\u5220\u9664\u7ed3\u679c\u5e76\u8ba1\u7b97\u654f\u611f\u5ea6\u5206\u6570\u4ee5\u6307\u5bfc\u53c2\u6570\u526a\u679d\uff09\u548c\u57df\u526a\u679d\uff08\u79fb\u9664\u56e0\u5220\u9664\u800c\u5b8c\u5168\u6d88\u5931\u7684\u503c\u57df\u652f\u6301\uff09\u3002", "result": "\u5728IMDB\u548cTPC-H\u6570\u636e\u96c6\u4e0a\u5bf9NeuroCard\u548cFACE\u6a21\u578b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCEP\u5728\u591a\u8868\u573a\u666f\u4e0b\u59cb\u7ec8\u53d6\u5f97\u6700\u4f4eQ-error\uff0c\u5c24\u5176\u5728\u9ad8\u5220\u9664\u6bd4\u4f8b\u4e0b\u5e38\u4f18\u4e8e\u5b8c\u5168\u91cd\u8bad\u7ec3\uff0c\u5e76\u5c06\u6536\u655b\u8fed\u4ee3\u6b21\u6570\u5927\u5e45\u51cf\u5c11\uff0c\u8ba1\u7b97\u5f00\u9500\u4ec5\u4e3a\u5fae\u8c03\u65f6\u95f4\u76840.3%-2.5%\u3002", "conclusion": "CEP\u662f\u9996\u4e2a\u4e13\u4e3a\u591a\u8868\u5b66\u4e60\u578b\u57fa\u6570\u4f30\u8ba1\u8bbe\u8ba1\u7684\u673a\u5668\u9057\u5fd8\u6846\u67b6\uff0c\u80fd\u9ad8\u6548\u3001\u51c6\u786e\u5730\u5904\u7406\u6570\u636e\u5220\u9664\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2511.19635", "categories": ["cs.SE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19635", "abs": "https://arxiv.org/abs/2511.19635", "authors": ["Abhi Chivukula", "Jay Somasundaram", "Vijay Somasundaram"], "title": "Agint: Agentic Graph Compilation for Software Engineering Agents", "comment": "18 pages, 5 figures, NeurIPS 2025: Deep Learning for Code in the Agentic Era", "summary": "LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.", "AI": {"tldr": "Agint \u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u667a\u80fd\u4f53\u7f16\u8bd1\u5668\u4e0e\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u9010\u6b65\u8f6c\u6362\u4e3a\u7c7b\u578b\u5316\u3001\u6548\u5e94\u611f\u77e5\u7684\u4ee3\u7801 DAG\uff0c\u89e3\u51b3\u4e86 LLM \u7f16\u7801\u667a\u80fd\u4f53\u5728\u4e0a\u4e0b\u6587\u7ba1\u7406\u3001\u5ef6\u8fdf\u3001\u53ef\u9760\u6027\u3001\u53ef\u590d\u73b0\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7f16\u7801\u667a\u80fd\u4f53\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u4e0a\u4e0b\u6587\u7ba1\u7406\u56f0\u96be\u3001\u5ef6\u8fdf\u9ad8\u3001\u53ef\u9760\u6027\u4e0d\u8db3\u3001\u7ed3\u679c\u4e0d\u53ef\u590d\u73b0\u4ee5\u53ca\u96be\u4ee5\u6269\u5c55\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u4e00\u79cd\u7ed3\u5408\u7f16\u8bd1\u5668\u6280\u672f\u4e0e\u667a\u80fd\u4f53\u67b6\u6784\u7684\u65b0\u8303\u5f0f\u6765\u63d0\u5347\u5f00\u53d1\u6548\u7387\u4e0e\u7cfb\u7edf\u6027\u80fd\u3002", "method": "Agint \u91c7\u7528\u5206\u5c42\u589e\u91cf\u5f0f\u7f16\u8bd1\u7b56\u7565\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u5316\u4e3a\u5177\u6709\u663e\u5f0f\u7c7b\u578b\u5c42\u7ea7\uff08\u6587\u672c \u2192 \u6570\u636e \u2192 \u89c4\u7ea6 \u2192 \u4ee3\u7801\uff09\u7684\u8bed\u4e49\u56fe\uff0c\u5e76\u7ed3\u5408\u6df7\u5408\u5f0f LLM \u4e0e\u51fd\u6570\u9a71\u52a8\u7684\u5373\u65f6\uff08JIT\uff09\u8fd0\u884c\u65f6\u3002\u5176\u6838\u5fc3\u7ec4\u4ef6\u5305\u62ec dagify\uff08DAG \u7f16\u8bd1\u5668\uff09\u3001dagent\uff08\u6df7\u5408 JIT \u8fd0\u884c\u65f6\uff09\u3001schemagin\uff08\u6a21\u5f0f\u751f\u6210\u5668\uff09\u548c datagin\uff08\u6570\u636e\u8f6c\u6362\u5668\uff09\uff0c\u652f\u6301\u52a8\u6001\u56fe\u4f18\u5316\u3001\u63a8\u6d4b\u6267\u884c\u3001\u5e76\u884c\u751f\u6210\u53ca\u4e0e\u73b0\u6709\u5f00\u53d1\u5de5\u5177\u7684\u4e92\u64cd\u4f5c\u3002", "result": "Agint \u5b9e\u73b0\u4e86\u66f4\u4f4e\u5ef6\u8fdf\u3001\u66f4\u9ad8\u541e\u5410\u91cf\u3001\u66f4\u9ad8\u6548\u4e0a\u4e0b\u6587\u5229\u7528\uff0c\u5e76\u652f\u6301\u4f7f\u7528\u66f4\u5c0f\u66f4\u5feb\u7684\u6a21\u578b\u8fdb\u884c\u53ef\u9760\u3001\u53ef\u7ec4\u5408\u7684\u5e76\u53d1\u4ee3\u7801\u751f\u6210\u3002\u5176\u56fe\u7ed3\u6784\u4fdd\u969c\u4e86\u53ef\u590d\u73b0\u6027\uff0c\u4e14\u901a\u8fc7 CLI \u4e0e GUI\uff08Agint Flow\uff09\u652f\u6301\u6280\u672f\u4eba\u5458\u4e0e\u975e\u6280\u672f\u4eba\u5458\u534f\u540c\u8fdb\u884c\u539f\u578b\u8bbe\u8ba1\u5230\u751f\u4ea7\u90e8\u7f72\u7684\u5168\u6d41\u7a0b\u3002", "conclusion": "Agint \u901a\u8fc7\u878d\u5408\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u7c7b\u578b\u5316\u56fe\u8868\u793a\u4e0e\u7f16\u8bd1\u5668\u6280\u672f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u7ec4\u5408\u3001\u56e2\u961f\u534f\u4f5c\u5bfc\u5411\u7684\u7f16\u7801\u667a\u80fd\u4f53\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u6269\u5c55\u3001\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u667a\u80fd\u7f16\u7a0b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.19468", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19468", "abs": "https://arxiv.org/abs/2511.19468", "authors": ["Blaise Ag\u00fcera y Arcas", "Travis Beals", "Maria Biggs", "Jessica V. Bloom", "Thomas Fischbacher", "Konstantin Gromov", "Urs K\u00f6ster", "Rishiraj Pravahan", "James Manyika"], "title": "Towards a future space-based, highly scalable AI infrastructure system design", "comment": "19 pages, 4 figures", "summary": "If AI is a foundational general-purpose technology, we should anticipate that demand for AI compute -- and energy -- will continue to grow. The Sun is by far the largest energy source in our solar system, and thus it warrants consideration how future AI infrastructure could most efficiently tap into that power. This work explores a scalable compute system for machine learning in space, using fleets of satellites equipped with solar arrays, inter-satellite links using free-space optics, and Google tensor processing unit (TPU) accelerator chips. To facilitate high-bandwidth, low-latency inter-satellite communication, the satellites would be flown in close proximity. We illustrate the basic approach to formation flight via a 81-satellite cluster of 1 km radius, and describe an approach for using high-precision ML-based models to control large-scale constellations. Trillium TPUs are radiation tested. They survive a total ionizing dose equivalent to a 5 year mission life without permanent failures, and are characterized for bit-flip errors. Launch costs are a critical part of overall system cost; a learning curve analysis suggests launch to low-Earth orbit (LEO) may reach $\\lesssim$\\$200/kg by the mid-2030s.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u536b\u661f\u7fa4\u7684\u592a\u7a7a\u673a\u5668\u5b66\u4e60\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5229\u7528\u592a\u9633\u80fd\u4f9b\u7535\u3001\u81ea\u7531\u7a7a\u95f4\u5149\u901a\u4fe1\u548c\u6297\u8f90\u5c04TPU\u82af\u7247\uff0c\u4ee5\u5e94\u5bf9\u672a\u6765AI\u5bf9\u7b97\u529b\u4e0e\u80fd\u6e90\u4e0d\u65ad\u589e\u957f\u7684\u9700\u6c42\u3002", "motivation": "\u7531\u4e8eAI\u4f5c\u4e3a\u57fa\u7840\u6027\u901a\u7528\u6280\u672f\u5c06\u5e26\u6765\u5bf9\u7b97\u529b\u548c\u80fd\u6e90\u9700\u6c42\u7684\u6301\u7eed\u589e\u957f\uff0c\u800c\u592a\u9633\u662f\u592a\u9633\u7cfb\u4e2d\u6700\u4e30\u5bcc\u7684\u80fd\u6e90\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u9ad8\u6548\u5229\u7528\u592a\u9633\u80fd\u652f\u6301\u672a\u6765\u7684AI\u57fa\u7840\u8bbe\u65bd\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u8bbe\u8ba1\u7531\u914d\u5907\u592a\u9633\u80fd\u9635\u5217\u3001\u661f\u95f4\u81ea\u7531\u7a7a\u95f4\u5149\u94fe\u8def\u548cGoogle TPU\u52a0\u901f\u82af\u7247\u7684\u536b\u661f\u7ec4\u6210\u7684\u8fd1\u8ddd\u7f16\u961f\u96c6\u7fa4\uff1b\u91c7\u7528\u9ad8\u7cbe\u5ea6\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63a7\u5236\u5927\u89c4\u6a21\u661f\u5ea7\uff0c\u5e76\u5bf9Trillium TPU\u8fdb\u884c\u8f90\u5c04\u6d4b\u8bd5\u4ee5\u9a8c\u8bc1\u5176\u5728\u8f68\u53ef\u9760\u6027\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u534a\u5f841\u516c\u91cc\u3001\u5305\u542b81\u9897\u536b\u661f\u7684\u96c6\u7fa4\u6784\u578b\uff1bTPU\u53ef\u627f\u53d7\u76f8\u5f53\u4e8e5\u5e74\u4efb\u52a1\u5bff\u547d\u7684\u603b\u7535\u79bb\u5242\u91cf\u4e14\u65e0\u6c38\u4e45\u6545\u969c\uff0c\u5e76\u91cf\u5316\u4e86\u4f4d\u7ffb\u8f6c\u9519\u8bef\uff1b\u9884\u8ba1\u52302030\u5e74\u4ee3\u4e2d\u671f\uff0cLEO\u53d1\u5c04\u6210\u672c\u53ef\u80fd\u964d\u81f3\u7ea6200\u7f8e\u5143/\u516c\u65a4\u3002", "conclusion": "\u6784\u5efa\u57fa\u4e8e\u592a\u9633\u80fd\u7684\u592a\u7a7aAI\u8ba1\u7b97\u7cfb\u7edf\u5728\u6280\u672f\u4e0a\u5177\u5907\u53ef\u884c\u6027\uff0c\u6709\u671b\u6210\u4e3a\u6ee1\u8db3\u672a\u6765AI\u80fd\u6e90\u4e0e\u7b97\u529b\u9700\u6c42\u7684\u4e00\u79cd\u9ad8\u6548\u65b9\u6848\u3002"}}
{"id": "2511.20419", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.20419", "abs": "https://arxiv.org/abs/2511.20419", "authors": ["Gianna Lisa Nicolai", "Patrick Hansert", "Sebastian Michel"], "title": "The Case for Intent-Based Query Rewriting", "comment": "Published in the 2nd International Workshop on Data-driven AI (DATAI) 2025", "summary": "With this work, we describe the concept of intent-based query rewriting and present a first viable solution. The aim is to allow rewrites to alter the structure and syntactic outcome of an original query while keeping the obtainable insights intact. This drastically differs from traditional query rewriting, which typically aims to decrease query evaluation time by using strict equivalence rules and optimization heuristics on the query plan. Rewriting queries to queries that only provide a similar insight but otherwise can be entirely different can remedy inaccessible original data tables due to access control, privacy, or expensive data access regarding monetary cost or remote access. In this paper, we put forward INQURE, a system designed for INtent-based QUery REwriting. It uses access to a large language model (LLM) for the query understanding and human-like derivation of alternate queries. Around the LLM, INQURE employs upfront table filtering and subsequent candidate rewrite pruning and ranking. We report on the results of an evaluation using a benchmark set of over 900 database table schemas and discuss the pros and cons of alternate approaches regarding runtime and quality of the rewrites of a user study.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u610f\u56fe\u7684\u67e5\u8be2\u91cd\u5199\u65b9\u6cd5INQURE\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u67e5\u8be2\u610f\u56fe\u5e76\u751f\u6210\u8bed\u4e49\u7b49\u6548\u4f46\u7ed3\u6784\u4e0d\u540c\u7684\u66ff\u4ee3\u67e5\u8be2\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u8bbf\u95ee\u53d7\u9650\u3001\u9690\u79c1\u6216\u6210\u672c\u7b49\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u67e5\u8be2\u91cd\u5199\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u7b49\u4ef7\u53d8\u6362\u4f18\u5316\u6267\u884c\u6548\u7387\uff0c\u4f46\u5728\u6570\u636e\u56e0\u8bbf\u95ee\u63a7\u5236\u3001\u9690\u79c1\u6216\u9ad8\u6210\u672c\u800c\u4e0d\u53ef\u7528\u65f6\u65e0\u6cd5\u53d1\u6325\u4f5c\u7528\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u4fdd\u7559\u539f\u59cb\u67e5\u8be2\u201c\u53ef\u83b7\u5f97\u6d1e\u5bdf\u201d\u4f46\u5141\u8bb8\u7ed3\u6784\u548c\u8bed\u6cd5\u53d8\u5316\u7684\u65b0\u578b\u91cd\u5199\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u7cfb\u7edfINQURE\uff0c\u56f4\u7ed5\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6784\u5efa\uff0c\u7ed3\u5408\u524d\u7f6e\u8868\u8fc7\u6ee4\u3001\u5019\u9009\u91cd\u5199\u751f\u6210\u3001\u526a\u679d\u4e0e\u6392\u5e8f\u673a\u5236\uff0c\u5b9e\u73b0\u57fa\u4e8e\u610f\u56fe\u7684\u67e5\u8be2\u91cd\u5199\u3002", "result": "\u5728\u5305\u542b900\u591a\u4e2a\u6570\u636e\u5e93\u8868\u6a21\u5f0f\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u4e86INQURE\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u5728\u91cd\u5199\u8d28\u91cf\u548c\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u7684\u4f18\u52a3\u3002", "conclusion": "\u57fa\u4e8e\u610f\u56fe\u7684\u67e5\u8be2\u91cd\u5199\u662f\u4e00\u79cd\u53ef\u884c\u7684\u65b0\u8303\u5f0f\uff0cINQURE\u5c55\u793a\u4e86\u5229\u7528LLM\u5b9e\u73b0\u8be5\u76ee\u6807\u7684\u6709\u6548\u6027\uff0c\u4e3a\u89e3\u51b3\u6570\u636e\u8bbf\u95ee\u9650\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.19875", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19875", "abs": "https://arxiv.org/abs/2511.19875", "authors": ["Qingyu Zhang", "Puzhuo Liu", "Peng Di", "Chenxiong Qian"], "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection", "comment": null, "summary": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u68c0\u6d4b\u63d0\u4ea4\u4fe1\u606f\u4e0e\u4ee3\u7801\u53d8\u66f4\u4e0d\u4e00\u81f4\uff08MCI\uff09\u7684\u57fa\u51c6CODEFUSE-COMMITEVAL\uff0c\u57fa\u4e8eApacheCM\u6570\u636e\u96c6\u6784\u5efa\u5305\u542b\u4e03\u7c7b\u4e0d\u4e00\u81f4\u6837\u672c\uff0c\u5e76\u8bc4\u4f30\u4e86\u516d\u79cd\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u589e\u5f3a\u7b56\u7565\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u4e0d\u4e00\u81f4\u6837\u672c\u8bc6\u522b\u6548\u679c\u4f18\u4e8e\u4e00\u81f4\u6837\u672c\uff0c\u4e14\u4e0d\u540c\u7c7b\u578b\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u96be\u5ea6\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u63d0\u4ea4\u4fe1\u606f\u5e38\u4e0e\u4ee3\u7801\u53d8\u66f4\u4e0d\u4e00\u81f4\uff08MCI\uff09\uff0c\u8bef\u5bfc\u5ba1\u67e5\u8005\u3001\u963b\u788d\u7ef4\u62a4\u3001\u6c61\u67d3\u7814\u7a76\u6570\u636e\u5e76\u53ef\u80fd\u63a9\u76d6\u5b89\u5168\u8865\u4e01\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30MCI\u68c0\u6d4b\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u57fa\u4e8eApacheCM\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u89c4\u5219\u5f15\u5bfc\u53d8\u5f02\u751f\u6210\u4e03\u7c7b\u4e0d\u4e00\u81f4\u63d0\u4ea4\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u53cc\u91cd\u9a8c\u8bc1\u786e\u4fdd\u6b63\u8d1f\u6837\u672c\u8d28\u91cf\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u8bc4\u4f30\u516d\u79cd\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u539f\u59cb\u8bbe\u7f6e\u53ca\u4e09\u79cd\u589e\u5f3a\u7b56\u7565\uff08\u5c11\u6837\u672c\u63d0\u793a\u3001\u601d\u7ef4\u94fe\u3001\u6269\u5c55\u4e0a\u4e0b\u6587\uff09\u4e0b\u7684MCI\u68c0\u6d4b\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5bf9\u4e0d\u4e00\u81f4\u63d0\u4ea4\u7684\u68c0\u6d4b\u66f4\u53ef\u9760\uff08\u5e73\u5747\u53ec\u56de\u738785.95%\uff0c\u7cbe\u786e\u738780.28%\uff0c\u7279\u5f02\u602763.8%\uff09\uff1bgpt-oss-20B\u6574\u4f53\u8868\u73b0\u6700\u4f73\u4f46token\u6d88\u8017\u8d85\u4e24\u500d\uff1b\u589e\u5f3a\u7b56\u7565\u6548\u679c\u5404\u5f02\uff1a\u90bb\u8fd1\u4e0a\u4e0b\u6587\u5bf9\u5927\u6a21\u578b\u6709\u76ca\u4f46\u5bf9\u5c0f\u6a21\u578b\u5f15\u5165\u566a\u58f0\uff0c\u5c11\u6837\u672c\u63d0\u5347\u51c6\u786e\u7387\u4f46\u589e\u52a0\u9519\u8bef\u9884\u6d4b\uff0c\u601d\u7ef4\u94fe\u63d0\u9ad8\u7cbe\u786e\u7387\u548c\u7279\u5f02\u6027\u4f46\u964d\u4f4e\u53ec\u56de\u7387\uff1b\u7ec4\u4ef6\u3001\u6587\u4ef6\u8def\u5f84\u548c\u64cd\u4f5c\u7c7b\u4e0d\u4e00\u81f4\u6027\u66f4\u6613\u68c0\u6d4b\uff0c\u800c\u610f\u56fe\u5c42\u9762\u7684\u201c\u76ee\u7684\u201d\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u51c6\u786e\u7387\u4f4e\u4e14token\u6210\u672c\u9ad8\u3002", "conclusion": "CODEFUSE-COMMITEVAL\u4e3aMCI\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e25\u8c28\u7684\u8bc4\u4f30\u57fa\u7840\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u6355\u6349\u9ad8\u5c42\u8bed\u4e49\u5dee\u8ddd\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u9700\u5f15\u5165\u66f4\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u548c\u5e73\u8861\u7684\u6570\u636e\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2511.19479", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19479", "abs": "https://arxiv.org/abs/2511.19479", "authors": ["Sangam Ghimire", "Paribartan Timalsina", "Nirjal Bhurtel", "Bishal Neupane", "Bigyan Byanju Shrestha", "Subarna Bhattarai", "Prajwal Gaire", "Jessica Thapa", "Sudan Jha"], "title": "Federated Learning Framework for Scalable AI in Heterogeneous HPC and Cloud Environments", "comment": null, "summary": "As the demand grows for scalable and privacy-aware AI systems, Federated Learning (FL) has emerged as a promising solution, allowing decentralized model training without moving raw data. At the same time, the combination of high- performance computing (HPC) and cloud infrastructure offers vast computing power but introduces new complexities, especially when dealing with heteroge- neous hardware, communication limits, and non-uniform data. In this work, we present a federated learning framework built to run efficiently across mixed HPC and cloud environments. Our system addresses key challenges such as system het- erogeneity, communication overhead, and resource scheduling, while maintaining model accuracy and data privacy. Through experiments on a hybrid testbed, we demonstrate strong performance in terms of scalability, fault tolerance, and convergence, even under non-Independent and Identically Distributed (non-IID) data distributions and varied hardware. These results highlight the potential of federated learning as a practical approach to building scalable Artificial Intelligence (AI) systems in modern, distributed computing settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u6df7\u5408\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u4e0e\u4e91\u73af\u5883\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u5e94\u5bf9\u7cfb\u7edf\u5f02\u6784\u6027\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8d44\u6e90\u8c03\u5ea6\u7b49\u6311\u6218\uff0c\u5728\u4fdd\u969c\u6a21\u578b\u7cbe\u5ea6\u4e0e\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\uff0c\u5c55\u73b0\u51fa\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3001\u5bb9\u9519\u6027\u548c\u6536\u655b\u6027\u3002", "motivation": "\u968f\u7740\u5bf9\u53ef\u6269\u5c55\u4e14\u6ce8\u91cd\u9690\u79c1\u7684AI\u7cfb\u7edf\u9700\u6c42\u589e\u957f\uff0c\u8054\u90a6\u5b66\u4e60\u6210\u4e3a\u5728\u4e0d\u79fb\u52a8\u539f\u59cb\u6570\u636e\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u7684\u6709\u6548\u65b9\u6cd5\uff1b\u7136\u800c\uff0c\u5c06\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u4e8e\u6df7\u5408HPC\u4e0e\u4e91\u73af\u5883\u4e2d\u9762\u4e34\u786c\u4ef6\u5f02\u6784\u3001\u901a\u4fe1\u9650\u5236\u548c\u6570\u636e\u5206\u5e03\u4e0d\u5747\u7b49\u65b0\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e13\u4e3a\u6df7\u5408HPC\u4e0e\u4e91\u73af\u5883\u4f18\u5316\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u91cd\u70b9\u89e3\u51b3\u7cfb\u7edf\u5f02\u6784\u6027\u3001\u901a\u4fe1\u5f00\u9500\u548c\u8d44\u6e90\u8c03\u5ea6\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u6a21\u578b\u51c6\u786e\u6027\u548c\u6570\u636e\u9690\u79c1\u3002", "result": "\u5728\u6df7\u5408\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u548c\u591a\u6837\u5316\u786c\u4ef6\u6761\u4ef6\u4e0b\u4ecd\u5177\u6709\u4f18\u5f02\u7684\u53ef\u6269\u5c55\u6027\u3001\u5bb9\u9519\u80fd\u529b\u548c\u6536\u655b\u6027\u80fd\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u5728\u73b0\u4ee3\u5206\u5e03\u5f0f\u8ba1\u7b97\u73af\u5883\u4e2d\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u6784\u5efa\u53ef\u6269\u5c55\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2511.20403", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20403", "abs": "https://arxiv.org/abs/2511.20403", "authors": ["Andrea Lops", "Fedelucio Narducci", "Azzurra Ragone", "Michelantonio Trizio", "Claudio Barto"], "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework", "comment": "Accepted at 40th IEEE/ACM International Conference on Automated Software Engineering", "summary": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 AgoneTest\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210 Java \u5355\u5143\u6d4b\u8bd5\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5e76\u5f15\u5165 Classes2Test \u6570\u636e\u96c6\u548c\u7efc\u5408\u8bc4\u4f30\u6307\u6807\uff0c\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u6bd4\u8f83\u4e0d\u540c LLM \u548c\u63d0\u793a\u7b56\u7565\u7684\u6548\u679c\u3002", "motivation": "\u5355\u5143\u6d4b\u8bd5\u662f\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5173\u952e\u4f46\u8017\u8d39\u8d44\u6e90\u7684\u73af\u8282\u3002\u5f53\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u53ca\u5176\u63d0\u793a\u7b56\u7565\u5728\u751f\u6210\u5355\u5143\u6d4b\u8bd5\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efa AgoneTest \u6846\u67b6\uff0c\u6574\u5408 Classes2Test \u6570\u636e\u96c6\uff08\u5305\u542b Java \u88ab\u6d4b\u7c7b\u4e0e\u5176\u5bf9\u5e94\u6d4b\u8bd5\u7c7b\u7684\u6620\u5c04\uff09\uff0c\u5e76\u91c7\u7528\u53d8\u5f02\u5206\u6570\u3001\u6d4b\u8bd5\u5f02\u5473\u7b49\u9ad8\u7ea7\u8bc4\u4f30\u6307\u6807\uff0c\u5efa\u7acb\u7aef\u5230\u7aef\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u53ef\u7f16\u8bd1\u7684\u6d4b\u8bd5\u5b50\u96c6\u4e2d\uff0cLLM \u751f\u6210\u7684\u6d4b\u8bd5\u5728\u8986\u76d6\u7387\u548c\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u4e0a\u53ef\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u4eba\u5de5\u7f16\u5199\u7684\u6d4b\u8bd5\uff1b\u589e\u5f3a\u578b\u63d0\u793a\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u8d28\u91cf\u3002", "conclusion": "AgoneTest \u63ed\u793a\u4e86 LLM \u5728\u8f6f\u4ef6\u6d4b\u8bd5\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u6d4b\u8bd5\u5b9e\u8df5\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2511.19832", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.19832", "abs": "https://arxiv.org/abs/2511.19832", "authors": ["Aurelio Vivas", "Harold Castro"], "title": "Enabling Scientific Workflow Scheduling Research in Non-Uniform Memory Access Architectures", "comment": null, "summary": "Data-intensive scientific workflows increasingly rely on high-performance computing (HPC) systems, complementing traditional Grid and Cloud platforms. However, workflow scheduling on HPC infrastructures remains challenging due to the prevalence of non-uniform memory access (NUMA) architectures. These systems require schedulers to account for data locality not only across distributed environments but also within each node. Modern HPC nodes integrate multiple NUMA domains and heterogeneous memory regions, such as high-bandwidth memory (HBM) and DRAM, and frequently attach accelerators (GPUs or FPGAs) and network interface cards (NICs) to specific NUMA nodes. This design increases the variability of data-access latency and complicates the placement of both tasks and data. Despite these constraints, most workflow scheduling strategies were originally developed for Grid or Cloud environments and rarely incorporate NUMA-aware considerations. To address this gap, this work introduces nFlows, a NUMA-aware Workflow Execution Runtime System that enables the modeling, bare-metal execution, simulation, and validation of scheduling algorithms for data-intensive workflows on NUMA-based HPC systems. The system's design, implementation, and validation methodology are presented. nFlows supports the construction of simulation models and their direct execution on physical systems, enabling studies of NUMA effects on scheduling, the design of NUMA-aware algorithms, the analysis of data-movement behavior, the identification of performance bottlenecks, and the exploration of in-memory workflow execution.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86nFlows\uff0c\u4e00\u4e2a\u9762\u5411NUMA\u67b6\u6784\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u5de5\u4f5c\u6d41\u6267\u884c\u8fd0\u884c\u65f6\u7cfb\u7edf\uff0c\u7528\u4e8e\u5efa\u6a21\u3001\u88f8\u673a\u6267\u884c\u3001\u4eff\u771f\u548c\u9a8c\u8bc1\u6570\u636e\u5bc6\u96c6\u578b\u5de5\u4f5c\u6d41\u7684\u8c03\u5ea6\u7b97\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u5de5\u4f5c\u6d41\u8c03\u5ea6\u7b56\u7565\u9488\u5bf9\u7f51\u683c\u6216\u4e91\u73af\u5883\u8bbe\u8ba1\uff0c\u672a\u5145\u5206\u8003\u8651HPC\u7cfb\u7edf\u4e2d\u666e\u904d\u5b58\u5728\u7684\u975e\u7edf\u4e00\u5185\u5b58\u8bbf\u95ee\uff08NUMA\uff09\u67b6\u6784\u6240\u5e26\u6765\u7684\u6570\u636e\u5c40\u90e8\u6027\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0nFlows\u7cfb\u7edf\uff0c\u652f\u6301\u6784\u5efa\u4eff\u771f\u6a21\u578b\u5e76\u5728\u7269\u7406\u7cfb\u7edf\u4e0a\u76f4\u63a5\u6267\u884c\uff0c\u4ee5\u7814\u7a76NUMA\u5bf9\u8c03\u5ea6\u7684\u5f71\u54cd\u3001\u8bbe\u8ba1NUMA\u611f\u77e5\u7b97\u6cd5\u3001\u5206\u6790\u6570\u636e\u79fb\u52a8\u884c\u4e3a\u7b49\u3002", "result": "nFlows\u80fd\u591f\u6709\u6548\u652f\u6301NUMA\u611f\u77e5\u8c03\u5ea6\u7b97\u6cd5\u7684\u7814\u7a76\u4e0e\u9a8c\u8bc1\uff0c\u8bc6\u522b\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u63a2\u7d22\u5185\u5b58\u4e2d\u5de5\u4f5c\u6d41\u6267\u884c\u7684\u4f18\u5316\u7a7a\u95f4\u3002", "conclusion": "nFlows\u4e3a\u5728NUMA\u67b6\u6784HPC\u7cfb\u7edf\u4e0a\u9ad8\u6548\u6267\u884c\u6570\u636e\u5bc6\u96c6\u578b\u79d1\u5b66\u5de5\u4f5c\u6d41\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8c03\u5ea6\u7b56\u7565\u5728NUMA\u611f\u77e5\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2511.19949", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.19949", "abs": "https://arxiv.org/abs/2511.19949", "authors": ["Qingda Hu", "Xinjun Yang", "Feifei Li", "Junru Li", "Ya Lin", "Yuqi Zhou", "Yicong Zhu", "Junwei Zhang", "Rongbiao Xie", "Ling Zhou", "Bin Wu", "Wenchao Zhou"], "title": "PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases", "comment": "13 pages, accepted by FAST'26", "summary": "In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters.", "AI": {"tldr": "PolarStore \u662f\u4e00\u79cd\u9762\u5411\u4e91\u539f\u751f\u5173\u7cfb\u578b\u6570\u636e\u5e93\u7684\u538b\u7f29\u5171\u4eab\u5b58\u50a8\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u7684\u53cc\u5c42\u538b\u7f29\u673a\u5236\uff0c\u5728\u663e\u8457\u964d\u4f4e\u5b58\u50a8\u6210\u672c\uff08\u7ea660%\uff09\u7684\u540c\u65f6\u4fdd\u6301\u4e0e\u672a\u538b\u7f29\u96c6\u7fa4\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u4e91\u539f\u751fRDBMS\u867d\u901a\u8fc7\u8ba1\u7b97\u4e0e\u5b58\u50a8\u5206\u79bb\u5b9e\u73b0\u4e86\u8ba1\u7b97\u5f39\u6027\uff0c\u4f46\u5b58\u50a8\u6210\u672c\u4ecd\u662f\u7528\u6237\u5173\u6ce8\u91cd\u70b9\u3002\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5728\u6027\u80fd\u5f00\u9500\u4e0e\u7075\u6d3b\u6027\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u6743\u8861\uff1a\u8f6f\u4ef6\u538b\u7f29\u5e26\u6765\u8f83\u5927\u6027\u80fd\u635f\u8017\uff0c\u786c\u4ef6\u538b\u7f29\u5219\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u6570\u636e\u5e93\u8d1f\u8f7d\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa PolarStore \u7cfb\u7edf\uff0c\u91c7\u7528\u53cc\u5c42\u538b\u7f29\u673a\u5236\uff1a\u7ed3\u5408 PolarCSD \u786c\u4ef6\u4e2d\u7684\u5b58\u50a8\u5185\u538b\u7f29\u4e0e\u8f6f\u4ef6\u4e2d\u7684\u8f7b\u91cf\u7ea7\u538b\u7f29\uff1b\u5f15\u5165\u9762\u5411\u6570\u636e\u5e93\u7684 I/O \u8def\u5f84\u4f18\u5316\uff1b\u57fa\u4e8e\u5927\u89c4\u6a21\u90e8\u7f72\u7ecf\u9a8c\uff0c\u6539\u8fdb PolarCSD \u786c\u4ef6\u4ee5\u63d0\u5347\u4e3b\u673a\u7a33\u5b9a\u6027\uff0c\u5e76\u8bbe\u8ba1\u538b\u7f29\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\u4ee5\u63d0\u9ad8\u96c6\u7fa4\u7a7a\u95f4\u6548\u7387\u3002", "result": "PolarStore \u5df2\u5728 PolarDB \u4e2d\u90e8\u7f72\u4e8e\u6570\u5343\u53f0\u5b58\u50a8\u670d\u52a1\u5668\uff0c\u7ba1\u7406\u8d85 100 PB \u6570\u636e\uff0c\u5b9e\u73b0 3.55 \u500d\u538b\u7f29\u7387\uff0c\u964d\u4f4e\u7ea6 60% \u5b58\u50a8\u6210\u672c\uff0c\u540c\u65f6\u6027\u80fd\u4e0e\u672a\u538b\u7f29\u96c6\u7fa4\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u548c\u6570\u636e\u5e93\u611f\u77e5\u4f18\u5316\uff0cPolarStore \u6210\u529f\u5728\u4e91\u539f\u751f RDBMS \u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u4ef7\u6bd4\u7684\u5b58\u50a8\u538b\u7f29\uff0c\u5728\u5927\u5e45\u8282\u7701\u6210\u672c\u7684\u540c\u65f6\u4fdd\u969c\u4e86\u5173\u952e\u8def\u5f84\u7684\u9ad8\u6027\u80fd\u3002"}}
{"id": "2511.20100", "categories": ["cs.DC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2511.20100", "abs": "https://arxiv.org/abs/2511.20100", "authors": ["Xinguo Zhu", "Shaohui Peng", "Jiaming Guo", "Yunji Chen", "Qi Guo", "Yuanbo Wen", "Hang Qin", "Ruizhi Chen", "Qirui Zhou", "Ke Gao", "Yanjun Wu", "Chen Zhao", "Ling Li"], "title": "QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation", "comment": "9 pages, 2 figures, accepted by AAAI 2026", "summary": "Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5b8f\u89c2\u601d\u8003\u5fae\u89c2\u7f16\u7801\u201d\uff08MTMC\uff09\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u6b63\u786e\u5730\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fdGPU\u5185\u6838\u3002\u8be5\u65b9\u6cd5\u5c06\u4f18\u5316\u7b56\u7565\u4e0e\u5177\u4f53\u5b9e\u73b0\u89e3\u8026\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u8f7b\u91cf\u7ea7LLM\u63a2\u7d22\u9ad8\u5c42\u4f18\u5316\u7b56\u7565\uff0c\u5e76\u5229\u7528\u901a\u7528LLM\u9010\u6b65\u5b9e\u73b0\u8fd9\u4e9b\u7b56\u7565\uff0c\u4ece\u800c\u5728\u51c6\u786e\u6027\u548c\u8fd0\u884c\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5927\u6a21\u578b\u548c\u4e13\u5bb6\u624b\u5de5\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u5f00\u53d1\u9ad8\u6027\u80fdGPU\u5185\u6838\u5bf9AI\u548c\u79d1\u5b66\u8ba1\u7b97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u624b\u5de5\u7f16\u5199\uff0c\u53ef\u79fb\u690d\u6027\u5dee\u3002\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\u5728\u751f\u6210\u5b8c\u6574\u5e95\u5c42\u4ee3\u7801\u65f6\u9762\u4e34\u6b63\u786e\u6027\u4e0e\u6548\u7387\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u6839\u6e90\u5728\u4e8e\u9700\u540c\u65f6\u63a2\u7d22\u5e9e\u5927\u7684\u4f18\u5316\u7b56\u7565\u7a7a\u95f4\u548c\u5b9e\u73b0\u7ec6\u8282\u7a7a\u95f4\u3002", "method": "\u63d0\u51faMTMC\u6846\u67b6\uff1a1\uff09Macro Thinking\u9636\u6bb5\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6307\u5bfc\u8f7b\u91cf\u7ea7LLM\u5b66\u4e60\u9ad8\u5c42\u8bed\u4e49\u4f18\u5316\u7b56\u7565\u4ee5\u6700\u5927\u5316\u786c\u4ef6\u5229\u7528\u7387\uff1b2\uff09Micro Coding\u9636\u6bb5\u5229\u7528\u901a\u7528LLM\u6839\u636eMacro Thinking\u63d0\u51fa\u7684\u9010\u6b65\u4f18\u5316\u5efa\u8bae\u589e\u91cf\u5f0f\u751f\u6210\u4ee3\u7801\uff0c\u907f\u514d\u4e00\u6b21\u6027\u751f\u6210\u6574\u4e2a\u5185\u6838\u5e26\u6765\u7684\u9519\u8bef\u3002", "result": "\u5728KernelBench\u4e0a\uff0cMTMC\u5728Level 1-2\u8fbe\u5230\u8fd1100%\u51c6\u786e\u7387\uff0cLevel 3\u8fbe70%\uff0c\u6bd4\u5f53\u524d\u6700\u4f18\u901a\u7528\u548c\u9886\u57df\u5fae\u8c03LLM\u9ad8\u51fa50%\u4ee5\u4e0a\uff0c\u5e76\u5b9e\u73b0\u6700\u9ad87.3\u500d\u4e8eLLM\u30012.2\u500d\u4e8ePyTorch Eager\u4e13\u5bb6\u5185\u6838\u7684\u901f\u5ea6\u63d0\u5347\uff1b\u5728\u66f4\u5177\u6311\u6218\u6027\u7684TritonBench\u4e0a\uff0c\u51c6\u786e\u7387\u8fbe59.64%\uff0c\u901f\u5ea6\u63d0\u5347\u8fbe34\u500d\u3002", "conclusion": "MTMC\u901a\u8fc7\u5206\u5c42\u89e3\u8026\u4f18\u5316\u7b56\u7565\u4e0e\u4ee3\u7801\u5b9e\u73b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728GPU\u5185\u6838\u751f\u6210\u4e2d\u6b63\u786e\u6027\u4e0e\u6548\u7387\u96be\u4ee5\u517c\u987e\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.20172", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20172", "abs": "https://arxiv.org/abs/2511.20172", "authors": ["Xinjun Yang", "Qingda Hu", "Junru Li", "Feifei Li", "Yuqi Zhou", "Yicong Zhu", "Qiuru Lin", "Jian Dai", "Yang Kong", "Jiayu Zhang", "Guoqiang Xu", "Qiang Liu"], "title": "Beluga: A CXL-Based Memory Architecture for Scalable and Efficient LLM KVCache Management", "comment": "13 pages, accepted by SIGMOD'26", "summary": "The rapid increase in LLM model sizes and the growing demand for long-context inference have made memory a critical bottleneck in GPU-accelerated serving systems. Although high-bandwidth memory (HBM) on GPUs offers fast access, its limited capacity necessitates reliance on host memory (CPU DRAM) to support larger working sets such as the KVCache. However, the maximum DRAM capacity is constrained by the limited number of memory channels per CPU socket. To overcome this limitation, current systems often adopt RDMA-based disaggregated memory pools, which introduce significant challenges including high access latency, complex communication protocols, and synchronization overhead. Fortunately, the emerging CXL technology introduces new opportunities in KVCache design. In this paper, we propose Beluga, a novel memory architecture that enables GPUs and CPUs to access a shared, large-scale memory pool through CXL switches. By supporting native load/store access semantics over the CXL fabric, our design delivers near-local memory latency, while reducing programming complexity and minimizing synchronization overhead. We conduct a systematic characterization of a commercial CXL switch-based memory pool and propose a set of design guidelines. Based on Beluga, we design and implement Beluga-KVCache, a system tailored for managing the large-scale KVCache in LLM inference. Beluga-KVCache achieves an 89.6% reduction in Time-To-First-Token (TTFT) and 7.35x throughput improvement in the vLLM inference engine compared to RDMA-based solutions. To the best of our knowledge, Beluga is the first system that enables GPUs to directly access large-scale memory pools through CXL switches, marking a significant step toward low-latency, shared access to vast memory resources by GPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBeluga\uff0c\u4e00\u79cd\u57fa\u4e8eCXL\u7684\u65b0\u578b\u5185\u5b58\u67b6\u6784\uff0c\u4f7fGPU\u548cCPU\u53ef\u901a\u8fc7CXL\u4ea4\u6362\u673a\u5171\u4eab\u5927\u89c4\u6a21\u5185\u5b58\u6c60\uff0c\u663e\u8457\u964d\u4f4eLLM\u63a8\u7406\u4e2dKVCache\u8bbf\u95ee\u7684\u5ef6\u8fdf\u5e76\u63d0\u5347\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89c4\u6a21\u6269\u5927\u548c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u9700\u6c42\u589e\u957f\uff0cGPU\u9ad8\u5e26\u5bbd\u5185\u5b58\uff08HBM\uff09\u5bb9\u91cf\u4e0d\u8db3\uff0c\u9700\u4f9d\u8d56\u4e3b\u673a\u5185\u5b58\uff08CPU DRAM\uff09\u5b58\u50a8KVCache\uff1b\u7136\u800cCPU\u5185\u5b58\u901a\u9053\u6570\u91cf\u6709\u9650\uff0c\u73b0\u6709\u57fa\u4e8eRDMA\u7684\u89e3\u8026\u5185\u5b58\u65b9\u6848\u5b58\u5728\u9ad8\u5ef6\u8fdf\u3001\u534f\u8bae\u590d\u6742\u548c\u540c\u6b65\u5f00\u9500\u5927\u7b49\u95ee\u9898\u3002", "method": "\u5229\u7528\u65b0\u5174CXL\u6280\u672f\u6784\u5efa\u5171\u4eab\u5185\u5b58\u6c60\u67b6\u6784Beluga\uff0c\u652f\u6301GPU\u901a\u8fc7CXL\u4ea4\u6362\u673a\u4ee5\u672c\u5730\u8bbf\u5b58\u8bed\u4e49\u76f4\u63a5\u8bbf\u95ee\u5927\u89c4\u6a21\u5185\u5b58\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5b9e\u73b0\u9762\u5411LLM\u63a8\u7406\u7684KVCache\u7ba1\u7406\u7cfb\u7edfBeluga-KVCache\u3002", "result": "\u5728vLLM\u63a8\u7406\u5f15\u64ce\u4e2d\uff0c\u76f8\u6bd4\u57fa\u4e8eRDMA\u7684\u65b9\u6848\uff0cBeluga-KVCache\u5c06\u9996Token\u751f\u6210\u65f6\u95f4\uff08TTFT\uff09\u51cf\u5c1189.6%\uff0c\u541e\u5410\u91cf\u63d0\u53477.35\u500d\u3002", "conclusion": "Beluga\u662f\u9996\u4e2a\u652f\u6301GPU\u901a\u8fc7CXL\u4ea4\u6362\u673a\u76f4\u63a5\u8bbf\u95ee\u5927\u89c4\u6a21\u5185\u5b58\u6c60\u7684\u7cfb\u7edf\uff0c\u4e3a\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u9ad8\u6548\u7387\u7684GPU\u5171\u4eab\u5185\u5b58\u8bbf\u95ee\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
