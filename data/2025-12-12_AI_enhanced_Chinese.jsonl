{"id": "2512.10217", "categories": ["cs.DB", "cs.IT", "math.PR"], "pdf": "https://arxiv.org/pdf/2512.10217", "abs": "https://arxiv.org/abs/2512.10217", "authors": ["Mahmoud Abo Khamis", "Hung Q. Ngo", "Dan Suciu"], "title": "PANDAExpress: a Simpler and Faster PANDA Algorithm", "comment": null, "summary": "PANDA is a powerful generic algorithm for answering conjunctive queries (CQs) and disjunctive datalog rules (DDRs) given input degree constraints. In the special case where degree constraints are cardinality constraints and the query is Boolean, PANDA runs in $\\tilde O (N^{subw})$-time, where $N$ is the input size, and $subw$ is the submodular width of the query, a notion introduced by Daniel Marx (JACM 2013). When specialized to certain classes of sub-graph pattern finding problems, the $\\tilde O(N^{subw})$ runtime matches the optimal runtime possible, modulo some conjectures in fine-grained complexity (Bringmann and Gorbachev (STOC 25)). The PANDA framework is much more general, as it handles arbitrary input degree constraints, which capture common statistics and integrity constraints used in relational database management systems, it works for queries with free variables, and for both CQs and DDRs.\n  The key weakness of PANDA is the large $polylog(N)$-factor hidden in the $\\tilde O(\\cdot)$ notation. This makes PANDA completely impractical, and fall short of what is achievable with specialized algorithms. This paper resolves this weakness with two novel ideas. First, we prove a new probabilistic inequality that upper-bounds the output size of DDRs under arbitrary degree constraints. Second, the proof of this inequality directly leads to a new algorithm named PANDAExpress that is both simpler and faster than PANDA. The novel feature of PANDAExpress is a new partitioning scheme that uses arbitrary hyperplane cuts instead of axis-parallel hyperplanes used in PANDA. These hyperplanes are dynamically constructed based on data-skewness statistics carefully tracked throughout the algorithm's execution. As a result, PANDAExpress removes the $polylog(N)$-factor from the runtime of PANDA, matching the runtimes of intricate specialized algorithms, while retaining all its generality and power.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPANDAExpress\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u6982\u7387\u4e0d\u7b49\u5f0f\u548c\u57fa\u4e8e\u6570\u636e\u504f\u659c\u52a8\u6001\u6784\u5efa\u7684\u8d85\u5e73\u9762\u5212\u5206\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u539fPANDA\u7b97\u6cd5\u4e2d\u9690\u85cf\u7684\u8f83\u5927polylog(N)\u56e0\u5b50\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u901a\u7528\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002", "motivation": "PANDA\u7b97\u6cd5\u867d\u5728\u7406\u8bba\u4e0a\u5177\u6709\u901a\u7528\u6027\u548c\u6700\u4f18\u65f6\u95f4\u590d\u6742\u5ea6\uff08\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff09\uff0c\u4f46\u5176\u8fd0\u884c\u65f6\u95f4\u4e2d\u9690\u85cf\u7684\u8f83\u5927polylog(N)\u56e0\u5b50\u4f7f\u5176\u5728\u5b9e\u8df5\u4e2d\u4e0d\u53ef\u884c\uff0c\u8fdc\u900a\u4e8e\u4e13\u7528\u7b97\u6cd5\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u65e2\u4fdd\u7559PANDA\u901a\u7528\u6027\u53c8\u5177\u5907\u5b9e\u7528\u6548\u7387\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc1\u660e\u4e86\u4e00\u4e2a\u65b0\u7684\u6982\u7387\u4e0d\u7b49\u5f0f\uff0c\u7528\u4e8e\u5728\u4efb\u610f\u5ea6\u7ea6\u675f\u4e0b\u4e0a\u754c\u4f30\u8ba1\u6790\u53d6\u6570\u636e\u65e5\u5fd7\u89c4\u5219\uff08DDRs\uff09\u7684\u8f93\u51fa\u89c4\u6a21\uff1b\u968f\u540e\uff0c\u57fa\u4e8e\u8be5\u4e0d\u7b49\u5f0f\u7684\u8bc1\u660e\u601d\u8def\uff0c\u8bbe\u8ba1\u4e86\u65b0\u7b97\u6cd5PANDAExpress\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u91c7\u7528\u52a8\u6001\u6784\u9020\u7684\u4efb\u610f\u65b9\u5411\u8d85\u5e73\u9762\uff08\u800c\u975ePANDA\u4e2d\u7684\u5750\u6807\u8f74\u5e73\u884c\u8d85\u5e73\u9762\uff09\u8fdb\u884c\u5212\u5206\uff0c\u5e76\u5728\u6574\u4e2a\u6267\u884c\u8fc7\u7a0b\u4e2d\u8ddf\u8e2a\u6570\u636e\u504f\u659c\u7edf\u8ba1\u4fe1\u606f\u4ee5\u6307\u5bfc\u8d85\u5e73\u9762\u9009\u62e9\u3002", "result": "PANDAExpress\u6210\u529f\u53bb\u9664\u4e86PANDA\u4e2d\u7684polylog(N)\u56e0\u5b50\uff0c\u5728\u4fdd\u6301\u5bf9\u4efb\u610f\u5ea6\u7ea6\u675f\u3001\u81ea\u7531\u53d8\u91cf\u3001\u5408\u53d6\u67e5\u8be2\uff08CQs\uff09\u4e0e\u6790\u53d6\u6570\u636e\u65e5\u5fd7\u89c4\u5219\uff08DDRs\uff09\u7684\u901a\u7528\u652f\u6301\u7684\u540c\u65f6\uff0c\u8fbe\u5230\u4e86\u4e0e\u590d\u6742\u4e13\u7528\u7b97\u6cd5\u76f8\u5339\u914d\u7684\u8fd0\u884c\u6548\u7387\u3002", "conclusion": "PANDAExpress\u5728\u4e0d\u727a\u7272PANDA\u6846\u67b6\u901a\u7528\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u9645\u8fd0\u884c\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u539f\u7b97\u6cd5\u56e0polylog\u56e0\u5b50\u8fc7\u5927\u800c\u7f3a\u4e4f\u5b9e\u7528\u6027\u7684\u5173\u952e\u7f3a\u9677\uff0c\u4e3a\u5904\u7406\u5e26\u5ea6\u7ea6\u675f\u7684\u590d\u6742\u67e5\u8be2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b0\u5de5\u5177\u3002"}}
{"id": "2512.10621", "categories": ["cs.DB", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.10621", "abs": "https://arxiv.org/abs/2512.10621", "authors": ["Siwoo Song", "Wonseok Shin", "Kunsoo Park", "Giuseppe F. Italiano", "Zhengyi Yang", "Wenjie Zhang"], "title": "Efficient Hypergraph Pattern Matching via Match-and-Filter and Intersection Constraint", "comment": null, "summary": "A hypergraph is a generalization of a graph, in which a hyperedge can connect multiple vertices, modeling complex relationships involving multiple vertices simultaneously. Hypergraph pattern matching, which is to find all isomorphic embeddings of a query hypergraph in a data hypergraph, is one of the fundamental problems. In this paper, we present a novel algorithm for hypergraph pattern matching by introducing (1) the intersection constraint, a necessary and sufficient condition for valid embeddings, which significantly speeds up the verification process, (2) the candidate hyperedge space, a data structure that stores potential mappings between hyperedges in the query hypergraph and the data hypergraph, and (3) the Match-and-Filter framework, which interleaves matching and filtering operations to maintain only compatible candidates in the candidate hyperedge space during backtracking. Experimental results on real-world datasets demonstrate that our algorithm significantly outperforms the state-of-the-art algorithms, by up to orders of magnitude in terms of query processing time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u56fe\u6a21\u5f0f\u5339\u914d\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4ea4\u96c6\u7ea6\u675f\u3001\u5019\u9009\u8d85\u8fb9\u7a7a\u95f4\u548cMatch-and-Filter\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u5904\u7406\u6548\u7387\u3002", "motivation": "\u8d85\u56fe\u6a21\u5f0f\u5339\u914d\u662f\u8d85\u56fe\u5206\u6790\u4e2d\u7684\u57fa\u672c\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u5bf9\u591a\u5173\u7cfb\u65f6\u6548\u7387\u8f83\u4f4e\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a\uff081\uff09\u4ea4\u96c6\u7ea6\u675f\uff0c\u4f5c\u4e3a\u6709\u6548\u5d4c\u5165\u7684\u5145\u8981\u6761\u4ef6\u4ee5\u52a0\u901f\u9a8c\u8bc1\uff1b\uff082\uff09\u5019\u9009\u8d85\u8fb9\u7a7a\u95f4\uff0c\u7528\u4e8e\u5b58\u50a8\u67e5\u8be2\u4e0e\u6570\u636e\u8d85\u56fe\u4e4b\u95f4\u53ef\u80fd\u7684\u8d85\u8fb9\u6620\u5c04\uff1b\uff083\uff09Match-and-Filter\u6846\u67b6\uff0c\u5728\u56de\u6eaf\u8fc7\u7a0b\u4e2d\u4ea4\u66ff\u6267\u884c\u5339\u914d\u4e0e\u8fc7\u6ee4\uff0c\u4ec5\u4fdd\u7559\u517c\u5bb9\u7684\u5019\u9009\u6620\u5c04\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u67e5\u8be2\u5904\u7406\u65f6\u95f4\u4e0a\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u4ea4\u96c6\u7ea6\u675f\u3001\u5019\u9009\u8d85\u8fb9\u7a7a\u95f4\u548cMatch-and-Filter\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d85\u56fe\u6a21\u5f0f\u5339\u914d\u7684\u6548\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.10227", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.10227", "abs": "https://arxiv.org/abs/2512.10227", "authors": ["Pengwei Liu", "Xingyu Ren", "Pengkai Wang", "Hangjie Yuan", "Zhongkai Hao", "Guanyu Chen", "Chao Xu", "Dong Ni", "Shengze Cai"], "title": "An Efficient Graph-Transformer Operator for Learning Physical Dynamics with Manifolds Embedding", "comment": "https://github.com/pengwei07/PhysGTO", "summary": "Accurate and efficient physical simulations are essential in science and engineering, yet traditional numerical solvers face significant challenges in computational cost when handling simulations across dynamic scenarios involving complex geometries, varying boundary/initial conditions, and diverse physical parameters. While deep learning offers promising alternatives, existing methods often struggle with flexibility and generalization, particularly on unstructured meshes, which significantly limits their practical applicability. To address these challenges, we propose PhysGTO, an efficient Graph-Transformer Operator for learning physical dynamics through explicit manifold embeddings in both physical and latent spaces. In the physical space, the proposed Unified Graph Embedding module aligns node-level conditions and constructs sparse yet structure-preserving graph connectivity to process heterogeneous inputs. In the latent space, PhysGTO integrates a lightweight flux-oriented message-passing scheme with projection-inspired attention to capture local and global dependencies, facilitating multilevel interactions among complex physical correlations. This design ensures linear complexity relative to the number of mesh points, reducing both the number of trainable parameters and computational costs in terms of floating-point operations (FLOPs), and thereby allowing efficient inference in real-time applications. We introduce a comprehensive benchmark spanning eleven datasets, covering problems with unstructured meshes, transient flow dynamics, and large-scale 3D geometries. PhysGTO consistently achieves state-of-the-art accuracy while significantly reducing computational costs, demonstrating superior flexibility, scalability, and generalization in a wide range of simulation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPhysGTO\uff0c\u4e00\u79cd\u57fa\u4e8e\u56feTransformer\u7684\u9ad8\u6548\u7b97\u5b50\uff0c\u901a\u8fc7\u5728\u7269\u7406\u7a7a\u95f4\u548c\u6f5c\u5728\u7a7a\u95f4\u4e2d\u663e\u5f0f\u5d4c\u5165\u6d41\u5f62\u7ed3\u6784\uff0c\u5b9e\u73b0\u5bf9\u590d\u6742\u7269\u7406\u52a8\u6001\u7684\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u8ba1\u7b97\u6210\u672c\u5efa\u6a21\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u7f51\u683c\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u5728\u5904\u7406\u590d\u6742\u51e0\u4f55\u3001\u591a\u53d8\u8fb9\u754c/\u521d\u59cb\u6761\u4ef6\u53ca\u591a\u6837\u7269\u7406\u53c2\u6570\u7684\u52a8\u6001\u6a21\u62df\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff1b\u800c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u7f51\u683c\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0e\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "PhysGTO\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u7269\u7406\u7a7a\u95f4\u4e2d\u7684\u7edf\u4e00\u56fe\u5d4c\u5165\u6a21\u5757\uff0c\u7528\u4e8e\u5bf9\u9f50\u8282\u70b9\u6761\u4ef6\u5e76\u6784\u5efa\u7a00\u758f\u4f46\u7ed3\u6784\u4fdd\u6301\u7684\u56fe\u8fde\u63a5\uff1b\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7ed3\u5408\u8f7b\u91cf\u7ea7\u901a\u91cf\u5bfc\u5411\u6d88\u606f\u4f20\u9012\u4e0e\u6295\u5f71\u542f\u53d1\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u6355\u6349\u5c40\u90e8\u4e0e\u5168\u5c40\u4f9d\u8d56\u5173\u7cfb\u3002\u6574\u4f53\u8bbe\u8ba1\u5177\u6709\u76f8\u5bf9\u4e8e\u7f51\u683c\u70b9\u6570\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "result": "\u5728\u6db5\u76d6\u975e\u7ed3\u6784\u5316\u7f51\u683c\u3001\u77ac\u6001\u6d41\u52a8\u548c\u5927\u89c4\u6a213D\u51e0\u4f55\u768411\u4e2a\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPhysGTO\u5728\u4fdd\u6301SOTA\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff08\u53c2\u6570\u91cf\u4e0eFLOPs\uff09\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u7075\u6d3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PhysGTO\u4e3a\u590d\u6742\u7269\u7406\u7cfb\u7edf\u7684\u9ad8\u6548\u3001\u51c6\u786e\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u5b9e\u7528\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u5b9e\u65f6\u63a8\u7406\u7684\u5de5\u7a0b\u4e0e\u79d1\u5b66\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.10089", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.10089", "abs": "https://arxiv.org/abs/2512.10089", "authors": ["Jeongeun Kim", "Sabrina Yarzada", "Paul Chen", "Christopher Torng"], "title": "Algorithm-Driven On-Chip Integration for High Density and Low Cost", "comment": null, "summary": "Growing interest in semiconductor workforce development has generated demand for platforms capable of supporting large numbers of independent hardware designs for research and training without imposing high per-project overhead. Traditional multi-project wafer (MPW) services based solely on physical co-placement have historically met this need, yet their scalability breaks down as project counts rise. Recent efforts towards scalable chip tapeouts mitigate these limitations by integrating many small designs within a shared die and attempt to amortize costly resources such as IO pads and memory macros. However, foundational principles for arranging, linking, and validating such densely integrated design sites have received limited systematic investigation. This work presents a new approach with three key techniques to address this gap. First, we establish a structured formulation of the design space that enables automated, algorithm-driven packing of many projects, replacing manual layout practices. Second, we introduce an architecture that exploits only the narrow-area regions between sites to deliver on off-chip communication and other shared needs. Third, we provide a practical approach for on-chip power domains enabling per-project power characterization at a standard laboratory bench and requiring no expertise in low-power ASIC design. Experimental results show that our approach achieves substantial area reductions of up to 13x over state-of-the-art physical-only aggregation methods, offering a scalable and cost-effective path forward for large-scale tapeout environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u53ef\u6269\u5c55\u82af\u7247\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bbe\u8ba1\u7a7a\u95f4\u3001\u7a84\u8fb9\u901a\u4fe1\u67b6\u6784\u548c\u7247\u4e0a\u7535\u6e90\u57df\u6280\u672f\uff0c\u663e\u8457\u964d\u4f4e\u591a\u9879\u76ee\u6676\u5706\uff08MPW\uff09\u4e2d\u6bcf\u4e2a\u9879\u76ee\u7684\u9762\u79ef\u5f00\u9500\uff0c\u76f8\u6bd4\u73b0\u6709\u7269\u7406\u805a\u5408\u65b9\u6cd5\u6700\u591a\u8282\u770113\u500d\u9762\u79ef\u3002", "motivation": "\u968f\u7740\u534a\u5bfc\u4f53\u4eba\u624d\u57f9\u8bad\u548c\u7814\u7a76\u5bf9\u652f\u6301\u5927\u91cf\u72ec\u7acb\u786c\u4ef6\u8bbe\u8ba1\u5e73\u53f0\u7684\u9700\u6c42\u589e\u957f\uff0c\u4f20\u7edf\u4ec5\u4f9d\u8d56\u7269\u7406\u5171\u5e03\u5c40\u7684\u591a\u9879\u76ee\u6676\u5706\uff08MPW\uff09\u670d\u52a1\u5728\u9879\u76ee\u6570\u91cf\u589e\u52a0\u65f6\u96be\u4ee5\u6269\u5c55\u3002\u73b0\u6709\u53ef\u6269\u5c55\u65b9\u6848\u867d\u5c1d\u8bd5\u6574\u5408\u5c0f\u578b\u8bbe\u8ba1\u5e76\u5206\u644aIO\u548c\u5b58\u50a8\u7b49\u6602\u8d35\u8d44\u6e90\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5bc6\u96c6\u96c6\u6210\u8bbe\u8ba1\u7ad9\u70b9\u6392\u5e03\u3001\u4e92\u8fde\u4e0e\u9a8c\u8bc1\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a1\uff09\u5efa\u7acb\u7ed3\u6784\u5316\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5b9e\u73b0\u7b97\u6cd5\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u9879\u76ee\u5e03\u5c40\uff1b2\uff09\u5229\u7528\u7ad9\u70b9\u95f4\u72ed\u7a84\u533a\u57df\u6784\u5efa\u7247\u5916\u901a\u4fe1\u53ca\u5171\u4eab\u529f\u80fd\u67b6\u6784\uff1b3\uff09\u8bbe\u8ba1\u5b9e\u7528\u7684\u7247\u4e0a\u7535\u6e90\u57df\u65b9\u6848\uff0c\u4f7f\u6bcf\u4e2a\u9879\u76ee\u53ef\u5728\u6807\u51c6\u5b9e\u9a8c\u5ba4\u73af\u5883\u4e0b\u8fdb\u884c\u529f\u8017\u8868\u5f81\uff0c\u65e0\u9700\u4f4e\u529f\u8017ASIC\u8bbe\u8ba1\u4e13\u4e1a\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7eaf\u7269\u7406\u805a\u5408\u65b9\u6cd5\uff0c\u6700\u591a\u53ef\u51cf\u5c1113\u500d\u7684\u9762\u79ef\u5f00\u9500\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u82af\u7247\u6d41\u7247\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b0\u8def\u5f84\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u9879\u76ee\u5bc6\u5ea6\u4e0b\u7684\u96c6\u6210\u4e0e\u9a8c\u8bc1\u96be\u9898\u3002"}}
{"id": "2512.09942", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09942", "abs": "https://arxiv.org/abs/2512.09942", "authors": ["Zhiming Liang", "Bin Chen", "Litao Ye", "Chen Sun", "Shuo Wang", "Zhe Peng"], "title": "A study of the spectrum resource leasing method based on ERC4907 extension", "comment": null, "summary": "The ERC4907 standard enables rentable Non-Fungible Tokens (NFTs) but is limited to single-user, single-time-slot authorization, which severely limits its applicability and efficiency in decentralized multi-slot scheduling scenarios. To address this limitation, this paper proposes Multi-slot ERC4907 (M-ERC4907) extension method. The M-ERC4907 method introduces novel functionalities to support the batch configuration of multiple time slots and simultaneous authorization of multiple users, thereby effectively eliminating the rigid sequential authorization constraint of ERC4907. The experiment was conducted on the Remix development platform. Experimental results show that the M-ERC4907 method significantly reduces on-chain transactions and overall Gas consumption, leading to enhanced scalability and resource allocation efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM-ERC4907\u7684ERC4907\u6807\u51c6\u6269\u5c55\u65b9\u6cd5\uff0c\u652f\u6301\u6279\u91cf\u914d\u7f6e\u591a\u4e2a\u65f6\u95f4\u6bb5\u548c\u591a\u7528\u6237\u540c\u65f6\u6388\u6743\uff0c\u4ece\u800c\u663e\u8457\u964d\u4f4e\u94fe\u4e0a\u4ea4\u6613\u6b21\u6570\u4e0eGas\u6d88\u8017\uff0c\u63d0\u5347\u53ef\u6269\u5c55\u6027\u4e0e\u8d44\u6e90\u5206\u914d\u6548\u7387\u3002", "motivation": "ERC4907\u6807\u51c6\u4ec5\u652f\u6301\u5355\u7528\u6237\u3001\u5355\u65f6\u95f4\u6bb5\u7684\u6388\u6743\u673a\u5236\uff0c\u5728\u53bb\u4e2d\u5fc3\u5316\u591a\u65f6\u95f4\u6bb5\u8c03\u5ea6\u573a\u666f\u4e2d\u5b58\u5728\u5e94\u7528\u5c40\u9650\u6027\u548c\u6548\u7387\u74f6\u9888\u3002", "method": "\u63d0\u51faM-ERC4907\u6269\u5c55\u65b9\u6cd5\uff0c\u5f15\u5165\u652f\u6301\u591a\u65f6\u95f4\u6bb5\u6279\u91cf\u914d\u7f6e\u548c\u591a\u7528\u6237\u5e76\u53d1\u6388\u6743\u7684\u65b0\u529f\u80fd\uff0c\u6253\u7834ERC4907\u539f\u6709\u7684\u987a\u5e8f\u6388\u6743\u9650\u5236\u3002", "result": "\u5728Remix\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cM-ERC4907\u663e\u8457\u51cf\u5c11\u4e86\u94fe\u4e0a\u4ea4\u6613\u6570\u91cf\u548c\u603bGas\u6d88\u8017\u3002", "conclusion": "M-ERC4907\u6709\u6548\u63d0\u5347\u4e86NFT\u79df\u8d41\u5728\u591a\u7528\u6237\u591a\u65f6\u6bb5\u573a\u666f\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u8d44\u6e90\u5229\u7528\u6548\u7387\uff0c\u662f\u5bf9ERC4907\u7684\u91cd\u8981\u6539\u8fdb\u3002"}}
{"id": "2512.10079", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10079", "abs": "https://arxiv.org/abs/2512.10079", "authors": ["Federico Formica", "Mark Lawford", "Claudio Menghi"], "title": "Search-based Software Testing Driven by Domain Knowledge: Reflections and New Perspectives", "comment": null, "summary": "Search-based Software Testing (SBST) can automatically generate test cases to search for requirements violations. Unlike manual test case development, it can generate a substantial number of test cases in a limited time. However, SBST does not possess the domain knowledge of engineers. Several techniques have been proposed to integrate engineers' domain knowledge within existing SBST frameworks. This paper will reflect on recent experimental results by highlighting bold and unexpected results. It will help re-examine SBST techniques driven by domain knowledge from a new perspective, suggesting new directions for future research.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u8fd1\u671f\u5c06\u5de5\u7a0b\u5e08\u9886\u57df\u77e5\u8bc6\u878d\u5165\u641c\u7d22\u5f0f\u8f6f\u4ef6\u6d4b\u8bd5\uff08SBST\uff09\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u4e00\u4e9b\u51fa\u4eba\u610f\u6599\u7684\u53d1\u73b0\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "SBST\u867d\u80fd\u81ea\u52a8\u751f\u6210\u5927\u91cf\u6d4b\u8bd5\u7528\u4f8b\uff0c\u4f46\u7f3a\u4e4f\u5de5\u7a0b\u5e08\u6240\u5177\u5907\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5982\u4f55\u6709\u6548\u878d\u5408\u9886\u57df\u77e5\u8bc6\u4ee5\u63d0\u5347\u6d4b\u8bd5\u6548\u679c\u3002", "method": "\u56de\u987e\u548c\u5206\u6790\u8fd1\u671f\u5c06\u9886\u57df\u77e5\u8bc6\u6574\u5408\u8fdbSBST\u6846\u67b6\u7684\u5b9e\u9a8c\u7814\u7a76\uff0c\u91cd\u70b9\u5173\u6ce8\u5176\u4e2d\u663e\u8457\u4e14\u610f\u5916\u7684\u7ed3\u679c\u3002", "result": "\u63ed\u793a\u4e86\u73b0\u6709\u57fa\u4e8e\u9886\u57df\u77e5\u8bc6\u7684SBST\u65b9\u6cd5\u4e2d\u4e00\u4e9b\u51fa\u4eba\u610f\u6599\u7684\u5b9e\u9a8c\u73b0\u8c61\uff0c\u6311\u6218\u4e86\u65e2\u6709\u5047\u8bbe\u3002", "conclusion": "\u9700\u4ece\u65b0\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u878d\u5408\u9886\u57df\u77e5\u8bc6\u7684SBST\u6280\u672f\uff0c\u5e76\u636e\u6b64\u63d0\u51fa\u672a\u6765\u7814\u7a76\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.10358", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.10358", "abs": "https://arxiv.org/abs/2512.10358", "authors": ["Runhao Liu", "Ziming Chen", "You Li", "Zequn Xie", "Peng Zhang"], "title": "Integrated Planning and Machine-Level Scheduling for High-Mix Discrete Manufacturing: A Profit-Driven Heuristic Framework", "comment": null, "summary": "Modern manufacturing enterprises struggle to create efficient and reliable production schedules under multi-variety, small-batch, and rush-order conditions. High-mix discrete manufacturing systems require jointly optimizing mid-term production planning and machine-level scheduling under heterogeneous resources and stringent delivery commitments. We address this problem with a profit-driven integrated framework that couples a mixed-integer planning model with a machine-level scheduling heuristic. The planning layer allocates production, accessory co-production, and outsourcing under aggregate economic and capacity constraints, while the scheduling layer refines these allocations using a structure-aware procedure that enforces execution feasibility and stabilizes daily machine behavior. This hierarchical design preserves the tractability of aggregated optimization while capturing detailed operational restrictions. Evaluations are conducted on a real industrial scenario. A flexible machine-level execution scheme yields 73.3% on-time completion and significant outsourcing demand, revealing bottleneck congestion. In contrast, a stability-enforcing execution policy achieves 100% on-time completion, eliminates all outsourcing, and maintains balanced machine utilization with only 1.9 to 4.6% capacity loss from changeovers. These results show that aligning planning decisions with stability-oriented execution rules enables practical and interpretable profit-maximizing decisions in complex manufacturing environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u6da6\u9a71\u52a8\u7684\u96c6\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e2d\u671f\u751f\u4ea7\u8ba1\u5212\u4e0e\u673a\u5668\u7ea7\u8c03\u5ea6\uff0c\u5728\u591a\u54c1\u79cd\u3001\u5c0f\u6279\u91cf\u548c\u7d27\u6025\u8ba2\u5355\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u53ef\u9760\u7684\u751f\u4ea7\u8c03\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u5236\u9020\u4f01\u4e1a\u5728\u591a\u54c1\u79cd\u3001\u5c0f\u6279\u91cf\u548c\u7d27\u6025\u8ba2\u5355\u6761\u4ef6\u4e0b\u96be\u4ee5\u5236\u5b9a\u9ad8\u6548\u53ef\u9760\u7684\u751f\u4ea7\u8ba1\u5212\uff0c\u9700\u5728\u5f02\u6784\u8d44\u6e90\u548c\u4e25\u683c\u4ea4\u4ed8\u627f\u8bfa\u4e0b\u8054\u5408\u4f18\u5316\u4e2d\u671f\u8ba1\u5212\u4e0e\u673a\u5668\u7ea7\u8c03\u5ea6\u3002", "method": "\u91c7\u7528\u5206\u5c42\u65b9\u6cd5\uff1a\u4e0a\u5c42\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u914d\u751f\u4ea7\u3001\u914d\u4ef6\u5171\u751f\u4ea7\u53ca\u5916\u5305\uff1b\u4e0b\u5c42\u4e3a\u7ed3\u6784\u611f\u77e5\u8c03\u5ea6\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u786e\u4fdd\u6267\u884c\u53ef\u884c\u6027\u548c\u673a\u5668\u884c\u4e3a\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4e1a\u573a\u666f\u4e2d\uff0c\u7a33\u5b9a\u6027\u4f18\u5148\u7684\u6267\u884c\u7b56\u7565\u5b9e\u73b0\u4e86100%\u51c6\u65f6\u4ea4\u4ed8\u3001\u96f6\u5916\u5305\uff0c\u5e76\u5728\u4ec5\u635f\u59311.9%\u20134.6%\u4ea7\u80fd\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u673a\u5668\u8d1f\u8f7d\u5747\u8861\u3002", "conclusion": "\u5c06\u8ba1\u5212\u51b3\u7b56\u4e0e\u7a33\u5b9a\u6027\u5bfc\u5411\u7684\u6267\u884c\u89c4\u5219\u5bf9\u9f50\uff0c\u53ef\u5728\u590d\u6742\u5236\u9020\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u89e3\u91ca\u4e14\u5b9e\u7528\u7684\u5229\u6da6\u6700\u5927\u5316\u8c03\u5ea6\u65b9\u6848\u3002"}}
{"id": "2512.10155", "categories": ["cs.AR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10155", "abs": "https://arxiv.org/abs/2512.10155", "authors": ["Jeongeun Kim", "Christopher Torng"], "title": "A Vertically Integrated Framework for Templatized Chip Design", "comment": null, "summary": "Developers who primarily engage with software often struggle to incorporate custom hardware into their applications, even though specialized silicon can provide substantial benefits to machine learning and AI, as well as to the application domains that they enable. This work investigates how a chip can be generated from a high-level object-oriented software specification, targeting introductory-level chip design learners with only very light performance requirements, while maintaining mental continuity between the chip layout and the software source program. In our approach, each software object is represented as a corresponding region on the die, producing a one-to-one structural mapping that preserves these familiar abstractions throughout the design flow. To support this mapping, we employ a modular construction strategy in which vertically composed IP blocks implement the behavioral protocols expressed in software. A direct syntactic translation, however, cannot meet hardware-level efficiency or communication constraints. For this reason, we leverage formal type systems based on sequences that check whether interactions between hardware modules adhere to the communication patterns described in the software model. We further examine hardware interconnect strategies for composing many such modules and develop layout techniques suited to this object-aligned design style. Together, these contributions preserve mental continuity from software to chip design for new learners and enables practical layout generation, ultimately reducing the expertise required for software developers to participate in chip creation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u9762\u5411\u521d\u5b66\u8005\u7684\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8f6f\u4ef6\u5bf9\u8c61\u76f4\u63a5\u6620\u5c04\u4e3a\u82af\u7247\u4e0a\u7684\u7269\u7406\u533a\u57df\uff0c\u4fdd\u6301\u4ece\u9ad8\u7ea7\u9762\u5411\u5bf9\u8c61\u7a0b\u5e8f\u5230\u82af\u7247\u5e03\u5c40\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u5e8f\u5217\u7684\u7c7b\u578b\u7cfb\u7edf\u786e\u4fdd\u6a21\u5757\u95f4\u901a\u4fe1\u7b26\u5408\u8f6f\u4ef6\u6a21\u578b\uff0c\u4ece\u800c\u964d\u4f4e\u8f6f\u4ef6\u5f00\u53d1\u8005\u53c2\u4e0e\u82af\u7247\u8bbe\u8ba1\u7684\u95e8\u69db\u3002", "motivation": "\u8f6f\u4ef6\u5f00\u53d1\u8005\u96be\u4ee5\u5c06\u5b9a\u5236\u786c\u4ef6\u96c6\u6210\u5230\u5176\u5e94\u7528\u4e2d\uff0c\u5c3d\u7ba1\u4e13\u7528\u82af\u7247\u5728\u673a\u5668\u5b66\u4e60\u548cAI\u7b49\u9886\u57df\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u4ece\u9ad8\u7ea7\u8f6f\u4ef6\u62bd\u8c61\u5230\u786c\u4ef6\u5b9e\u73b0\u7684\u76f4\u89c2\u6620\u5c04\uff0c\u589e\u52a0\u4e86\u521d\u5b66\u8005\u53c2\u4e0e\u82af\u7247\u8bbe\u8ba1\u7684\u96be\u5ea6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5bf9\u8c61\u5bf9\u9f50\u7684\u82af\u7247\u751f\u6210\u65b9\u6cd5\uff1a\u6bcf\u4e2a\u8f6f\u4ef6\u5bf9\u8c61\u5bf9\u5e94\u82af\u7247\u4e0a\u7684\u4e00\u4e2a\u7269\u7406\u533a\u57df\uff0c\u91c7\u7528\u5782\u76f4\u5806\u53e0\u7684IP\u6a21\u5757\u5b9e\u73b0\u8f6f\u4ef6\u884c\u4e3a\u534f\u8bae\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u5e8f\u5217\u7684\u7c7b\u578b\u7cfb\u7edf\u9a8c\u8bc1\u6a21\u5757\u95f4\u901a\u4fe1\u662f\u5426\u7b26\u5408\u8f6f\u4ef6\u6a21\u578b\uff1b\u540c\u65f6\u5f00\u53d1\u9002\u7528\u4e8e\u8be5\u8bbe\u8ba1\u98ce\u683c\u7684\u4e92\u8fde\u7b56\u7565\u4e0e\u5e03\u5c40\u6280\u672f\u3002", "result": "\u5b9e\u73b0\u4e86\u4ece\u9762\u5411\u5bf9\u8c61\u8f6f\u4ef6\u89c4\u8303\u5230\u82af\u7247\u5e03\u5c40\u7684\u4e00\u5bf9\u4e00\u7ed3\u6784\u6620\u5c04\uff0c\u5728\u6ee1\u8db3\u57fa\u672c\u6027\u80fd\u8981\u6c42\u7684\u524d\u63d0\u4e0b\uff0c\u4fdd\u6301\u4e86\u8f6f\u4ef6\u4e0e\u786c\u4ef6\u4e4b\u95f4\u7684\u8ba4\u77e5\u8fde\u7eed\u6027\uff0c\u5e76\u652f\u6301\u5b9e\u7528\u7684\u81ea\u52a8\u5e03\u5c40\u751f\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u4e86\u8f6f\u4ef6\u5f00\u53d1\u8005\u53c2\u4e0e\u82af\u7247\u8bbe\u8ba1\u6240\u9700\u7684\u4e13\u4e1a\u77e5\u8bc6\u95e8\u69db\uff0c\u4e3a\u8f6f\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9762\u5411\u521d\u5b66\u8005\u7684\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2512.09946", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09946", "abs": "https://arxiv.org/abs/2512.09946", "authors": ["Hung-Yueh Chiang", "Bokun Wang", "Diana Marculescu"], "title": "ELANA: A Simple Energy and Latency Analyzer for LLMs", "comment": null, "summary": "The latency and power consumption of large language models (LLMs) are major constraints when serving them across a wide spectrum of hardware platforms, from mobile edge devices to cloud GPU clusters. Benchmarking is crucial for optimizing efficiency in both model deployment and next-generation model development. To address this need, we open-source a simple profiling tool, \\textbf{ELANA}, for evaluating LLMs. ELANA is designed as a lightweight, academic-friendly profiler for analyzing model size, key-value (KV) cache size, prefilling latency (Time-to-first-token, TTFT), generation latency (Time-per-output-token, TPOT), and end-to-end latency (Time-to-last-token, TTLT) of LLMs on both multi-GPU and edge GPU platforms. It supports all publicly available models on Hugging Face and offers a simple command-line interface, along with optional energy consumption logging. Moreover, ELANA is fully compatible with popular Hugging Face APIs and can be easily customized or adapted to compressed or low bit-width models, making it ideal for research on efficient LLMs or for small-scale proof-of-concept studies. We release the ELANA profiling tool at: https://github.com/enyac-group/Elana.", "AI": {"tldr": "\u672c\u6587\u5f00\u6e90\u4e86\u4e00\u4e2a\u540d\u4e3aELANA\u7684\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u591aGPU\u548c\u8fb9\u7f18GPU\u5e73\u53f0\u4e0a\u7684\u5ef6\u8fdf\u3001\u7f13\u5b58\u5927\u5c0f\u3001\u80fd\u8017\u7b49\u5173\u952e\u6307\u6807\uff0c\u517c\u5bb9Hugging Face\u6240\u6709\u516c\u5f00\u6a21\u578b\u53caAPI\uff0c\u9002\u7528\u4e8e\u9ad8\u6548LLM\u7814\u7a76\u548c\u5c0f\u89c4\u6a21\u9a8c\u8bc1\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ece\u79fb\u52a8\u8fb9\u7f18\u8bbe\u5907\u5230\u4e91\u7aefGPU\u96c6\u7fa4\u7b49\u591a\u79cd\u786c\u4ef6\u5e73\u53f0\u4e0a\u90e8\u7f72\u65f6\uff0c\u9762\u4e34\u5ef6\u8fdf\u9ad8\u548c\u529f\u8017\u5927\u7684\u95ee\u9898\uff0c\u4e9f\u9700\u6709\u6548\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u6765\u4f18\u5316\u6a21\u578b\u90e8\u7f72\u6548\u7387\u5e76\u6307\u5bfc\u4e0b\u4e00\u4ee3\u6a21\u578b\u5f00\u53d1\u3002", "method": "\u5f00\u53d1\u5e76\u5f00\u6e90\u4e86\u540d\u4e3aELANA\u7684\u8f7b\u91cf\u7ea7\u547d\u4ee4\u884c\u6027\u80fd\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u6d4b\u91cf\u6a21\u578b\u5927\u5c0f\u3001KV\u7f13\u5b58\u5927\u5c0f\u3001\u9996token\u5ef6\u8fdf\uff08TTFT\uff09\u3001\u6bcftoken\u751f\u6210\u5ef6\u8fdf\uff08TPOT\uff09\u3001\u7aef\u5230\u7aef\u5ef6\u8fdf\uff08TTLT\uff09\u4ee5\u53ca\u53ef\u9009\u7684\u80fd\u8017\u65e5\u5fd7\uff0c\u517c\u5bb9Hugging Face\u6240\u6709\u516c\u5f00\u6a21\u578b\u548cAPI\uff0c\u5e76\u53ef\u9002\u914d\u538b\u7f29\u6216\u4f4e\u6bd4\u7279\u6a21\u578b\u3002", "result": "ELANA\u80fd\u591f\u6709\u6548\u5728\u591aGPU\u548c\u8fb9\u7f18GPU\u5e73\u53f0\u4e0a\u5bf9\u5404\u7c7bLLM\u8fdb\u884c\u6027\u80fd\u5256\u6790\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u5ef6\u8fdf\u4e0e\u8d44\u6e90\u4f7f\u7528\u6570\u636e\uff0c\u4e14\u6613\u4e8e\u5b9a\u5236\uff0c\u9002\u5408\u5b66\u672f\u7814\u7a76\u548c\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u3002", "conclusion": "ELANA\u4f5c\u4e3a\u4e00\u4e2a\u8f7b\u91cf\u3001\u6613\u7528\u4e14\u9ad8\u5ea6\u517c\u5bb9\u7684\u5f00\u6e90\u5de5\u5177\uff0c\u586b\u8865\u4e86LLM\u9ad8\u6548\u6027\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u9ad8\u6548\u5927\u6a21\u578b\u7684\u7814\u7a76\u4e0e\u90e8\u7f72\u3002"}}
{"id": "2512.10173", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10173", "abs": "https://arxiv.org/abs/2512.10173", "authors": ["Mantas Baksys", "Stefan Zetzsche", "Olivier Bouissou", "Remi Delmas", "Soonho Kong"], "title": "ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis", "comment": null, "summary": "Large language models have shown potential for program verification, but progress is hindered by the scarcity of verified code for training. We present ATLAS, an automated pipeline that synthesizes verified programs at scale to address this data bottleneck. ATLAS generates complete Dafny programs with specifications, implementations, and proofs, producing 2.7K verified programs from which we extract over 19K training examples--more than 7 per verified program--by decomposing the synthesis process into multiple specialized tasks. Fine-tuning Qwen 2.5 7B Coder on this dataset produces substantial gains: +23 percentage points on DafnyBench and +50 percentage points on DafnySynthesis. These results demonstrate that synthetic verified code can effectively enhance LLM capabilities for formal verification.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faATLAS\uff0c\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u5927\u89c4\u6a21\u5df2\u9a8c\u8bc1Dafny\u7a0b\u5e8f\u7684\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u5206\u89e3\u5408\u6210\u8fc7\u7a0b\u751f\u6210\u8d85\u8fc719K\u8bad\u7ec3\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7a0b\u5e8f\u9a8c\u8bc1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u5e26\u9a8c\u8bc1\u7684\u4ee3\u7801\u6570\u636e\u9650\u5236\u4e86\u5176\u53d1\u5c55\u3002", "method": "\u6784\u5efaATLAS\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5408\u6210\u5305\u542b\u89c4\u8303\u3001\u5b9e\u73b0\u548c\u8bc1\u660e\u7684\u5b8c\u6574Dafny\u7a0b\u5e8f\uff0c\u5e76\u5c06\u5408\u6210\u8fc7\u7a0b\u62c6\u89e3\u4e3a\u591a\u4e2a\u4e13\u95e8\u4efb\u52a1\u4ee5\u63d0\u53d6\u5927\u91cf\u8bad\u7ec3\u6837\u672c\uff1b\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u5fae\u8c03Qwen 2.5 7B Coder\u6a21\u578b\u3002", "result": "\u5728DafnyBench\u4e0a\u63d0\u534723\u4e2a\u767e\u5206\u70b9\uff0c\u5728DafnySynthesis\u4e0a\u63d0\u534750\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u5408\u6210\u7684\u5df2\u9a8c\u8bc1\u4ee3\u7801\u80fd\u6709\u6548\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2512.10396", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.10396", "abs": "https://arxiv.org/abs/2512.10396", "authors": ["Runhao Liu", "Ziming Chen", "You Li", "Peng Zhang"], "title": "Robust Crop Planning under Uncertainty: Aligning Economic Optimality with Agronomic Sustainability", "comment": null, "summary": "Long-horizon agricultural planning requires optimizing crop allocation under complex spatial heterogeneity, temporal agronomic dependencies, and multi-source environmental uncertainty. Existing approaches often treat crop interactions, such as legume-cereal complementarity, which implicitly or rely on static deterministic formulations that fail to guarantee resilience against market and climate volatility. To address these challenges, we propose a Multi-Layer Robust Crop Planning Framework (MLRCPF) that integrates spatial reasoning, temporal dynamics, and robust optimization. Specifically, we formalize crop-to-crop relationships through a structured interaction matrix embedded within the state-transition logic, and employ a distributionally robust optimization layer to mitigate worst-case risks defined by a data-driven ambiguity set. Evaluations on a real-world high-mix farming dataset from North China demonstrate the effectiveness of the proposed approach. The framework autonomously generates sustainable checkerboard rotation patterns that restore soil fertility, significantly increasing the legume planting ratio compared to deterministic baselines. Economically, it successfully resolves the trade-off between optimality and stability. These results highlight the importance of explicitly encoding domain-specific structural priors into optimization models for resilient decision-making in complex agricultural systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c42\u9c81\u68d2\u4f5c\u7269\u89c4\u5212\u6846\u67b6\uff08MLRCPF\uff09\uff0c\u901a\u8fc7\u878d\u5408\u7a7a\u95f4\u63a8\u7406\u3001\u65f6\u95f4\u52a8\u6001\u548c\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff0c\u5728\u8003\u8651\u4f5c\u7269\u95f4\u76f8\u4e92\u4f5c\u7528\u4e0e\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u751f\u6210\u53ef\u6301\u7eed\u7684\u8f6e\u4f5c\u6a21\u5f0f\uff0c\u63d0\u5347\u571f\u58e4\u80a5\u529b\u5e76\u5e73\u8861\u7ecf\u6d4e\u6536\u76ca\u4e0e\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u4f5c\u7269\u95f4\u7684\u590d\u6742\u4e92\u4f5c\u5173\u7cfb\uff08\u5982\u8c46\u7c7b\u4e0e\u8c37\u7269\u7684\u4e92\u8865\u6027\uff09\uff0c\u4e14\u591a\u91c7\u7528\u9759\u6001\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u96be\u4ee5\u5e94\u5bf9\u5e02\u573a\u4e0e\u6c14\u5019\u6ce2\u52a8\u5e26\u6765\u7684\u98ce\u9669\uff0c\u7f3a\u4e4f\u957f\u671f\u97e7\u6027\u3002", "method": "\u6784\u5efa\u591a\u5c42\u9c81\u68d2\u4f5c\u7269\u89c4\u5212\u6846\u67b6\uff08MLRCPF\uff09\uff1a1\uff09\u901a\u8fc7\u7ed3\u6784\u5316\u4ea4\u4e92\u77e9\u9635\u5728\u72b6\u6001\u8f6c\u79fb\u903b\u8f91\u4e2d\u5f62\u5f0f\u5316\u4f5c\u7269\u95f4\u5173\u7cfb\uff1b2\uff09\u5f15\u5165\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u6a21\u7cca\u96c6\u7684\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u5c42\u4ee5\u5e94\u5bf9\u6700\u574f\u60c5\u51b5\u98ce\u9669\u3002", "result": "\u5728\u534e\u5317\u9ad8\u6df7\u5408\u4f5c\u7269\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u81ea\u52a8\u751f\u6210\u6062\u590d\u571f\u58e4\u80a5\u529b\u7684\u68cb\u76d8\u5f0f\u8f6e\u4f5c\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u9ad8\u8c46\u7c7b\u79cd\u690d\u6bd4\u4f8b\uff0c\u5e76\u6709\u6548\u6743\u8861\u7ecf\u6d4e\u6700\u4f18\u6027\u4e0e\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5c06\u9886\u57df\u7279\u5b9a\u7684\u7ed3\u6784\u5148\u9a8c\uff08\u5982\u4f5c\u7269\u4e92\u4f5c\uff09\u663e\u5f0f\u5d4c\u5165\u4f18\u5316\u6a21\u578b\uff0c\u5bf9\u63d0\u5347\u590d\u6742\u519c\u4e1a\u7cfb\u7edf\u4e2d\u957f\u671f\u51b3\u7b56\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2512.10218", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10218", "abs": "https://arxiv.org/abs/2512.10218", "authors": ["Thanosan Prathifkumar", "Noble Saji Mathews", "Meiyappan Nagappan"], "title": "Does SWE-Bench-Verified Test Agent Ability or Model Memory?", "comment": null, "summary": "SWE-Bench-Verified, a dataset comprising 500 issues, serves as a de facto benchmark for evaluating various large language models (LLMs) on their ability to resolve GitHub issues. But this benchmark may overlap with model training data. If that is true, scores may reflect training recall, not issue-solving skill. To study this, we test two Claude models that frequently appear in top-performing agents submitted to the benchmark. We ask them to find relevant files using only issue text, and then issue text plus file paths. We then run the same setup on BeetleBox and SWE-rebench. Despite both benchmarks involving popular open-source Python projects, models performed 3 times better on SWE-Bench-Verified. They were also 6 times better at finding edited files, without any additional context about the projects themselves. This gap suggests the models may have seen many SWE-Bench-Verified tasks during training. As a result, scores on this benchmark may not reflect an agent's ability to handle real software issues, yet it continues to be used in ways that can misrepresent progress and lead to choices that favour agents that use certain models over strong agent design. Our setup tests the localization step with minimal context to the extent that the task should be logically impossible to solve. Our results show the risk of relying on older popular benchmarks and support the shift toward newer datasets built with contamination in mind.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6307\u51faSWE-Bench-Verified\u57fa\u51c6\u53ef\u80fd\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u91cd\u53e0\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u53cd\u6620\u7684\u662f\u8bb0\u5fc6\u80fd\u529b\u800c\u975e\u771f\u5b9e\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u98ce\u9669\u3002", "motivation": "\u5f53\u524d\u5e7f\u6cdb\u4f7f\u7528\u7684SWE-Bench-Verified\u57fa\u51c6\u53ef\u80fd\u56e0\u4e0e\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u91cd\u53e0\uff0c\u4f7f\u5f97\u8bc4\u4f30\u7ed3\u679c\u65e0\u6cd5\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u89e3\u51b3\u5b9e\u9645\u8f6f\u4ef6\u95ee\u9898\u7684\u80fd\u529b\uff0c\u4ece\u800c\u8bef\u5bfc\u7814\u7a76\u8fdb\u5c55\u548c\u6a21\u578b\u9009\u62e9\u3002", "method": "\u4f5c\u8005\u6d4b\u8bd5\u4e86\u4e24\u4e2a\u5728SWE-Bench-Verified\u4e0a\u8868\u73b0\u4f18\u5f02\u7684Claude\u6a21\u578b\uff0c\u5728\u4ec5\u63d0\u4f9b\u95ee\u9898\u63cf\u8ff0\u6216\u95ee\u9898\u63cf\u8ff0\u52a0\u6587\u4ef6\u8def\u5f84\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc4\u4f30\u5176\u5b9a\u4f4d\u76f8\u5173\u6587\u4ef6\u7684\u80fd\u529b\uff0c\u5e76\u5c06\u76f8\u540c\u8bbe\u7f6e\u5e94\u7528\u4e8eBeetleBox\u548cSWE-rebench\u4e24\u4e2a\u65b0\u57fa\u51c6\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u6a21\u578b\u5728SWE-Bench-Verified\u4e0a\u7684\u8868\u73b0\u6bd4\u5728\u5176\u4ed6\u4e24\u4e2a\u57fa\u51c6\u4e0a\u9ad8\u51fa3\u500d\uff0c\u4e14\u5728\u65e0\u989d\u5916\u4e0a\u4e0b\u6587\u60c5\u51b5\u4e0b\u5b9a\u4f4d\u88ab\u7f16\u8f91\u6587\u4ef6\u7684\u80fd\u529b\u9ad8\u51fa6\u500d\uff0c\u8868\u660e\u5176\u53ef\u80fd\u5728\u8bad\u7ec3\u4e2d\u63a5\u89e6\u8fc7SWE-Bench-Verified\u4e2d\u7684\u4efb\u52a1\u3002", "conclusion": "\u4f9d\u8d56\u53ef\u80fd\u5b58\u5728\u6570\u636e\u6c61\u67d3\u7684\u65e7\u6709\u70ed\u95e8\u57fa\u51c6\uff08\u5982SWE-Bench-Verified\uff09\u4f1a\u9ad8\u4f30\u6a21\u578b\u80fd\u529b\uff0c\u5e94\u8f6c\u5411\u8bbe\u8ba1\u65f6\u5c31\u8003\u8651\u6570\u636e\u6c61\u67d3\u95ee\u9898\u7684\u65b0\u57fa\u51c6\u4ee5\u66f4\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u7684\u771f\u5b9e\u6027\u80fd\u3002"}}
{"id": "2512.10231", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.10231", "abs": "https://arxiv.org/abs/2512.10231", "authors": ["Zhenguo Liu", "Chengao Shi", "Chen Ding", "Jiang Xu"], "title": "SemanticBBV: A Semantic Signature for Cross-Program Knowledge Reuse in Microarchitecture Simulation", "comment": "Accepted by ASP-DAC 2026 conference", "summary": "For decades, sampling-based techniques have been the de facto standard for accelerating microarchitecture simulation, with the Basic Block Vector (BBV) serving as the cornerstone program representation. Yet, the BBV's fundamental limitations: order-dependent IDs that prevent cross-program knowledge reuse and a lack of semantic content predictive of hardware performance have left a massive potential for optimization untapped.\n  To address these gaps, we introduce SemanticBBV, a novel, two-stage framework that generates robust, performance-aware signatures for cross-program simulation reuse. First, a lightweight RWKV-based semantic encoder transforms assembly basic blocks into rich Basic Block Embeddings (BBEs), capturing deep functional semantics. Second, an order-invariant Set Transformer aggregates these BBEs, weighted by execution frequency, into a final signature. Crucially, this stage is co-trained with a dual objective: a triplet loss for signature distinctiveness and a Cycles Per Instruction (CPI) regression task, directly imbuing the signature with performance sensitivity. Our evaluation demonstrates that SemanticBBV not only matches traditional BBVs in single-program accuracy but also enables unprecedented cross-program analysis. By simulating just 14 universal program points, we estimated the performance of ten SPEC CPU benchmarks with 86.3% average accuracy, achieving a 7143x simulation speedup. Furthermore, the signature shows strong adaptability to new microarchitectures with minimal fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSemanticBBV\uff0c\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u7f16\u7801\u548c\u987a\u5e8f\u4e0d\u53d8\u805a\u5408\u751f\u6210\u5177\u6709\u6027\u80fd\u611f\u77e5\u80fd\u529b\u7684\u57fa\u672c\u5757\u7b7e\u540d\uff0c\u5728\u4fdd\u6301\u5355\u7a0b\u5e8f\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u7a0b\u5e8f\u5fae\u67b6\u6784\u4eff\u771f\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u57fa\u672c\u5757\u5411\u91cf\uff08BBV\uff09\u7684\u91c7\u6837\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a\u4f9d\u8d56\u987a\u5e8f\u7684ID\u963b\u788d\u8de8\u7a0b\u5e8f\u77e5\u8bc6\u590d\u7528\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u786c\u4ef6\u6027\u80fd\u6709\u9884\u6d4b\u80fd\u529b\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u4eff\u771f\u4f18\u5316\u6f5c\u529b\u3002", "method": "SemanticBBV\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a\u9996\u5148\u4f7f\u7528\u8f7b\u91cf\u7ea7RWKV\u7f16\u7801\u5668\u5c06\u6c47\u7f16\u57fa\u672c\u5757\u8f6c\u5316\u4e3a\u5bcc\u542b\u8bed\u4e49\u7684\u57fa\u672c\u5757\u5d4c\u5165\uff08BBE\uff09\uff1b\u7136\u540e\u901a\u8fc7\u4e00\u4e2a\u6309\u6267\u884c\u9891\u7387\u52a0\u6743\u3001\u987a\u5e8f\u4e0d\u53d8\u7684Set Transformer\u805a\u5408BBE\uff0c\u5e76\u8054\u5408\u8bad\u7ec3\u4e09\u5143\u7ec4\u635f\u5931\u548cCPI\u56de\u5f52\u4efb\u52a1\uff0c\u4f7f\u6700\u7ec8\u7b7e\u540d\u5177\u5907\u6027\u80fd\u654f\u611f\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSemanticBBV\u5728\u5355\u7a0b\u5e8f\u4eff\u771f\u4e2d\u7cbe\u5ea6\u4e0e\u4f20\u7edfBBV\u76f8\u5f53\uff0c\u540c\u65f6\u4ec5\u9700\u6a21\u62df14\u4e2a\u901a\u7528\u7a0b\u5e8f\u70b9\u5373\u53ef\u5728\u5341\u4e2aSPEC CPU\u57fa\u51c6\u4e0a\u8fbe\u523086.3%\u7684\u5e73\u5747\u6027\u80fd\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5b9e\u73b07143\u500d\u7684\u4eff\u771f\u52a0\u901f\uff0c\u5e76\u5bf9\u65b0\u5fae\u67b6\u6784\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\u3002", "conclusion": "SemanticBBV\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edfBBV\u7684\u8bed\u4e49\u7f3a\u5931\u4e0e\u987a\u5e8f\u4f9d\u8d56\u95ee\u9898\uff0c\u4e3a\u8de8\u7a0b\u5e8f\u3001\u9ad8\u6027\u80fd\u611f\u77e5\u7684\u5fae\u67b6\u6784\u4eff\u771f\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u884c\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2512.10238", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10238", "abs": "https://arxiv.org/abs/2512.10238", "authors": ["Antu Saha"], "title": "Studying and Automating Issue Resolution for Software Quality", "comment": "3 pages", "summary": "Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u5347\u95ee\u9898\u62a5\u544a\u8d28\u91cf\u3001\u523b\u753b\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\u548c\u81ea\u52a8\u5316\u9ad8\u96be\u5ea6\u4fee\u590d\u4efb\u52a1\uff0c\u63a8\u52a8AI\u9a71\u52a8\u7684\u8f6f\u4ef6\u95ee\u9898\u89e3\u51b3\u3002", "motivation": "\u5f00\u53d1\u8005\u5728\u8f6f\u4ef6\u7ef4\u62a4\u4e2d\u5e38\u9762\u4e34\u4f4e\u8d28\u91cf\u7684\u95ee\u9898\u62a5\u544a\u3001\u5bf9\u771f\u5b9e\u5de5\u4f5c\u6d41\u7406\u89e3\u4e0d\u8db3\u4ee5\u53ca\u7f3a\u4e4f\u81ea\u52a8\u5316\u652f\u6301\u7b49\u6311\u6218\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u4e0e\u5e94\u7528\u7279\u5b9a\u4fe1\u606f\u6539\u8fdb\u95ee\u9898\u62a5\u544a\uff1b\u5b9e\u8bc1\u5206\u6790\u4f20\u7edf\u4e0eAI\u589e\u5f3a\u73af\u5883\u4e0b\u7684\u5f00\u53d1\u8005\u5de5\u4f5c\u6d41\uff1b\u5229\u7528\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u548cLLM\u5b9e\u73b0UI\u7f3a\u9677\u5b9a\u4f4d\u4e0e\u89e3\u51b3\u65b9\u6848\u8bc6\u522b\u7b49\u8ba4\u77e5\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8eAI\u8f85\u52a9\u5f00\u53d1\u7684\u5b9e\u8bc1\u6d1e\u5bdf\uff0c\u5f00\u53d1\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u5e76\u5b9e\u73b0\u4e86\u82e5\u5e72\u5173\u952e\u4fee\u590d\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u591a\u65b9\u5411\u534f\u540c\uff0c\u63d0\u5347\u4e86AI\u9a71\u52a8\u7684\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u6709\u52a9\u4e8e\u6784\u5efa\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u6613\u7ef4\u62a4\u7684\u8f6f\u4ef6\u7cfb\u7edf\u3002"}}
{"id": "2512.09963", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09963", "abs": "https://arxiv.org/abs/2512.09963", "authors": ["Phuong Tran", "Tzu-Hao Liu", "Long Tan Le", "Tung-Anh Nguyen", "Van Quan La", "Eason Yu", "Han Shu", "Choong Seon Hong", "Nguyen H. Tran"], "title": "GoodSpeed: Optimizing Fair Goodput with Adaptive Speculative Decoding in Distributed Edge Inference", "comment": "Accepted at INFOCOM 2026", "summary": "Large language models (LLMs) have revolutionized natural language processing, yet their high computational demands pose significant challenges for real-time inference, especially in multi-user server speculative decoding and resource-constrained environments. Speculative decoding has emerged as a promising technique to accelerate LLM inference by using lightweight draft models to generate candidate tokens, which are subsequently verified by a larger, more accurate model. However, ensuring both high goodput (the effective rate of accepted tokens) and fairness across multiple draft servers cooperating with a central verification server remains an open challenge. This paper introduces GOODSPEED, a novel distributed inference framework that optimizes goodput through adaptive speculative decoding. GOODSPEED employs a central verification server that coordinates a set of heterogeneous draft servers, each running a small language model to generate speculative tokens. To manage resource allocation effectively, GOODSPEED incorporates a gradient scheduling algorithm that dynamically assigns token verification tasks, maximizing a logarithmic utility function to ensure proportional fairness across servers. By processing speculative outputs from all draft servers in parallel, the framework enables efficient collaboration between the verification server and distributed draft generators, streamlining both latency and throughput. Through rigorous fluid sample path analysis, we show that GOODSPEED converges to the optimal goodput allocation in steady-state conditions and maintains near-optimal performance with provably bounded error under dynamic workloads. These results demonstrate that GOODSPEED provides a scalable, fair and efficient solution for multi- in distributed LLM inference systems.", "AI": {"tldr": "GOODSPEED \u662f\u4e00\u79cd\u7528\u4e8e\u5206\u5e03\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a8\u6d4b\u89e3\u7801\u548c\u57fa\u4e8e\u68af\u5ea6\u8c03\u5ea6\u7684\u8d44\u6e90\u5206\u914d\uff0c\u5728\u591a\u7528\u6237\u73af\u5883\u4e0b\u540c\u65f6\u4f18\u5316\u541e\u5410\u91cf\uff08goodput\uff09\u4e0e\u516c\u5e73\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u5728\u591a\u7528\u6237\u670d\u52a1\u5668\u73af\u5883\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u54cd\u5e94\uff1b\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5728\u591a\u8349\u7a3f\u670d\u52a1\u5668\u534f\u540c\u5de5\u4f5c\u65f6\u96be\u4ee5\u517c\u987e\u9ad8 goodput \u4e0e\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51fa GOODSPEED \u6846\u67b6\uff0c\u7531\u4e00\u4e2a\u4e2d\u5fc3\u9a8c\u8bc1\u670d\u52a1\u5668\u534f\u8c03\u591a\u4e2a\u5f02\u6784\u8349\u7a3f\u670d\u52a1\u5668\uff0c\u91c7\u7528\u68af\u5ea6\u8c03\u5ea6\u7b97\u6cd5\u52a8\u6001\u5206\u914d\u9a8c\u8bc1\u4efb\u52a1\uff0c\u5e76\u6700\u5927\u5316\u5bf9\u6570\u6548\u7528\u51fd\u6570\u4ee5\u5b9e\u73b0\u6bd4\u4f8b\u516c\u5e73\uff1b\u6240\u6709\u8349\u7a3f\u670d\u52a1\u5668\u5e76\u884c\u751f\u6210\u63a8\u6d4b token\uff0c\u63d0\u5347\u6574\u4f53\u6548\u7387\u3002", "result": "\u901a\u8fc7\u6d41\u4f53\u6837\u672c\u8def\u5f84\u5206\u6790\u8bc1\u660e GOODSPEED \u5728\u7a33\u6001\u4e0b\u6536\u655b\u5230\u6700\u4f18 goodput \u5206\u914d\uff0c\u5728\u52a8\u6001\u8d1f\u8f7d\u4e0b\u4e5f\u4fdd\u6301\u8fd1\u4f18\u6027\u80fd\u4e14\u8bef\u5dee\u6709\u7406\u8bba\u754c\uff1b\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3001\u516c\u5e73\u6027\u548c\u6548\u7387\u3002", "conclusion": "GOODSPEED \u4e3a\u5206\u5e03\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u541e\u5410\u91cf\u3001\u516c\u5e73\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.10393", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10393", "abs": "https://arxiv.org/abs/2512.10393", "authors": ["Guoqiang Chen", "Lingyun Ying", "Ziyang Song", "Daguang Liu", "Qiang Wang", "Zhiqi Wang", "Li Hu", "Shaoyin Cheng", "Weiming Zhang", "Nenghai Yu"], "title": "Cross-modal Retrieval Models for Stripped Binary Analysis", "comment": null, "summary": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86BinSeek\uff0c\u4e00\u79cd\u7528\u4e8e\u5265\u79bb\u7b26\u53f7\u4fe1\u606f\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e4b\u95f4\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u542bBinSeekEmbedding\u548cBinSeek-Reranker\u4e24\u4e2a\u6a21\u578b\uff0c\u5e76\u901a\u8fc7LLM\u9a71\u52a8\u7684\u6570\u636e\u5408\u6210\u6d41\u7a0b\u6784\u5efa\u8bad\u7ec3\u6570\u636e\u548c\u9886\u57df\u57fa\u51c6\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u667a\u80fd\u4f53\u7684\u4e8c\u8fdb\u5236\u4ee3\u7801\u5206\u6790\u4e2d\uff0c\u5982\u4f55\u4ece\u5927\u91cf\u5265\u79bb\u7b26\u53f7\u4fe1\u606f\u7684\u4e8c\u8fdb\u5236\u51fd\u6570\u4e2d\u6839\u636e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u51c6\u786e\u68c0\u7d22\u76f8\u5173\u51fd\u6570\u4ecd\u662f\u4e00\u4e2a\u672a\u88ab\u5145\u5206\u7814\u7a76\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u7b26\u53f7\u4fe1\u606f\u4f7f\u5176\u533a\u522b\u4e8e\u6e90\u4ee3\u7801\u68c0\u7d22\u3002", "method": "\u63d0\u51faBinSeek\u4e24\u9636\u6bb5\u8de8\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528BinSeekEmbedding\u6a21\u578b\u5b66\u4e60\u4e8c\u8fdb\u5236\u4ee3\u7801\u4e0e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u5173\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7BinSeek-Reranker\u6a21\u578b\u7ed3\u5408\u4e0a\u4e0b\u6587\u589e\u5f3a\u5bf9\u5019\u9009\u4ee3\u7801\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6027\u8fdb\u884c\u7cbe\u7ec6\u91cd\u6392\u5e8f\u3002\u540c\u65f6\u6784\u5efa\u4e86\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u8bad\u7ec3\u6570\u636e\u5408\u6210\u6d41\u6c34\u7ebf\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u9886\u57df\u57fa\u51c6\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cBinSeek\u5728Rec@3\u4e0a\u6bd4\u540c\u89c4\u6a21\u6a21\u578b\u63d0\u534731.42%\uff0c\u5728MRR@3\u4e0a\u63d0\u534727.17%\uff0c\u5e76\u4f18\u4e8e\u53c2\u6570\u91cf\u8fbe\u517616\u500d\u7684\u5148\u8fdb\u901a\u7528\u6a21\u578b\u3002", "conclusion": "BinSeek\u662f\u9996\u4e2a\u9762\u5411\u5265\u79bb\u7b26\u53f7\u4fe1\u606f\u4e8c\u8fdb\u5236\u4ee3\u7801\u5206\u6790\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u6846\u67b6\uff0c\u5728\u6027\u80fd\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u4e8c\u8fdb\u5236\u4ee3\u7801\u7406\u89e3\u4e0e\u68c0\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u548c\u57fa\u51c6\u3002"}}
{"id": "2512.10271", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10271", "abs": "https://arxiv.org/abs/2512.10271", "authors": ["Shruti Dongare", "Redwan Ibne Seraj Khan", "Hadeel Albahar", "Nannan Zhao", "Diego Melendez Maita", "Ali R. Butt"], "title": "Hybrid Learning and Optimization-Based Dynamic Scheduling for DL Workloads on Heterogeneous GPU Clusters", "comment": null, "summary": "Modern cloud platforms increasingly host large-scale deep learning (DL) workloads, demanding high-throughput, low-latency GPU scheduling. However, the growing heterogeneity of GPU clusters and limited visibility into application characteristics pose major challenges for existing schedulers, which often rely on offline profiling or application-specific assumptions. We present RLTune, an application-agnostic reinforcement learning (RL)-based scheduling framework that dynamically prioritizes and allocates DL jobs on heterogeneous GPU clusters. RLTune integrates RL-driven prioritization with MILP-based job-to-node mapping to optimize system-wide objectives such as job completion time (JCT), queueing delay, and resource utilization. Trained on large-scale production traces from Microsoft Philly, Helios, and Alibaba, RLTune improves GPU utilization by up to 20%, reduces queueing delay by up to 81%, and shortens JCT by as much as 70 percent. Unlike prior approaches, RLTune generalizes across diverse workloads without requiring per-job profiling, making it practical for cloud providers to deploy at scale for more efficient, fair, and sustainable DL workload management.", "AI": {"tldr": "RLTune \u662f\u4e00\u4e2a\u65e0\u9700\u5e94\u7528\u5148\u9a8c\u77e5\u8bc6\u7684\u5f3a\u5316\u5b66\u4e60\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff0c\u5728\u5f02\u6784 GPU \u96c6\u7fa4\u4e0a\u663e\u8457\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u5e76\u964d\u4f4e\u4f5c\u4e1a\u6392\u961f\u5ef6\u8fdf\u548c\u5b8c\u6210\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709 GPU \u8c03\u5ea6\u5668\u96be\u4ee5\u5e94\u5bf9\u65e5\u76ca\u5f02\u6784\u7684\u96c6\u7fa4\u73af\u5883\u548c\u5bf9\u5e94\u7528\u7279\u5f81\u7f3a\u4e4f\u53ef\u89c1\u6027\u7684\u95ee\u9898\uff0c\u901a\u5e38\u4f9d\u8d56\u79bb\u7ebf\u5206\u6790\u6216\u7279\u5b9a\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u89c4\u6a21\u4e91\u5e73\u53f0\u4e0a\u7684\u901a\u7528\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa RLTune \u6846\u67b6\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f5c\u4e1a\u52a8\u6001\u4f18\u5148\u7ea7\u6392\u5e8f\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e MILP \u7684\u4f5c\u4e1a\u5230\u8282\u70b9\u6620\u5c04\u7b56\u7565\uff0c\u4ee5\u4f18\u5316\u6574\u4f53\u7cfb\u7edf\u76ee\u6807\uff08\u5982\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u3001\u6392\u961f\u5ef6\u8fdf\u548c\u8d44\u6e90\u5229\u7528\u7387\uff09\u3002", "result": "\u5728\u5fae\u8f6f Philly\u3001Helios \u548c\u963f\u91cc\u4e91\u7684\u771f\u5b9e\u751f\u4ea7\u8f68\u8ff9\u4e0a\u9a8c\u8bc1\uff0cRLTune \u6700\u591a\u63d0\u5347 GPU \u5229\u7528\u7387 20%\uff0c\u51cf\u5c11\u6392\u961f\u5ef6\u8fdf 81%\uff0c\u7f29\u77ed\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\u8fbe 70%\u3002", "conclusion": "RLTune \u65e0\u9700\u9010\u4f5c\u4e1a\u5206\u6790\u5373\u53ef\u6cdb\u5316\u4e8e\u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u4e3a\u4e91\u670d\u52a1\u5546\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u516c\u5e73\u4e14\u53ef\u6269\u5c55\u7684\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u8c03\u5ea6\u65b9\u6848\u3002"}}
{"id": "2512.10452", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10452", "abs": "https://arxiv.org/abs/2512.10452", "authors": ["Yang Yang", "Li Kuang", "Jiakun Liu", "Zhongxin Liu", "Yingjie Xia", "David Lo"], "title": "UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval", "comment": "Accepted by the 48th IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "summary": "Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faUniCoR\uff0c\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u7edf\u4e00\u4e14\u9c81\u68d2\u7684\u4ee3\u7801\u8868\u793a\uff0c\u4ee5\u89e3\u51b3\u6df7\u5408\u67e5\u8be2\uff08\u81ea\u7136\u8bed\u8a00+\u4ee3\u7801\uff09\u5728\u8de8\u8bed\u8a00\u4ee3\u7801\u68c0\u7d22\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u3001\u6a21\u6001\u878d\u5408\u4f4e\u6548\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u5f31\u4e09\u5927\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6df7\u5408\u6a21\u5f0f\uff08\u81ea\u7136\u8bed\u8a00\u4e0e\u4ee3\u7801\u7247\u6bb5\u7ed3\u5408\uff09\u4e0b\u7684\u4ee3\u7801\u68c0\u7d22\uff0c\u5c24\u5176\u662f\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\uff0c\u96be\u4ee5\u6709\u6548\u5229\u7528\u6df7\u5408\u67e5\u8be2\uff0c\u5b58\u5728\u8bed\u4e49\u7406\u89e3\u4e0d\u8db3\u3001\u878d\u5408\u6548\u7387\u4f4e\u548c\u6cdb\u5316\u80fd\u529b\u5f31\u7684\u95ee\u9898\u3002", "method": "UniCoR\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u4e00\u662f\u591a\u89c6\u89d2\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u4ece\u4ee3\u7801-\u4ee3\u7801\u3001\u81ea\u7136\u8bed\u8a00-\u4ee3\u7801\u3001\u81ea\u7136\u8bed\u8a00-\u81ea\u7136\u8bed\u8a00\u4e09\u4e2a\u89c6\u89d2\u5bf9\u9f50\u8868\u793a\uff0c\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u548c\u6a21\u6001\u878d\u5408\uff1b\u4e8c\u662f\u8868\u793a\u5206\u5e03\u4e00\u81f4\u6027\u5b66\u4e60\u6a21\u5757\uff0c\u663e\u5f0f\u5bf9\u9f50\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u7684\u7279\u5f81\u5206\u5e03\uff0c\u63d0\u5347\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUniCoR\u5728MRR\u548cMAP\u6307\u6807\u4e0a\u5206\u522b\u5e73\u5747\u4f18\u4e8e\u6700\u5f3a\u57fa\u7ebf8.64%\u548c11.54%\uff0c\u5e76\u5728\u6df7\u5408\u68c0\u7d22\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7a33\u5b9a\u6027\u4e0e\u80fd\u529b\u3002", "conclusion": "UniCoR\u901a\u8fc7\u7edf\u4e00\u7684\u4ee3\u7801\u8868\u793a\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u6df7\u5408\u67e5\u8be2\u4e0b\u4ee3\u7801\u68c0\u7d22\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u4ee3\u7801\u68c0\u7d22\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.10312", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10312", "abs": "https://arxiv.org/abs/2512.10312", "authors": ["Julian Rodriguez", "Piotr Lopez", "Emiliano Lerma", "Rafael Medrano", "Jacobo Hernandez"], "title": "High-Dimensional Data Processing: Benchmarking Machine Learning and Deep Learning Architectures in Local and Distributed Environments", "comment": "8 pages, 3 figures", "summary": "This document reports the sequence of practices and methodologies implemented during the Big Data course. It details the workflow beginning with the processing of the Epsilon dataset through group and individual strategies, followed by text analysis and classification with RestMex and movie feature analysis with IMDb. Finally, it describes the technical implementation of a distributed computing cluster with Apache Spark on Linux using Scala.", "AI": {"tldr": "\u672c\u6587\u8bb0\u5f55\u4e86\u5927\u6570\u636e\u8bfe\u7a0b\u4e2d\u5b9e\u65bd\u7684\u4e00\u7cfb\u5217\u5b9e\u8df5\u4e0e\u65b9\u6cd5\uff0c\u5305\u62ecEpsilon\u6570\u636e\u96c6\u5904\u7406\u3001RestMex\u6587\u672c\u5206\u6790\u4e0e\u5206\u7c7b\u3001IMDb\u7535\u5f71\u7279\u5f81\u5206\u6790\uff0c\u4ee5\u53ca\u57fa\u4e8eLinux\u548cScala\u7684Apache Spark\u5206\u5e03\u5f0f\u96c6\u7fa4\u642d\u5efa\u3002", "motivation": "\u4e3a\u7cfb\u7edf\u6027\u5730\u638c\u63e1\u5927\u6570\u636e\u5904\u7406\u6280\u672f\uff0c\u901a\u8fc7\u8bfe\u7a0b\u9879\u76ee\u6574\u5408\u591a\u79cd\u5de5\u5177\u4e0e\u65b9\u6cd5\uff0c\u63d0\u5347\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5206\u6790\u4e0e\u5de5\u7a0b\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5206\u7ec4\u4e0e\u4e2a\u4eba\u7b56\u7565\u5904\u7406Epsilon\u6570\u636e\u96c6\uff1b\u5229\u7528RestMex\u8fdb\u884c\u6587\u672c\u5206\u6790\u4e0e\u5206\u7c7b\uff1b\u4f7f\u7528IMDb\u6570\u636e\u8fdb\u884c\u7535\u5f71\u7279\u5f81\u5206\u6790\uff1b\u5728Linux\u73af\u5883\u4e0b\u4f7f\u7528Scala\u6784\u5efa\u57fa\u4e8eApache Spark\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u96c6\u7fa4\u3002", "result": "\u6210\u529f\u5b8c\u6210\u4e86\u4ece\u6570\u636e\u9884\u5904\u7406\u3001\u6587\u672c\u4e0e\u7535\u5f71\u7279\u5f81\u5206\u6790\u5230\u5206\u5e03\u5f0f\u96c6\u7fa4\u90e8\u7f72\u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u6240\u9009\u65b9\u6cd5\u5728\u5b9e\u9645\u5927\u6570\u636e\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u8bfe\u7a0b\u5b9e\u8df5\u6709\u6548\u6574\u5408\u4e86\u591a\u79cd\u5927\u6570\u636e\u6280\u672f\uff0c\u4e3a\u540e\u7eed\u590d\u6742\u6570\u636e\u5206\u6790\u4e0e\u5206\u5e03\u5f0f\u7cfb\u7edf\u5f00\u53d1\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2512.10493", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10493", "abs": "https://arxiv.org/abs/2512.10493", "authors": ["Binquan Zhang", "Li Zhang", "Haoyuan Zhang", "Fang Liu", "Song Wang", "Bo Shen", "An Fu", "Lin Shi"], "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild", "comment": null, "summary": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8eLMSYS-Chat-1M\u548cWildChat\u6570\u636e\u96c6\uff0c\u5bf9\u4eba\u7c7b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7f16\u7801\u534f\u4f5c\u4e2d\u7684\u4ea4\u4e92\u6a21\u5f0f\u3001\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u53ca\u7528\u6237\u6ee1\u610f\u5ea6\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4efb\u52a1\u7c7b\u578b\u5982\u4f55\u5f71\u54cd\u4ea4\u4e92\u7ed3\u6784\u3001LLM\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u6307\u4ee4\u9075\u5faa\u8868\u73b0\u4ee5\u53ca\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f16\u7801\u573a\u666f\u4e2d\u534f\u4f5c\u673a\u5236\u7684\u7cfb\u7edf\u6027\u63a2\u7d22\uff0c\u5c24\u5176\u4e0d\u6e05\u695a\u7528\u6237\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7ecf\u5386\u7684\u56f0\u96be\u8def\u5f84\u3001LLM\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4ee5\u53ca\u7528\u6237\u6ee1\u610f\u5ea6\u60c5\u51b5\u3002", "method": "\u4f5c\u8005\u5229\u7528LMSYS-Chat-1M\u548cWildChat\u4e24\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u4eba\u673a\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5f00\u5c55\u5b9e\u8bc1\u5206\u6790\uff0c\u4ece\u4ea4\u4e92\u6a21\u5f0f\u3001\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u7814\u7a76\u4eba\u7c7b\u4e0eLLM\u5728\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u534f\u4f5c\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u4efb\u52a1\u7c7b\u578b\u5851\u9020\u4e86\u4e09\u79cd\u4ea4\u4e92\u6a21\u5f0f\uff08\u7ebf\u6027\u3001\u661f\u578b\u3001\u6811\u72b6\uff09\uff0c\u4e0d\u540c\u7f16\u7801\u4efb\u52a1\u504f\u597d\u4e0d\u540c\u6a21\u5f0f\uff1b2\uff09\u5728\u4ee3\u7801\u4fee\u590d\u4e0e\u91cd\u6784\u4efb\u52a1\u4e2d\uff0cLLM\u7684\u6307\u4ee4\u4e0d\u9075\u4ece\u7387\u663e\u8457\u9ad8\u4e8e\u4fe1\u606f\u67e5\u8be2\u4efb\u52a1\uff1b3\uff09\u4ee3\u7801\u8d28\u91cf\u4f18\u5316\u548c\u9700\u6c42\u9a71\u52a8\u5f00\u53d1\u4efb\u52a1\u7684\u7528\u6237\u6ee1\u610f\u5ea6\u8f83\u4f4e\uff0c\u800c\u7ed3\u6784\u5316\u77e5\u8bc6\u67e5\u8be2\u548c\u7b97\u6cd5\u8bbe\u8ba1\u4efb\u52a1\u6ee1\u610f\u5ea6\u8f83\u9ad8\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6539\u8fdbLLM\u7f16\u7801\u534f\u4f5c\u754c\u9762\u548c\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u8df5\u5efa\u8bae\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u5728\u81ea\u9002\u5e94\u5bf9\u8bdd\u7cfb\u7edf\u65b9\u5411\u7684\u7814\u7a76\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u6df1\u5316\u5bf9\u4eba\u673a\u534f\u540c\u7f16\u7a0b\u7684\u7406\u89e3\u3002"}}
{"id": "2512.10425", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10425", "abs": "https://arxiv.org/abs/2512.10425", "authors": ["Fan Yu", "Guodong Li", "Si Wu", "Weijun Fang", "Sihuang Hu"], "title": "Making Wide Stripes Practical: Cascaded Parity LRCs for Efficient Repair and High Reliability", "comment": null, "summary": "Erasure coding with wide stripes is increasingly adopted to reduce storage overhead in large-scale storage systems. However, existing Locally Repairable Codes (LRCs) exhibit structural limitations in this setting: inflated local groups increase single-node repair cost, multi-node failures frequently trigger expensive global repair, and reliability degrades sharply. We identify a key root cause: local and global parity blocks are designed independently, preventing them from cooperating during repair. We present Cascaded Parity LRCs (CP-LRCs), a new family of wide stripe LRCs that embed structured dependency between parity blocks by decomposing a global parity block across all local parity blocks. This creates a cascaded parity group that preserves MDS-level fault tolerance while enabling low-bandwidth single-node and multi-node repairs. We provide a general coefficient-generation framework, develop repair algorithms exploiting cascading, and instantiate the design with CP-Azure and CP-Uniform. Evaluations on Alibaba Cloud show reductions in repair time of up to 41% for single-node failures and 26% for two-node failures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5bbd\u6761\u5e26\u5c40\u90e8\u53ef\u4fee\u590d\u7f16\u7801\uff08CP-LRCs\uff09\uff0c\u901a\u8fc7\u5728\u5c40\u90e8\u4e0e\u5168\u5c40\u6821\u9a8c\u5757\u4e4b\u95f4\u5f15\u5165\u7ea7\u8054\u4f9d\u8d56\u7ed3\u6784\uff0c\u663e\u8457\u964d\u4f4e\u5355\u8282\u70b9\u548c\u591a\u8282\u70b9\u6545\u969c\u7684\u4fee\u590d\u5f00\u9500\uff0c\u5e76\u5728\u963f\u91cc\u4e91\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709LRC\u5728\u5bbd\u6761\u5e26\u573a\u666f\u4e0b\u9762\u4e34\u5c40\u90e8\u7ec4\u81a8\u80c0\u5bfc\u81f4\u5355\u8282\u70b9\u4fee\u590d\u6210\u672c\u9ad8\u3001\u591a\u8282\u70b9\u6545\u969c\u9891\u7e41\u89e6\u53d1\u6602\u8d35\u7684\u5168\u5c40\u4fee\u590d\u4ee5\u53ca\u53ef\u9760\u6027\u6025\u5267\u4e0b\u964d\u7b49\u95ee\u9898\uff0c\u6839\u672c\u539f\u56e0\u5728\u4e8e\u5c40\u90e8\u4e0e\u5168\u5c40\u6821\u9a8c\u5757\u72ec\u7acb\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u534f\u540c\u4fee\u590d\u3002", "method": "\u63d0\u51fa\u7ea7\u8054\u6821\u9a8cLRC\uff08CP-LRCs\uff09\uff0c\u5c06\u5168\u5c40\u6821\u9a8c\u5757\u5206\u89e3\u5e76\u5d4c\u5165\u6240\u6709\u5c40\u90e8\u6821\u9a8c\u5757\u4e2d\uff0c\u6784\u5efa\u7ea7\u8054\u6821\u9a8c\u7ec4\uff1b\u63d0\u4f9b\u901a\u7528\u7cfb\u6570\u751f\u6210\u6846\u67b6\uff0c\u8bbe\u8ba1\u5229\u7528\u7ea7\u8054\u7279\u6027\u7684\u4fee\u590d\u7b97\u6cd5\uff0c\u5e76\u5b9e\u4f8b\u5316\u4e3aCP-Azure\u548cCP-Uniform\u4e24\u79cd\u65b9\u6848\u3002", "result": "\u5728\u963f\u91cc\u4e91\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cCP-LRCs\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6700\u591a\u53ef\u51cf\u5c1141%\u7684\u5355\u8282\u70b9\u6545\u969c\u4fee\u590d\u65f6\u95f4\u548c26%\u7684\u53cc\u8282\u70b9\u6545\u969c\u4fee\u590d\u65f6\u95f4\u3002", "conclusion": "CP-LRCs\u901a\u8fc7\u5728\u5c40\u90e8\u4e0e\u5168\u5c40\u6821\u9a8c\u5757\u95f4\u5efa\u7acb\u7ed3\u6784\u5316\u4f9d\u8d56\uff0c\u5728\u4fdd\u6301MDS\u7ea7\u522b\u5bb9\u9519\u80fd\u529b\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bbd\u6761\u5e26\u5b58\u50a8\u7cfb\u7edf\u4e2d\u5355\u8282\u70b9\u4e0e\u591a\u8282\u70b9\u6545\u969c\u7684\u4fee\u590d\u6548\u7387\u3002"}}
{"id": "2512.10618", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.10618", "abs": "https://arxiv.org/abs/2512.10618", "authors": ["Georgia M. Kapitsaki", "Maria Papoutsoglou", "Christoph Treude", "Ioanna Theophilou"], "title": "Analyzing developer discussions on EU and US privacy legislation compliance in GitHub repositories", "comment": "40 pages", "summary": "Context: Privacy legislation has impacted the way software systems are developed, prompting practitioners to update their implementations. Specifically, the EU General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have forced the community to focus on users' data privacy. Despite the vast amount of data on developer issues available in GitHub repositories, there is a lack of empirical evidence on the issues developers of Open Source Software discuss to comply with privacy legislation. Method: In this work, we examine such discussions by mining and analyzing 32,820 issues from GitHub repositories. We partially analyzed the dataset automatically to identify law user rights and principles indicated, and manually analyzed a sample of 1,186 issues based on the type of concern addressed. Results: We devised 24 discussion categories placed in six clusters: features/bugs, consent-related, documentation, data storing/sharing, adaptability, and general compliance. Our results show that developers mainly focus on specific user rights from the legislation (right to erasure, right to opt-out, right to access), addressing other rights less frequently, while most discussions concern user consent, user rights functionality, bugs and cookies management. Conclusion: The created taxonomy can help practitioners understand which issues are discussed for law compliance, so that they ensure they address them first in their systems. In addition, the educational community can reshape curricula to better educate future engineers on the privacy law concerns raised, and the research community can identify gaps and areas for improvement to support and accelerate data privacy law compliance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790GitHub\u4e0a32,820\u4e2a\u8bae\u9898\uff0c\u8bc6\u522b\u51fa\u5f00\u6e90\u8f6f\u4ef6\u5f00\u53d1\u8005\u5728\u9075\u5b88GDPR\u548cCCPA\u7b49\u9690\u79c1\u6cd5\u89c4\u65f6\u8ba8\u8bba\u768424\u7c7b\u95ee\u9898\uff0c\u5e76\u5f52\u7eb3\u4e3a\u516d\u5927\u4e3b\u9898\u96c6\u7fa4\uff0c\u65e8\u5728\u5e2e\u52a9\u5b9e\u8df5\u8005\u3001\u6559\u80b2\u754c\u548c\u7814\u7a76\u754c\u66f4\u597d\u5730\u7406\u89e3\u548c\u5e94\u5bf9\u9690\u79c1\u5408\u89c4\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1\u9690\u79c1\u7acb\u6cd5\uff08\u5982GDPR\u548cCCPA\uff09\u5bf9\u8f6f\u4ef6\u5f00\u53d1\u4ea7\u751f\u4e86\u91cd\u5927\u5f71\u54cd\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u5f00\u6e90\u8f6f\u4ef6\u5f00\u53d1\u8005\u5982\u4f55\u5728\u5b9e\u9645\u5f00\u53d1\u4e2d\u8ba8\u8bba\u548c\u5904\u7406\u5408\u89c4\u95ee\u9898\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u7814\u7a76\u8005\u4eceGitHub\u4ed3\u5e93\u4e2d\u6316\u6398\u5e76\u5206\u6790\u4e8632,820\u4e2a\u8bae\u9898\uff0c\u90e8\u5206\u901a\u8fc7\u81ea\u52a8\u65b9\u6cd5\u8bc6\u522b\u6d89\u53ca\u7684\u6cd5\u5f8b\u6743\u5229\u4e0e\u539f\u5219\uff0c\u5e76\u5bf9\u5176\u4e2d1,186\u4e2a\u8bae\u9898\u8fdb\u884c\u4eba\u5de5\u5206\u7c7b\uff0c\u4f9d\u636e\u5176\u6240\u6d89\u53ca\u7684\u5173\u6ce8\u7c7b\u578b\u8fdb\u884c\u7f16\u7801\u3002", "result": "\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b24\u4e2a\u8ba8\u8bba\u7c7b\u522b\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u5f52\u5165\u516d\u5927\u96c6\u7fa4\uff1a\u529f\u80fd/\u7f3a\u9677\u3001\u540c\u610f\u76f8\u5173\u3001\u6587\u6863\u3001\u6570\u636e\u5b58\u50a8/\u5171\u4eab\u3001\u9002\u5e94\u6027\u4ee5\u53ca\u4e00\u822c\u5408\u89c4\u3002\u5f00\u53d1\u8005\u4e3b\u8981\u5173\u6ce8\u201c\u5220\u9664\u6743\u201d\u3001\u201c\u9000\u51fa\u6743\u201d\u548c\u201c\u8bbf\u95ee\u6743\u201d\uff0c\u8ba8\u8bba\u7126\u70b9\u96c6\u4e2d\u5728\u7528\u6237\u540c\u610f\u3001\u7528\u6237\u6743\u5229\u529f\u80fd\u5b9e\u73b0\u3001\u7f3a\u9677\u4fee\u590d\u53caCookie\u7ba1\u7406\u7b49\u65b9\u9762\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u7c7b\u6cd5\u53ef\u5e2e\u52a9\u4ece\u4e1a\u8005\u4f18\u5148\u5904\u7406\u5408\u89c4\u76f8\u5173\u8bae\u9898\uff0c\u6559\u80b2\u754c\u53ef\u636e\u6b64\u4f18\u5316\u8bfe\u7a0b\u5185\u5bb9\uff0c\u7814\u7a76\u754c\u5219\u53ef\u8bc6\u522b\u73b0\u6709\u4e0d\u8db3\u5e76\u63a8\u52a8\u9690\u79c1\u5408\u89c4\u652f\u6301\u5de5\u5177\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.10443", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.10443", "abs": "https://arxiv.org/abs/2512.10443", "authors": ["Sabtain Ahmad", "Meerzhan Kanatbekova", "Ivona Brandic", "Atakan Aral"], "title": "Clustered Federated Learning with Hierarchical Knowledge Distillation", "comment": null, "summary": "Clustered Federated Learning (CFL) has emerged as a powerful approach for addressing data heterogeneity and ensuring privacy in large distributed IoT environments. By clustering clients and training cluster-specific models, CFL enables personalized models tailored to groups of heterogeneous clients. However, conventional CFL approaches suffer from fragmented learning for training independent global models for each cluster and fail to take advantage of collective cluster insights. This paper advocates a shift to hierarchical CFL, allowing bi-level aggregation to train cluster-specific models at the edge and a unified global model at the cloud. This shift improves training efficiency yet might introduce communication challenges. To this end, we propose CFLHKD, a novel personalization scheme for integrating hierarchical cluster knowledge into CFL. Built upon multi-teacher knowledge distillation, CFLHKD enables inter-cluster knowledge sharing while preserving cluster-specific personalization. CFLHKD adopts a bi-level aggregation to bridge the gap between local and global learning. Extensive evaluations of standard benchmark datasets demonstrate that CFLHKD outperforms representative baselines in cluster-specific and global model accuracy and achieves a performance improvement of 3.32-7.57\\%.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCFLHKD\uff0c\u4e00\u79cd\u57fa\u4e8e\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u7684\u5206\u5c42\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u805a\u5408\u673a\u5236\u5728\u4fdd\u7559\u7c07\u7279\u5f02\u6027\u4e2a\u6027\u5316\u7684\u540c\u65f6\u5b9e\u73b0\u8de8\u7c07\u77e5\u8bc6\u5171\u4eab\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\uff08CFL\uff09\u4e3a\u6bcf\u4e2a\u7c07\u72ec\u7acb\u8bad\u7ec3\u5168\u5c40\u6a21\u578b\uff0c\u5bfc\u81f4\u5b66\u4e60\u788e\u7247\u5316\uff0c\u65e0\u6cd5\u5229\u7528\u5404\u7c07\u95f4\u7684\u96c6\u4f53\u77e5\u8bc6\uff1b\u540c\u65f6\u7f3a\u4e4f\u9ad8\u6548\u7684\u5c42\u7ea7\u7ed3\u6784\u6765\u517c\u987e\u672c\u5730\u4e2a\u6027\u5316\u4e0e\u5168\u5c40\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u63d0\u51faCFLHKD\u65b9\u6848\uff0c\u91c7\u7528\u5206\u5c42CFL\u67b6\u6784\u7ed3\u5408\u53cc\u5c42\u805a\u5408\u673a\u5236\uff0c\u5e76\u57fa\u4e8e\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u7c07\u95f4\u77e5\u8bc6\u5171\u4eab\uff0c\u5728\u8fb9\u7f18\u7aef\u8bad\u7ec3\u7c07\u7279\u5b9a\u6a21\u578b\uff0c\u5728\u4e91\u7aef\u8bad\u7ec3\u7edf\u4e00\u5168\u5c40\u6a21\u578b\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCFLHKD\u5728\u7c07\u7279\u5b9a\u6a21\u578b\u548c\u5168\u5c40\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e0a\u5747\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u8fbe3.32\u20137.57%\u3002", "conclusion": "CFLHKD\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfCFL\u4e2d\u7684\u5b66\u4e60\u788e\u7247\u5316\u95ee\u9898\uff0c\u5728\u4fdd\u969c\u4e2a\u6027\u5316\u7684\u540c\u65f6\u63d0\u5347\u4e86\u6574\u4f53\u8bad\u7ec3\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u5f02\u6784IoT\u73af\u5883\u4e0b\u7684\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.10713", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.10713", "abs": "https://arxiv.org/abs/2512.10713", "authors": ["Itay Dreyfuss", "Antonio Abu Nassar", "Samuel Ackerman", "Axel Ben David", "Rami Katan", "Orna Raz", "Marcel Zalmanovici"], "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "comment": null, "summary": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86PACIFIC\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u53ef\u8c03\u8282\u96be\u5ea6\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u4e25\u683c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u9075\u5faa\u6307\u4ee4\u548c\u5e72\u8fd0\u884c\uff08dry-running\uff09\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u7b80\u5355\u8f93\u51fa\u6bd4\u5bf9\u5b9e\u73b0\u53ef\u9760\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5e38\u4f9d\u8d56\u5de5\u5177\u8c03\u7528\u6216\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u96be\u4ee5\u51c6\u786e\u8861\u91cfLLM\u672c\u8eab\u5728\u65e0\u6267\u884c\u73af\u5883\u4e0b\u9010\u6b65\u63a8\u7406\u4ee3\u7801\u884c\u4e3a\u53ca\u9075\u5faa\u7528\u6237\u6307\u4ee4\u7684\u80fd\u529b\uff1b\u540c\u65f6\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u9694\u79bb\u5e76\u7cbe\u51c6\u8bc4\u4f30LLM\u5185\u5728\u6307\u4ee4\u9075\u5faa\u4e0e\u5e72\u8fd0\u884c\u80fd\u529b\u3001\u4e14\u53ef\u907f\u514d\u6570\u636e\u6c61\u67d3\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faPACIFIC\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u81ea\u52a8\u751f\u6210\u5177\u6709\u660e\u786e\u5b9a\u4e49\u9884\u671f\u8f93\u51fa\u7684\u57fa\u51c6\u6d4b\u8bd5\u53d8\u4f53\uff0c\u652f\u6301\u63a7\u5236\u96be\u5ea6\u7ea7\u522b\uff0c\u901a\u8fc7\u76f4\u63a5\u6bd4\u8f83\u6a21\u578b\u8f93\u51fa\u4e0e\u9884\u671f\u7ed3\u679c\u8fdb\u884c\u8bc4\u4f30\uff0c\u65e0\u9700\u6267\u884c\u4ee3\u7801\u6216\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\u3002", "result": "\u5728\u591a\u4e2a\u524d\u6cbfLLM\u4e0a\u9a8c\u8bc1\u8868\u660e\uff0cPACIFIC\u751f\u6210\u7684\u57fa\u51c6\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u548c\u5e72\u8fd0\u884c\u80fd\u529b\u4e0a\u7684\u5dee\u5f02\uff0c\u4e14\u968f\u7740\u96be\u5ea6\u63d0\u5347\uff0c\u8bc4\u4f30\u6548\u679c\u66f4\u52a0\u663e\u8457\u3002", "conclusion": "PACIFIC\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6297\u6570\u636e\u6c61\u67d3\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8861\u91cfLLM\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u7684\u6838\u5fc3\u80fd\u529b\uff0c\u5c24\u5176\u5728\u65e0\u6267\u884c\u73af\u5883\u4e0b\u7684\u6307\u4ee4\u9075\u5faa\u4e0e\u9010\u6b65\u63a8\u7406\u65b9\u9762\u3002"}}
{"id": "2512.10576", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.10576", "abs": "https://arxiv.org/abs/2512.10576", "authors": ["Xinhang Chen", "Chao Zhang", "Jiahuan He", "Wei Liu", "Jianming Zhang", "Wenlong Zhou", "Xiao Li", "Pai Zeng", "Shiyong Li", "Yuanpan Qian", "Dong Li", "Zhaogeng Li"], "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp", "comment": null, "summary": "DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.\n  To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.\n  Our high-fidelity simulations show that ESS delivers 69.4\\% throughput improvement at 32K context length and up to 123\\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.", "AI": {"tldr": "DeepSeek-V3.2-Exp \u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u4ecd\u53d7\u9650\u4e8e Decode \u9636\u6bb5\u7684 Latent-Cache \u5185\u5b58\u74f6\u9888\uff1b\u4e3a\u6b64\u63d0\u51fa\u7684 ESS \u7cfb\u7edf\u901a\u8fc7\u5c06 Latent-Cache \u9009\u62e9\u6027\u5378\u8f7d\u81f3 CPU \u5185\u5b58\uff0c\u663e\u8457\u63d0\u5347 Decode \u9636\u6bb5\u541e\u5410\u91cf\uff0c\u5728 128K \u4e0a\u4e0b\u6587\u4e2d\u5b9e\u73b0\u6700\u9ad8 123% \u7684\u541e\u5410\u589e\u76ca\u3002", "motivation": "\u5728 DeepSeek-V3.2-Exp \u4e2d\uff0c\u5c3d\u7ba1\u6574\u4f53\u541e\u5410\u6709\u6240\u63d0\u5347\uff0c\u4f46 PD \u89e3\u8026\u67b6\u6784\u4e0b\u7684 Decode \u9636\u6bb5\u56e0 Latent-Cache \u968f\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u800c\u53d7\u9650\u4e8e GPU \u663e\u5b58\u5bb9\u91cf\uff0c\u5bfc\u81f4\u6279\u5904\u7406\u89c4\u6a21\u65e0\u6cd5\u6269\u5927\uff0c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa ESS\uff08Extended Sparse Server\uff09\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4ee5\u5378\u8f7d\u4e3a\u6838\u5fc3\u7b56\u7565\uff0c\u5c06 Latent-Cache \u9009\u62e9\u6027\u5730\u8fc1\u79fb\u81f3 CPU \u5185\u5b58\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u5173\u952e\u7ec4\u4ef6\u5728 GPU \u4e0a\uff0c\u4ece\u800c\u89e3\u9664\u6279\u5904\u7406\u89c4\u6a21\u4e0e GPU \u663e\u5b58\u4e4b\u95f4\u7684\u8026\u5408\u3002", "result": "\u9ad8\u4fdd\u771f\u4eff\u771f\u8868\u660e\uff0cESS \u5728 32K \u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u63d0\u5347\u541e\u5410 69.4%\uff0c\u5728 128K \u4e0b\u6700\u9ad8\u63d0\u5347 123%\uff0c\u663e\u8457\u4f18\u5316\u4e86\u957f\u4e0a\u4e0b\u6587\u5927\u6a21\u578b\u63a8\u7406\u7684 Decode \u9636\u6bb5\u6027\u80fd\u3002", "conclusion": "ESS \u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u957f\u4e0a\u4e0b\u6587 LLM \u63a8\u7406\u4e2d\u7531 GPU \u663e\u5b58\u9650\u5236\u5e26\u6765\u7684 Decode \u9636\u6bb5\u74f6\u9888\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u5b9e\u9645\u90e8\u7f72\u6210\u672c\u3002"}}
{"id": "2512.10799", "categories": ["cs.SE", "cs.PL"], "pdf": "https://arxiv.org/pdf/2512.10799", "abs": "https://arxiv.org/abs/2512.10799", "authors": ["Karolina Gorna", "Nicolas Iooss", "Yannick Seurin", "Rida Khatoun"], "title": "Zorya: Automated Concolic Execution of Single-Threaded Go Binaries", "comment": null, "summary": "Go's adoption in critical infrastructure intensifies the need for systematic vulnerability detection, yet existing symbolic execution tools struggle with Go binaries due to runtime complexity and scalability challenges. In this work, we build upon Zorya, a concolic execution framework that translates Go binaries to Ghidra's P-Code intermediate representation to address these challenges. We added the detection of bugs in concretely not taken paths and a multi-layer filtering mechanism to concentrate symbolic reasoning on panic-relevant paths. Evaluation on five Go vulnerabilities demonstrates that panic-reachability gating achieves 1.8-3.9x speedups when filtering 33-70% of branches, and that Zorya detects all panics while existing tools detect at most two. Function-mode analysis proved essential for complex programs, running roughly two orders of magnitude faster than starting from main. This work establishes that specialized concolic execution can achieve practical vulnerability detection in language ecosystems with runtime safety checks.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86Zorya\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165panic\u53ef\u8fbe\u6027\u95e8\u63a7\u548c\u591a\u5c42\u8fc7\u6ee4\u673a\u5236\uff0c\u5728Go\u4e8c\u8fdb\u5236\u7a0b\u5e8f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7cbe\u51c6\u7684\u6f0f\u6d1e\u68c0\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5de5\u5177\u3002", "motivation": "Go\u8bed\u8a00\u5728\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u4f7f\u5176\u6f0f\u6d1e\u68c0\u6d4b\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7b26\u53f7\u6267\u884c\u5de5\u5177\u56e0Go\u8fd0\u884c\u65f6\u590d\u6742\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u96be\u4ee5\u6709\u6548\u5206\u6790Go\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002", "method": "\u5728Zorya\uff08\u4e00\u79cd\u5c06Go\u4e8c\u8fdb\u5236\u8f6c\u6362\u4e3aGhidra P-Code\u4e2d\u95f4\u8868\u793a\u7684\u6df7\u5408\u6267\u884c\u6846\u67b6\uff09\u57fa\u7840\u4e0a\uff0c\u589e\u52a0\u4e86\u5bf9\u5177\u4f53\u672a\u6267\u884c\u8def\u5f84\u4e2d\u6f0f\u6d1e\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u5c42\u8fc7\u6ee4\u673a\u5236\uff0c\u805a\u7126\u4e8e\u4e0epanic\u76f8\u5173\u7684\u8def\u5f84\uff1b\u540c\u65f6\u91c7\u7528\u51fd\u6570\u7ea7\u5206\u6790\u6a21\u5f0f\u63d0\u5347\u6548\u7387\u3002", "result": "\u5728\u4e94\u4e2aGo\u6f0f\u6d1e\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cpanic\u53ef\u8fbe\u6027\u95e8\u63a7\u53ef\u5728\u8fc7\u6ee433%-70%\u5206\u652f\u7684\u540c\u65f6\u5b9e\u73b01.8-3.9\u500d\u52a0\u901f\uff1bZorya\u6210\u529f\u68c0\u6d4b\u51fa\u5168\u90e8panic\uff0c\u800c\u73b0\u6709\u5de5\u5177\u6700\u591a\u4ec5\u68c0\u6d4b\u51fa\u4e24\u4e2a\uff1b\u51fd\u6570\u7ea7\u5206\u6790\u6bd4\u4ecemain\u51fd\u6570\u5f00\u59cb\u5feb\u7ea6\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u9488\u5bf9\u5177\u6709\u8fd0\u884c\u65f6\u5b89\u5168\u68c0\u67e5\u7684\u8bed\u8a00\u751f\u6001\uff08\u5982Go\uff09\uff0c\u4e13\u95e8\u4f18\u5316\u7684\u6df7\u5408\u6267\u884c\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u6f0f\u6d1e\u68c0\u6d4b\u3002"}}
