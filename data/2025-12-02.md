<div id=toc></div>

# Table of Contents

- [cs.DB](#cs.DB) [Total: 4]
- [cs.CE](#cs.CE) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.SE](#cs.SE) [Total: 19]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.NI](#cs.NI) [Total: 4]


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [1] [A Conceptual Model for Context Awareness in Ethical Data Management](https://arxiv.org/abs/2511.21942)
*Elisa Quintarelli,Fabio Alberto Schreiber,Kostas Stefanidis,Letizia Tanca,Barbara Oliboni*

Main category: cs.DB

TL;DR: 本文提出了一种双分支概念模型，用于根据应用场景的上下文动态适配数据伦理规则，以确保数据分析与学习系统在不同情境下符合相应的伦理要求。


<details>
  <summary>Details</summary>
Motivation: 由于算法和数据在使用过程中可能产生不道德行为，而伦理规则又随应用情境（上下文）变化，因此需要一种能根据上下文灵活调整伦理规范的方法来指导数据管理。

Method: 作者构建了一个由“上下文维度树”（CDT）和“伦理需求树”（ERT）组成的双分支概念模型。CDT 描述可能的应用上下文，ERT 表示在各上下文中所需遵守的伦理规则，用以指导数据集的定制与预处理。

Result: 该模型为不同上下文中的数据伦理需求提供了结构化表达方式，并通过示例展示了如何利用 CDT 和 ERT 指导数据预处理和系统设计。

Conclusion: 所提出的双分支概念模型有助于将伦理规则嵌入数据管理流程中，提升信息管理系统在多样化应用场景下的伦理合规性。

Abstract: Ethics has become a major concern to the information management community, as both algorithms and data should satisfy ethical rules that guarantee not to generate dishonourable behaviours when they are used. However, these ethical rules may vary according to the situation-the context-in which the application programs must work. In this paper, after reviewing the basic ethical concepts and their possible influence on data management, we propose a bipartite conceptual model, composed of the Context Dimensions Tree (CDT), which describes the possible contexts, and the Ethical Requirements Tree (ERT), representing the ethical rules necessary to tailor and preprocess the datasets that should be fed to Data Analysis and Learning Systems in each possible context. We provide some examples and suggestions on how these conceptual tools can be used.

</details>


### [2] [Relation-Stratified Sampling for Shapley Values Estimation in Relational Databases](https://arxiv.org/abs/2511.22035)
*Amirhossein Alizad,Mostafa Milani*

Main category: cs.DB

TL;DR: 该论文提出了一种用于关系查询中元组级归因的采样方法——关系分层采样（RSS）及其自适应变体ARSS，通过考虑关系结构和连接信息，在减少样本数量的同时显著降低了估计误差和方差。


<details>
  <summary>Details</summary>
Motivation: 精确计算Shapley类值在关系查询中不可行，因其需对指数级多的排列或子集进行聚合；现有基于采样的估计方法未充分利用关系模式和连接结构。

Method: 提出关系分层采样（RSS），按每个关系中抽取的元组数量向量划分样本空间，以聚焦于结构有效且信息丰富的联盟；进一步提出自适应版本ARSS，根据采样过程中的方差估计动态分配各层采样预算，并利用编译视图降低单次采样查询开销。

Result: 在TPCH工作负载上的实验表明，RSS和ARSS在多种含多关系连接与聚合的查询中，相比经典蒙特卡洛采样（MCS）和基于大小的分层采样（SS），能以更少样本获得更低的误差和方差。

Conclusion: 关系感知的分层策略与自适应预算分配相结合，使ARSS成为一种简单、高效且适用于任意时间的数据库中心Shapley归因估计器。

Abstract: Shapley-like values, including the Shapley and Banzhaf values, provide a principled way to quantify how individual tuples contribute to a query result. Their exact computation, however, is intractable because it requires aggregating marginal contributions over exponentially many permutations or subsets. While sampling-based estimators have been studied in cooperative game theory, their direct use for relational query answering remains underexplored and often ignores the structure of schemas and joins.
  We study tuple-level attribution for relational queries through sampling and introduce Relation-Stratified Sampling (RSS). Instead of stratifying coalitions only by size, RSS partitions the sample space by a relation-wise count vector that records how many tuples are drawn from each relation. This join-aware stratification concentrates samples on structurally valid and informative coalitions and avoids strata that cannot satisfy query conditions. We further develop an adaptive variant, ARSS, that reallocates budget across strata using variance estimates obtained during sampling, improving estimator efficiency without increasing the total number of samples. We analyze these estimators, describe a practical implementation that reuses compiled views to reduce per-sample query cost, and evaluate them on TPCH workloads.
  Across diverse queries with multi-relation joins and aggregates, RSS and ARSS consistently outperform classical Monte Carlo (MCS) and size-based Stratified Sampling (SS), yielding lower error and variance with fewer samples. An ablation shows that relation-aware stratification and adaptive allocation contribute complementary gains, making ARSS a simple, effective, and anytime estimator for database-centric Shapley attribution.

</details>


### [3] [Structured Multi-Step Reasoning for Entity Matching Using Large Language Model](https://arxiv.org/abs/2511.22832)
*Rohan Bopardikar,Jin Wang,Jia Zou*

Main category: cs.DB

TL;DR: 本文提出一种基于多步结构化推理的LLM实体匹配方法，通过分阶段识别匹配/不匹配项、关键属性判断和最终决策，并引入辩论机制提升鲁棒性，在多个基准数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的实体匹配方法多采用单步提示，缺乏对结构化推理策略的深入探索，限制了匹配性能的进一步提升。

Method: 提出一个三步框架：首先识别两条记录中匹配与不匹配的词元，其次确定对匹配决策影响最大的属性，最后判断是否指向同一现实实体；同时引入支持与反对论点对比的辩论机制以增强决策鲁棒性。

Result: 在多个真实世界实体匹配基准数据集上的实验表明，结构化多步推理在若干场景下提升了匹配性能，同时也揭示了当前方法仍面临的挑战和改进空间。

Conclusion: 结构化多步推理能有效提升LLM在实体匹配任务中的表现，但需进一步优化推理引导策略以应对复杂匹配场景。

Abstract: Entity matching is a fundamental task in data cleaning and data integration. With the rapid adoption of large language models (LLMs), recent studies have explored zero-shot and few-shot prompting to improve entity matching accuracy. However, most existing approaches rely on single-step prompting and offer limited investigation into structured reasoning strategies. In this work, we investigate how to enhance LLM-based entity matching by decomposing the matching process into multiple explicit reasoning stages. We propose a three-step framework that first identifies matched and unmatched tokens between two records, then determines the attributes most influential to the matching decision, and finally predicts whether the records refer to the same real-world entity. In addition, we explore a debate-based strategy that contrasts supporting and opposing arguments to improve decision robustness. We evaluate our approaches against multiple existing baselines on several real-world entity matching benchmark datasets. Experimental results demonstrate that structured multi-step reasoning can improve matching performance in several cases, while also highlighting remaining challenges and opportunities for further refinement of reasoning-guided LLM approaches.

</details>


### [4] [Extended Serial Safety Net: A Refined Serializability Criterion for Multiversion Concurrency Control](https://arxiv.org/abs/2511.22956)
*Atsushi Kitazawa,Chihaya Ito,Yuta Yoshida,Takamitsu Shioi*

Main category: cs.DB

TL;DR: 本文提出了ESSN，一种对Serial Safety Net（SSN）协议的推广，通过放宽排除条件，在保证多版本可串行化（MVSR）的同时允许更多事务安全提交，并在混合工作负载下显著降低长事务的中止率。


<details>
  <summary>Details</summary>
Motivation: 现有基于单一序列化点（如开始或提交时刻）的并发控制协议无法兼容快照隔离（SI），而SSN虽提供轻量级提交时检测，但过于保守且仅以提交时间为锚点，限制了并发性能。

Method: ESSN引入基于多版本序列化图（MVSG）的判定准则，并利用事务的已知全序（KTO，如按开始或提交排序）来推理可串行性；其在提交时进行一次检查，维持各数据项版本链上的单调性，避免链遍历，且开销与SSN相当。

Result: 在提交有序的KTO下，使用开始快照读取可使长事务的中止率相比SSN最多降低约0.25（绝对值），即相对减少约50%。

Conclusion: ESSN在保持多版本可串行化的同时严格包含并优于SSN，显著提升了混合工作负载下的事务提交成功率和系统并发性能。

Abstract: A long line of concurrency-control (CC) protocols argues correctness via a single serialization point (begin or commit), an assumption that is incompatible with snapshot isolation (SI), where read-write anti-dependencies arise. Serial Safety Net (SSN) offers a lightweight commit-time test but is conservative and effectively anchored on commit time as the sole point. We present ESSN, a principled generalization of SSN that relaxes the exclusion condition to allow more transactions to commit safely, and we prove that this preserves multiversion serializability (MVSR) and that it strictly subsumes SSN. ESSN states an MVSG (Multiversion Serialization Graph)-based criterion and introduces a known total order over transactions (KTO; e.g., begin-ordered or commit-ordered) for reasoning about the graph's serializability. With a single commit-time check under invariant-based semantics, ESSN's exclusion condition preserves monotonicity along per-item version chains, and eliminates chain traversal. The protocol is Direct Serialization Graph (DSG)-based with commit-time work linear in the number of reads and writes, matching SSN's per-version footprint. We also make mixed workloads explicit by defining a Long transaction via strict interval containment of Short transactions, and we evaluate ESSN on reproducible workloads. Under a commit-ordered KTO, using begin-snapshot reads reduces the long-transaction abort rate by up to approximately 0.25 absolute (about 50% relative) compared with SSN.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [5] [Textual semantics and machine learning methods for data product pricing](https://arxiv.org/abs/2511.22185)
*Ruize Gao,Feng Xiao,Jinpu Li,Shaoze Cui*

Main category: cs.CE

TL;DR: 本文通过多种文本表示方法和机器学习模型预测数据产品的价格，发现Word2Vec在连续价格预测中表现最佳，而Bag-of-Words和TF-IDF在价格等级分类任务中更优；SHAP分析揭示了不同语义特征对价格的影响。


<details>
  <summary>Details</summary>
Motivation: 探索数据产品描述文本的语义特征如何影响其定价，以提升数据交易平台的收入与市场发展。

Method: 采用五种文本表示方法（如Word2Vec、Bag-of-Words、TF-IDF等）对数据产品描述文本进行编码，并结合六种机器学习模型（如线性回归、XGBoost等）进行价格预测；同时利用mRMR和SHAP方法进行特征重要性分析。

Result: 在连续价格预测任务中，基于语义相似性的Word2Vec表现最优；在价格等级分类任务中，非语义方法（如Bag-of-Words和TF-IDF）效果更好。SHAP分析显示医疗与人口统计相关语义特征推高价格，而天气与环境类特征则与较低价格相关。

Conclusion: 文本语义对数据产品定价具有显著影响，结合合适的文本表示方法和可解释性技术可有效提升定价模型的性能与透明度。

Abstract: Reasonable pricing of data products enables data trading platforms to maximize revenue and foster the growth of the data trading market. The textual semantics of data products are vital for pricing and contain significant value that remains largely underexplored. Therefore, to investigate how textual features influence data product pricing, we employ five prevalent text representation techniques to encode the descriptive text of data products. And then, we employ six machine learning methods to predict data product prices, including linear regression, neural networks, decision trees, support vector machines, random forests, and XGBoost. Our empirical design consists of two tasks: a regression task that predicts the continuous price of data products, and a classification task that discretizes price into ordered categories. Furthermore, we conduct feature importance analysis by the mRMR feature selection method and SHAP-based interpretability techniques. Based on empirical data from the AWA Data Exchange, we find that for predicting continuous prices, Word2Vec text representations capturing semantic similarity yield superior performance. In contrast, for price-tier classification tasks, simpler representations that do not rely on semantic similarity, such as Bag-of-Words and TF-IDF, perform better. SHAP analysis reveals that semantic features related to healthcare and demographics tend to increase prices, whereas those associated with weather and environmental topics are linked to lower prices. This analytical framework significantly enhances the interpretability of pricing models.

</details>


### [6] [Maritime Activities Observed Through Open-Access Positioning Data: Moving and Stationary Vessels in the Baltic Sea](https://arxiv.org/abs/2511.23016)
*Moritz Hütten*

Main category: cs.CE

TL;DR: 该研究利用公开的AIS数据，通过数据清洗与重建方法，在接收器覆盖不完整的情况下高精度重构了波罗的海沿岸船舶活动，并构建了行程模型以生成船舶数量、交通流量及约400米分辨率的船舶密度图。


<details>
  <summary>Details</summary>
Motivation: 理解过去和现在的海上活动模式对航行安全、环境评估和商业运营至关重要；而目前越来越多的服务通过地面接收器公开提供AIS定位数据，但这些数据存在质量有限和覆盖不全的问题。

Method: 研究采用2024年8月至10月波罗的海三个月的公开AIS数据，提出（i）数据清洗与重建方法以提升数据质量，（ii）一种将AIS消息转化为船舶数量、交通估计和空间分辨船舶密度（约400米分辨率）的行程模型。

Result: 研究提供了移动和静止船舶的数量及其不确定性，识别出港口位置，并推断出波罗的海最拥挤和最繁忙的沿岸区域；平均每天有超过300艘船进出该海域，同时有超过4000艘船在区域内运行；结果与依赖专有数据的先前研究相差在20%以内。

Conclusion: 公开AIS数据即使在覆盖不全和质量有限的情况下，也能有效用于高精度重建沿海船舶活动，为航行安全、环境监测和商业决策提供可靠支持。

Abstract: Understanding past and present maritime activity patterns is critical for navigation safety, environmental assessment, and commercial operations. An increasing number of services now openly provide positioning data from the Automatic Identification System (AIS) via ground-based receivers. We show that coastal vessel activity can be reconstructed from open access data with high accuracy, even with limited data quality and incomplete receiver coverage. For three months of open AIS data in the Baltic Sea from August to October 2024, we present (i) cleansing and reconstruction methods to improve the data quality, and (ii) a journey model that converts AIS message data into vessel counts, traffic estimates, and spatially resolved vessel density at a resolution of $\sim$400 m. Vessel counts are provided, along with their uncertainties, for both moving and stationary activity. Vessel density maps also enable the identification of port locations, and we infer the most crowded and busiest coastal areas in the Baltic Sea. We find that on average, $\gtrsim$4000 vessels simultaneously operate in the Baltic Sea, and more than 300 vessels enter or leave the area each day. Our results agree within 20\% with previous studies relying on proprietary data.

</details>


### [7] [Iterative convergence in phase-field brittle fracture computations: exact line search is all you need](https://arxiv.org/abs/2511.23064)
*Jonas Heinzmann,Francesco Vicentini,Pietro Carrara,Laura De Lorenzis*

Main category: cs.CE

TL;DR: 本文提出一种基于二分法的精确线搜索算法，用于保证牛顿法在变分相场断裂模型交替最小化子问题中的全局收敛性，并通过多组基准测试验证其鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 变分相场断裂模型中的能量泛函具有强非线性，导致交替最小化过程中子问题可能无法迭代收敛，影响整体求解稳定性。

Method: 提出一种基于二分法的精确线搜索算法，在满足一定条件下确保每个子问题中牛顿法的全局收敛，并结合交替最小化策略求解能量泛函的临界点。

Result: 通过多种应变能分解方式和两种不可逆约束处理策略，在二维和三维基准算例中验证了所提方法的鲁棒性，并与其他常用线搜索算法进行了效率对比。

Conclusion: 所提出的线搜索算法能有效提升变分相场断裂模型中交替最小化过程的稳定性和可靠性，具备良好的实际应用潜力。

Abstract: Variational phase-field models of brittle fracture pose a local constrained minimization problem of a non-convex energy functional. In the discrete setting, the problem is most often solved by alternate minimization, exploiting the separate convexity of the energy with respect to the two unknowns. This approach is theoretically guaranteed to converge, provided each of the individual subproblems is solved successfully. However, strong non-linearities of the energy functional may lead to failure of iterative convergence within one or both subproblems. In this paper, we propose an exact line search algorithm based on bisection, which (under certain conditions) guarantees global convergence of Newton's method for each subproblem and consequently the successful determination of critical points of the energy through the alternate minimization scheme. Through several benchmark tests computed with various strain energy decompositions and two strategies for the enforcement of the irreversibility constraint in two and three dimensions, we demonstrate the robustness of the approach and assess its efficiency in comparison with other commonly used line search algorithms.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [A Sustainable and Reward Incentivized High-Performance Cluster Computing for Artificial Intelligence: A Novel Bayesian-Time-Decay Trust Mechanism in Blockchain](https://arxiv.org/abs/2511.21844)
*Murat Yaslioglu*

Main category: cs.DC

TL;DR: 本文提出了一种结合高性能集群计算、智能算法与区块链的新框架，通过改进的工作量证明机制、动态信任评分和统计抽签系统，实现高效、公平且环保的智能算法部署。


<details>
  <summary>Details</summary>
Motivation: 当前高性能计算和智能算法虽重要，但能耗高、资源集中，不利于可持续发展和广泛参与，亟需一种更包容、可扩展且环保的解决方案。

Method: 设计了一个融合高性能集群计算与智能算法的区块链框架，包含改进的PoW共识机制、基于验证准确性的动态信任评分机制，以及允许低算力节点参与的统计抽签系统。

Result: 该框架提升了资源利用效率，扩大了不同计算能力节点的参与机会，并通过信任机制激励准确贡献，从而在保障性能的同时增强公平性与可持续性。

Conclusion: 所提出的框架有效平衡了计算效率、参与公平性与能源可持续性，为未来智能算法在分布式环境中的部署提供了可行路径。

Abstract: In an age where sustainability is of paramount importance, the significance of both high-performance computing and intelligent algorithms cannot be understated. Yet, these domains often demand hefty computational power, translating to substantial energy usage and potentially sidelining less robust computing systems. It's evident that we need an approach that is more encompassing, scalable, and eco-friendly for intelligent algorithm development and implementation. The strategy we present in this paper offers a compelling answer to these issues. We unveil a fresh framework that seamlessly melds high-performance cluster computing with intelligent algorithms, all within a blockchain infrastructure. This promotes both efficiency and a broad-based participation. At its core, our design integrates an evolved proof-of-work consensus process, which links computational efforts directly to rewards for producing blocks. This ensures both optimal resource use and participation from a wide spectrum of computational capacities. Additionally, our approach incorporates a dynamic 'trust rating' that evolves based on a track record of accurate block validations. This rating determines the likelihood of a node being chosen for block generation, creating a merit-based system that recognizes and rewards genuine and precise contributions. To level the playing field further, we suggest a statistical 'draw' system, allowing even less powerful nodes a chance to be part of the block creation process.

</details>


### [9] [Equivalence and Separation between Heard-Of and Asynchronous Message-Passing Models](https://arxiv.org/abs/2511.21859)
*Hagit Attiya,Armando Castañeda,Dhrubajyoti Ghosh,Thomas Nowak*

Main category: cs.DC

TL;DR: 本文研究了异步消息传递模型（AMP_f）与Heard-Of模型（HO_f）在容错分布式计算中的等价性，发现对于无色任务在 n > 2f 时两者等价，而对有色任务仅在 f = 1 且 n > 2 时等价；差异源于 HO_f 中存在“静默”进程，可能导致不一致决策。


<details>
  <summary>Details</summary>
Motivation: 厘清两种主流分布式计算模型（AMP_f 与 HO_f）之间的表达能力关系，特别是在不同故障数量和任务类型下的可解性等价性，以明确基于轮次的抽象模型在多大程度上能准确刻画异步计算。

Method: 通过构建双向模拟，在 AMP_f 与 HO_f 之间引入一个中间模型来刻画“静默”行为，并分别证明在无色任务和有色任务下两者的等价或分离条件；同时将结果扩展至非自适应敌手下的随机协议。

Result: 证明了当 n > 2f 时，AMP_f 与 HO_f 对无色任务等价；对有色任务，仅当 f = 1 且 n > 2 时等价；更大的 f 值会导致 HO_f 中因静默进程而无法达成一致，从而破坏等价性；该结论在随机协议下依然成立。

Conclusion: 基于轮次的 Heard-Of 模型在特定条件下（如无色任务或 f=1 的有色任务）能够精确捕捉异步消息传递模型的计算能力，但在更一般的有色任务和高容错场景中存在结构性局限。

Abstract: We revisit the relationship between two fundamental models of distributed computation: the asynchronous message-passing model with up to $f$ crash failures ($\operatorname{AMP}_f$) and the Heard-Of model with up to $f$ message omissions ($\operatorname{HO}_f$). We show that for $n > 2f$, the two models are equivalent with respect to the solvability of colorless tasks, and that for colored tasks the equivalence holds only when $f = 1$ (and $n > 2$). The separation for larger $f$ arises from the presence of silenced processes in $\operatorname{HO}_f$, which may lead to incompatible decisions. The proofs proceed through bidirectional simulations between $\operatorname{AMP}_f$ and $\operatorname{HO}_f$ via an intermediate model that captures this notion of silencing. The results extend to randomized protocols against a non-adaptive adversary, indicating that the expressive limits of canonical rounds are structural rather than probabilistic. Together, these results delineate precisely where round-based abstractions capture asynchronous computation, and where they do not.

</details>


### [10] [OOCO: Latency-disaggregated Architecture for Online-Offline Co-locate LLM Serving](https://arxiv.org/abs/2511.21862)
*Siyu Wu,Zihan Tang,Yuting Zeng,Hui Chen,Guiguang Ding,Tongxuan Liu,Ke Zhang,Hailong Yang*

Main category: cs.DC

TL;DR: 提出一种面向延迟约束的解耦架构，通过分离延迟敏感与非敏感任务池，并结合基于瓶颈的调度器和快速抢占机制，在保障在线请求SLO的同时，将离线吞吐量提升最高3倍。


<details>
  <summary>Details</summary>
Motivation: 在Prefill/Decode解耦系统中，直接共置延迟敏感的在线服务与成本敏感的离线任务会导致严重的负载不均衡，而现有动态调整方法难以应对在线服务突发流量。

Method: 设计延迟约束解耦架构，将集群资源划分为延迟严格与延迟宽松池；引入基于Roofline模型的瓶颈感知调度器，并实现快速抢占机制以保障在线请求的SLO。

Result: 在真实轨迹实验中，相比现有离线系统方法，该方法在满足在线请求SLO的前提下，将离线吞吐量最高提升3倍。

Conclusion: 所提架构有效缓解了P/D负载不均衡问题，在保障在线服务质量的同时显著提升了离线任务吞吐能力。

Abstract: Large Language Models (LLMs) are increasingly deployed in both latency-sensitive online services and cost-sensitive offline workloads. Co-locating these workloads on shared serving instances can improve resource utilization, but directly applying this approach to Prefill/Decode (P/D) disaggregated systems introduces severe load imbalance, as fluctuating request mixes alter the intrinsic P/D ratio. Existing dynamic adjustment techniques cannot keep up with the bursty traffic patterns of online services.
  We propose a latency-constraint disaggregated architecture, which separates cluster resources into latency-strict and latency-relaxed pools based on task latency requirements. This design enables flexible placement of offline decode tasks, mitigating P/D imbalance while preserving online performance. To fully exploit this flexibility, we propose (1) a bottleneck-based scheduler guided by a Roofline-based performance model for performance bottleneck based scheduling, and (2) a fast preemption mechanism that strictly enforces Service Level Objectives (SLOs) for online requests.
  Experiments on real-world traces show that compared to existing offline system approaches, our method improves offline throughput by up to 3x, while maintaining online request SLOs.

</details>


### [11] [Clock2Q+: A Simple and Efficient Replacement Algorithm for Metadata Cache in VMware vSAN](https://arxiv.org/abs/2511.21958)
*Yiyan Zhai,Bintang Dwi Marthen,Sarath Balivada,Vamsi Sudhakar Bojji,Eric Knauft,Jitender Rohilla,Jiaqi Zuo,Quanxing Liu,Maxime Austruy,Wenguang Wang,Juncheng Yang*

Main category: cs.DC

TL;DR: 本文提出了一种专为元数据缓存设计的新型缓存替换算法Clock2Q+，通过在Small FIFO队列中引入相关性窗口来避免将相关引用误判为热点块，从而显著降低缓存未命中率。该算法已在VMware的vSAN和VDFS产品中实现，具有低CPU开销、低内存开销、良好的多核扩展性以及易于调优和实现等优点。


<details>
  <summary>Details</summary>
Motivation: 元数据缓存中存在固有的相关引用，即使对应的数据访问没有相关性。这种相关引用会误导传统缓存替换算法，使其将非热点块误判为热点块，从而降低缓存效率。

Method: Clock2Q+基于S3-FIFO的三队列结构，在Small FIFO队列中引入一个“相关性窗口”，处于该窗口内的块不会设置引用位，从而避免将相关引用误认为频繁访问的热点块。

Result: 在元数据轨迹上，Clock2Q+相比次优算法S3-FIFO最多可降低28.5%的未命中率；同时在数据轨迹上也优于现有先进算法。此外，它具备低CPU开销（命中时）、低内存开销、良好的多核可扩展性，并且易于实现和调优。

Conclusion: Clock2Q+是一种高效、实用且适用于大规模存储系统的缓存替换算法，特别适合处理元数据缓存中的相关引用问题，在性能和工程实现上均具有显著优势。

Abstract: Cache replacement algorithms are critical building blocks of storage systems. This paper examines the characteristics of metadata caches and argues that they inherently exhibit correlated references, even when the corresponding data accesses do not contain correlated references. The presence of correlated references reduces the effectiveness of cache replacement algorithms because these references are often mistakenly categorized as hot blocks. Clock2Q+ is specifically designed for metadata caches and has been implemented in vSAN and VDFS, two flagship storage products of VMware by Broadcom. Similar to S3-FIFO, Clock2Q+ uses three queues; however, Clock2Q+ introduces a correlation window in the Small FIFO queue, where blocks in this window do not set the reference bit. This simple enhancement allows Clock2Q+ to outperform state-of-the-art replacement algorithms. Compared to S3-FIFO, the second-best performing algorithm, Clock2Q+ achieves up to a 28.5% lower miss ratio on metadata traces. Clock2Q+ possesses the essential properties required for large-scale storage systems: it has low CPU overhead on cache hits, low memory overhead, scales efficiently to multiple CPUs, and is both easy to tune and implement. Additionally, Clock2Q+ outperforms state-of-the-art cache replacement algorithms on data traces as well.

</details>


### [12] [An Empirical Study of Cross-Language Interoperability in Replicated Data Systems](https://arxiv.org/abs/2511.22010)
*Provakar Mondal,Eli Tilevich*

Main category: cs.DC

TL;DR: 本文通过实证研究比较了在多语言分布式系统中集成复制数据库（RDL）的两种策略——外部函数接口（FFI）和通用数据格式（CDF），发现CDF在软件质量、延迟、内存消耗和吞吐量方面更具优势，并据此设计了一个支持多语言且具有插件扩展能力的RDL。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统常需在多个执行站点间复制数据，并混合使用不同编程语言。现有复制数据库（RDL）通常仅支持单一语言或提供特定语言绑定，导致在多语言环境中集成时需编写专用代码，而这类代码的软件质量与性能特征尚不明确。

Method: 开展实证研究，对比两种RDL集成策略：外部函数接口（FFI）与通用数据格式（CDF），通过测量和比较它们的软件指标与性能表现，评估其在多语言复制数据系统中的适用性。

Result: 研究发现采用CDF进行跨语言交互在软件质量、延迟、内存占用和吞吐量方面均优于FFI；作者进一步构建了一个基于CDF的RDL，支持编译型、解释型和托管语言的混合使用，并通过插件机制实现单语言功能扩展的同时维持多语言集成。

Conclusion: 在现代多语言分布式系统背景下，本研究为设计适用于多语言环境的复制数据库（RDL）提供了新的实践指导和理论依据。

Abstract: BACKGROUND: Modern distributed systems replicate data across multiple execution sites. Business requirements and resource constraints often necessitate mixing different languages across replica sites. To facilitate the management of replicated data, modern software engineering practices integrate special-purpose replicated data libraries (RDLs) that provide read-write access to the data and ensure its synchronization. Irrespective of the implementation languages, an RDL typically uses a single language or offers bindings to a designated one. Hence, integrating existing RDLs in multilingual environments requires special-purpose code, whose software quality and performance characteristics are poorly understood.
  AIMS: We aim to bridge this knowledge gap to understand the software quality and performance characteristics of RDL integration in multilingual environments.
  METHOD: We conduct an empirical study of two key strategies for integrating RDLs in the context of multilingual replicated data systems: foreign-function interface (FFI) and a common data format (CDF); we measure and compare their respective software metrics and performance to understand their suitability for the task at hand.
  RESULTS: Our results reveal that adopting CDF for cross-language interaction offers software quality, latency, memory consumption, and throughput advantages. We further validate our findings by (1) creating a CDF-based RDL for mixing compiled, interpreted, and managed languages; and (2) enhancing our RDL with plug-in extensibility that enables adding functionality in a single language while maintaining integration within a multilingual environment.
  CONCLUSIONS: With modern distributed systems utilizing multiple languages, our findings provide novel insights for designing RDLs in multilingual replicated data systems.

</details>


### [13] [PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel](https://arxiv.org/abs/2511.22333)
*Jinjun Yi,Zhixin Zhao,Yitao Hu,Ke Yan,Weiwei Sun,Hao Wang,Laiping Zhao,Yuhao Zhang,Wenxin Li,Keqiu Li*

Main category: cs.DC

TL;DR: 本文提出PAT，一种面向LLM解码的前缀感知注意力核，通过打包共享前缀查询、定制多分块核及多流转发等技术，显著降低注意力延迟和TPOT。


<details>
  <summary>Details</summary>
Motivation: 现有注意力实现未能充分利用真实负载中请求间存在的大量层次化共享前缀（如系统提示、工具模板、RAG），导致重复加载KV缓存、片上资源闲置及内存带宽压力增大，从而拖慢内存受限的解码注意力操作。

Method: 提出PAT方法，采用“打包-前向-合并”范式：按共享前缀打包查询以减少重复内存访问；运行定制化的多分块核以提升资源利用率；结合多流前向与KV分割以减少资源气泡；最后通过在线softmax完成合并，开销可忽略。该方法以即插即用插件形式集成到vLLM中。

Result: 在真实和合成负载上的评估表明，相比当前最先进的注意力核，PAT在相同配置下平均降低注意力延迟67.4%，TPOT降低13.6%至83.4%。

Conclusion: PAT通过高效利用请求间的共享前缀信息，在不改变模型结构的前提下显著提升了LLM解码阶段的注意力计算效率，有效缓解了内存带宽瓶颈。

Abstract: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: *one-query-per-CTA* execution repeatedly loads shared prefix KV cache, while *one-size-fits-all* tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.
  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 67.4% on average and TPOT by 13.6-83.4% under the same configurations against state-of-the-art attention kernels.

</details>


### [14] [Optimality of Simultaneous Consensus with Limited Information Exchange (Extended Abstract)](https://arxiv.org/abs/2511.22380)
*Kaya Alpturer,Ron van der Meyden,Sushmita Ruj,Godfrey Wong*

Main category: cs.DC

TL;DR: 本文研究了在崩溃故障模型下，针对多种受限信息交换机制的同步共识问题，并提出了一个新机制，在接近最优决策延迟的同时降低了计算和存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识逻辑的容错共识协议多采用“全信息”交换方式，消息开销大；为降低通信成本，需研究在受限信息交换下的最优协议设计。

Method: 分析并比较了多种已有受限信息交换机制（如FloodSet及其变体），并提出一种新的信息交换机制；通过实现基于知识的程序，推导出在每种机制下最优的同步共识协议。

Result: 所提出的新信息交换机制可在最坏情况下仅比Dwork和Moses的最优协议晚一轮做出决策，同时显著降低计算复杂度和空间需求；针对各类信息交换机制均获得了最优协议。

Conclusion: 在崩溃故障模型中，通过精心设计受限信息交换机制，可以在保持接近最优响应时间的同时，有效减少资源消耗，从而实现高效且实用的同步共识协议。

Abstract: Work on the development of optimal fault-tolerant Agreement protocols using the logic of knowledge has concentrated on the "full information" approach to information exchange, which is costly with respect to message size. Alpturer, Halpern, and van der Meyden (PODC 2023) introduced the notion of optimality with respect to a limited information exchange, and studied the Eventual Agreement problem in the sending omissions failure model. The present paper studies the Simultaneous Agreement problem for the crash failures model, and a number of limited information exchanges from the literature. In particular, the paper considers information exchanges from a FloodSet protocol (Lynch, Distributed Algorithms 1996), a variant of this in which agents also count the number of failures (Castañeda et al, NETYS 2017), and a variant in which agents associate each agent with a value (Raynal, PRDC 2002). A new information exchange is also introduced that enables decisions to be made at worst one round later than the optimal protocol of Dwork and Moses (I&C 88), but with lower computation cost and space requirements. By determining implementations of a knowledge based program, protocols are derived that are optimal amongst protocols for each of these information exchanges.

</details>


### [15] [OmniInfer: System-Wide Acceleration Techniques for Optimizing LLM Serving Throughput and Latency](https://arxiv.org/abs/2511.22481)
*Jun Wang,Yunxiang Yao,Wenwei Kuang,Runze Mao,Zhenhao Sun,Zhuang Tao,Ziyang Zhang,Dengyu Li,Jiajun Chen,Zhili Wang,Kai Cui,Congzhi Cai,Longwen Lan,Ken Zhang*

Main category: cs.DC

TL;DR: OmniInfer 是一个统一的系统级加速框架，通过专家放置、缓存压缩和调度的细粒度优化，显著提升大语言模型推理的端到端效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在大规模部署中面临计算密集、延迟严格和吞吐瓶颈等挑战，亟需系统级优化方案以提升推理效率。

Method: OmniInfer 基于 vLLM 构建，整合三个核心组件：OmniPlacement（负载感知的混合专家调度）、OmniAttn（稀疏注意力加速）和 OmniProxy（面向资源解耦的请求调度），并通过自适应资源解耦、高效稀疏性利用及预填充与解码阶段的全局协调实现系统级优化。

Result: 在 10 节点 Ascend 910C 集群上使用 DeepSeek-R1 评估，OmniInfer 达到 616 QPM，整体框架降低 TPOT 36%，叠加 OmniProxy 后进一步减少 TTFT 38%。

Conclusion: OmniInfer 有效提升了大语言模型推理系统的性能与效率，具备良好的实用价值，并已开源。

Abstract: Large Language Models drive a wide range of modern AI applications but impose substantial challenges on large-scale serving systems due to intensive computation, strict latency constraints, and throughput bottlenecks. We introduce OmniInfer, a unified system-level acceleration framework designed to maximize end-to-end serving efficiency through fine-grained optimization of expert placement, cache compression, and scheduling. OmniInfer integrates three complementary components: OmniPlacement for load-aware Mixture-of-Experts scheduling, OmniAttn for sparse attention acceleration, and OmniProxy for disaggregation-aware request scheduling. Built atop vLLM, OmniInfer delivers system-wide performance gains through adaptive resource disaggregation, efficient sparsity exploitation, and global coordination across prefill and decode phases. Evaluated on DeepSeek-R1 within a 10-node Ascend 910C cluster, OmniInfer achieves 616 QPM, where the unified framework reduces TPOT by 36\%, and the superimposition of OmniProxy further slashes TTFT by 38\%. The project is open-sourced at [this https URL](https://gitee.com/omniai/omniinfer).

</details>


### [16] [Accelerating mesh-based Monte Carlo simulations using contemporary graphics ray-tracing hardware](https://arxiv.org/abs/2511.22779)
*Shijie Yan,Douglas Dwyer,David R. Kaeli,Qianqian Fang*

Main category: cs.DC

TL;DR: 本文提出了一种基于GPU光线追踪核心（RT-cores）的加速版网格蒙特卡洛光传输模拟算法RT-MMC，利用NVIDIA OptiX平台实现硬件加速的体光线追踪，在保持精度的同时获得1.5至4.5倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统网格蒙特卡洛（MMC）方法虽精度高，但因频繁进行光线-边界相交测试而计算开销大，限制了其在复杂组织结构中的实际应用效率。

Method: 采用NVIDIA OptiX平台，将图形学中的光线追踪管线扩展至浑浊介质中的体光线追踪，利用现代GPU上的RT-cores进行硬件加速，避免复杂的四面体网格生成，并原生支持宽场光源。

Result: RT-MMC与传统软件光线追踪MMC结果高度一致，在多种GPU架构上实现1.5x至4.5x的加速，显著提升MMC在常规模拟中的实用性。

Conclusion: 从软件光线追踪转向硬件光线追踪不仅简化了MMC工作流程，还带来显著性能提升，随着光线追踪硬件的普及，该方法将在生物光子学领域广泛应用。

Abstract: Significance: Monte Carlo (MC) methods are the gold-standard for modeling light-tissue interactions due to their accuracy. Mesh-based MC (MMC) offers enhanced precision for complex tissue structures using tetrahedral mesh models. Despite significant speedups achieved on graphics processing units (GPUs), MMC performance remains hindered by the computational cost of frequent ray-boundary intersection tests.
  Aim: We propose a highly accelerated MMC algorithm, RT-MMC, that leverages the hardware-accelerated ray traversal and intersection capabilities of ray-tracing cores (RT-cores) on modern GPUs.
  Approach: Implemented using NVIDIA's OptiX platform, RT-MMC extends graphics ray-tracing pipelines towards volumetric ray-tracing in turbid media, eliminating the need for challenging tetrahedral mesh generation while delivering significant speed improvements through hardware acceleration. It also intrinsically supports wide-field sources without complex mesh retesselation.
  Results: RT-MMC demonstrates excellent agreement with traditional software-ray-tracing MMC algorithms while achieving 1.5x to 4.5x speedups across multiple GPU architectures. These performance gains significantly enhance the practicality of MMC for routine simulations.
  Conclusion: Migration from software- to hardware-based ray-tracing not only greatly simplifies MMC simulation workflows, but also results in significant speedups that are expected to increase further as ray-tracing hardware rapidly gains adoption. Adoption of graphics ray-tracing pipelines in quantitative MMC simulations enables leveraging of emerging hardware resources and benefits a wide range of biophotonics applications.

</details>


### [17] [Serving Heterogeneous LoRA Adapters in Distributed LLM Inference Systems](https://arxiv.org/abs/2511.22880)
*Shashwat Jaiswal,Shrikara Arun,Anjaly Parayil,Ankur Mallick,Spyros Mastorakis,Alind Khare,Chloi Alverti,Renee St Amant,Chetan Bansal,Victor Rühle,Josep Torrellas*

Main category: cs.DC

TL;DR: LoRAServe 是一种面向 LoRA 服务的动态适配器放置与路由框架，通过感知工作负载并利用 GPU Direct RDMA 技术，在真实生产环境中显著提升吞吐量、降低首 Token 延迟，并减少所需 GPU 数量。


<details>
  <summary>Details</summary>
Motivation: 现有 LoRA 服务系统在多租户环境下对不同秩（rank）的适配器进行共批处理时未考虑其大小差异，导致性能严重不均衡，GPU 资源利用率低，难以满足服务等级目标（SLO），需额外增加 GPU。

Method: 提出 LoRAServe 框架，采用工作负载感知的动态适配器放置策略，并结合 GPU Direct RDMA 实现远程访问，以应对 LoRA 适配器秩多样性带来的挑战。

Result: 在 Company X 的真实生产轨迹评估中，LoRAServe 相比现有最先进系统，吞吐量最高提升 2 倍，首 Token 延迟（TTFT）最多降低 9 倍，并在满足 SLO 的前提下最多节省 50% 的 GPU 资源。

Conclusion: LoRAServe 有效解决了 LoRA 多租户服务中因适配器秩异构导致的资源利用不均问题，显著提升了系统效率和可扩展性。

Abstract: Low-Rank Adaptation (LoRA) has become the de facto method for parameter-efficient fine-tuning of large language models (LLMs), enabling rapid adaptation to diverse domains. In production, LoRA-based models are served at scale, creating multi-tenant environments with hundreds of adapters sharing a base model. However, state-of-the-art serving systems co-batch heterogeneous adapters without accounting for rank (size) variability, leading to severe performance skew, which ultimately requires adding more GPUs to satisfy service-level objectives (SLOs). Existing optimizations, focused on loading, caching, and kernel execution, ignore this heterogeneity, leaving GPU resources underutilized. We present LoRAServe, a workload-aware dynamic adapter placement and routing framework designed to tame rank diversity in LoRA serving. By dynamically rebalancing adapters across GPUs and leveraging GPU Direct RDMA for remote access, LoRAServe maximizes throughput and minimizes tail latency under real-world workload drift. Evaluations on production traces from Company X show that LoRAServe elicits up to 2$\times$ higher throughput, up to 9$\times$ lower TTFT, while using up to 50% fewer GPUs under SLO constraints compared to state-of-the-art systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [18] [Technical knowledge and soft skills in software startups within the Colombian entrepreneurial ecosystem](https://arxiv.org/abs/2511.21769)
*Royer David Estrada-Esponda,Gerardo Matturro,Jose Reinaldo Sabogal-Pinilla*

Main category: cs.SE

TL;DR: 该研究调查了哥伦比亚软件初创企业创始团队在不同发展阶段最重视的技术知识（如需求工程、软件测试、敏捷方法等）和软技能（如沟通、领导力、团队合作），为创业者、孵化器和研究人员提供参考。


<details>
  <summary>Details</summary>
Motivation: 初创企业的成败很大程度上取决于创始团队成员的质量，因此有必要了解在软件初创企业早期及成长阶段，哪些技术知识和软技能最受重视。

Method: 通过对哥伦比亚软件初创企业代表进行问卷调查，收集并分析创始团队对各类技术知识和软技能的重视程度及其随企业发展阶段的变化。

Result: 研究发现最受重视的技术知识包括需求工程、软件测试、项目规划与管理、敏捷方法、市场营销、商业模式定义和预算编制；最受重视的软技能是沟通、领导力和团队合作。

Conclusion: 该研究结果有助于软件创业者、孵化器和研究人员更好地理解初创团队能力建设的关键要素，从而在不同阶段有针对性地培养或招募具备相应知识与技能的人才。

Abstract: The technical knowledge and soft skills of entrepreneurial team members significantly impact the early stages of software startups. It is widely recognized that the success or failure of a startup is determined by the quality of the individuals who constitute the founding team. This article presents the findings of a study conducted within the Colombian entrepreneurial ecosystem, focusing on which technical knowledge and soft skills are the most valued by founding teams of software startups, and how the needs for knowledge and skills evolve as the startup grows. A survey of software startup representatives revealed that the most valued knowledge includes requirements engineering, software testing, project planning and management, agile methodologies, marketing, business model definition, and budgeting. The most valued soft skills are typically communication, leadership, and teamwork. The outcomes of this work are relevant to software entrepreneurs, incubators, and researchers.

</details>


### [19] [Code Refactoring with LLM: A Comprehensive Evaluation With Few-Shot Settings](https://arxiv.org/abs/2511.21788)
*Md. Raihan Tapader,Md. Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLMs）的多语言代码重构框架，结合提示工程与少样本学习，在C、C++、C#、Python和Java上实现高效重构，并通过多种指标评估重构质量。实验表明，Java在正确性和可编译性方面表现最佳，而Python在结构改动最小的同时保持了适度语义相似性。


<details>
  <summary>Details</summary>
Motivation: 现有代码重构方法依赖手工编写规则，难以泛化到多种编程语言和编码风格。随着对简洁、清晰、高效和可持续代码的需求增加，亟需一种通用且高效的自动化重构方法。

Method: 提出一个结合指令微调、提示工程（如Temperature控制和不同少样本策略）与少样本学习的大语言模型框架，用于多语言代码重构。

Result: 实验结果显示：Java在10-shot设置下正确率达99.99%，平均可编译性达94.78%，语义相似性约53–54%；Python结构距离最小（约277–294），语义相似性约44–48%，表明其重构一致性高且干扰小。

Conclusion: 所提出的LLM驱动框架在多语言代码重构中表现出良好效果，尤其在Java和Python上实现了结构优化与语义保留的良好平衡，验证了提示工程与少样本学习在提升重构质量方面的有效性。

Abstract: In today's world, the focus of programmers has shifted from writing complex, error-prone code to prioritizing simple, clear, efficient, and sustainable code that makes programs easier to understand. Code refactoring plays a critical role in this transition by improving structural organization and optimizing performance. However, existing refactoring methods are limited in their ability to generalize across multiple programming languages and coding styles, as they often rely on manually crafted transformation rules. The objectives of this study are to (i) develop an Large Language Models (LLMs)-based framework capable of performing accurate and efficient code refactoring across multiple languages (C, C++, C#, Python, Java), (ii) investigate the impact of prompt engineering (Temperature, Different shot algorithm) and instruction fine-tuning on refactoring effectiveness, and (iii) evaluate the quality improvements (Compilability, Correctness, Distance, Similarity, Number of Lines, Token, Character, Cyclomatic Complexity) in refactored code through empirical metrics and human assessment. To accomplish these goals, we propose a fine-tuned prompt-engineering-based model combined with few-shot learning for multilingual code refactoring. Experimental results indicate that Java achieves the highest overall correctness up to 99.99% the 10-shot setting, records the highest average compilability of 94.78% compared to the original source code and maintains high similarity (Approx. 53-54%) and thus demonstrates a strong balance between structural modifications and semantic preservation. Python exhibits the lowest structural distance across all shots (Approx. 277-294) while achieving moderate similarity ( Approx. 44-48%) that indicates consistent and minimally disruptive refactoring.

</details>


### [20] [LLM-Empowered Event-Chain Driven Code Generation for ADAS in SDV systems](https://arxiv.org/abs/2511.21877)
*Nenad Petrovic,Norbert Kroth,Axel Torschmied,Yinglei Song,Fengjunjie Pan,Vahid Zolfaghari,Nils Purschke,Sven Kirchner,Chengdong Wu,Andre Schamschurko,Yi Zhang,Alois Knoll*

Main category: cs.SE

TL;DR: 本文提出了一种基于事件链驱动、大语言模型（LLM）增强的工作流，从自然语言需求生成经过验证的汽车代码，利用RAG技术检索车辆信号规范（VSS）以减少幻觉并确保架构正确性。


<details>
  <summary>Details</summary>
Motivation: 当前从自然语言需求自动生成汽车代码面临大语言模型幻觉和架构不一致的问题，亟需一种能确保信号正确使用与行为一致性的方法。

Method: 引入检索增强生成（RAG）层从VSS目录中检索相关信号作为提示上下文；将检索到的信号映射验证后转化为编码因果与时序约束的事件链，用以引导和约束LLM进行代码合成。

Result: 在紧急制动案例研究中，该方法实现了无需重新训练LLM即可生成信号使用正确、行为一致的代码。

Conclusion: 所提方法有效提升了从自然语言需求生成汽车代码的准确性与实时可行性，为LLM在安全关键系统中的应用提供了可行路径。

Abstract: This paper presents an event-chain-driven, LLM-empowered workflow for generating validated, automotive code from natural-language requirements. A Retrieval-Augmented Generation (RAG) layer retrieves relevant signals from large and evolving Vehicle Signal Specification (VSS) catalogs as code generation prompt context, reducing hallucinations and ensuring architectural correctness. Retrieved signals are mapped and validated before being transformed into event chains that encode causal and timing constraints. These event chains guide and constrain LLM-based code synthesis, ensuring behavioral consistency and real-time feasibility. Based on our initial findings from the emergency braking case study, with the proposed approach, we managed to achieve valid signal usage and consistent code generation without LLM retraining.

</details>


### [21] [Advancing Automated In-Isolation Validation in Repository-Level Code Translation](https://arxiv.org/abs/2511.21878)
*Kaiyao Ke,Ali Reza Ibrahimzada,Rangeet Pan,Saurabh Sinha,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: TRAM is a novel approach for repository-level code translation that combines retrieval-augmented type resolution with mock-based isolated validation to achieve high-quality Java-to-Python translations without relying on costly agent-based validation or manual interoperability efforts.


<details>
  <summary>Details</summary>
Motivation: Validating automatically translated code across entire repositories remains a major challenge due to semantic differences between languages and the high cost or manual effort required by existing validation methods.

Method: TRAM retrieves API documentation and contextual code information to perform context-aware type resolution using a large language model (LLM). It then constructs mock objects in the target language via a custom serialization/deserialization workflow, enabling each method to be validated in isolation.

Result: TRAM achieves state-of-the-art performance in Java-to-Python repository-level translation, demonstrating the effectiveness of combining RAG-based type mapping with in-isolation validation.

Conclusion: Integrating context-aware type resolution with mock-based isolated validation significantly improves the quality and feasibility of automatic repository-level code translation.

Abstract: Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.

</details>


### [22] [Toward Automated and Trustworthy Scientific Analysis and Visualization with LLM-Generated Code](https://arxiv.org/abs/2511.21920)
*Apu Kumar Chakroborti,Yi Ding,Lipeng Wan*

Main category: cs.SE

TL;DR: 本文评估开源大语言模型（LLMs）在科学数据分析与可视化任务中自动生成Python代码的可靠性，发现其在无干预情况下存在执行失败和理解不足的问题，并提出三种改进策略以提升生成代码的可执行性与正确性。


<details>
  <summary>Details</summary>
Motivation: 现代科学研究日益依赖大规模复杂数据的分析与可视化，但许多领域科学家缺乏编程能力，难以构建定制化分析流程。大语言模型有望通过自然语言生成可执行代码来降低这一门槛，但其在科学场景下的可信度尚不明确。

Method: 构建一个基于真实科研任务的提示词基准集，系统评估开源LLM生成代码的可执行性与正确性；并设计三种策略：数据感知的提示消歧、检索增强的提示优化和迭代式错误修复，以提升性能。

Result: 未经人工干预时，LLM生成的代码可靠性有限，常因提示模糊或缺乏领域理解而失败；所提出的三种策略显著提高了代码执行成功率和输出质量。

Conclusion: LLM在科学工作流自动化中具有潜力，但仍存在局限；本研究提供了可复用的基准和实用技术，为构建更包容、易用且可信的AI辅助科研工具奠定基础。

Abstract: As modern science becomes increasingly data-intensive, the ability to analyze and visualize large-scale, complex datasets is critical to accelerating discovery. However, many domain scientists lack the programming expertise required to develop custom data analysis workflows, creating barriers to timely and effective insight. Large language models (LLMs) offer a promising solution by generating executable code from natural language descriptions. In this paper, we investigate the trustworthiness of open-source LLMs in autonomously producing Python scripts for scientific data analysis and visualization. We construct a benchmark suite of domain-inspired prompts that reflect real-world research tasks and systematically evaluate the executability and correctness of the generated code. Our findings show that, without human intervention, the reliability of LLM-generated code is limited, with frequent failures caused by ambiguous prompts and the models' insufficient understanding of domain-specific contexts. To address these challenges, we design and assess three complementary strategies: data-aware prompt disambiguation, retrieval-augmented prompt enhancement, and iterative error repair. While these methods significantly improve execution success rates and output quality, further refinement is needed. This work highlights both the promise and current limitations of LLM-driven automation in scientific workflows and introduces actionable techniques and a reusable benchmark for building more inclusive, accessible, and trustworthy AI-assisted research tools.

</details>


### [23] [Beyond Like-for-Like: A User-centered Approach to Modernizing Legacy Applications](https://arxiv.org/abs/2511.21956)
*M. Polzin,M. Guzman*

Main category: cs.SE

TL;DR: 在现代化遗留应用程序时，不应仅做表面翻新，而应通过用户参与优化用户体验，兼顾新老用户需求，并利用原有系统的优势指导新设计。


<details>
  <summary>Details</summary>
Motivation: 避免在现代化过程中简单复制旧系统，从而延续其低效流程和痛点，导致新应用无法满足用户实际需求。

Method: 让用户积极参与现代化过程，结合遗留系统的既有信息与新GUI设计理念，平衡“原样复刻”与创新之间的关系。

Result: 通过利用现有遗留应用作为设计参考，并融合用户反馈，可开发出更直观、高效且被用户接受的新应用。

Conclusion: 遗留应用并非现代化的障碍，而是宝贵资源；通过用户参与和合理利用既有系统，可打造真正解决用户问题的新一代应用。

Abstract: When modernizing a legacy application, it is easy to fall back on a like-for-like replica with new tools and updated design stylings, but this is an opportunity to explore making a more intuitive application that supports user tasks and efficiency. Rather than having a blank canvas-unburdened by legacy tech debt-to create a new application, you are working with an existing application that is integral to accelerator operations and one that expert users are already familiar with. Due to this, you might assume people will prefer the like-for-like, but you could be carrying forward the pain points, processes that are inefficient, and ultimately wind up with an application that no one wants to use because it doesn't solve existing problems. Getting users involved can make all the difference in your approach to modernizing a legacy application that caters to both newer and expert users. It also can bridge the gap between like-for-like and introducing new GUI design. Having a legacy application doesn't have to make the modernized one difficult to develop, as the existing application is a tool in how you move forward with the new application. It provides insight into areas that an application with a clean slate doesn't give you.

</details>


### [24] [DRS-OSS: LLM-Driven Diff Risk Scoring Tool for PR Risk Prediction](https://arxiv.org/abs/2511.21964)
*Ali Sayedsalehi,Peter C. Rigby,Audris Mockus*

Main category: cs.SE

TL;DR: 本文提出了DRS-OSS，一个开源的差异风险评分系统，利用微调后的Llama 3.1 8B模型对代码提交进行缺陷风险预测，在ApacheJIT数据集上达到SOTA性能，并通过GitHub插件、Web界面和API集成到开发者工作流中。


<details>
  <summary>Details</summary>
Motivation: 在大型开源项目中，每天有大量拉取请求合并，可能引入回归缺陷。需要一种方法来评估每个代码差异（diff）引入缺陷的可能性，以优化代码审查优先级、测试计划和CI/CD门禁策略。

Method: 基于ApacheJIT数据集，使用参数高效微调技术（4-bit QLoRA）和DeepSpeed ZeRO-3 CPU卸载，在单张20GB GPU上训练Llama 3.1 8B序列分类器，输入包含提交信息、结构化diff和变更指标的长上下文（最长22k token）。系统提供GitHub插件、React仪表盘和API网关。

Result: 在ApacheJIT基准测试中，DRS-OSS取得F1=0.64、ROC-AUC=0.89的SOTA性能；模拟显示，仅对风险最高的30%提交实施门禁，可阻止高达86.4%的缺陷引入。

Conclusion: DRS-OSS是一个实用且高性能的开源差异风险评分系统，能有效支持缺陷预防和开发流程集成，并已公开发布完整复现包和部署资源。

Abstract: In large-scale open-source projects, hundreds of pull requests land daily, each a potential source of regressions. Diff Risk Scoring (DRS) estimates the likelihood that a diff will introduce a defect, enabling better review prioritization, test planning, and CI/CD gating. We present DRS-OSS, an open-source DRS system equipped with a public API, web UI, and GitHub plugin. DRS-OSS uses a fine-tuned Llama 3.1 8B sequence classifier trained on the ApacheJIT dataset, consuming long-context representations that combine commit messages, structured diffs, and change metrics. Through parameter-efficient adaptation, 4-bit QLoRA, and DeepSpeed ZeRO-3 CPU offloading, we train 22k-token contexts on a single 20 GB GPU. On the ApacheJIT benchmark, DRS-OSS achieves state-of-the-art performance (F1 = 0.64, ROC-AUC = 0.89). Simulations show that gating only the riskiest 30% of commits can prevent up to 86.4% of defect-inducing changes. The system integrates with developer workflows through an API gateway, a React dashboard, and a GitHub App that posts risk labels on pull requests. We release the full replication package, fine-tuning scripts, deployment artifacts, code, demo video, and public website.

</details>


### [25] [Statistical Independence Aware Caching for LLM Workflows](https://arxiv.org/abs/2511.22118)
*Yihan Dai,Dimitrios Stamatios Bouras,Haoxiang Jia,Sergey Mechtaev*

Main category: cs.SE

TL;DR: 本文提出Mnimi，一种新型缓存设计模式，在支持模块化大语言模型（LLM）工作流的同时，通过在LLM引用类型中封装统计独立性约束，确保组件级别的统计完整性，从而兼顾效率、可复现性与统计正确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理成本高、速度慢，本地缓存虽能降低成本和延迟并提升可复现性，但简单复用缓存会破坏概率性工作流所需的统计独立性，而现有缓存系统无法保障这一关键属性。

Method: 提出Mnimi缓存设计模式，将统计约束封装在LLM引用的类型中，用户可根据算法范围和需求管理与转换这些类型；使用Python结合装饰器和无限序列迭代器实现该模式。

Result: 在SpecFix（自动程序规范修复系统）案例研究中，Mnimi在保持统计正确性的同时，提升了可复现性、调试便利性以及时间和成本效率。

Conclusion: Mnimi有效解决了LLM缓存中统计独立性缺失的问题，为模块化LLM工作流提供了一种兼顾效率与统计完整性的实用方案。

Abstract: Large language models (LLMs) inference is both expensive and slow. Local caching of responses offers a practical solution to reduce the cost and latency of LLM queries. In research contexts, caching also enhances reproducibility and provides flexibility for experimentation. However, naive reuse of cached responses compromises statistical independence, a critical property for probabilistic workflows. In applications of LLM for code, it underpins performance metrics such as Pass@k and uncertainty estimation, as well as algorithms like program repair loops and retries. Existing LLM caching systems lack ways to enforce statistical independence constraints. To address this, we introduce Mnimi, a cache design pattern that supports modular LLM workflows while ensuring statistical integrity at the component level. Its core innovation lies in encapsulating statistical constraints within the type of LLM references, allowing users to manage and transform these types according to the scope and requirements of their algorithm. We implemented this design pattern in Python using a combination of decorators and iterators over infinite sequences. A case study on SpecFix, an recent automated program specification repair system, highlights how Mnimi improves reproducibility, ease of debugging, time and cost efficiency while preserving statistical correctness.

</details>


### [26] [Exploring the SECURITY.md in the Dependency Chain: Preliminary Analysis of the PyPI Ecosystem](https://arxiv.org/abs/2511.22186)
*Chayanid Termphaiboon,Raula Gaikovina Kula,Youmei Fan,Morakot Choetkiertikul,Chaiyong Ragkhitwetsagul,Thanwadee Sunetnanta,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: 该研究发现，拥有 SECURITY.md 安全策略的 PyPI 项目倾向于使用更多直接依赖项，并更频繁地更新依赖，表明安全策略与更模块化、功能丰富的项目相关，并有助于提升依赖管理的主动性。


<details>
  <summary>Details</summary>
Motivation: 尽管 SECURITY.md 等安全策略在开源项目中日益普及，但其对软件依赖结构和演化的影响尚不明确。本研究旨在探究安全策略与依赖管理之间的关系。

Method: 通过对比分析包含与不包含 SECURITY.md 文件的 PyPI 项目，考察其依赖树结构及随时间变化的依赖更新行为。

Result: 拥有安全策略的项目通常具有更广泛的直接依赖，但整体依赖深度和传递依赖相似；特别是后期采用 SECURITY.md 的项目表现出更频繁的依赖更新。

Conclusion: SECURITY.md 与更模块化、功能丰富的项目相关，并在促进主动依赖管理、降低软件供应链风险方面发挥积极作用。

Abstract: Security policies, such as SECURITY.md files, are now common in open-source projects. They help guide responsible vulnerability reporting and build trust among users and contributors. Despite their growing use, it is still unclear how these policies influence the structure and evolution of software dependencies. Software dependencies are external packages or libraries that a project relies on, and their interconnected nature affects both functionality and security. This study explores the relationship between security policies and dependency management in PyPI projects. We analyzed projects with and without a SECURITY.md file by examining their dependency trees and tracking how dependencies change over time. The analysis shows that projects with a security policy tend to rely on a broader set of direct dependencies, while overall depth and transitive dependencies remain similar. Historically, projects created after the introduction of SECURITY.md, particularly later adopters, show more frequent dependency updates. These results suggest that security policies are linked to more modular and feature-rich projects, and highlight the role of SECURITY.md in promoting proactive dependency management and reducing risks in the software supply chain.

</details>


### [27] [NOMAD: A Multi-Agent LLM System for UML Class Diagram Generation from Natural Language Requirements](https://arxiv.org/abs/2511.22409)
*Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.SE

TL;DR: 本文提出了NOMAD，一个受认知启发的多智能体框架，用于生成UML类图。该框架将建模任务分解为多个专业化子任务，提升了可解释性与验证能力，在实验中优于现有基线，并首次系统性地对LLM生成UML中的错误进行了分类。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在软件工程中的应用日益广泛，但其生成如UML图等结构化制品的能力尚未得到充分探索。现有方法缺乏对建模过程的细粒度控制、可解释性和可靠验证机制。

Method: 提出NOMAD框架，采用模块化多智能体架构，将UML生成分解为实体提取、关系分类和图表合成等角色专用的子任务，模拟工程师的目标导向推理过程，并结合针对性验证策略。

Result: 在Northwind案例和人工编写的UML练习上评估表明，NOMAD优于所有基线模型；同时揭示了属性提取等细粒度任务的持续挑战，并构建了首个LLM生成UML错误的系统分类体系。

Conclusion: NOMAD不仅是一个高效的UML类图生成框架，也为语言到模型转换流程中的可靠性研究提供了新视角，验证机制虽效果不一，但自适应策略展现出潜力。

Abstract: Large Language Models (LLMs) are increasingly utilised in software engineering, yet their ability to generate structured artefacts such as UML diagrams remains underexplored. In this work we present NOMAD, a cognitively inspired, modular multi-agent framework that decomposes UML generation into a series of role-specialised subtasks. Each agent handles a distinct modelling activity, such as entity extraction, relationship classification, and diagram synthesis, mirroring the goal-directed reasoning processes of an engineer. This decomposition improves interpretability and allows for targeted verification strategies. We evaluate NOMAD through a mixed design: a large case study (Northwind) for in-depth probing and error analysis, and human-authored UML exercises for breadth and realism. NOMAD outperforms all selected baselines, while revealing persistent challenges in fine-grained attribute extraction. Building on these observations, we introduce the first systematic taxonomy of errors in LLM-generated UML diagrams, categorising structural, relationship, and semantic/logical. Finally, we examine verification as a design probe, showing its mixed effects and outlining adaptive strategies as promising directions. Together, these contributions position NOMAD as both an effective framework for UML class diagram generation and a lens onto the broader research challenges of reliable language-to-model workflows.

</details>


### [28] [Declarative Policy Control for Data Spaces: A DSL-Based Approach for Manufacturing-X](https://arxiv.org/abs/2511.22513)
*Jérôme Pfeiffer,Nicolai Maisch,Sebastian Friedl,Matthias Milan Strljic,Armin Lechler,Oliver Riedel,Andreas Wortmann*

Main category: cs.SE

TL;DR: 本文提出一种基于领域特定语言（DSL）的方法，使非编程背景的领域专家能够以声明式、可读且可执行的方式定义细粒度的数据使用策略，用于联邦数据空间中的主权数据共享。


<details>
  <summary>Details</summary>
Motivation: 在工业4.0背景下，尽管已有如AAS、EDC、ID-Link和OPC UA等技术框架支持跨组织的安全数据共享，但如何让非软件工程背景的领域专家有效描述和实施上下文相关的数据使用策略仍是一个关键挑战。

Method: 引入一种领域特定语言（DSL），支持声明式、人类可读且机器可执行的策略定义，使领域专家无需编写命令式代码即可设定细粒度的数据治理规则。

Result: 该方法使领域专家能够直接指定如“仅限特定生产批次的数据访问”或“在保留期后自动删除数据”等策略，提升了数据治理的可用性和可操作性。

Conclusion: 通过DSL赋能领域专家参与数据策略制定，有效弥合了技术实现与业务需求之间的鸿沟，为联邦数据空间中的主权数据共享提供了实用解决方案。

Abstract: The growing adoption of federated data spaces, such as in the GAIA-X and the International Data Spaces (IDS) initiative, promises secure and sovereign data sharing across organizational boundaries in Industry 4.0. In manufacturing ecosystems, this enables use cases, such as cross-factory process optimization, predictive maintenance, and supplier integration. Frameworks and standards, such as the Asset Administration Shell (AAS), Eclipse Dataspace Connector (EDC), ID-Link and Open Platform Communications Unified Architecture (OPC UA) provide a strong foundation to realize this ecosystem. However, a major open challenge is the practical description and enforcement of context-dependent data usage policies using these base technologies - especially by domain experts without software engineering backgrounds. Therefore, this article proposes a method for leveraging domain-specific languages (DSLs) to enable declarative, human-readable, and machine-executable policy definitions for sovereign data sharing via data space connectors. The DSL empowers domain experts to specify fine-grained data governance requirements - such as restricting access to data from specific production batches or enforcing automatic deletion after a defined retention period - without writing imperative code.

</details>


### [29] [The Repeat Offenders: Characterizing and Predicting Extremely Bug-Prone Source Methods](https://arxiv.org/abs/2511.22726)
*Ethan Friesen,Sasha Morton-Salmon,Md Nahidul Islam Opu,Shahidul Islam,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: 该论文对98个开源Java项目中超过125万方法进行了大规模研究，发现“极缺陷方法”（ExtremelyBuggy methods）虽占比极小，却集中了大量缺陷；这些方法在初始阶段就表现出更大、更复杂、可读性和可维护性更差等特征，但因数据不平衡、项目异质性及缺陷多在演化中产生等原因，早期预测仍不可靠。通过主题分析揭示了其常见视觉问题、上下文角色和缺陷模式，为高风险方法的早期识别提供了实践指导。


<details>
  <summary>Details</summary>
Motivation: 识别反复引发缺陷的少量源代码子集对于降低长期维护成本至关重要，但现有研究缺乏对这类“极缺陷方法”的系统性理解与有效预测手段。

Method: 结合大规模定量分析（基于1.25M+方法的数据集与5种机器学习模型）与定性主题分析（对265个极缺陷方法进行人工编码），从代码特征、可预测性及演化角度展开研究。

Result: 极缺陷方法占比极小但缺陷集中；初始即具更高复杂度与更低质量；现有模型难以可靠预测；主题分析揭示了三类共性：视觉混乱、核心功能角色、典型缺陷模式。

Conclusion: 极缺陷方法具有显著早期特征但难以准确预测，需发展融合演化信息的代码表征方法，研究结果为开发者优先审查高风险代码提供了实证依据与实用洞见。

Abstract: Identifying the small subset of source code that repeatedly attracts bugs is critical for reducing long-term maintenance effort. We define ExtremelyBuggy methods as those involved in more than one bug fix and present the first large-scale study of their prevalence, characteristics, and predictability. Using a dataset of over 1.25 million methods from 98 open-source Java projects, we find that ExtremelyBuggy methods constitute only a tiny fraction of all methods, yet frequently account for a disproportionately large share of bugs. At their inception, these methods are significantly larger, more complex, less readable, and less maintainable than both singly-buggy and non-buggy methods. However, despite these measurable differences, a comprehensive evaluation of five machine learning models shows that early prediction of ExtremelyBuggy methods remains highly unreliable due to data imbalance, project heterogeneity, and the fact that many bugs emerge through subsequent evolution rather than initial implementation. To complement these quantitative findings, we conduct a thematic analysis of 265 ExtremelyBuggy methods, revealing recurring visual issues (e.g., confusing control flow, poor readability), contextual roles (e.g., core logic, data transformation, external resource handling), and common defect patterns (e.g., faulty conditionals, fragile error handling, misuse of variables). These results highlight the need for richer, evolution-aware representations of code and provide actionable insights for practitioners seeking to prioritize high-risk methods early in the development lifecycle.

</details>


### [30] [MBFL-DKMR: Improving Mutation-based Fault Localization through Denoising-based Kill Matrix Refinement](https://arxiv.org/abs/2511.22921)
*Hengyuan Liu,Xia Song,Yong Liu,Zheng Li*

Main category: cs.SE

TL;DR: 本文提出了一种基于信号去噪思想的新方法DKMR，用于优化变异体-测试关系矩阵（kill matrix），从而提升基于变异的故障定位（MBFL）的准确性。实验表明，该方法在Defects4J数据集上显著优于现有技术，且计算开销极低。


<details>
  <summary>Details</summary>
Motivation: 现有MBFL方法受到“噪声”（即错误的变异体-测试杀死关系）的严重影响，而当前的改进策略仅修正最终定位结果，未从根源上处理噪声问题。

Method: 将kill matrix视为包含故障相关信号和高频噪声的混合信号，提出DKMR框架：首先通过混合矩阵构建增强信号，然后在频域进行滤波去噪，最后利用去噪后的模糊值矩阵进行可疑度计算。

Result: 在Defects4J v2.0.0上的实验显示，MBFL-DKMR在Top-1定位了129个故障，优于BLMu（85个）和Delta4Ms（103个），且额外计算开销仅为0.11秒。

Conclusion: 将信号处理中的去噪思想应用于kill matrix的优化，能有效提升MBFL的定位效果，为软件调试提供了新思路。

Abstract: Software debugging is a critical and time-consuming aspect of software development, with fault localization being a fundamental step that significantly impacts debugging efficiency. Mutation-Based Fault Localization (MBFL) has gained prominence due to its robust theoretical foundations and fine-grained analysis capabilities. However, recent studies have identified a critical challenge: noise phenomena, specifically the false kill relationships between mutants and tests, which significantly degrade localization effectiveness. While several approaches have been proposed to rectify the final localization results, they do not directly address the underlying noise. In this paper, we propose a novel approach to refine the kill matrix, a core data structure capturing mutant-test relationships in MBFL, by treating it as a signal that contains both meaningful fault-related patterns and high-frequency noise. Inspired by signal processing theory, we introduce DKMR (Denoising-based Kill Matrix Refinement), which employs two key stages: (1) signal enhancement through hybrid matrix construction to improve the signal-to-noise ratio for better denoising, and (2) signal denoising via frequency domain filtering to suppress noise while preserving fault-related patterns. Building on this foundation, we develop MBFL-DKMR, a fault localization framework that utilizes the refined matrix with fuzzy values for suspiciousness calculation. Our evaluation on Defects4J v2.0.0 demonstrates that MBFL-DKMR effectively mitigates the noise and outperforms the state-of-the-art MBFL techniques. Specifically, MBFL-DKMR achieves 129 faults localized at Top-1 compared to 85 for BLMu and 103 for Delta4Ms, with negligible additional computational overhead (0.11 seconds, 0.001\% of total time). This work highlights the potential of signal processing techniques to enhance the effectiveness of MBFL by refining the kill matrix.

</details>


### [31] [Software for Studying CASCADE Error Correction Protocols in Quantum Communications](https://arxiv.org/abs/2511.23050)
*Nikita Repnkiov,Vladimir Faerman*

Main category: cs.SE

TL;DR: 本文针对量子计算威胁下的量子通信需求，聚焦CASCADE协议，开发了一个用于研究与教学的软件原型，通过基于Actor模型的并行纠错算法提升了密钥协调效率，并提出了若干架构与功能改进方向。


<details>
  <summary>Details</summary>
Motivation: 随着量子计算的发展，传统加密面临威胁，量子通信中的密钥协调变得至关重要；现有方法在效率和可扩展性方面存在不足，亟需优化与验证工具。

Method: 设计并实现了一个基于Actor模型的并行错误校正算法的CASCADE协议软件原型，用于密钥协调，并对其性能和实现细节进行评估。

Result: 原型验证了CASCADE核心算法的正确实现，提高了密钥协调效率并减少了通信数据量，但也暴露出消息传递开销大、错误处理复杂和代码冗余等问题。

Conclusion: 所开发的原型为密钥协调研究提供了有效平台，未来工作将聚焦于系统架构重构、中间数据导出接口、通信信道模块化及更完善的验证与分析工具。

Abstract: This article addresses the development of quantum communication methods in the context of emerging quantum computing threats and emphasizes the importance of key reconciliation in quantum communication systems. The study focuses on the CASCADE protocol and the design of a software prototype intended for research and educational purposes. A parallel error-correction algorithm based on the actor model was implemented, improving the efficiency of key reconciliation and reducing the amount of exchanged data. Evaluation of the prototype revealed limitations, including the computational cost of message passing, complexity of error handling, and code redundancy due to iterative development. Experimental results confirmed the correct implementation of the core CASCADE algorithms and informed the design of future improvements. Proposed enhancements include redesigning the system architecture, developing interfaces for exporting intermediate data, defining the communication channel as a separate component, and expanding tools for systematic verification and comparative analysis of blind key-reconciliation methods.

</details>


### [32] [Amplifiers or Equalizers? A Longitudinal Study of LLM Evolution in Software Engineering Project-Based Learning](https://arxiv.org/abs/2511.23157)
*Hana Kataoka,Jialong Li,Yutaka Matsuno*

Main category: cs.SE

TL;DR: 本文通过两年纵向研究，探讨了最新大语言模型（LLM）在软件工程项目式学习中的双重作用：既提升整体学生表现（尤其是编程能力较弱者），又加剧了成绩差距。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）重塑软件开发，将其整合进软件工程（SE）教育变得迫切；然而现有研究多聚焦于入门编程或孤立任务，缺乏对开放性项目式学习（PBL）中LLM影响的探索。

Method: 开展为期两年的纵向研究，比较2024年使用早期免费LLM（n=48）与2025年使用最新付费LLM（n=46）的两个学生群体在软件工程项目式学习中的表现差异。

Result: 最新强大的LLM兼具“均衡器”和“放大器”双重角色：一方面提升整体平均表现，使编程能力较弱的学生也能参与更真实的软件工程实践；另一方面显著拉大绝对成绩差距，带来新的教育公平挑战。

Conclusion: 在软件工程教育中引入先进LLM虽能促进实践真实性并提升弱势学生表现，但也可能加剧教育不平等，需设计相应教学策略以应对这一双重效应。

Abstract: As LLMs reshape software development, integrating LLM-augmented practices into SE education has become imperative. While existing studies explore LLMs' educational use in introductory programming or isolated SE tasks, their impact in more open-ended Project-Based Learning (PBL) remains unexplored. This paper introduces a two-year longitudinal study comparing a 2024 (using early free LLMs, $n$=48) and 2025 (using the latest paid LLMs, $n$=46) cohort. Our findings suggest the latest powerful LLMs' dual role: they act as "equalizers," boosting average performance even for programming-weak students, providing opportunities for more authentic SE practices; yet also as "amplifiers," dramatically widening absolute performance gaps, creating new pedagogical challenges for addressing educational inequities.

</details>


### [33] [AI for software engineering: from probable to provable](https://arxiv.org/abs/2511.23159)
*Bertrand Meyer*

Main category: cs.SE

TL;DR: Vibe coding struggles with goal specification and hallucinations; combining AI creativity with formal methods and verification offers a solution.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of vibe coding—namely, the difficulty of accurately specifying programming goals and the risk of AI-generated code hallucinations that compromise correctness.

Method: Integrate artificial intelligence with formal specification techniques and formal program verification, leveraging modern proof tools to ensure correctness.

Result: A hybrid approach that retains AI's creativity while enforcing the rigor needed for producing correct or near-correct programs.

Conclusion: The fusion of AI and formal methods is essential to overcome the limitations of current AI-assisted programming and achieve reliable software development.

Abstract: Vibe coding, the much-touted use of AI techniques for programming, faces two overwhelming obstacles: the difficulty of specifying goals ("prompt engineering" is a form of requirements engineering, one of the toughest disciplines of software engineering); and the hallucination phenomenon. Programs are only useful if they are correct or very close to correct.
  The solution? Combine the creativity of artificial intelligence with the rigor of formal specification methods and the power of formal program verification, supported by modern proof tools.

</details>


### [34] [GAPS: Guiding Dynamic Android Analysis with Static Path Synthesis](https://arxiv.org/abs/2511.23213)
*Samuele Doria,Eleonora Losiouk*

Main category: cs.SE

TL;DR: GAPS 是首个结合静态方法导向调用图分析与动态交互驱动执行的系统，显著优于现有工具在 Android 应用中动态触达目标方法的能力。


<details>
  <summary>Details</summary>
Motivation: 当前 Android 应用中的方法可达性问题尚未有效解决，尤其对于未嵌入图形界面的方法（如库函数），现有 GUI 测试和静态调用图构建工具难以可靠地驱动执行到达这些目标方法，而这对漏洞验证、调试和行为分析至关重要。

Method: GAPS 采用轻量级的、由数据流分析引导的调用图后向遍历，重构通往目标方法的路径，并将这些路径转化为运行时应用探索的指令，从而实现静态与动态分析的结合。

Result: 在 AndroTest 基准上，GAPS 静态识别 88.24% 目标方法路径（平均 4.27 秒/应用），动态触达 57.44%；远超 APE（12.82%）、GoalExplorer（9.69%）和 Guardian（17.12%）等动态工具，也优于 FlowDroid（58.81%）和 DroidReach（9.48%）等静态工具。在 50 个热门真实应用中，GAPS 平均静态分析耗时 278.9 秒，静态覆盖 62.03% 目标方法，动态触达 59.86%。

Conclusion: GAPS 在静态路径重构和动态方法触达方面均显著优于现有技术，展现出在真实场景下分析安全关键代码的强大实用性。

Abstract: Dynamically resolving method reachability in Android applications remains a critical and largely unsolved problem. Despite notable advancements in GUI testing and static call graph construction, current tools are insufficient for reliably driving execution toward specific target methods, especially those not embedded in a graphical component (e.g., libraries' methods), a capability essential for tasks such as vulnerability validation, debugging, and behavioral analysis.
  We present GAPS (Graph-based Automated Path Synthesizer), the first system that integrates static, method-guided call graph analysis with dynamic, interaction-driven execution. GAPS performs a lightweight backward traversal of the call graph, guided by data-flow analysis, to reconstruct paths reaching the target methods. These paths are then translated into instructions that guide runtime app exploration.
  On the AndroTest benchmark, GAPS statically identifies paths to reach 88.24\% of the target methods in just 4.27 seconds per app and dynamically reaches 57.44\% of them. In contrast, state-of-the-art dynamic interaction tools show significantly lower reachability over three runs: APE, one of the best model-based GUI testers, achieves 12.82\%, while GoalExplorer, a hybrid analysis tool, reaches 9.69\%, and Guardian, an LLM-based UI automator, reaches 17.12\%. Static analysis tools also fall short: FlowDroid and DroidReach identify paths to reach 58.81\% and 9.48\% of the targets, requiring 35.06 seconds and 23.46 seconds per app, respectively.
  Finally, an evaluation on the 50 most downloaded real-world apps demonstrates GAPS's practical utility in analyzing security-critical code under a realistic scenario. With an average static analysis time of 278.9 seconds, GAPS statically reconstructs paths to 62.03\% of the target methods and dynamically reaches 59.86\% of them.

</details>


### [35] [FLIMs: Fault Localization Interference Mutants, Definition, Recognition and Mitigation](https://arxiv.org/abs/2511.23302)
*Hengyuan Liu,Zheng Li,Donghua Wang,Yankai Wu,Xiang Chen,Yong Liu*

Main category: cs.SE

TL;DR: 本文提出MBFL-FLIM框架，通过引入故障定位干扰变异体（FLIMs）的概念，并利用基于大语言模型（LLM）的语义分析识别和缓解这些干扰变异体，从而提升基于变异的故障定位（MBFL）的效果。在Defects4J基准上的实验表明，该方法在Top-1指标上平均多定位44个故障，优于传统SBFL、MBFL及其他先进方法。


<details>
  <summary>Details</summary>
Motivation: MBFL方法在自动化调试中受到干扰变异体（即非故障代码生成但能被失败测试杀死的变异体）的影响，这些变异体会模仿真实故障行为，降低定位精度。为解决此问题，作者提出识别并缓解这类干扰变异体。

Method: 基于RIPR模型理论分析干扰原因，提出利用LLM进行语义分析以识别FLIMs，并结合微调与置信度估计策略增强LLM输出稳定性；随后将识别出的FLIMs用于修正MBFL中的可疑度评分，构建MBFL-FLIM框架。

Result: 在Defects4J的395个程序版本上使用8种LLM进行实验，MBFL-FLIM在Top-1指标上平均比基线方法多定位44个故障，且在多故障场景下表现稳健，消融实验证实了微调和置信度估计的有效性。

Conclusion: 通过语义识别与缓解FLIMs，MBFL-FLIM显著提升了MBFL的故障定位能力，证明了结合LLM语义分析与传统MBFL流程的有效性。

Abstract: Mutation-based Fault Localization (MBFL) has been widely explored for automated software debugging, leveraging artificial mutants to identify faulty code entities. However, MBFL faces significant challenges due to interference mutants generated from non-faulty code entities but can be killed by failing tests. These mutants mimic the test sensitivity behaviors of real faulty code entities and weaken the effectiveness of fault localization. To address this challenge, we introduce the concept of Fault Localization Interference Mutants (FLIMs) and conduct a theoretical analysis based on the Reachability, Infection, Propagation, and Revealability (RIPR) model, identifying four distinct interference causes. Building on this, we propose a novel approach to semantically recognize and mitigate FLIMs using LLM-based semantic analysis, enhanced by fine-tuning techniques and confidence estimation strategies to address LLM output instability. The recognized FLIMs are then mitigated by refining the suspiciousness scores calculated from MBFL techniques. We integrate FLIM recognition and mitigation into the MBFL workflow, developing MBFL-FLIM, a fault localization framework that enhances MBFL's effectiveness by reducing misleading interference while preserving real fault-revealing information. Our empirical experiments on the Defects4J benchmark with 395 program versions using eight LLMs demonstrate MBFL-FLIM's superiority over traditional SBFL and MBFL methods, advanced dynamic feature-based approaches, and recent LLM-based fault localization techniques. Specifically, MBFL-FLIM achieves an average improvement of 44 faults in the Top-1 metric, representing a significant enhancement over baseline methods. Further evaluation confirms MBFL-FLIM's robust performance in multi-fault scenarios, with ablation experiments validating the contributions of the fine-tuning and confidence estimation components.

</details>


### [36] [Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing](https://arxiv.org/abs/2511.23321)
*Yifei Wang,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Yuchen Cao*

Main category: cs.SE

TL;DR: 本文提出C2C-MoLA框架，结合Mixture of Experts（MoE）与Low-Rank Adaptation（LoRA），在Chart-to-code任务中显著提升生成准确性、内存效率和训练收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图表到代码生成任务中难以兼顾跨类型泛化能力、内存效率和模块化设计，亟需更高效的多模态模型架构。

Method: C2C-MoLA采用复杂度感知的MoE路由机制，结合领域专家和负载均衡稀疏门控，并利用LoRA实现参数高效微调；同时设计对齐路由稳定性与语义准确性的训练策略。

Result: 在Chart2Code-160k数据集上，相比标准微调和仅LoRA基线，该方法生成准确率最高提升17%，峰值GPU内存减少18%，收敛速度加快20%。

Conclusion: C2C-MoLA有效解决了图表到代码生成中的关键挑战，具备良好的可扩展性和实际应用潜力，为多模态代码生成提供了新思路。

Abstract: Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [37] [Aquas: Enhancing Domain Specialization through Holistic Hardware-Software Co-Optimization based on MLIR](https://arxiv.org/abs/2511.22267)
*Yuyang Zou,Youwei Xiao,Yansong Xu,Chenyun Yin,Yuhao Luo,Yitian Sun,Ruifan Xu,Renze Chen,Yun Liang*

Main category: cs.AR

TL;DR: Aquas is a hardware-software co-design framework based on MLIR that improves RISC-V-based ASIP performance through burst DMA and HLS optimizations in hardware, and an e-graph–based retargetable compiler with a novel matching engine, achieving up to 9.27x speedup on real-world workloads.


<details>
  <summary>Details</summary>
Motivation: Existing open-source RISC-V frameworks for ASIPs suffer from limited performance due to constrained hardware synthesis capabilities and inflexible compiler support, hindering effective application specialization.

Method: The authors propose Aquas, a co-design framework leveraging MLIR. It introduces a burst DMA engine and advanced HLS techniques for hardware acceleration, and employs an e-graph–based retargetable compilation approach with a new instruction-matching engine on the software side.

Result: Evaluation shows Aquas achieves up to 9.27× speedup on practical applications such as point cloud processing and large language model (LLM) inference.

Conclusion: Aquas effectively bridges hardware and compiler limitations in current RISC-V ASIP frameworks, delivering significant performance gains through integrated co-design innovations.

Abstract: Application-Specific Instruction-Set Processors (ASIPs) built on the RISC-V architecture offer specialization opportunities for various applications. However, existing frameworks from the open-source RISC-V ecosystem suffer from limited performance due to restricted hardware synthesis and rigid compiler support. To address these challenges, we introduce Aquas, a holistic hardware-software co-design framework built upon MLIR. Aquas enhances ASIP synthesis with fast memory access capability via a burst DMA engine and advanced high-level synthesis (HLS) optimizations. On the compiler side, we propose an e-graph based retargetable approach with a novel matching engine for efficient instruction matching. Evaluation demonstrates up to 9.27x speedup on real-world workloads, including point cloud processing and LLM inference.

</details>


### [38] [3RSeT: Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison](https://arxiv.org/abs/2511.22551)
*Elham Cheshmikhani,Hamed Farbeh,Hossein Asad*

Main category: cs.AR

TL;DR: 本文提出了一种名为3RSeT的低成本方案，通过选择性标签比较显著降低STT-MRAM缓存中的读干扰错误率，从而提升可靠性、降低能耗，并几乎不影响性能和面积。


<details>
  <summary>Details</summary>
Motivation: STT-MRAM作为SRAM的潜在替代品，在片上缓存中面临严重的读干扰错误问题，尤其在并行标签比较操作时未被充分研究，亟需有效缓解方法。

Method: 提出3RSeT方案，利用标签低位信息在每次访问时主动禁用不可能命中（hit）的标签，从而减少不必要的标签读取，降低读干扰发生率。

Result: 实验表明，3RSeT将标签阵列的读干扰率降低71.8%，平均故障间隔时间（MTTF）提升3.6倍，能耗降低62.1%，且性能无损、面积开销低于0.4%。

Conclusion: 3RSeT是一种高效、低成本的解决方案，能显著提升STT-MRAM缓存的可靠性与能效，适用于未来高密度非易失性缓存设计。

Abstract: Recent development in memory technologies has introduced Spin-Transfer Torque Magnetic RAM (STT-MRAM) as the most promising replacement for SRAMs in on-chip cache memories. Besides its lower leakage power, higher density, immunity to radiation-induced particles, and non-volatility, an unintentional bit flip during read operation, referred to as read disturbance error, is a severe reliability challenge in STT-MRAM caches. One major source of read disturbance error in STT-MRAM caches is simultaneous accesses to all tags for parallel comparison operation in a cache set, which has not been addressed in previous work. This paper first demonstrates that high read accesses to tag array extremely increase the read disturbance rate and then proposes a low-cost scheme, so-called Read Disturbance Rate Reduction in STT-MRAM Caches by Selective Tag Comparison (3RSeT), to reduce the error rate by eliminating a significant portion of tag reads. 3RSeT proactively disables the tags that have no chance for hit, using low significant bits of the tags on each access request. Our evaluations using gem5 full-system cycle-accurate simulator show that 3RSeT reduces the read disturbance rate in the tag array by 71.8%, which results in 3.6x improvement in Mean Time To Failure (MTTF). In addition, the energy consumption is reduced by 62.1% without compromising performance and with less than 0.4% area overhead.

</details>


### [39] [The Immutable Tensor Architecture: A Pure Dataflow Approach for Secure, Energy-Efficient AI Inference](https://arxiv.org/abs/2511.22889)
*Fang Li*

Main category: cs.AR

TL;DR: 提出了一种名为“不可变张量架构”（ITA）的新硬件架构，通过将大语言模型权重固化到ASIC的金属互连与逻辑中，彻底消除内存层级，从而突破边缘设备部署LLM时面临的“内存墙”瓶颈。


<details>
  <summary>Details</summary>
Motivation: 当前在消费级边缘设备上部署大语言模型受限于“内存墙”问题——每次生成token都需要从DRAM中读取GB级模型权重，带来巨大的带宽和能耗开销；现有GPU/NPU架构将权重视为可变软件数据，为保持通用可编程性而付出高昂能耗代价。

Method: 提出ITA架构，将模型权重编码进成熟工艺节点（28nm/40nm）ASIC的物理电路拓扑中，使其成为不可变的硬件结构；并采用“Split-Brain”系统设计，由主机CPU处理动态KV缓存，ITA ASIC作为无状态、内嵌ROM的数据流引擎。

Result: 该方法消除了传统内存层次结构，显著降低权重访问的能耗与延迟，为在边缘设备高效部署LLM提供新路径。

Conclusion: 将模型权重视为物理电路而非软件数据，是一种突破“内存墙”的有效范式转变，ITA架构展示了在成熟制程下实现高能效LLM推理的可行性。

Abstract: The deployment of Large Language Models (LLMs) on consumer edge devices is throttled by the "Memory Wall" -- the prohibitive bandwidth and energy cost of fetching gigabytes of model weights from DRAM for every token generated. Current architectures (GPUs, NPUs) treat model weights as mutable software data, incurring massive energy penalties to maintain general-purpose programmability. We propose The Immutable Tensor Architecture (ITA), a paradigm shift that treats model weights not as data, but as physical circuit topology. By encoding parameters directly into the metal interconnects and logic of mature-node ASICs (28nm/40nm), ITA eliminates the memory hierarchy entirely. We present a "Split-Brain" system design where a host CPU manages dynamic KV-cache operations while the ITA ASIC acts as a stateless, ROM-embedded dataflow engine.

</details>


### [40] [Cohet: A CXL-Driven Coherent Heterogeneous Computing Framework with Hardware-Calibrated Full-System Simulation](https://arxiv.org/abs/2511.23011)
*Yanjing Wang,Lizhou Wu,Sunfeng Gao,Yibo Tang,Junhui Luo,Zicong Wang,Yang Ou,Dezun Dong,Nong Xiao,Mingche Lai*

Main category: cs.AR

TL;DR: 本文提出了Cohet——首个基于CXL的缓存一致异构计算框架，通过解耦计算与内存资源构建统一一致内存池，并提供标准内存接口；同时开发了高精度全系统模拟器SimCXL，实验表明CXL在延迟、带宽及典型应用（如RAO和RPC）上显著优于传统PCIe方案。


<details>
  <summary>Details</summary>
Motivation: 传统基于PCIe的异构计算系统存在细粒度主机-设备交互效率低和编程模型复杂的问题；尽管CXL等缓存一致互连标准兴起，但受限于硬件平台稀缺、软硬件生态不成熟及应用场景不明，相关研究进展缓慢。

Method: 提出Cohet框架，将CPU与XPU的计算和内存资源解耦，构建共享的统一缓存一致内存池，并向计算线程暴露标准malloc/mmap接口；同时开发支持所有CXL子协议和设备类型的周期级全系统模拟器SimCXL，并通过真实CXL测试平台校准。

Result: 评估显示，相比DMA传输，CXL.cache在缓存行粒度下延迟降低68%，带宽提升14.4倍；在远程原子操作（RAO）和远程过程调用（RPC）两个典型应用中，CXL-NIC相较PCIe-NIC分别实现5.5–40.2倍和平均1.86倍的加速。

Conclusion: CXL驱动的缓存一致异构计算能显著提升系统性能与编程效率，Cohet框架与SimCXL模拟器为该领域研究提供了有效工具和验证基础，展示了CXL在下一代异构计算中的巨大潜力。

Abstract: Conventional heterogeneous computing systems built on PCIe interconnects suffer from inefficient fine-grained host-device interactions and complex programming models. In recent years, many proprietary and open cache-coherent interconnect standards have emerged, among which compute express link (CXL) prevails in the open-standard domain after acquiring several competing solutions. Although CXL-based coherent heterogeneous computing holds the potential to fundamentally transform the collaborative computing mode of CPUs and XPUs, research in this direction remains hampered by the scarcity of available CXL-supported platforms, immature software/hardware ecosystems, and unclear application prospects. This paper presents Cohet, the first CXL-driven coherent heterogeneous computing framework. Cohet decouples the compute and memory resources to form unbiased CPU and XPU pools which share a single unified and coherent memory pool. It exposes a standard malloc/mmap interface to both CPU and XPU compute threads, leaving the OS dealing with smart memory allocation and management of heterogeneous resources. To facilitate Cohet research, we also present a full-system cycle-level simulator named SimCXL, which is capable of modeling all CXL sub-protocols and device types. SimCXL has been rigorously calibrated against a real CXL testbed with various CXL memory and accelerators, showing an average simulation error of 3%. Our evaluation reveals that CXL.cache reduces latency by 68% and increases bandwidth by 14.4x compared to DMA transfers at cacheline granularity. Building upon these insights, we demonstrate the benefits of Cohet with two killer apps, which are remote atomic operation (RAO) and remote procedure call (RPC). Compared to PCIe-NIC design, CXL-NIC achieves a 5.5 to 40.2x speedup for RAO offloading and an average speedup of 1.86x for RPC (de)serialization offloading.

</details>


### [41] [GAVINA: flexible aggressive undervolting for bit-serial mixed-precision DNN acceleration](https://arxiv.org/abs/2511.23203)
*Jordi Fornt,Pau Fontova-Musté,Adrian Gras,Omar Lahyani,Martí Caro,Jaume Abella,Francesc Moll,Josep Altet*

Main category: cs.AR

TL;DR: 本文提出了一种名为GAV的新技术，结合欠压与位串行计算，在保证精度的同时显著提升能效，并基于此构建了支持混合精度的加速器GAVINA。


<details>
  <summary>Details</summary>
Motivation: 电压过缩放（欠压）虽能显著降低功耗，但错误率高且现有方案多限于8位运算，难以与当前低精度（<8位）架构竞争，因此亟需一种兼顾能效与精度的新型近似计算方法。

Method: 提出Guarded Aggressive underVolting（GAV）技术，通过在选定的最低有效位组合上激进地降低供电电压，结合位串行计算实现灵活近似；并据此设计了支持任意混合精度和灵活欠压的加速器GAVINA。

Result: GAVINA在最激进配置下能效达89 TOP/sW；通过误差建模表明，GAV可在ResNet-18上实现20%的能效提升，同时精度损失可忽略。

Conclusion: GAV通过智能选择欠压位组合，在保持模型精度的同时显著提升能效，为低功耗DNN加速提供了一种可行且高效的解决方案。

Abstract: Voltage overscaling, or undervolting, is an enticing approximate technique in the context of energy-efficient Deep Neural Network (DNN) acceleration, given the quadratic relationship between power and voltage. Nevertheless, its very high error rate has thwarted its general adoption. Moreover, recent undervolting accelerators rely on 8-bit arithmetic and cannot compete with state-of-the-art low-precision (<8b) architectures. To overcome these issues, we propose a new technique called Guarded Aggressive underVolting (GAV), which combines the ideas of undervolting and bit-serial computation to create a flexible approximation method based on aggressively lowering the supply voltage on a select number of least significant bit combinations. Based on this idea, we implement GAVINA (GAV mIxed-precisioN Accelerator), a novel architecture that supports arbitrary mixed precision and flexible undervolting, with an energy efficiency of up to 89 TOP/sW in its most aggressive configuration. By developing an error model of GAVINA, we show that GAV can achieve an energy efficiency boost of 20% via undervolting, with negligible accuracy degradation on ResNet-18.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [42] [Secure Command, Control and Communications Systems (C3) for Army UxVs](https://arxiv.org/abs/2511.21936)
*T. Rebolo,A. Grilo,C. Ribeiro*

Main category: cs.NI

TL;DR: 本文提出并实现了一种名为NC2S的安全指挥控制系统，通过零信任模型和轻量级安全协议，为无人载具（UxV）提供具备机密性、完整性与认证能力的实时指挥控制架构，并在Wi-Fi与战术无线电上进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 当前军用无人载具广泛采用如MAVLink等缺乏认证与加密机制的不安全通信协议，存在安全风险；同时需支持多地面控制站间的实时控制权切换，因此亟需一种兼顾安全性与实时性的新型指挥控制架构。

Method: 设计并实现了基于零信任模型的NC2S系统，采用mTLS结合ECDSA证书与ECDH密钥交换保障通信安全，使用HMAC确保消息完整性，并开发了轻量级协议用于凭证管理、密钥更新与控制权交接。

Result: 在Wi-Fi和Rohde&Schwarz HR-5000H战术无线电上对NC2S原型进行了验证。结果显示，HR-5000H链路延迟比宽带技术高约两个数量级，但仍能维持稳定通信且消息丢失极少，适用于战术指挥终端与地面控制站之间的安全连接。

Conclusion: 所提出的NC2S架构有效解决了现有无人载具指挥控制系统中的安全缺陷，在保证CIA三要素的同时支持实时控制权转移，且在战术通信环境中具有良好的适用性与稳定性。

Abstract: Unmanned Vehicles (UxVs) are increasingly used in modern military operations for reconnaissance, surveillance, and strike missions, enhancing situational awareness while reducing risk to personnel. Their affordability and rapid deployment have encouraged the adoption of commercial solutions. However, many rely on insecure protocols such as MAVLink, which lack authentication and encryption mechanisms. This paper designed, implemented, and evaluated a new secure command-and-control architecture that ensures confidentiality, integrity, and authentication (CIA) while supporting real-time control delegation between Ground Control Stations (GCSs). The proposed solution, named New Command and Control System (NC2S), enforces a zero-trust model integrating hierarchical credential-based privileges to regulate access and control among Tactical Commanders (TC), GCSs, and UxVs. It employs mutual Transport Layer Security (mTLS) with Elliptic Curve Digital Signature Algorithm (ECDSA) certificates and Elliptic Curve Diffie-Hellman (ECDH) key exchange, while message integrity is ensured through Hash-based Message Authentication Codes (HMAC). Multiple lightweight protocols were developed for credential management, key renewal, and control handover. The NC2S prototype was experimentally validated over Wi-Fi and Rohde&Schwarz HR-5000H tactical radios. Results showed that HR-5000H links introduce latencies roughly two orders of magnitude higher than broadband technologies (e.g., Wi-Fi or 5G&Beyond technologies) but are still able to maintain stable communication with minimal message loss, making them suitable for the NC2S links among TC terminals and GCSs.

</details>


### [43] [AutoRec: Accelerating Loss Recovery for Live Streaming in a Multi-Supplier Market](https://arxiv.org/abs/2511.22046)
*Tong Li,Xu Yan,Bo Wu,Cheng Luo,Fuyu Wang,Jiuxiang Zhu,Haoyi Fang,Xinle Du,Ke Xu*

Main category: cs.NI

TL;DR: 本文通过大规模测量发现直播流中丢包具有动态性且存在频繁的开关模式切换，提出了一种无需客户端修改的增强恢复机制AutoRec，可在控制开销的同时显著降低恢复延迟，提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有基于ARQ的丢包恢复机制仅在服务端进行修改，无法有效应对直播流中动态丢包和频繁开关模式切换带来的高恢复延迟问题，影响客户端QoE。

Method: 提出AutoRec机制，在QUIC协议上实现，利用直播流的开关模式切换特性，在不修改客户端的前提下自适应调整恢复策略，以满足用户对恢复延迟和开销的容忍度。

Result: 通过测试床和真实商业部署验证，AutoRec在控制额外开销的同时有效降低了丢包恢复延迟，提升了直播服务质量，展现出良好的实用性和收益性。

Conclusion: AutoRec是一种实用且高效的丢包恢复机制，能够在多供应商CDN市场限制下，无需客户端改动即可显著改善直播流的QoE。

Abstract: Due to the limited permissions for upgrading dualside (i.e., server-side and client-side) loss tolerance schemes from the perspective of CDN vendors in a multi-supplier market, modern large-scale live streaming services are still using the automatic-repeat-request (ARQ) based paradigm for loss recovery, which only requires server-side modifications. In this paper, we first conduct a large-scale measurement study with up to 50 million live streams. We find that loss shows dynamics and live streaming contains frequent on-off mode switching in the wild. We further find that the recovery latency, enlarged by the ubiquitous retransmission loss, is a critical factor affecting live streaming's client-side QoE (e.g., video freezing). We then propose an enhanced recovery mechanism called AutoRec, which can transform the disadvantages of on-off mode switching into an advantage for reducing loss recovery latency without any modifications on the client side. AutoRec allows users to customize overhead tolerance and recovery latency tolerance and adaptively adjusts strategies as the network environment changes to ensure that recovery latency meets user demands whenever possible while keeping overhead under control. We implement AutoRec upon QUIC and evaluate it via testbed and real-world commercial services deployments. The experimental results demonstrate the practicability and profitability of AutoRec.

</details>


### [44] [Semantic-Aware Caching for Efficient Image Generation in Edge Computing](https://arxiv.org/abs/2511.22421)
*Hanshuai Cui,Zhiqing Tang,Zhi Yao,Weijia Ji,Wei Zhao*

Main category: cs.NI

TL;DR: CacheGenius 是一种面向边缘计算的混合图像生成系统，通过结合文本到图像与图像到图像流程，并利用语义对齐的缓存参考图像加速扩散模型的去噪过程，在保持生成质量的同时显著降低延迟和计算开销。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽能生成高质量图像，但其多步去噪过程在资源受限的移动和边缘设备上效率低下，亟需加速方案。

Method: 提出 CacheGenius 系统，采用语义感知的分类存储策略、请求调度算法以确保参考图像与目标语义对齐，并通过基于相关性分析的缓存维护策略主动淘汰过时条目。

Result: 在分布式边缘计算环境中，相比基线方法，CacheGenius 将生成延迟降低 41%，计算成本减少 48%，同时保持有竞争力的评估指标。

Conclusion: 通过有效利用缓存参考图像并保障语义一致性，CacheGenius 显著提升了扩散模型在边缘环境中的推理效率，为资源受限场景下的高效文本到图像生成提供了可行方案。

Abstract: Text-to-image generation employing diffusion models has attained significant popularity due to its capability to produce high-quality images that adhere to textual prompts. However, the integration of diffusion models faces critical challenges into resource-constrained mobile and edge environments because it requires multiple denoising steps from the original random noise. A practical way to speed up denoising is to initialize the process with a noised reference image that is similar to the target, since both images share similar layouts, structures, and details, allowing for fewer denoising steps. Based on this idea, we present CacheGenius, a hybrid image generation system in edge computing that accelerates generation by combining text-toimage and image-to-image workflows. It generates images from user text prompts using cached reference images. CacheGenius introduces a semantic-aware classified storage scheme and a request-scheduling algorithm that ensures semantic alignment between references and targets. To ensure sustained performance, it employs a cache maintenance policy that proactively evicts obsolete entries via correlation analysis. Evaluated in a distributed edge computing system, CacheGenius reduces generation latency by 41% and computational costs by 48% relative to baselines, while maintaining competitive evaluation metrics.

</details>


### [45] [RetryGuard: Preventing Self-Inflicted Retry Storms in Cloud Microservices Applications](https://arxiv.org/abs/2511.23278)
*Jhonatan Tavori,Anat Bremler-Barr,Hanoch Levy,Ofek Lavi*

Main category: cs.NI

TL;DR: RetryGuard 是一个分布式框架，用于控制微服务间的重试行为，有效防止重试风暴、降低资源消耗与运营成本。


<details>
  <summary>Details</summary>
Motivation: 现代云应用中微服务架构和自动扩缩容机制导致服务间协调困难，不当的重试策略易引发重试风暴，造成资源浪费和“钱包拒绝”（Denial-of-Wallet）问题。

Method: 提出 RetryGuard 框架，基于分析模型对每个服务独立管理重试策略，并行决策以控制重试行为，综合考虑重试、吞吐量、延迟与成本之间的关系。

Result: 实验表明，RetryGuard 相较于 AWS 默认和高级重试策略显著降低资源使用和成本，并在 Kubernetes 与 Istio 环境中展现出良好可扩展性与性能优势。

Conclusion: RetryGuard 能有效缓解微服务系统中的重试风暴问题，提升系统稳定性并控制运营开销，适用于复杂云原生环境。

Abstract: Modern cloud applications are built on independent, diverse microservices, offering scalability, flexibility, and usage-based billing. However, the structural design of these varied services, along with their reliance on auto-scalers for dynamic internet traffic, introduces significant coordination challenges. As we demonstrate in this paper, common default retry patterns used between misaligned services can turn into retry storms which drive up resource usage and costs, leading to self-inflicted Denial-of-Wallet (DoW) scenarios. To overcome these problems we introduce RetryGuard, a distributed framework for productive control of retry patterns across interdependent microservices. By managing retry policy on a per-service basis and making parallel decisions, RetryGuard prevents retry storms, curbs resource contention, and mitigates escalating operational costs. RetryGuard makes its decisions based on an analytic model that captures the relationships among retries, throughput (rejections), delays, and costs. Experimental results show that RetryGuard significantly reduces resource usage and costs compared to AWS standard and advanced retry policies. We further demonstrate its scalability and superior performance in a more complex Kubernetes deployment with the Istio service mesh, where it achieves substantial improvements.

</details>
