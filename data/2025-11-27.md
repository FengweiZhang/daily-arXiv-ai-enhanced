<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 8]
- [cs.AR](#cs.AR) [Total: 5]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms](https://arxiv.org/abs/2511.20813)
*Simon Hacks*

Main category: cs.SE

TL;DR: 本文探讨了支持“边战边训”（TWYF）理念的先进分布式学习（ADL）平台所需满足的技术要求，并通过设计科学研究方法，将来自北约文档与实践中的七大技术挑战映射到已验证的软件工程模式上，辅以德国武装部队的国家用例加以说明。


<details>
  <summary>Details</summary>
Motivation: 为实现“边战边训”（TWYF）所倡导的在作战过程中持续学习，需构建能够支持该理念的先进分布式学习（ADL）平台，因此有必要明确其关键技术需求并寻找可行的软件工程解决方案。

Method: 采用设计科学研究方法：（i）从PfPC/北约文档和近期实践中提炼挑战；（ii）定义解决方案目标；（iii）系统地将挑战映射到已有的软件工程模式。

Result: 识别出七大技术挑战：互操作性、弹性、多语言支持、数据安全与隐私、可扩展性、平台无关性和模块化，并通过德国武装部队的实际用例展示了相关软件模式的应用。

Conclusion: 现有软件工程模式能够有效应对支持TWYF的ADL平台所面临的关键技术挑战，为未来军事训练系统的开发提供了可行路径。

Abstract: "Train While You Fight" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.

</details>


### [2] [Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code](https://arxiv.org/abs/2511.20933)
*Mootez Saad,Boqi Chen,José Antonio Hernández López,Dániel Varró,Tushar Sharma*

Main category: cs.SE

TL;DR: 本文评估了大语言模型（DeepSeek-R1系列）对软件工程中内聚性和耦合性概念的理解能力，发现在理想条件下表现良好，但在噪声干扰和开放式任务中推理能力显著下降，尤其对耦合性的理解更为脆弱。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在软件工程中应用日益广泛，但其对核心软件设计概念（如内聚与耦合）的理解稳健性尚不明确，亟需系统性评估。

Method: 通过程序化生成设计不良的代码片段，在不同引导程度（验证、引导、开放式生成）和不同噪声水平（注入干扰元素）下，测试DeepSeek-R1系列模型（14B、32B、70B）对内聚性和耦合性的识别与推理能力，并分析其推理轨迹。

Result: 模型在理想条件下对两个概念有较好理解，但实际应用中表现脆弱且不对称：耦合性推理在噪声和开放式场景中F1分数下降超50%；内聚性在引导任务中对内部噪声鲁棒，但无引导时同样失效。推理轨迹显示模型对耦合采取“认知捷径”，对内聚则进行更详尽但仍失败的分析。

Conclusion: 大语言模型可在识别设计缺陷方面提供可靠辅助，但在嘈杂、真实场景中缺乏自主推理能力，凸显提升其程序理解可扩展性与鲁棒性的必要性。

Abstract: Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \textit{Verification} to \textit{Guided} and \textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.

</details>


### [3] [SpaceX: Exploring metrics with the SPACE model for developer productivity](https://arxiv.org/abs/2511.20955)
*Sanchit Kaul,Kevin Nhu,Jason Eissayou,Ivan Eser,Victor Borup*

Main category: cs.SE

TL;DR: 该研究通过挖掘开源仓库数据，结合统计模型与情感分析，提出了一种多维度的开发者生产力综合评估指标（CPS），揭示负面情绪与提交频率正相关，并证明基于协作拓扑的分析优于传统数量指标。


<details>
  <summary>Details</summary>
Motivation: 现有生产力评估方法过于依赖单一、确定性的启发式指标，无法全面反映开发者在复杂协作环境中的真实效能，因此需要构建更全面、多维的评估框架。

Method: 基于开源仓库数据，运用广义线性混合模型（GLMM）和RoBERTa情感分类方法，操作化SPACE框架，构建包含情感状态与协作拓扑的复合生产力评分（CPS）。

Result: 研究发现负面情绪状态与代码提交频率呈显著正相关，且基于贡献者互动拓扑结构的分析比传统基于数量的指标更能准确刻画协作动态。

Conclusion: 开发者生产力应通过多维度、情境敏感的综合指标来衡量，所提出的CPS能更有效地捕捉开发效能的异质性。

Abstract: This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.

</details>


### [4] [Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations](https://arxiv.org/abs/2511.21022)
*Guancheng Lin,Xiao Yu,Jacky Keung,Xing Hu,Xin Xia,Alex X. Liu*

Main category: cs.SE

TL;DR: 本文首次系统评估了10种主流模型编辑方法在更新大语言模型（LLMs）中已弃用API知识方面的效果，并提出了改进方法AdaLoRA-L，在保持性能的同时显著提升了编辑的特异性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码补全任务中表现优异，但其训练数据时效性有限，常生成已被弃用的API。重新训练成本高昂，而现有轻量级模型编辑方法是否能有效更新此类知识尚不明确。

Method: 构建包含70多个已弃用API和3000多个编辑实例的基准EDA PI Bench；在Qwen2.5-Coder、StarCoder2和DeepSeek-Coder上评估10种模型编辑技术；提出AdaLoRA-L方法，通过区分“通用API层”与“特定API层”，仅对后者进行编辑以提升特异性。

Result: AdaLoRA在生成正确、最新API方面表现最佳，但特异性不足；AdaLoRA-L在保持其他指标相当的同时，显著提高了特异性。

Conclusion: 模型编辑可有效更新LLM中的过时API知识，而通过分层编辑策略（如AdaLoRA-L）能兼顾性能与编辑精度，为高效维护模型代码知识提供新思路。

Abstract: Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines "Common API Layers" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to "Specific API Layers" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.

</details>


### [5] [Exploring Hidden Geographic Disparities in Android Apps](https://arxiv.org/abs/2511.21151)
*M. Alecci,P. Jiménez,J. Samhi,T. Bissyandé,J. Klein*

Main category: cs.SE

TL;DR: 该论文揭示了Android应用在不同地理区域存在显著但被忽视的行为差异，包括功能相似但包名不同的“GeoTwins”应用以及同一应用基础APK文件的区域性变化，这些差异影响安全评估、隐私透明度和研究可复现性。


<details>
  <summary>Details</summary>
Motivation: 尽管移动应用演化已被广泛研究，但其在地理维度上的行为差异尚未得到充分探索。作者旨在揭示这种地域性差异及其对安全、公平性和研究可复现性的潜在影响。

Method: 构建了一个跨地区的分布式应用收集管道，采集并分析数千款应用；识别出81,963个GeoTwins样本，并对比其权限请求、第三方库和隐私声明等属性；同时检查Android App Bundle生态系统中base.apk文件的区域性差异。

Result: 发现GeoTwins在不同国家虽外观和功能相似，但在权限、库依赖和隐私披露方面存在显著差异；即使base.apk文件也因地区而异，导致同一应用在不同地区可能被判定为良性或可疑，从而引入地理偏见。

Conclusion: 移动应用存在系统性的地域差异，这不仅削弱了安全与隐私研究的可复现性，还带来伦理和透明度问题，呼吁研究人员、开发者、平台设计者和政策制定者关注并应对这一现象。

Abstract: While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.
  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.
  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.

</details>


### [6] [Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools](https://arxiv.org/abs/2511.21197)
*Paolo Buono,Mary Cerullo,Stefano Cirillo,Giuseppe Desolda,Francesco Greco,Emanuela Guglielmi,Grazia Margarella,Giuseppe Polese,Simone Scalabrino,Cesare Tucci*

Main category: cs.SE

TL;DR: 该研究通过六场与58名开发者的联合设计工作坊，探索了开发者对AI辅助工具（如缺陷检测和代码可读性评估）的心理模型，并提炼出以人为本的AI集成开发环境（IDE）设计原则。


<details>
  <summary>Details</summary>
Motivation: 尽管AI辅助工具在技术上不断进步，但人们对开发者如何理解这些工具、心理模型与实际功能之间的不匹配如何影响信任、控制和采纳仍知之甚少。

Method: 开展六场联合设计工作坊，共邀请58名开发者参与，以引出他们对AI辅助缺陷检测和代码可读性功能的心理模型。

Result: 研究发现开发者将缺陷检测工具视为“缺陷侦探”，强调关键问题预警、透明性、可操作反馈和信心提示；而将可读性评估工具视为“质量教练”，强调情境化、个性化和渐进式指导。信任依赖于解释清晰度、时机和用户控制。

Conclusion: 研究提炼出一套面向IDE中人本AI的设计原则，旨在平衡干扰与支持、简洁与深度、自动化与人类能动性。

Abstract: AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.

</details>


### [7] [Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions](https://arxiv.org/abs/2511.21380)
*Jingyi Chen,Xiaoyan Guo,Songqiang Chen,Shing-Chi Cheung,Jiasi Shen*

Main category: cs.SE

TL;DR: 本文首次实证研究了基于大语言模型的多智能体系统（如GitHub Copilot）在软件工程研究工件跨数据集适配任务中的表现，发现当前系统虽能识别关键文件并生成部分适配代码，但极少产出功能正确的实现；通过提供执行错误信息和参考代码等提示干预，可显著提升生成代码与真实结果的结构相似性。


<details>
  <summary>Details</summary>
Motivation: 自动化适配软件工程研究工件以适应不同数据集对可扩展性和可复现性至关重要，但目前对此缺乏系统研究；而基于大语言模型的多智能体系统展现出自动化复杂开发流程的潜力，值得深入评估。

Method: 作者构建了一个五阶段评估流程（文件理解、代码编辑、命令生成、验证和最终执行），在ROCODE和LogHub2.0等基准仓库上，评估由GPT-4.1和Claude Sonnet 4支持的Copilot多智能体系统，并测试了多种提示干预策略的效果。

Result: 当前多智能体系统能识别关键文件并生成部分适配，但功能正确率低；引入执行错误信息和参考代码等提示干预后，生成代码与真实实现的结构相似性从7.25%显著提升至67.14%。

Conclusion: 现有基于LLM的多智能体系统在数据集适配任务中展现出潜力但仍有明显局限；未来应聚焦于构建更具上下文感知能力和自我纠错能力的智能体，以提升其在软件工程研究中的可靠性。

Abstract: Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.

</details>


### [8] [Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead](https://arxiv.org/abs/2511.21382)
*Bei Chu,Yang Feng,Kui Liu,Zifan Nan,Zhaoqiang Guo,Baowen Xu*

Main category: cs.SE

TL;DR: 本文对2021年5月至2025年8月间发表的115篇关于大语言模型（LLMs）在单元测试生成中应用的文献进行了系统性综述，提出了一种基于单元测试生成生命周期的统一分类法，并分析了提示工程、迭代验证修复等关键技术，指出当前LLM生成测试在缺陷检测能力和评估标准方面仍存在挑战，未来应发展自主测试智能体与混合系统。


<details>
  <summary>Details</summary>
Motivation: 传统自动化单元测试方法虽能有效探索程序结构，但缺乏语义信息以生成真实输入和断言；而大语言模型（LLMs）凭借其对代码语义和编程模式的数据驱动理解，有望弥补这一不足。为厘清该领域的研究现状并指导未来发展，作者开展了系统性文献综述。

Method: 作者对2021年5月至2025年8月间发表的115篇相关论文进行了系统性文献回顾，提出了一个基于单元测试生成生命周期的统一分类框架，将LLM视为需系统工程约束的随机生成器，并据此分析核心生成策略及从上下文增强到质量保障的各类优化技术。

Result: 研究发现，提示工程是主流策略，占所调研研究的89%；迭代验证与修复机制已成为提升生成测试通过率的标准做法；但生成测试的缺陷检测能力较弱，且缺乏标准化评估基准。

Conclusion: 未来研究应聚焦于发展自主测试智能体及融合LLM与传统软件工程工具的混合系统，以推动LLM在工业级测试解决方案中的实际应用。

Abstract: Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI](https://arxiv.org/abs/2511.21232)
*Muhammed Yildirim,Ozcan Ozturk*

Main category: cs.AR

TL;DR: 本文提出一种新型硬件加速器架构，通过融合像素级数据流消除中间缓冲区需求，显著减少数据移动并提升边缘AI中轻量级CNN的执行效率。


<details>
  <summary>Details</summary>
Motivation: 在边缘AI和TinyML应用中，现代CNN（如MobileNetV2）虽采用深度可分离卷积降低计算复杂度，但其多阶段逐层执行方式导致中间特征图频繁访问片上或片外存储，造成高能耗与延迟，形成“内存墙”瓶颈。

Method: 设计一种基于RISC-V处理器的定制功能单元（CFU），采用融合像素级数据流架构，在紧密耦合的流水线中一次性完成单个输出像素的所有DSC阶段（扩展、深度卷积、投影）计算，无需写入中间缓冲区。

Result: 在Xilinx Artix-7 FPGA上实现最高59.3倍于RISC-V软件基线的加速；ASIC综合结果显示在28nm工艺下面积0.284 mm²、功耗910 mW（2 GHz），在40nm工艺下面积1.20 mm²、功耗233 mW（300 MHz）；数据移动减少高达87%。

Conclusion: 该工作验证了在TinyML资源限制下实现零缓冲数据流的可行性，为解决边缘AI加速器中的内存墙问题提供了有效新策略。

Abstract: The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.

</details>


### [10] [Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration](https://arxiv.org/abs/2511.21346)
*Mohamed Shahawy,Julien de Castelnau,Paolo Ienne*

Main category: cs.AR

TL;DR: 本文提出了Bombyx编译器工具链，将OpenCilk程序转换为类Cilk-1的中间表示，以更高效地将任务级并行（TLP）应用映射到FPGA架构上，并支持多种编译目标和自动优化。


<details>
  <summary>Details</summary>
Motivation: 现有OpenCilk的任务模型在硬件中需要昂贵的上下文切换，难以高效映射到FPGA；而FPGA更适合显式延续传递风格的任务模型，因此需要一种新的编译方法来桥接CPU导向的TLP程序与空间架构。

Method: Bombyx将OpenCilk程序降级为类Cilk-1的中间表示，采用显式延续传递模型，并提供两个编译目标：一是兼容OpenCilk运行时的后端，二是面向HLS工具（如Vitis HLS）的可综合处理单元（PE）生成器；同时引入解耦访问-执行优化以提升性能。

Result: Bombyx能自动生成高性能PE，改善内存与计算的重叠，提高整体吞吐量，并有效支持将CPU导向的TLP程序部署到FPGA上。

Conclusion: 通过采用类Cilk-1的显式任务模型和编译优化，Bombyx显著提升了TLP程序在FPGA上的执行效率，为高层综合和异构编程提供了实用的编译基础设施。

Abstract: Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.

</details>


### [11] [A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm](https://arxiv.org/abs/2511.21451)
*Flurin Arquint,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 本文首次实现了抗干扰的多天线时间同步ASIC芯片，支持单天线发射端与16天线接收端之间的同步，并能抵御最多两根天线的智能干扰。


<details>
  <summary>Details</summary>
Motivation: 为应对针对同步信号的智能干扰攻击，提升无线通信系统在复杂电磁环境下的鲁棒性，作者提出并实现了一种基于多天线处理的抗干扰时间同步方案。

Method: 采用65 nm工艺设计并流片了一款专用集成电路（ASIC），实现了一种利用多天线处理技术来抑制干扰的同步算法。该芯片支持单发16收的天线配置，可对抗最多两根天线的智能干扰机。

Result: 所实现的ASIC核心面积为2.87 mm²，功耗为310 mW，采样率达1.28 MS/s，成功验证了该抗干扰同步算法的硬件可行性。

Conclusion: 该工作展示了多天线抗干扰时间同步算法在ASIC上的有效实现，为高安全性和高鲁棒性的无线同步系统提供了硬件基础。

Abstract: We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).

</details>


### [12] [A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection](https://arxiv.org/abs/2511.21461)
*Jonas Elmiger,Fabian Stuber,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 本文提出了一款新型单输入多输出（SIMO）接收机ASIC，采用名为MAED的算法，在22 nm FD-SOI工艺下实现干扰抑制、信道估计与数据检测的一体化处理，相较现有技术在吞吐量和面积效率方面分别提升3倍和4.5倍。


<details>
  <summary>Details</summary>
Motivation: 现有抗干扰接收机在面对智能干扰和压制式干扰时，通常将干扰抑制、信道估计与数据检测分步处理，导致性能受限。为提升在强干扰环境下的误码率性能和系统效率，亟需一种能联合优化上述任务的高效解决方案。

Method: 该工作基于siMultaneous mitigAtion, Estimation, and Detection（MAED）算法，通过构建一个非线性优化问题，统一实现干扰估计与空间滤波抑制、信道估计及数据检测，并将其集成于专用集成电路（ASIC）中，支持8根接收天线。

Result: 所设计的ASIC在22 nm FD-SOI工艺下实现，核心面积为0.32 mm²，功耗223 mW，吞吐率达100 Mb/s，相较当前最先进的抗干扰检测器，每用户吞吐量提升3倍，面积效率提升4.5倍，并能有效应对智能干扰与压制式干扰。

Conclusion: 该研究成功实现了首个集干扰抑制、信道估计与数据检测于一体的SIMO接收机ASIC，验证了MAED算法在硬件上的高效性与实用性，显著提升了抗干扰通信系统的性能与能效。

Abstract: We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\times$ higher per-user throughput and 4.5$\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.

</details>


### [13] [Modeling and Optimizing Performance Bottlenecks for Neuromorphic Accelerators](https://arxiv.org/abs/2511.21549)
*Jason Yik,Walter Gallego Gomez,Andrew Cheng,Benedetto Leto,Alessandro Pierro,Noah Pacik-Nelson,Korneel Van den Berghe,Vittorio Fra,Andreea Danielescu,Gianvito Urgese,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: 本文首次对神经形态加速器进行了全面的性能边界与瓶颈分析，提出了“floorline性能模型”以指导工作负载优化，并结合稀疏感知训练与模型驱动的划分方法，在保持精度不变的前提下实现了最高3.86倍的运行时间提升和3.38倍的能耗降低。


<details>
  <summary>Details</summary>
Motivation: 现有针对神经形态加速器的工作负载优化方法依赖于全网稀疏度和操作计数等指标，但这些指标是否真正提升实际部署性能尚不清楚。因此，亟需深入理解影响神经形态加速器工作负载性能的关键因素。

Method: 通过理论建模与对三种真实神经形态加速器（Brainchip AKD1000、Synsense Speck 和 Intel Loihi 2）的广泛实证分析，识别出内存受限、计算受限和通信受限三种瓶颈状态，并据此构建了floorline性能模型；进一步提出结合稀疏感知训练与floorline指导的划分优化方法。

Result: 该优化方法在保持相同精度下，相比以往手动调优配置，最多实现了3.86倍的运行时间提升和3.38倍的能耗降低。

Conclusion: 传统基于稀疏度的优化指标不足以准确预测神经形态加速器的实际性能；本文提出的floorline模型和优化方法能有效识别瓶颈并显著提升性能与能效。

Abstract: Neuromorphic accelerators offer promising platforms for machine learning (ML) inference by leveraging event-driven, spatially-expanded architectures that naturally exploit unstructured sparsity through co-located memory and compute. However, their unique architectural characteristics create performance dynamics that differ fundamentally from conventional accelerators. Existing workload optimization approaches for neuromorphic accelerators rely on aggregate network-wide sparsity and operation counting, but the extent to which these metrics actually improve deployed performance remains unknown. This paper presents the first comprehensive performance bound and bottleneck analysis of neuromorphic accelerators, revealing the shortcomings of the conventional metrics and offering an understanding of what facets matter for workload performance. We present both theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators: Brainchip AKD1000, Synsense Speck, and Intel Loihi 2. From these, we establish three distinct accelerator bottleneck states, memory-bound, compute-bound, and traffic-bound, and identify which workload configuration features are likely to exhibit these bottleneck states. We synthesize all of our insights into the floorline performance model, a visual model that identifies performance bounds and informs how to optimize a given workload, based on its position on the model. Finally, we present an optimization methodology that combines sparsity-aware training with floorline-informed partitioning. Our methodology achieves substantial performance improvements at iso-accuracy: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [14] [MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference](https://arxiv.org/abs/2511.21160)
*Wu Sai,Xia Ruichen,Yang Dingyu,Wang Rui,Lai Huihang,Guan Jiarui,Bai Jiameng,Zhang Dongxiang,Tang Xiu,Xie Zhongle,Lu Peng,Chen Gang*

Main category: cs.DB

TL;DR: MorphingDB 是一个任务导向的 AI 原生数据库系统，集成于 PostgreSQL，通过自动化的模型存储、选择与推理机制，在保证准确率的同时显著提升吞吐量并降低资源开销。


<details>
  <summary>Details</summary>
Motivation: 现有 AI 原生数据库系统要么依赖模型中心化设计，导致开发维护成本高；要么采用任务中心化的 AutoML 方法，计算开销大且与数据库集成差。因此需要一种兼顾自动化、高效性与良好集成度的新方案。

Method: MorphingDB 引入专用模式和多维张量数据类型支持灵活高效的模型存储；设计两阶段迁移学习框架进行模型选择（离线构建可迁移子空间 + 在线特征感知映射）；并通过预嵌入向量共享和基于 DAG 的批处理流水线优化推理吞吐。

Result: 在九个公开数据集上的实验表明，MorphingDB 在准确性、资源消耗和时间成本之间取得良好平衡，并在吞吐量和资源效率方面显著优于现有 AI 原生 DBMS 和 AutoML 平台。

Conclusion: MorphingDB 有效解决了 AI 原生数据库中模型管理与推理效率的问题，为数据库内深度学习推理提供了一种高效、自动且易于集成的解决方案。

Abstract: The increasing demand for deep neural inference within database environments has driven the emergence of AI-native DBMSs. However, existing solutions either rely on model-centric designs requiring developers to manually select, configure, and maintain models, resulting in high development overhead, or adopt task-centric AutoML approaches with high computational costs and poor DBMS integration. We present MorphingDB, a task-centric AI-native DBMS that automates model storage, selection, and inference within PostgreSQL. To enable flexible, I/O-efficient storage of deep learning models, we first introduce specialized schemas and multi-dimensional tensor data types to support BLOB-based all-in-one and decoupled model storage. Then we design a transfer learning framework for model selection in two phases, which builds a transferability subspace via offline embedding of historical tasks and employs online projection through feature-aware mapping for real-time tasks. To further optimize inference throughput, we propose pre-embedding with vectoring sharing to eliminate redundant computations and DAG-based batch pipelines with cost-aware scheduling to minimize the inference time. Implemented as a PostgreSQL extension with LibTorch, MorphingDB outperforms AI-native DBMSs (EvaDB, Madlib, GaussML) and AutoML platforms (AutoGluon, AutoKeras, AutoSklearn) across nine public datasets, encompassing series, NLP, and image tasks. Our evaluation demonstrates a robust balance among accuracy, resource consumption, and time cost in model selection and significant gains in throughput and resource efficiency.

</details>


### [15] [HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads](https://arxiv.org/abs/2511.21307)
*Xinyi Zhang,Liang Liang,Anastasia Ailamaki,Jianliang Xu*

Main category: cs.DB

TL;DR: 本文提出了一种名为HIRE的混合内存索引结构，结合传统索引的鲁棒性与学习型索引的预测能力，在点查、范围查询、尾延迟和动态数据更新等方面实现高效且稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习型索引在点查上表现优异，但在尾延迟、范围查询性能以及不同工作负载下的稳定性方面存在不足，亟需一种兼顾效率与鲁棒性的新索引结构。

Method: HIRE采用四种关键技术：(1) 自适应数据分布与工作负载的混合叶节点；(2) 结合模型加速与日志更新的内部节点；(3) 无阻塞、成本驱动的动态重校准机制；(4) 考虑叶节点与内部节点误差的跨层级优化批量加载算法。

Result: 在多个真实数据集上的实验表明，HIRE在范围查询吞吐量、尾延迟和整体稳定性方面均优于当前最先进的学习型索引和传统索引，混合工作负载下吞吐量最高提升41.7倍，尾延迟最多降低98%。

Conclusion: HIRE通过融合传统索引结构的稳定性与学习模型的预测能力，有效解决了现有学习型索引在实际应用中的关键瓶颈，为现代数据库提供了一种高效、鲁棒的索引方案。

Abstract: Indexes are critical for efficient data retrieval and updates in modern databases. Recent advances in machine learning have led to the development of learned indexes, which model the cumulative distribution function of data to predict search positions and accelerate query processing. While learned indexes substantially outperform traditional structures for point lookups, they often suffer from high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads. To address these challenges, this paper proposes HIRE, a hybrid in-memory index structure designed to deliver efficient performance consistently. HIRE combines the structural and performance robustness of traditional indexes with the predictive power of model-based prediction to reduce search overhead while maintaining worst-case stability. Specifically, it employs (1) hybrid leaf nodes adaptive to varying data distributions and workloads, (2) model-accelerated internal nodes augmented by log-based updates for efficient updates, (3) a nonblocking, cost-driven recalibration mechanism for dynamic data, and (4) an inter-level optimized bulk-loading algorithm accounting for leaf and internal-node errors. Experimental results on multiple real-world datasets demonstrate that HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. Compared to state-of-the-art learned indexes and traditional indexes, HIRE achieves up to 41.7$\times$ higher throughput under mixed workloads, reduces tail latency by up to 98% across varying scenarios.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [16] [Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures](https://arxiv.org/abs/2511.20780)
*Alison Silva,Gustavo Callou*

Main category: cs.DC

TL;DR: 本文提出一种基于随机Petri网（SPN）的方法，用于评估私有云中Nextcloud文件服务器在不同冗余策略下的可用性，结果表明主机与虚拟机双重冗余能显著提升系统可用性。


<details>
  <summary>Details</summary>
Motivation: 随着云存储平台在学术和商业环境中日益普及，组织对可靠性的需求不断增长，尤其是在寻求公有云替代方案时。因此，评估私有云存储系统的可用性变得至关重要。

Method: 采用随机Petri网（SPNs）对运行在Apache CloudStack私有云上的Nextcloud文件服务器进行建模，分析四种架构配置：基线、主机级冗余、虚拟机（VM）冗余以及两者结合。

Result: 主机级和虚拟机级双重冗余显著提高了系统可用性，并有效减少了预期停机时间。

Conclusion: 所提出的基于SPN的建模方法可有效评估私有云环境中的系统可用性，为基础设施设计和冗余策略选择提供决策支持。

Abstract: Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.

</details>


### [17] [A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving](https://arxiv.org/abs/2511.20982)
*Junhan Liao,Minxian Xu,Wanyi Zheng,Yan Wang,Kejiang Ye,Rajkumar Buyya,Chengzhong Xu*

Main category: cs.DC

TL;DR: DOPD is a dynamic LLM inference system that optimizes the prefill-to-decoding (P/D) ratio in real time to address workload imbalance in disaggregated GPU architectures, significantly improving goodput and latency while maintaining high SLO compliance.


<details>
  <summary>Details</summary>
Motivation: Contemporary LLM inference systems decouple prefill and decoding stages onto separate GPUs, but heterogeneous workloads cause producer-consumer imbalance between these stages, leading to inefficiencies and resource mismatches.

Method: DOPD dynamically adjusts instance allocations for prefill and decoding based on real-time load monitoring and employs an effective request-scheduling policy. It also uses historical load data for proactive reconfiguration to maintain optimal P/D ratios.

Result: DOPD improves system goodput by up to 1.5×, reduces P90 TTFT by up to 67.5%, and decreases P90 TPOT by up to 22.8% compared to vLLM and DistServe. It achieves over 99% SLO attainment with minimal additional resources.

Conclusion: Dynamic adjustment of the prefill/decoding instance ratio based on real-time and historical workload data effectively balances resource utilization, enhances performance, and ensures high SLO compliance in disaggregated LLM inference systems.

Abstract: To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.

</details>


### [18] [Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM](https://arxiv.org/abs/2511.21413)
*Tim Trappen,Robert Keßler,Roland Pabel,Viktor Achter,Stefan Wesner*

Main category: cs.DC

TL;DR: 本文提出了一种在超级计算机RAMSES上结合vLLM、Slurm和Kubernetes的架构，以高效支持大语言模型（LLM）的同步、用户交互式AI推理负载，并在高并发请求下仅引入约500毫秒的端到端延迟开销。


<details>
  <summary>Details</summary>
Motivation: 传统高性能计算（HPC）的运行模式难以满足同步、面向用户的动态AI应用负载的需求，尤其是在高等教育领域对AI推理需求不断增长的背景下，亟需利用现有基础设施提出新的解决方案。

Method: 在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes，构建支持大语言模型服务的新型架构。

Result: 初步基准测试表明，该架构在100、500和1000个并发请求下均能高效扩展，端到端延迟仅增加约500毫秒。

Conclusion: 所提出的集成架构有效解决了传统HPC在支持动态AI推理负载方面的不足，为在现有超算基础设施上部署高效LLM服务提供了可行方案。

Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

</details>


### [19] [MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training](https://arxiv.org/abs/2511.21431)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Yueqiang Chen,Baoguo He,Hongfeng Sun,Ziqing Yin,Shangchao Su,Zhiyan Cui,Liang Dong,Xiyuan Li,Lingbin Wang,Jianwei He,Jiesong Ma,Weikang Huang,Jianglei Tong,Dongdong Gao,Jian Zhang,Hong Tian*

Main category: cs.DC

TL;DR: MemFine 是一种面向内存优化的细粒度调度框架，通过分块重计算策略有效缓解大规模 Mixture of Experts（MoE）模型训练中的内存瓶颈问题，在有限显存下实现高效稳定的训练。


<details>
  <summary>Details</summary>
Motivation: 大规模 MoE 模型训练因动态 token 路由导致严重的负载不均衡，引发 GPU 内存溢出，限制了模型可扩展性；现有基于专家容量限制的负载均衡方法在内存受限设备上会损害模型精度且效果不佳。

Method: 提出 MemFine 框架，将 token 分配与专家计算分解为可管理的小块，并采用基于理论内存模型动态优化的分块重计算策略，在内存效率与吞吐量之间取得平衡。

Result: 实验表明，相比基于全重计算的基线方法，MemFine 减少了 48.03% 的激活内存占用，并提升了 4.42% 的吞吐量，支持在内存受限 GPU 上稳定训练大规模 MoE 模型。

Conclusion: MemFine 有效解决了 MoE 训练中的内存瓶颈问题，在保障模型精度的同时显著提升内存效率和训练吞吐，为在资源受限硬件上扩展 MoE 模型提供了可行方案。

Abstract: The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.

</details>


### [20] [Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation](https://arxiv.org/abs/2511.21535)
*Morteza Sadeghi*

Main category: cs.DC

TL;DR: 本文通过引入数据冗余提升MLFMA中近场（P2P）算子在GPU上的空间局部性，从而改善缓存行为并实现最高7倍的核函数加速；尽管整体应用加速受限于数据重构开销，但所提出的基于局部性指标的分析模型能有效预测性能趋势，且该方法易于集成到现有代码中。


<details>
  <summary>Details</summary>
Motivation: MLFMA中的近场（P2P）算子在GPU上因内存局部性差而成为性能瓶颈。

Method: 引入数据冗余以减少内存访问分散性，提升空间局部性；提出一个结合数据量与访问分散度的局部性指标分析模型，用于预测加速趋势，无需依赖硬件性能分析。

Result: 在两个MLFMA应用（DBIM-MLFMA和PhotoNs-2.0）上验证，核函数最高获得7倍加速，但端到端应用加速仅达1.04倍；分析模型虽不能精确预测绝对加速比，但能可靠反映不同问题规模和密度下的性能趋势。

Conclusion: 数据冗余可有效提升GPU上P2P算子的性能，前提是局部性收益超过数据移动带来的开销；该技术易于嵌入现有实现，具有实用价值。

Abstract: The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [21] [Numerical Optimization of Nozzle Shapes for Fused Deposition Modeling](https://arxiv.org/abs/2511.21449)
*Steffen Tillmann,Felipe A. González,Stefanie Elgeti*

Main category: cs.CE

TL;DR: 本文研究了熔融沉积成型（FDM）中喷嘴几何形状对内部压降的影响，比较了粘性与粘弹性流变模型在喷嘴优化中的表现，并提出了一种兼顾简单角度与样条参数化的优化框架。


<details>
  <summary>Details</summary>
Motivation: 在高速FDM打印中，喷嘴几何形状对压降有显著影响，而现有应用多采用标准设计，缺乏针对高速性能的系统优化研究。

Method: 采用两种流变模型（温度依赖剪切稀化粘性模型和等温粘弹性模型），分别结合基于半锥角和样条曲线的参数化方法，对喷嘴形状进行优化以最小化压降。

Result: 粘性模型下最优半锥角随进料速率显著变化（高速时约30°更优），而粘弹性模型下该依赖性较弱；样条参数化相比角度优化仅带来微小压降改善。

Conclusion: 模型选择显著影响最优喷嘴几何形状，所提出的优化框架为高速FDM打印中的喷嘴设计提供了理论支持和实用指导。

Abstract: Purpose: In fused deposition modeling (FDM), the nozzle plays a critical role in enabling high printing speeds while maintaining precision. Despite its importance, most applications still rely on standard nozzle designs. This work investigates the influence of nozzle geometry on pressure loss inside the nozzle, a key factor in high-speed printing performance. Design/methodology/approach: We focus on optimizing the nozzle shape to minimize the pressure loss and establish a framework that allows both sim- ple angle-based optimization and more advanced spline-based parametrization. To model the polymer melt flow, we compare two constitutive descriptions commonly employed in the literature: a temperature-dependent, shear-thinning viscous model and an isothermal viscoelastic model. Findings: For the viscous model, the optimal half-opening angle exhibits a strong dependence on the feeding rate, with higher rates favoring half-opening angles near 30°, whereas lower rates are more efficient at larger angles. In con- trast, the viscoelastic model predicts a weaker dependence of the optimal angle on the feeding rate. For both models, spline-based parametrization yields only marginal improvements over angle optimization in terms of reducing pressure loss. Originality/value: This paper presents a comparative study of FDM nozzle shape optimization using different simulation models. We introduce a flexible optimization framework that accommodates both simple and advanced geomet- ric parametrizations. The results highlight the impact of model choice on the optimal nozzle geometry and provide support for improving nozzle design in high-speed printing applications.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [22] [DynamicAdaptiveClimb: Adaptive Cache Replacement with Dynamic Resizing](https://arxiv.org/abs/2511.21235)
*Daniel Berend,Shlomi Dolev,Sweta Kumari,Dhruv Mishra,Marina Kogan-Sadetsky,Archit Somani*

Main category: cs.OS

TL;DR: 本文提出了两种新的缓存替换策略AdaptiveClimb和DynamicAdaptiveClimb，通过动态调整缓存项的提升距离和自动调节缓存大小，在多种真实工作负载下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统缓存替换策略（如LRU和CLIMB）难以在动态变化的访问模式下高效适应，且往往需要维护复杂的每项统计信息或多个调优参数，导致开销较高。因此，亟需一种轻量级、自适应能力强的缓存管理机制。

Method: AdaptiveClimb根据最近的命中/未命中模式动态调整缓存对象的提升距离，仅需一个可调参数且无需每项统计；其扩展版本DynamicAdaptiveClimb进一步引入缓存大小的自动调节机制以应对工作负载变化。

Result: 在涵盖6个数据集共1067条真实轨迹的评估中，DynamicAdaptiveClimb相比FIFO基准最高提升29%命中率，并显著降低未命中惩罚；相较次优方法AdaptiveClimb和SIEVE，性能提升约10%–15%，尤其在工作集大小波动的场景中表现突出。

Conclusion: 所提出的缓存策略在保持低开销的同时实现了高效的自适应能力，在现代动态缓存环境中具有显著优势和实用价值。

Abstract: Efficient cache management is critical for optimizing the system performance, and numerous caching mechanisms have been proposed, each exploring various insertion and eviction strategies. In this paper, we present AdaptiveClimb and its extension, DynamicAdaptiveClimb, two novel cache replacement policies that leverage lightweight, cache adaptation to outperform traditional approaches. Unlike classic Least Recently Used (LRU) and Incremental Rank Progress (CLIMB) policies, AdaptiveClimb dynamically adjusts the promotion distance (jump) of the cached objects based on recent hit and miss patterns, requiring only a single tunable parameter and no per-item statistics. This enables rapid adaptation to changing access distributions while maintaining low overhead. Building on this foundation, DynamicAdaptiveClimb further enhances adaptability by automatically tuning the cache size in response to workload demands. Our comprehensive evaluation across a diverse set of real-world traces, including 1067 traces from 6 different datasets, demonstrates that DynamicAdaptiveClimb consistently achieves substantial speedups and higher hit ratios compared to other state-of-the-art algorithms. In particular, our approach achieves up to a 29% improvement in hit ratio and a substantial reduction in miss penalties compared to the FIFO baseline. Furthermore, it outperforms the next-best contenders, AdaptiveClimb and SIEVE [43], by approximately 10% to 15%, especially in environments characterized by fluctuating working set sizes. These results highlight the effectiveness of our approach in delivering efficient performance, making it well-suited for modern, dynamic caching environments.

</details>
