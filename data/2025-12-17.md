<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing](https://arxiv.org/abs/2512.14290)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本文提出了一种结合机器学习预测与实时反馈的混合自动扩缩容算法，用于边缘计算环境中满足服务等级协议（SLA）的应用，在Kubernetes中实现并显著降低了SLA违规率。


<details>
  <summary>Details</summary>
Motivation: 现有边缘计算中的自动扩缩容策略在应对SLA要求时存在性能不足和配置复杂的问题，难以有效保障低延迟、高可靠性的服务质量。

Method: 提出一种新型混合自动扩缩容算法，集成基于机器学习的预测性扩缩容与基于当前资源使用和SLA约束的反应式扩缩容，并将其作为Kubernetes扩展实现。

Result: 实验表明，该方法将SLA违规率从现有方案的最高23%降低至6%，在多种真实边缘应用中实现了更稳定的SLA合规性。

Conclusion: 所提出的混合自动扩缩容算法能有效提升边缘计算环境下SLA的遵守程度，优于现有基线方法。

Abstract: Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.

</details>


### [2] [Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs](https://arxiv.org/abs/2512.14445)
*Brenton Walker,Markus Fidler*

Main category: cs.DC

TL;DR: 本文研究了在并行计算中引入同步屏障（如Apache Spark的Barrier Execution Mode）对系统稳定性和性能造成的负面影响，分析了$(s,k,l)$类屏障系统的稳定性，推导了混合任务负载下的性能边界，并通过仿真和真实Spark系统实验验证了模型。


<details>
  <summary>Details</summary>
Motivation: 在许多并行机器学习任务中，任务之间需要同步启动或结束，这种同步机制（如Spark的Barrier Execution Mode）会引发工作节点空闲，从而降低系统稳定性和性能。因此有必要量化和分析此类屏障带来的性能损失。

Method: 作者分析了$(s,k,l)$屏障系统的稳定性，推导混合任务（含屏障与不含屏障）系统的性能边界；针对纯1-barrier情形，结合Spark实测数据，通过仿真对比理论边界；并基于实测开销分布，构建调度机制相关的开销模型并通过仿真验证。

Result: 研究得出了屏障系统对稳定性和性能的影响边界，仿真结果与Spark实测数据吻合；发现实际系统中的开销主要源于事件驱动与轮询调度机制共存，并据此建立了有效的开销模型。

Conclusion: 同步屏障虽能支持特定并行任务需求，但会带来显著性能和稳定性代价；通过建模和分析可有效理解和预测此类开销，为优化Barrier Execution Mode提供理论依据。

Abstract: In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.
  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.

</details>


### [3] [PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning](https://arxiv.org/abs/2512.14628)
*Alireza Olama,Andreas Lundell,Izzat El Hajj,Johan Lilius,Jerker Björkqvist*

Main category: cs.DC

TL;DR: PruneX 是一种结合剪枝算法与集群层次结构的分布式训练系统，通过引入分层结构化 ADMM 算法，在节点间同步前施加节点级结构化稀疏性，从而显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 在多节点 GPU 集群上进行大规模分布式训练时，节点间通信带宽成为主要瓶颈；传统剪枝方法因无法有效利用非结构化稀疏性，难以减少通信开销。

Method: 提出 PruneX 系统，采用分层结构化 ADMM（H-SADMM）算法，在节点级强制结构化稀疏，并结合领导者-跟随者执行模型，将密集集合操作应用于压缩张量，同时将完整同步限制在高带宽的节点内互连中。

Result: 在 64 个 GPU 上对 ResNet 架构的评估表明，PruneX 将节点间通信量减少了约 60%，并实现了 6.75 倍的强扩展加速比，优于稠密基线（5.81 倍）和 Top-K 梯度压缩（3.71 倍）。

Conclusion: PruneX 通过协同设计剪枝策略与系统架构，有效缓解了分布式训练中的通信瓶颈，在保持模型性能的同时显著提升了扩展效率。

Abstract: Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [4] [ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework](https://arxiv.org/abs/2512.14172)
*Qijun Zhang,Shang Liu,Yao Lu,Mengming Li,Zhiyao Xie*

Main category: cs.AR

TL;DR: 本文提出了一种新的分析型功耗建模框架ReadyPower，通过引入架构级、实现级和技术级参数改进传统模型（如McPAT），在可靠性和可解释性方面优于基于机器学习的功耗模型，并在BOOM和香山CPU架构上实现了更低的平均绝对百分比误差（MAPE）和更高的相关系数。


<details>
  <summary>Details</summary>
Motivation: 现代处理器设计中，功耗是核心目标之一，需要既准确又高效的功耗建模方法。传统的分析型架构级功耗模型（如McPAT）精度不足，而新兴的基于机器学习的模型虽精度高但存在可靠性差、可解释性弱和使用困难等问题，限制了其工业应用。

Method: 作者在广泛使用的McPAT分析模型基础上，引入三个层级（架构级、实现级、技术级）的参数以弥合实际处理器实现与分析模型之间的差距，构建出名为ReadyPower的新分析型功耗建模框架。各层级参数采用不同方式确定。

Result: 实验表明，在不同训练场景下，ReadyPower相比基于机器学习的基线模型，在BOOM和香山CPU架构上平均实现了超过20%更低的MAPE和超过0.2更高的相关系数R。

Conclusion: ReadyPower是一种可靠、可解释且易于使用的分析型功耗建模框架，有效提升了架构级功耗模型的准确性，同时克服了机器学习模型在工业部署中的主要障碍。

Abstract: Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.

</details>


### [5] [TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips](https://arxiv.org/abs/2512.14256)
*Huizheng Wang,Taiquan Wei,Zichuan Wang,Dingcheng Jiang,Qize Yang,Jiaxin Liu,Jingxiang Hou,Chao Li,Jinyi Deng,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: 本文提出了一种名为 TEMP 的新框架，用于在晶圆级芯片（WSC）上高效训练大语言模型（LLM），通过拓扑感知的张量流分区、流量感知映射和双层级晶圆求解策略，显著提升吞吐量并缓解片上内存限制。


<details>
  <summary>Details</summary>
Motivation: 晶圆级芯片虽具备高算力和高带宽，但受限于晶圆面积，在片上内存与计算资源之间存在独特权衡。现有张量并行策略未能有效利用其通信优势同时保持内存效率，难以充分发挥 WSC 性能。

Method: 提出张量流分区范式（TSPP），并构建 TEMP 框架，包含拓扑感知的张量流分区、流量感知的映射策略以及双层级晶圆求解方法，以应对 WSC 上 LLM 训练中的硬件约束和并行挑战。

Result: 实验表明，TEMP 在多种模型上平均吞吐量比当前最先进的 LLM 训练系统提升 1.7 倍。

Conclusion: TEMP 成功解决了晶圆级芯片上大语言模型训练中的内存与通信瓶颈，显著提升了训练效率，验证了 TSPP 范式在 WSC 平台上的有效性与潜力。

Abstract: Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.
  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.
  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.

</details>


### [6] [PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion](https://arxiv.org/abs/2512.14322)
*Huizheng Wang,Hongbin Wang,Zichuan Wang,Zhiheng Yue,Yang Wang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: 本文提出PADE，一种无需额外稀疏性预测器的算法-硬件协同设计，通过位串行使能阶段融合机制实现动态稀疏注意力加速，在22个基准测试中相比Nvidia H100 GPU实现7.43倍加速和31.1倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制计算和内存开销大，现有稀疏注意力方法因引入高开销的稀疏性预测器而缺乏实用性，导致硬件效率低下。

Method: 提出PADE框架，包含三项关键技术：1）基于位级不确定性区间的守卫过滤策略（BUI-GF）；2）基于双向稀疏性的乱序执行机制（BS-OOE）；3）基于交织的稀疏分块注意力机制（ISTA），并结合定制加速器设计。

Result: 在22个基准测试中，PADE相比Nvidia H100 GPU实现7.43倍速度提升和31.1倍能效提升；相比SOTA加速器Sanger、DOTA和SOFA，分别实现5.1倍、4.3倍和3.4倍的能耗节省。

Conclusion: PADE通过算法与硬件协同设计，有效解决了稀疏注意力中预测器开销大、硬件利用率低和分块困难等问题，实现了高效且实用的动态稀疏注意力加速。

Abstract: Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.
  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.
  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [7] [Co-simulation errors due to step size changes](https://arxiv.org/abs/2512.13845)
*Lars T. Kyllingstad*

Main category: cs.CE

TL;DR: 在连续时间协同仿真中，当两个仿真单元通过变量 $q$ 连接且各自内部状态表示 $q$ 的时间积分时，由于外推误差会导致状态不一致；通常减小宏观时间步长可减小该误差，但本文指出在某些情况下，即使减小步长也可能导致误差增大。


<details>
  <summary>Details</summary>
Motivation: 研究连续时间协同仿真中外推误差对系统状态一致性的影响，特别是在减小时间步长时误差反而增大的反常现象。

Method: 理论分析与数值实验相结合，探讨在特定条件下时间步长变化如何影响由外推引起的积分状态偏差。

Result: 发现并验证了在某些情形下，减小宏观时间步长反而会导致两个仿真单元间状态差异增大。

Conclusion: 外推误差在协同仿真中可能导致非直观的行为，减小步长并不总能改善状态一致性，需谨慎处理时间步长选择和状态同步策略。

Abstract: When two simulation units in a continuous-time co-simulation are connected via some variable $q$, and both simulation units have an internal state which represents the time integral of $q$, there will generally be a discrepancy between those states due to extrapolation errors. Normally, such extrapolation errors diminish if the macro time step size is reduced. Here we show that, under certain circumstances, step size changes can cause such discrepancies to increase even when the change is towards smaller steps.

</details>


### [8] [A data-physics hybrid generative model for patient-specific post-stroke motor rehabilitation using wearable sensor data](https://arxiv.org/abs/2512.14329)
*Yanning Dai,Chenyu Tang,Ruizhi Zhang,Wenyu Yang,Yilan Zhang,Yuhui Wang,Junliang Chen,Xuhang Chen,Ruimou Xie,Yangyue Cao,Qiaoying Li,Jin Cao,Tao Li,Hubin Zhao,Yu Pan,Arokia Nathan,Xin Gao,Peter Smielewski,Shuo Gao*

Main category: cs.CE

TL;DR: 本文提出了一种数据-物理混合生成框架，通过单次平地步行试验重建卒中患者的神经肌肉控制，并预测其在斜坡行走和爬楼梯等康复任务中的运动表现。该方法结合可穿戴传感器、物理控制器、健康运动图谱及深度强化学习技术，在临床试验中显著提升了康复效果。


<details>
  <summary>Details</summary>
Motivation: 当前卒中后运动能力评估仅提供静态损伤评分，无法动态预测患者是否能安全执行特定任务（如上下坡或楼梯），限制了康复方案的个性化制定。

Method: 结合可穿戴传感器运动学数据、比例-微分物理控制器、群体健康运动图谱，以及目标条件深度强化学习（含行为克隆与生成对抗模仿学习），构建个体化生成模型，模拟患者在不同任务下的步态。

Result: 在11名卒中患者中，该方法在保留个体步态特征的同时，关节角度和末端点保真度分别提升4.73%和12.10%，训练时间缩短至物理基线的25.56%；在21名住院患者的多中心试点中，使用该预测指导康复的临床组Fugl-Meyer下肢评分平均提高6.0分，显著优于对照组的3.7分。

Conclusion: 该生成式、任务预测框架可有效辅助临床决策，为卒中后步态康复提供动态个性化策略的新范式。

Abstract: Dynamic prediction of locomotor capacity after stroke is crucial for tailoring rehabilitation, yet current assessments provide only static impairment scores and do not indicate whether patients can safely perform specific tasks such as slope walking or stair climbing. Here, we develop a data-physics hybrid generative framework that reconstructs an individual stroke survivor's neuromuscular control from a single 20 m level-ground walking trial and predicts task-conditioned locomotion across rehabilitation scenarios. The system combines wearable-sensor kinematics, a proportional-derivative physics controller, a population Healthy Motion Atlas, and goal-conditioned deep reinforcement learning with behaviour cloning and generative adversarial imitation learning to generate physically plausible, patient-specific gait simulations for slopes and stairs. In 11 stroke survivors, the personalized controllers preserved idiosyncratic gait patterns while improving joint-angle and endpoint fidelity by 4.73% and 12.10%, respectively, and reducing training time to 25.56% relative to a physics-only baseline. In a multicentre pilot involving 21 inpatients, clinicians who used our locomotion predictions to guide task selection and difficulty obtained larger gains in Fugl-Meyer lower-extremity scores over 28 days of standard rehabilitation than control clinicians (mean change 6.0 versus 3.7 points). These findings indicate that our generative, task-predictive framework can augment clinical decision-making in post-stroke gait rehabilitation and provide a template for dynamically personalized motor recovery strategies.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [9] [Time and Relations into Focus: Ontological Foundations of Object-Centric Event Data](https://arxiv.org/abs/2512.14425)
*Hosna Hooshyar,Mattia Fumagalli,Marco Montali,Giancarlo Guizzardi*

Main category: cs.DB

TL;DR: 本文提出了一种基于本体论的改进方法，通过引入轻量级本体gUFO来增强现有对象中心事件数据（OCED）元模型，从而解决其在时间与动态关系表达上的模糊性和表达力不足问题，形成新元模型gOCED。


<details>
  <summary>Details</summary>
Motivation: 传统以案例为中心的事件数据模型无法有效处理对象间复杂交互，而现有对象中心事件数据（OCED）模型在时间维度和动态关系方面存在模糊性与表达能力不足的问题。

Method: 作者采用三步法：首先分析文献中OCED元模型的关键问题并用示例说明其局限；其次将当前OCED核心模型与轻量级本体gUFO结合，构建新的元模型gOCED；最后验证gOCED在保持简洁性的同时增强了表达能力。

Result: 提出的gOCED元模型不仅保留了现有模型的简洁性，还有效解决了文献中指出的模糊性和表达力不足问题，支持更全面的时间和动态关系建模。

Conclusion: 通过引入本体论基础，gOCED为对象中心过程挖掘提供了更严谨、更具表达力的数据模型，有望成为未来对象中心事件数据标准的基础。

Abstract: Object-centric process mining is a new branch of process mining where events are associated with multiple objects, and where object-to-object interactions are essential to understand the process dynamics. Traditional event data models, also called case-centric, are unable to cope with the complexity introduced by these more refined relationships. Several models have been made to move from case-centric to Object-Centric Event Data (OCED), trying to retain simplicity as much as possible. Still, these suffer from inherent ambiguities, and lack a comprehensive support of essential dimensions related to time and (dynamic) relations. In this work, we propose to fill this gap by leveraging a well-founded ontology of events and bringing ontological foundations to OCED, with a three-step approach. First, we start from key open issues reported in the literature regarding current OCED metamodels, and witness their ambiguity and expressiveness limitations on illustrative and representative examples proposed therein. Second, we consider the OCED Core Model, currently proposed as the basis for defining a new standard for object-centric event data, and we enhance it by grounding it on a lightweight version of UFO-B called gUFO, a well-known foundational ontology tailored to the representation of objects, events, time, and their (dynamic) relations. This results in a new metamodel, which we call gOCED. The third contribution then shows how gOCED at once covers the features of existing metamodels preserving their simplicity, and extends them with the essential features needed to overcome the ambiguity and expressiveness issues reported in the literature.

</details>


### [10] [Beyond Text-to-SQL: Autonomous Research-Driven Database Exploration with DAR](https://arxiv.org/abs/2512.14622)
*Ostap Vykhopen,Viktoria Skorik,Maxim Tereschenko,Veronika Solopova*

Main category: cs.DB

TL;DR: DAR 是一个无需人工干预的多智能体系统，可在云数据仓库内自主完成端到端数据库研究，比人类分析师快约32倍。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽能查询数据库，但大多依赖用户显式提示，缺乏主动探索数据的能力。

Method: DAR 采用三层多智能体架构：初始化（意图推断与元数据提取）、执行（SQL 与 AI 查询生成及迭代验证）、合成（带质量控制的报告生成），所有推理均在 BigQuery 内通过原生生成式 AI 函数完成。

Result: 在真实资产-事件数据集上，DAR 在16分钟内完成分析任务，而专业分析师需8.5小时，且 DAR 能产出基于模式的洞见和有证据支持的建议。

Conclusion: DAR 将数据库交互从被动查询辅助转变为在云数据仓库中自主、研究驱动的探索，显著提升探索性分析效率。

Abstract: Large language models can already query databases, yet most existing systems remain reactive: they rely on explicit user prompts and do not actively explore data. We introduce DAR (Data Agnostic Researcher), a multi-agent system that performs end-to-end database research without human-initiated queries. DAR orchestrates specialized AI agents across three layers: initialization (intent inference and metadata extraction), execution (SQL and AI-based query synthesis with iterative validation), and synthesis (report generation with built-in quality control). All reasoning is executed directly inside BigQuery using native generative AI functions, eliminating data movement and preserving data governance. On a realistic asset-incident dataset, DAR completes the full analytical task in 16 minutes, compared to 8.5 hours for a professional analyst (approximately 32x times faster), while producing useful pattern-based insights and evidence-grounded recommendations. Although human experts continue to offer deeper contextual interpretation, DAR excels at rapid exploratory analysis. Overall, this work shifts database interaction from query-driven assistance toward autonomous, research-driven exploration within cloud data warehouses.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [Energy-Efficient Multi-Radio Microwave and IAB-Based Fixed Wireless Access for Rural Areas](https://arxiv.org/abs/2512.13922)
*Anselme Ndikumana,Kim Khoa Nguyen,Adel Larabi,Mohamed Cheriet*

Main category: cs.NI

TL;DR: 本文提出了一种面向农村地区的节能型多跳5G固定无线接入（FWA）架构，融合微波、IAB与FWA技术，在保证数据速率的前提下通过动态调整设备状态和资源分配以降低能耗。


<details>
  <summary>Details</summary>
Motivation: 在农村地区部署光纤成本过高，而5G FWA虽具潜力但受限于单跳覆盖范围；同时，现有研究忽视了多跳架构中随跳数增加而上升的能耗问题。

Method: 提出一种统一的多跳框架，结合长距离微波、IAB和FWA；通过优化微波射频单元的开关状态（关闭、启动、服务、深度睡眠、唤醒）及IAB节点的资源块分配，构建以最小化能耗为目标、满足数据速率约束的优化问题，并采用对偶分解、多凸规划与动态规划求解。

Result: 仿真结果表明所提方法在保障农村区域高容量覆盖的同时有效降低了整体能耗。

Conclusion: 该节能型多跳FWA框架为农村宽带接入提供了一种高效可行的解决方案，兼顾覆盖、容量与能效。

Abstract: Deploying fiber optics as a last-mile solution in rural areas is not economically viable due to low population density. Nevertheless, providing high-speed internet access in these regions is essential to promote digital inclusion. 5G Fixed Wireless Access (5G FWA) has emerged as a promising alternative; however, its one-hop topology limits coverage. To overcome this limitation, a multi-hop architecture is required. This work proposes a unified multi-hop framework that integrates long-haul microwave, Integrated Access and Backhaul (IAB), and FWA to provide wide coverage and high capacity in rural areas. As the number of hops increases, total energy consumption also rises, a challenge often overlooked in existing literature. To address this, we propose an energy-efficient multi-radio microwave and IAB-based FWA framework for rural area connectivity. When the network is underutilized, the proposed approach dynamically operates at reduced capacity to minimize energy consumption. We optimize the off, start-up, serving, deep sleep, and wake-up sates of microwave radios to balance energy use and satisfying data rate requirements. Additionally, we optimize resource block allocation for IAB-based FWA nodes connected to microwave backhaul. The formulated optimization problems aim to minimize the energy consumption of long-haul microwave and multi-hop IAB-based network while satisfying data rate constraints. These problems are solved using dual decomposition and multi-convex programming, supported by dynamic programming. Simulation results demonstrates

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [12] [Practitioner Insights on Fairness Requirements in the AI Development Life Cycle: An Interview Study](https://arxiv.org/abs/2512.13830)
*Chaima Boufaied,Thanh Nguyen,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 该研究通过26位来自23个国家、不同背景的从业者的半结构化访谈，从软件工程视角探讨了AI/ML系统中的公平性需求。研究发现，尽管从业者普遍认识到公平性的重要性，但在实践中存在做法不一致、优先级低和知识缺口等问题，亟需在利益相关者之间就公平性定义、评估指标和流程达成共识。


<details>
  <summary>Details</summary>
Motivation: 随着AI（尤其是机器学习和大语言模型）的广泛应用，其“黑箱”特性可能导致对不同人群的不公平对待。因此，除了传统关注模型有效性外，AI公平性日益受到重视。然而，在软件开发生命周期（SDLC）中如何有效融入公平性仍缺乏系统性实践，促使作者从软件工程角度开展此项研究。

Method: 采用定性研究方法，对来自23个国家、不同应用领域和背景的26名AI/ML从业者进行半结构化访谈，围绕他们在软件开发生命周期中对公平性的认知、实践、评估维度（实现、验证、评价）以及与其他开发目标（如功能完整性、交付期限）之间的权衡进行深入探讨，并通过主题分析提炼关键发现。

Result: 研究发现：（1）从业者普遍意识到AI公平性的多个维度；（2）实际操作中公平性实践不一致且常被降级处理；（3）存在明显的公平性知识缺口；（4）在将公平性纳入需求、早期评估及与其他优先级平衡方面缺乏标准化流程。

Conclusion: 为有效将公平性融入AI/ML项目，需在相关利益方之间就具体场景下的公平性定义、对应的评估指标以及正式的集成流程达成共识，并加强从业者在公平性方面的知识与能力建设。

Abstract: Nowadays, Artificial Intelligence (AI), particularly Machine Learning (ML) and Large Language Models (LLMs), is widely applied across various contexts. However, the corresponding models often operate as black boxes, leading them to unintentionally act unfairly towards different demographic groups. This has led to a growing focus on fairness in AI software recently, alongside the traditional focus on the effectiveness of AI models. Through 26 semi-structured interviews with practitioners from different application domains and with varied backgrounds across 23 countries, we conducted research on fairness requirements in AI from software engineering perspective. Our study assesses the participants' awareness of fairness in AI / ML software and its application within the Software Development Life Cycle (SDLC), from translating fairness concerns into requirements to assessing their arising early in the SDLC. It also examines fairness through the key assessment dimensions of implementation, validation, evaluation, and how it is balanced with trade-offs involving other priorities, such as addressing all the software functionalities and meeting critical delivery deadlines. Findings of our thematic qualitative analysis show that while our participants recognize the aforementioned AI fairness dimensions, practices are inconsistent, and fairness is often deprioritized with noticeable knowledge gaps. This highlights the need for agreement with relevant stakeholders on well-defined, contextually appropriate fairness definitions, the corresponding evaluation metrics, and formalized processes to better integrate fairness into AI/ML projects.

</details>


### [13] [Verification-Guided Context Optimization for Tool Calling via Hierarchical LLMs-as-Editors](https://arxiv.org/abs/2512.13860)
*Henger Li,Shuangjie You,Flavio Di Palo,Yiyue Qian,Ayush Jain*

Main category: cs.SE

TL;DR: 本文提出了一种名为VGCO的框架，利用大语言模型（LLMs）作为编辑器自动优化工具调用所需的文档和知识库上下文，以提升LLM在单轮、大规模工具调用场景中的准确性、鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有工具调用依赖为人类编写的文档和知识库，这些内容与LLM的理解方式不一致，尤其在工业环境中存在工具数量多、功能重叠、上下文模糊等问题，限制了工具调用的效果。

Method: VGCO框架包含两个阶段：1）评估阶段，收集真实失败案例并识别工具与上下文之间的不匹配；2）优化阶段，通过离线学习进行具有结构感知能力的上下文内分层编辑。该方法利用状态感知、动作特定且验证引导的LLM编辑器，在单轮调用中实现高效、定向的上下文优化。

Result: VGCO在多个LLM上显著提升了工具调用的准确性、鲁棒性和泛化能力，特别适用于单轮、大规模的工业级应用场景。

Conclusion: 通过将LLM作为上下文编辑器，并结合验证引导的分层优化策略，VGCO有效解决了工具文档与LLM理解之间的错位问题，为大规模工具调用提供了实用且高效的解决方案。

Abstract: Tool calling enables large language models (LLMs) to interact with external environments through tool invocation, providing a practical way to overcome the limitations of pretraining. However, the effectiveness of tool use depends heavily on the quality of the associated documentation and knowledge base context. These materials are usually written for human users and are often misaligned with how LLMs interpret information. This problem is even more pronounced in industrial settings, where hundreds of tools with overlapping functionality create challenges in scalability, variability, and ambiguity. We propose Verification-Guided Context Optimization (VGCO), a framework that uses LLMs as editors to automatically refine tool-related documentation and knowledge base context. VGCO works in two stages. First, Evaluation collects real-world failure cases and identifies mismatches between tools and their context. Second, Optimization performs hierarchical editing through offline learning with structure-aware, in-context optimization. The novelty of our LLM editors has three main aspects. First, they use a hierarchical structure that naturally integrates into the tool-calling workflow. Second, they are state-aware, action-specific, and verification-guided, which constrains the search space and enables efficient, targeted improvements. Third, they enable cost-efficient sub-task specialization, either by prompt engineering large editor models or by post-training smaller editor models. Unlike prior work that emphasizes multi-turn reasoning, VGCO focuses on the single-turn, large-scale tool-calling problem and achieves significant improvements in accuracy, robustness, and generalization across LLMs.

</details>


### [14] [Context Branching for LLM Conversations: A Version Control Approach to Exploratory Programming](https://arxiv.org/abs/2512.13914)
*Bhargav Chickmagalur Nanjundappa,Spandan Maaheshwari*

Main category: cs.SE

TL;DR: 本文提出ContextBranch系统，通过引入类似版本控制的分支机制管理大语言模型（LLM）多轮对话，有效缓解因上下文污染导致的性能下降问题，在复杂探索性编程任务中显著提升响应质量并减少无关上下文。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中性能显著下降（平均下降39%），尤其在需要探索多种方案的编程任务中，现有方法迫使用户在混乱上下文和丢失历史之间做选择。

Method: 提出ContextBranch系统，提供checkpoint、branch、switch和inject四个原语，允许用户保存对话状态、隔离探索替代方案，并有选择地合并见解。

Result: 在30个软件工程场景的实验中，分支对话相比线性对话显著提升响应质量，尤其在概念距离较远的复杂探索中；上下文消息数减少58.1%（从31.0降至13.0）。

Conclusion: 对话分支是一种基础性机制，能有效防止探索替代方案时的上下文污染，提升AI辅助探索性工作的效果。

Abstract: Large Language Models (LLMs) have become integral to software engineering workflows, yet their effectiveness degrades significantly in multi-turn conversations. Recent studies demonstrate an average 39% performance drop when instructions are delivered across multiple turns, with models making premature assumptions and failing to course correct (Laban et al., 2025). This degradation is particularly problematic in exploratory programming tasks where developers need to investigate alternative approaches without committing to a single path. Current solutions force users into a false dichotomy: continue in a context-polluted conversation where the LLM becomes increasingly confused, or start fresh and lose all accumulated context.
  We present ContextBranch, a conversation management system that applies version control semantics to LLM interactions. ContextBranch provides four core primitives--checkpoint, branch, switch, and inject--enabling users to capture conversation state, explore alternatives in isolation, and selectively merge insights. We evaluate ContextBranch through a controlled experiment with 30 software engineering scenarios featuring intentionally polluting explorations. Branched conversations achieved higher response quality compared to linear conversations, with large improvements in focus and context awareness. Benefits were concentrated in complex scenarios involving conceptually distant explorations. Branching reduced context size by 58.1% (31.0 to 13.0 messages), eliminating irrelevant exploratory content. Our work establishes conversation branching as a fundamental primitive for AI-assisted exploratory work, demonstrating that isolation prevents context pollution when exploring alternatives.

</details>


### [15] [Professional Software Developers Don't Vibe, They Control: AI Agent Use for Coding in 2025](https://arxiv.org/abs/2512.14012)
*Ruanqianqian Huang,Avery Reyna,Sorin Lerner,Haijun Xia,Brian Hempel*

Main category: cs.SE

TL;DR: 本文通过实地观察和问卷调查研究了经验丰富的开发者如何在软件开发中使用AI智能体，发现他们虽认可智能体对效率的提升，但仍主导关键设计与实现以保障软件质量，并对智能体持积极态度。


<details>
  <summary>Details</summary>
Motivation: 探讨AI智能体在专业软件开发中的实际角色，了解经验丰富的开发者使用智能体的动机、策略、任务适用性及态度。

Method: 结合实地观察（N=13）与定性问卷调查（N=99），分析开发者在真实开发环境中使用AI智能体的行为与观点。

Result: 经验丰富的开发者将智能体视为生产力工具，但在软件设计与实现中保持主导地位，以确保软件质量；他们采用基于自身专业知识的策略控制智能体行为，并对其持总体积极态度。

Conclusion: 研究强调了软件开发最佳实践在有效使用AI智能体中的重要性，指出了适合智能体的任务类型，并为未来智能体界面设计与使用指南提供了方向。

Abstract: The rise of AI agents is transforming how software can be built. The promise of agents is that developers might write code quicker, delegate multiple tasks to different agents, and even write a full piece of software purely out of natural language. In reality, what roles agents play in professional software development remains in question. This paper investigates how experienced developers use agents in building software, including their motivations, strategies, task suitability, and sentiments. Through field observations (N=13) and qualitative surveys (N=99), we find that while experienced developers value agents as a productivity boost, they retain their agency in software design and implementation out of insistence on fundamental software quality attributes, employing strategies for controlling agent behavior leveraging their expertise. In addition, experienced developers feel overall positive about incorporating agents into software development given their confidence in complementing the agents' limitations. Our results shed light on the value of software development best practices in effective use of agents, suggest the kinds of tasks for which agents may be suitable, and point towards future opportunities for better agentic interfaces and agentic use guidelines.

</details>


### [16] [PerfCoder: Large Language Models for Interpretable Code Performance Optimization](https://arxiv.org/abs/2512.14018)
*Jiuding Yang,Shengyao Lu,Hongxuan Liu,Shayan Shirahmad Gale Bagi,Zahra Fazel,Tomasz Czajkowski,Di Niu*

Main category: cs.SE

TL;DR: 本文提出了PerfCoder，一种专为生成高性能代码而设计的大语言模型，通过在带有可解释注释的真实优化轨迹上微调，并结合运行时指标进行强化对齐，显著提升了代码性能优化能力。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在自动生成高性能代码方面存在局限，不仅受限于数据稀缺，更缺乏引导其进行可解释且有效性能优化的监督信号。

Method: PerfCoder基于真实世界中带有人类可读注释的优化轨迹进行微调，并通过运行时测量指标进行强化微调，使其能够提出针对特定输入的优化策略并直接应用，无需迭代优化。此外，它还能生成可解释的反馈，用于与更大规模模型协同工作。

Result: 在PIE代码性能基准上，PerfCoder在运行时加速比和有效优化率方面均优于现有模型；同时，其生成的可解释反馈能显著提升32B模型和GPT-5在代码优化任务上的表现。

Conclusion: 仅靠模型规模无法实现有效的性能优化，必须引入对优化策略的理解；PerfCoder通过可解释优化路径和协同工作模式，为高性能代码生成提供了新思路。

Abstract: Large language models (LLMs) have achieved remarkable progress in automatic code generation, yet their ability to produce high-performance code remains limited--a critical requirement in real-world software systems. We argue that current LLMs struggle not only due to data scarcity but, more importantly, because they lack supervision that guides interpretable and effective performance improvements. In this work, we introduce PerfCoder, a family of LLMs specifically designed to generate performance-enhanced code from source code via interpretable, customized optimizations. PerfCoder is fine-tuned on a curated collection of real-world optimization trajectories with human-readable annotations, and preference-aligned by reinforcement fine-tuning using runtime measurements, enabling it to propose input-specific improvement strategies and apply them directly without relying on iterative refinement. On the PIE code performance benchmark, PerfCoder surpasses all existing models in both runtime speedup and effective optimization rate, demonstrating that performance optimization cannot be achieved by scale alone but requires optimization stratetgy awareness. In addition, PerfCoder can generate interpretable feedback about the source code, which, when provided as input to a larger LLM in a planner-and-optimizer cooperative workflow, can further improve outcomes. Specifically, we elevate the performance of 32B models and GPT-5 to new levels on code optimization, substantially surpassing their original performance.

</details>


### [17] [Aligning Security Compliance and DevOps: A Longitudinal Study](https://arxiv.org/abs/2512.14453)
*Fabiola Moyón,Florian Angermeir,Daniel Mendez,Tony Gorschek,Markus Voggenreiter,Pierre-Louis Bonvin*

Main category: cs.SE

TL;DR: 本文提出了一种基于IEC 62443-4-1标准的安全合规DevOps框架（RefA），并通过西门子公司的纵向研究验证其在支持非安全专家团队实施DevOps的同时满足安全规范的有效性。


<details>
  <summary>Details</summary>
Motivation: 企业在采用敏捷和DevOps方法时面临传统线性安全合规流程的挑战，尤其在关键基础设施相关产品开发中更为突出，亟需一种能融合安全合规要求与DevOps实践的方法。

Method: 作者基于IEC 62443-4-1标准构建了名为RefA的规范性安全合规DevOps生命周期模型，并在西门子开展纵向研究，涵盖框架的构思、验证与初步应用。

Result: 研究表明RefA能够有效将安全合规知识传递给产品开发团队，使跨职能团队具备交付合规产品所需的能力，从而兼顾敏捷性与安全性。

Conclusion: 所提出的RefA框架有助于企业在向DevOps转型过程中保持对安全标准的合规，同时提升团队整体能力，适用于包括非安全专家在内的各类专业人员。

Abstract: Companies adopt agile methodologies and DevOps to facilitate efficient development and deployment of software-intensive products. This, in turn, introduces challenges in relation to security standard compliance traditionally following a more linear workflow. This is especially a challenge for the engineering of products and services associated with critical infrastructures. To support companies in their transition towards DevOps, this paper presents an adaptation of DevOps according to security regulations and standards. We report on our longitudinal study at Siemens AG, consisting of several individual sub-studies in the inception, validation, and initial adoption of our framework based on RefA as well as the implications for practice. RefA is a prescriptive model of a security compliant DevOps lifecycle based on the IEC 62443-4-1 standard. The overall framework is aimed at professionals, not only security experts, being able to use it on implementing DevOps processes while remaining compliant with security norms. We demonstrate how RefA facilitates the transfer of security compliance knowledge to product development teams. This knowledge transfer supports the agility aim of ensuring that cross-functional teams have all the skills needed to deliver the compliant products.

</details>


### [18] [Teralizer: Semantics-Based Test Generalization from Conventional Unit Tests to Property-Based Tests](https://arxiv.org/abs/2512.14475)
*Johann Glock,Clemens Bauer,Martin Pinzger*

Main category: cs.SE

TL;DR: 本文提出了一种基于语义的方法 Teralizer，通过单路径符号分析自动将单元测试转化为属性测试，无需人工定义属性。实验表明其在部分数据集上能提升变异测试得分，但在真实项目中受限于符号分析和静态分析的能力，适用性有限。


<details>
  <summary>Details</summary>
Motivation: 传统单元测试仅验证单一输入输出对，无法覆盖执行路径中的多数输入；而属性测试虽能生成满足属性的多组输入，却需大量人工定义属性及其约束。因此，亟需一种自动化方法将单元测试泛化为属性测试。

Method: 提出 Teralizer 方法，利用单路径符号分析从实现中提取规范，并自动将 JUnit 单元测试转换为 jqwik 属性测试，避免依赖人工编写属性。

Result: 在 EvoSuite 生成的测试上，Teralizer 将变异分数提高了 1–4 个百分点；在开发者编写的成熟测试上仅提升 0.05–0.07 个百分点；在 632 个真实 Java 项目中，仅有 1.7% 成功完成泛化流程，失败主因是原型在类型支持和静态分析方面的局限。

Conclusion: Teralizer 展示了从程序语义自动推导测试属性的可行性，但要广泛应用于真实项目，仍需克服符号执行与静态分析中的多项研究与工程挑战。

Abstract: Conventional unit tests validate single input-output pairs, leaving most inputs of an execution path untested. Property-based testing addresses this shortcoming by generating multiple inputs satisfying properties but requires significant manual effort to define properties and their constraints. We propose a semantics-based approach that automatically transforms unit tests into property-based tests by extracting specifications from implementations via single-path symbolic analysis. We demonstrate this approach through Teralizer, a prototype for Java that transforms JUnit tests into property-based jqwik tests. Unlike prior work that generalizes from input-output examples, Teralizer derives specifications from program semantics.
  We evaluated Teralizer on three progressively challenging datasets. On EvoSuite-generated tests for EqBench and Apache Commons utilities, Teralizer improved mutation scores by 1-4 percentage points. Generalization of mature developer-written tests from Apache Commons utilities showed only 0.05-0.07 percentage points improvement. Analysis of 632 real-world Java projects from RepoReapers highlights applicability barriers: only 1.7% of projects completed the generalization pipeline, with failures primarily due to type support limitations in symbolic analysis and static analysis limitations in our prototype. Based on the results, we provide a roadmap for future work, identifying research and engineering challenges that need to be tackled to advance the field of test generalization.
  Artifacts available at: https://doi.org/10.5281/zenodo.17950381

</details>


### [19] [MoT: A Model-Driven Low-Code Approach for Simplifying Cloud-of-Things Application Development](https://arxiv.org/abs/2512.14613)
*Cristiano Welter,Kleinner Farias*

Main category: cs.SE

TL;DR: 本文提出了一种名为“物模型”（Model of Things, MoT）的基于模型且融合低代码理念的方法，用于简化云物融合（Cloud-of-Things, CoT）应用的开发。通过定制UML配置文件降低技术门槛，并通过案例研究与技术接受模型（TAM）问卷验证其可行性与易用性。


<details>
  <summary>Details</summary>
Motivation: 当前云物融合应用开发面临技术门槛高、缺乏标准化建模方法、互操作性差、自动化程度低等问题，亟需一种更高效、易用的开发范式。

Method: 提出MoT方法，结合低代码理念与定制化的UML Profile，支持对IoT与云服务进行建模；并通过案例研究和TAM问卷评估其可用性与有效性。

Result: 评估结果显示MoT能有效简化CoT应用的开发与部署流程，用户即使缺乏IoT经验也认为其易于使用且实用性强，定性反馈表明其显著降低了开发复杂度并加快了开发速度。

Conclusion: MoT为CoT应用开发提供了一种有前景的模型驱动解决方案，通过降低技术门槛和提升自动化水平，增强了开发效率与灵活性，有助于推动CoT技术的广泛采用。

Abstract: The integration of cloud computing and the Internet of Things (IoT) is essential for scalable, intelligent systems. However, developing cloud-of-things (CoT) applications remains challenging. It requires significant technical expertise and lacks standardized, model-driven methodologies. Current approaches fail to ensure interoperability, automation, and efficiency. This study introduces the Model of Things (MoT), a model-based approach that incorporates low-code principles to simplify CoT development. MoT reduces technical barriers by providing a custom UML profile designed for IoT and cloud services. To evaluate MoT, we conducted a case study and a Technology Acceptance Model (TAM) questionnaire. The results confirmed MoT's feasibility, demonstrating that it streamlines CoT application development and deployment. Users found MoT accessible, even with limited IoT experience, and reported high perceived ease of use and usefulness. Qualitative feedback highlighted MoT's ability to reduce complexity and speed up development. MoT offers a promising, model-driven solution for CoT application development. By lowering entry barriers and promoting automation, it enhances both efficiency and flexibility. This study represents a step toward a more user-friendly framework, enabling broader adoption of CoT technologies.

</details>


### [20] [Reconsidering Conversational Norms in LLM Chatbots for Sustainable AI](https://arxiv.org/abs/2512.14673)
*Ronnie de Souza Santos,Cleyton Magalhães,Italo Santos*

Main category: cs.SE

TL;DR: 本文指出，大型语言模型（LLM）聊天机器人在用户交互层面的行为显著影响其能耗与环境足迹，但这一因素常被忽视。作者从对话长度、即时响应期待、用户日常习惯和上下文累积四个维度分析了交互行为对系统可持续性的影响，并呼吁重新设计对话交互以纳入可持续考量。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM系统可持续性的评估主要聚焦于模型架构、硬件效率和部署基础设施，却忽略了用户交互行为对能耗的潜在影响。作者旨在揭示并强调交互层面因素在塑造LLM环境影响中的关键作用。

Method: 本文为一篇观点性论文（vision paper），通过概念分析和维度划分，系统性地探讨用户与LLM聊天机器人的交互行为如何从四个方面影响系统的能源消耗和可持续性。

Result: 论文识别出四个关键维度：1）延长的对话增加token生成和推理计算成本；2）对即时响应的期待限制了节能调度；3）用户日常习惯累积产生未被量化的运营需求；4）上下文累积增加内存负担并降低长对话效率。

Conclusion: 要提升LLM系统的可持续性，需重新思考聊天机器人交互的设计理念，将可持续性视为部分依赖于用户对话规范的问题，并在交互设计中融入节能意识。

Abstract: LLM based chatbots have become central interfaces in technical, educational, and analytical domains, supporting tasks such as code reasoning, problem solving, and information exploration. As these systems scale, sustainability concerns have intensified, with most assessments focusing on model architecture, hardware efficiency, and deployment infrastructure. However, existing mitigation efforts largely overlook how user interaction practices themselves shape the energy profile of LLM based systems. In this vision paper, we argue that interaction level behavior appears to be an underexamined factor shaping the environmental impact of LLM based systems, and we present this issue across four dimensions. First, extended conversational patterns increase token production and raise the computational cost of inference. Second, expectations of instant responses limit opportunities for energy aware scheduling and workload consolidation. Third, everyday user habits contribute to cumulative operational demand in ways that are rarely quantified. Fourth, the accumulation of context affects memory requirements and reduces the efficiency of long running dialogues. Addressing these challenges requires rethinking how chatbot interactions are designed and conceptualized, and adopting perspectives that recognize sustainability as partly dependent on the conversational norms through which users engage with LLM based systems.

</details>
