<div id=toc></div>

# Table of Contents

- [cs.OS](#cs.OS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CE](#cs.CE) [Total: 2]


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [1] [Taiji: A DPU Memory Elasticity Solution for In-production Cloud Environments](https://arxiv.org/abs/2511.09936)
*Hao Zheng,Longxiang Wang,Yun Xu,Qiang Wang,Yibin Shen,Xiaoshe Dong,Bang Di,Jia Wei,Shenyu Dong,Xingjun Zhang,Weichen Chen,Zhao Han,Sanqian Zhao,Dongdong Huang,Jie Qi,Yifan Yang,Zhao Gao,Yi Wang,Jinhu Li,Xudong Ren,Min He,Hang Yang,Xiao Zheng,Haijiao Hao,Jiesheng Wu*

Main category: cs.OS

TL;DR: 本文提出了Taiji，一种面向数据处理单元（DPU）的资源弹性架构，通过混合虚拟化与并行内存交换技术，实现DPU内存的高效超配和透明弹性，显著提升资源利用率并支持热切换与热升级。


<details>
  <summary>Details</summary>
Motivation: DPUs在云数据中心中面临硬件升级周期长、资源受限等问题，限制了其在高密度、高效率场景下的应用，亟需一种能动态扩展资源并兼容现有应用的弹性解决方案。

Method: Taiji将DPU的操作系统转为来宾操作系统，并引入轻量级虚拟化层，结合并行内存交换机制，使几乎全部DPU内存可交换，从而实现对上层应用完全透明的内存超配与弹性管理。

Result: 实验表明，Taiji可将DPU内存资源扩展超过50%，虚拟化开销维持在约5%，90%的内存换入操作在10微秒内完成，并已在超过3万台服务器的大规模生产系统中部署。

Conclusion: Taiji为DPUs提供了一种高效、可靠且低开销的资源弹性方案，有效解决了其内存受限与升级困难的问题，具备良好的实用性和可扩展性。

Abstract: The growth of cloud computing drives data centers toward higher density and efficiency. Data processing units (DPUs) enhance server network and storage performance but face challenges such as long hardware upgrade cycles and limited resources. To address these, we propose Taiji, a resource-elasticity architecture for DPUs. Combining hybrid virtualization with parallel memory swapping, Taiji switches the DPU's operating system (OS) into a guest OS and inserts a lightweight virtualization layer, making nearly all DPU memory swappable. It achieves memory overcommitment for the switched guest OS via high-performance memory elasticity, fully transparent to upper-layer applications, and supports hot-switch and hot-upgrade to meet in-production cloud requirements. Experiments show that Taiji expands DPU memory resources by over 50%, maintains virtualization overhead around 5%, and ensures 90% of swap-ins complete within 10 microseconds. Taiji delivers an efficient, reliable, low-overhead elasticity solution for DPUs and is deployed in large-scale production systems across more than 30,000 servers.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Evaluating Software Process Models for Multi-Agent Class-Level Code Generation](https://arxiv.org/abs/2511.09794)
*Wasique Islam Shafin,Md Nakhla Rafi,Zhenhao Li,Tse-Hsun Chen*

Main category: cs.SE

TL;DR: 该研究探讨了多智能体大语言模型（LLM）在类级代码生成中如何受软件开发流程结构和角色分工影响，发现瀑布式协作虽提升了代码可维护性，但通常降低了功能正确性，并改变了错误类型分布。


<details>
  <summary>Details</summary>
Motivation: 当前大多数LLM代码生成研究聚焦于单智能体、函数级别的任务，缺乏对多智能体协作及软件工程流程结构如何影响类级代码生成效果的理解。

Method: 在ClassEval基准的100个Python任务上，使用GPT-4o-mini、DeepSeek-Chat和Claude-3.5-Haiku三种LLM，模拟包含需求、设计、实现和测试四个阶段的瀑布式开发流程，评估多智能体协作对代码质量和错误类型的影响。

Result: 瀑布式多智能体工作流提高了代码的结构性和可维护性，但多数模型的功能正确性显著下降（GPT-4o-mini和DeepSeek-Chat分别下降37.8%和39.8%），仅Claude-3.5-Haiku提升9.5%；错误类型从结构性缺失转向语义和验证错误；测试阶段对结果影响最大。

Conclusion: 软件过程结构深刻影响LLM在多智能体协作中的推理方式与失败模式，在流程纪律性和灵活问题解决之间存在固有权衡，需谨慎设计协作机制以平衡代码质量维度。

Abstract: Modern software systems require code that is not only functional but also maintainable and well-structured. Although Large Language Models (LLMs) are increasingly used to automate software development, most studies focus on isolated, single-agent function-level generation. This work examines how process structure and role specialization shape multi-agent LLM workflows for class-level code generation. We simulate a Waterfall-style development cycle covering Requirement, Design, Implementation, and Testing using three LLMs (GPT-4o-mini, DeepSeek-Chat, and Claude-3.5-Haiku) on 100 Python tasks from the ClassEval benchmark. Our findings show that multi-agent workflows reorganize, rather than consistently enhance, model performance. Waterfall-style collaboration produces cleaner and more maintainable code but often reduces functional correctness (-37.8\% for GPT-4o-mini and -39.8\% for DeepSeek-Chat), with Claude-3.5-Haiku as a notable exception (+9.5\%). Importantly, process constraints shift failure characteristics: structural issues such as missing code decrease, while semantic and validation errors become more frequent. Among all stages, Testing exerts the strongest influence by improving verification coverage but also introducing new reasoning failures, whereas Requirement and Design have comparatively modest effects. Overall, this study provides empirical evidence that software process structure fundamentally alters how LLMs reason, collaborate, and fail, revealing inherent trade-offs between rigid workflow discipline and flexible problem-solving in multi-agent code generation.

</details>


### [3] [EnvTrace: Simulation-Based Semantic Evaluation of LLM Code via Execution Trace Alignment -- Demonstrated at Synchrotron Beamlines](https://arxiv.org/abs/2511.09964)
*Noah van der Vleuten,Anthony Flores,Shray Mathur,Max Rakitin,Thomas Hopkins,Kevin G. Yager,Esther H. R. Tsai*

Main category: cs.SE

TL;DR: 本文提出EnvTrace方法，通过基于仿真的执行轨迹评估语义代码等价性，用于评测大语言模型（LLMs）在仪器控制任务中的表现，并结合数字孪生技术实现对30多个LLM的功能正确性多维评分，表明顶尖模型在快速生成控制代码方面接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 传统无状态算法基准无法充分评估大语言模型在物理系统仪器控制中的行为，需引入能反映真实系统动态的方法。

Method: 提出EnvTrace方法，利用数字孪生仿真环境评估代码执行轨迹的语义等价性，并通过轨迹对齐对30多个LLM在关键行为维度上进行多面评分。

Result: 实验显示多个顶级大语言模型在仪器控制代码生成任务中可达到接近人类的性能水平。

Conclusion: 该研究是迈向大语言模型与数字孪生协同工作的第一步，前者提供直观控制与智能编排，后者提供安全高保真环境，共同推动自主具身AI的发展。

Abstract: Evaluating large language models (LLMs) for instrument control requires methods that go beyond standard, stateless algorithmic benchmarks, since the behavior of physical systems cannot be fully captured by unit tests alone. Here we introduce EnvTrace, a simulation-based method that evaluates execution traces to assess semantic code equivalence. EnvTrace is demonstrated with a beamline control-logic digital twin to facilitate the evaluation of instrument control code, with the digital twin itself also enabling the pre-execution validation of live experiments. Over 30 LLMs were evaluated using trace alignment to generate a multi-faceted score for functional correctness across key behavioral dimensions, showing that many top-tier models can approach human-level performance in rapid control-code generation. This is a first step toward a broader vision where LLMs and digital twins work symbiotically: LLMs providing intuitive control and agentic orchestration, and digital twins offering safe and high-fidelity environments, paving the way towards autonomous embodied AI.

</details>


### [4] [Continuous Benchmark Generation for Evaluating Enterprise-scale LLM Agents](https://arxiv.org/abs/2511.10049)
*Divyanshu Saxena,Rishikesh Maurya,Xiaoxuan Ou,Gagan Somashekar,Shachee Mishra Gupta,Arun Iyer,Yu Kang,Chetan Bansal,Aditya Akella,Saravan Rajmohan*

Main category: cs.SE

TL;DR: 本文提出一种动态生成基准的方法，通过少量半结构化文档和大语言模型，为不断演化的AI智能体构建可维护的评估框架，特别适用于企业级场景。


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体评估依赖固定基准，在企业级应用中难以应对持续变化的服务需求和稀疏的真实样本，因此需要一种能随需求演进的评估方法。

Method: 利用开发者提供的少量半结构化文档表达高层意图，结合先进大语言模型自动生成适配当前需求的评估基准，构建动态、可维护的评估流程。

Result: 在大型公共企业服务迁移案例中成功应用该方法，实现了对AI智能体性能的快速反馈和针对性改进。

Conclusion: 所提出的基准生成流程能有效支持企业级AI智能体的持续评估与优化，提升其部署效率和实用性。

Abstract: The rapid adoption of AI agents across domains has made systematic evaluation crucial for ensuring their usefulness and successful production deployment. Evaluation of AI agents typically involves using a fixed set of benchmarks and computing multiple evaluation metrics for the agent. While sufficient for simple coding tasks, these benchmarks fall short for enterprise-scale agents, where services and requirements evolve continuously and ground-truth examples are sparse. We propose a process of benchmark generation that helps evolve the benchmarks as the requirements change and perform robust evaluation of evolving AI agents. We instantiate this approach for a case study of service migration from one deployment platform to another at a large public enterprise. Our approach relies on semi-structured documents where developers express the high-level intent, and uses state-of-the-art LLMs to generate benchmarks from just a small number of such documents. Overall, this process results in a maintainable evaluation framework, enabling rapid feedback on agent performance and facilitating targeted improvements.

</details>


### [5] [Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics](https://arxiv.org/abs/2511.10271)
*Xin Sun,Daniel Ståhl,Kristian Sandahl,Christoph Kessler*

Main category: cs.SE

TL;DR: 该研究基于ISO/IEC 25010质量模型，通过文献综述、行业研讨会和实证实验，揭示了大语言模型（LLM）生成代码在非功能性质量（如安全性、可维护性和性能效率）方面的现状与挑战，指出学术界、工业界与模型表现之间存在错位，并呼吁将质量保障机制整合进代码生成流程。


<details>
  <summary>Details</summary>
Motivation: 当前对大语言模型生成代码的评估主要关注功能正确性（是否通过测试），而忽视了非功能性质量（如可维护性、安全性、性能等）。为系统理解这些质量维度的表现及其在学术与工业中的差异，作者开展本研究。

Method: 结合三种方法：（1）系统性文献综述（108篇论文）；（2）与多家机构从业者开展两场行业研讨会；（3）使用三个LLM对真实软件问题生成补丁，并从安全性、可维护性和性能效率三方面进行实证分析。

Result: 文献中主要关注安全性和性能效率，而可维护性等维度研究不足；工业界则更重视可维护性与可读性，担忧LLM加速技术债积累；实证显示，LLM生成的功能正确补丁在某一质量维度上的提升常以牺牲其他维度为代价，且不同模型和优化策略在运行时与内存表现上差异显著。

Conclusion: 学术关注点、工业优先级与LLM实际表现之间存在明显不匹配，亟需在LLM代码生成流程中引入质量保障机制，确保生成代码不仅“能用”，而且“高质量”。

Abstract: In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.

</details>


### [6] [A Large-Scale Collection Of (Non-)Actionable Static Code Analysis Reports](https://arxiv.org/abs/2511.10323)
*Dávid Kószó,Tamás Aladics,Rudolf Ferenc,Péter Hegedűs*

Main category: cs.SE

TL;DR: 本文提出了一种用于收集和分类静态代码分析（SCA）警告的新方法，构建了一个包含超过100万条Java代码警告的大规模数据集NASCAR，以区分可操作与不可操作的警告，并公开了该数据集及生成工具。


<details>
  <summary>Details</summary>
Motivation: 静态代码分析工具常产生大量不可操作的警告，导致开发者出现“警报疲劳”，影响代码质量和开发效率；同时，缺乏大规模标注数据集限制了利用机器学习改进SCA工具的研究。

Method: 提出一种新颖的方法论，用于收集和分类SCA警告，有效区分可操作与不可操作的警告，并基于此方法构建大规模Java SCA警告数据集NASCAR。

Result: 成功构建并公开了一个包含超过100万条Java源代码警告的大规模数据集NASCAR，以及用于生成该数据集的工具。

Conclusion: 该研究填补了Java领域SCA警告数据集的空白，为后续提升SCA工具准确性、缓解警报疲劳问题提供了重要资源和基础。

Abstract: Static Code Analysis (SCA) tools, while invaluable for identifying potential coding problems, functional bugs, or vulnerabilities, often generate an overwhelming number of warnings, many of which are non-actionable. This overload of alerts leads to ``alert fatigue'', a phenomenon where developers become desensitized to warnings, potentially overlooking critical issues and ultimately hindering productivity and code quality. Analyzing these warnings and training machine learning models to identify and filter them requires substantial datasets, which are currently scarce, particularly for Java. This scarcity impedes efforts to improve the accuracy and usability of SCA tools and mitigate the effects of alert fatigue. In this paper, we address this gap by introducing a novel methodology for collecting and categorizing SCA warnings, effectively distinguishing actionable from non-actionable ones. We further leverage this methodology to generate a large-scale dataset of over 1 million entries of Java source code warnings, named NASCAR: (Non-)Actionable Static Code Analysis Reports. To facilitate follow-up research in this domain, we make both the dataset and the tools used to generate it publicly available.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [7] [Dolphin: An Actor-Oriented Database for Reactive Moving Object Data Management](https://arxiv.org/abs/2511.10063)
*Yiwen Wang,Vivek Shah,Marcos Antonio Vaz Salles,Claudia Bauzer Medeiros,Julio Cesar Dos Reis,Yongluan Zhou*

Main category: cs.DB

TL;DR: 本文提出了一种基于“移动参与者”（moving actor）抽象的新型分布式反应式移动对象数据管理平台M-AODB，并实现了其原型系统Dolphin，该系统在Orleans框架上构建，具备良好的可扩展性和近实时的反应延迟。


<details>
  <summary>Details</summary>
Motivation: 现有移动对象应用中，反应式行为通常依赖复杂的用户端实现，缺乏对低延迟、高并发和空间数据管理的统一支持，亟需一种内建反应能力且兼顾性能与一致性的系统级解决方案。

Method: 通过扩展现有的参与者模型，引入具备反应感知、移动性和空间查询能力的“移动参与者”抽象，并在此基础上构建名为M-AODB的反应式移动对象数据库平台；具体实现为Dolphin系统，采用非侵入式设计，构建于Microsoft Orleans分布式虚拟参与者框架之上。

Result: 实验表明，Dolphin在多机环境下具有良好可扩展性，并能在真实反应式移动对象场景中实现近实时的反应延迟。

Conclusion: 将反应式行为、空间数据管理和并发语义集成到分布式参与者模型中是可行且有效的，移动参与者抽象显著降低了开发者在性能与一致性之间权衡的实现负担。

Abstract: Novel reactive moving object applications require solutions to support object reactive behaviors as a way to query and update dynamic data. While moving object scenarios have long been researched in the context of spatio-temporal data management, reactive behavior is usually left to complex end-user implementations. However, it is not just a matter of hardwiring reactive constraints: the required solutions need to satisfy tight low-latency computation requirements and be scalable. This paper explores a novel approach to enrich a distributed actor-based framework with reactive functionality and complex spatial data management along with concurrency semantics. Our approach relies on a proposal of the moving actor abstraction, which is a conceptual enhancement of the actor model with reactive sensing, movement, and spatial querying capabilities. This enhancement helps developers of reactive moving object applications avoid the significant burden of implementing application-level schemes to balance performance and consistency. Based on moving actors, we define a reactive moving object data management platform, named Moving Actor-Oriented Databases (M-AODBs), and build Dolphin -- an implementation of M-AODBs. Dolphin embodies a non-intrusive actor-based design layered on top of the Microsoft Orleans distributed virtual actor framework. In a set of experimental evaluations with realistic reactive moving object scenarios, Dolphin exhibits scalability on multi-machines and provides near-real-time reaction latency.

</details>


### [8] [CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models](https://arxiv.org/abs/2511.10418)
*Yaqiao Zhu,Hongkai Wen,Mark Birkin,Man Luo*

Main category: cs.DB

TL;DR: 本文提出了CityVerse，这是首个面向城市计算中大语言模型（LLM）系统评估的统一平台，整合了多源城市数据、基于能力的任务分类体系和动态仿真功能，支持对LLM在多样化城市任务中的可复现、标准化评测。


<details>
  <summary>Details</summary>
Motivation: 当前在城市计算领域评估大语言模型面临两大挑战：缺乏统一的数据访问平台，以及任务定义碎片化导致难以公平比较不同模型性能。

Method: CityVerse平台包含三大核心组件：1）基于坐标的Data APIs，整合十类城市数据（如空间特征、时间动态、人口统计和多模态图像），涵盖超3800万条记录；2）Task APIs将43项城市计算任务按认知层次划分为感知、空间理解、推理与预测、决策与交互四个层级；3）交互式可视化前端，支持实时数据检索、多图层展示和仿真回放。

Result: 通过对主流大语言模型在代表性任务上的评测，验证了CityVerse在支持可复现、系统性评估方面的有效性。

Conclusion: CityVerse为城市计算领域中大语言模型及多任务方法的研究提供了可复用的基础平台，有助于推动该领域的标准化评估与发展。

Abstract: Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [9] [P4-TAS: P4-Based Time-Aware Shaper for Time-Sensitive Networking](https://arxiv.org/abs/2511.10249)
*Fabian Ihle,Moritz Flüchter,Michael Menth*

Main category: cs.NI

TL;DR: 本文提出了P4-TAS，一种基于P4语言在硬件交换芯片上实现的TSN时间感知整形器（TAS），通过内部生成的控制帧实现周期性队列控制，量化了纳秒级内部延迟来源，并提供了MPLS/TSN转换层以支持DetNet环境下的高速传输。


<details>
  <summary>Details</summary>
Motivation: 商用TSN交换机虽支持TAS硬件实现，但通常不公开如队列开启延迟等内部处理延迟信息，而这些信息对精确调度至关重要；因此需要一个透明、可量化的TAS实现方案。

Method: 在硬件交换ASIC上使用P4语言实现TAS，采用持续生成的内部TAS控制帧进行周期性队列控制，并构建MPLS/TSN翻译层以兼容DetNet架构。

Result: 识别并量化了影响调度精度的三种纳秒级内部延迟来源，实现了支持400 Gb/s转发的TSN/DetNet集成方案，并评估了P4-TAS的可扩展性及与现有TAS实现的对比性能。

Conclusion: P4-TAS不仅提高了TAS实现的透明度，为未来调度算法提供关键延迟数据，还通过MPLS/TSN翻译层有效支持DetNet环境中的高带宽时间敏感通信。

Abstract: Time-Sensitive Networking (TSN) is a set of IEEE standards that extends Ethernet with real-time capabilities. Among its mechanisms, TSN can coordinate transmission times network-wide to minimize queueing, ensuring low latency and bounded delay. This coordination is computed offline and yields a network-wide schedule. The Time-Aware Shaper (TAS), implemented in TSN bridges, protects high-priority scheduled traffic from lower-priority (best-effort) flows by periodically opening and closing priority queues according to this schedule. Deterministic Networking (DetNet), standardized by the IETF, provides similar guarantees at Layer 3 and can leverage TSN mechanisms such as TAS for that purpose. Commercially available TSN-capable switches typically implement TAS in hardware but rarely disclose internal processing delays such as queue opening latency. Such information is essential for precise scheduling but largely unavailable to system designers. In this work, we present P4-TAS, a P4-based implementation of the TAS on a hardware switching ASIC. Our design introduces a novel approach for periodic queue control using a continuous stream of internally generated TAS control frames. We identify and quantify three sources of internal delay on a nanosecond scale which also exist in other implementations that directly affect the precision of executed schedules, providing transparency for future implementations and scheduling algorithms. Moreover, we provide an MPLS/TSN translation layer that enables P4-TAS to operate within DetNet environments, allowing TSN time-based traffic shaping to be carried over high-speed 400 Gb/s forwarding. Finally, we evaluate the scalability of P4-TAS and compare it to available TAS implementations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [10] [Ksurf-Drone: Attention Kalman Filter for Contextual Bandit Optimization in Cloud Resource Allocation](https://arxiv.org/abs/2511.09766)
*Michael Dang'ana,Yuqiu Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 本文评估了Ksurf在高度可变工作负载下的资源编排任务中的性能，将其作为Drone框架中上下文多臂赌博机的目标函数模型。实验表明，Ksurf显著降低了延迟方差（p95降低41%，p99降低47%），减少了CPU使用率和主节点内存占用，并在VarBench基准测试中实现了平均工作Pod数量7%的成本节约。


<details>
  <summary>Details</summary>
Motivation: 云数据中心中容器化基础设施面临配置搜索空间庞大和云环境不确定性高的挑战，现有编排方法（如Drone）在面对虚拟机数量变化引起的负载与资源指标波动时，决策准确性下降。因此，需要一种能有效应对高变异性云数据的资源估计方法。

Method: 将Ksurf——一种先进的方差最小化估计器——集成到Drone编排器中，作为上下文多臂赌博机的目标函数模型，用于处理高度可变的工作负载场景，并在Kubernetes平台上进行实验验证。

Result: Ksurf在p95和p99延迟方差上分别降低41%和47%，CPU使用率减少4%，主节点内存减少7MB，并在VarBench Kubernetes基准测试中实现平均工作Pod数量7%的成本节约。

Conclusion: Ksurf能有效提升在高变异性云环境下的资源编排准确性与效率，显著降低延迟方差和资源开销，具有良好的实用价值和成本效益。

Abstract: Resource orchestration and configuration parameter search are key concerns for container-based infrastructure in cloud data centers. Large configuration search space and cloud uncertainties are often mitigated using contextual bandit techniques for resource orchestration including the state-of-the-art Drone orchestrator. Complexity in the cloud provider environment due to varying numbers of virtual machines introduces variability in workloads and resource metrics, making orchestration decisions less accurate due to increased nonlinearity and noise. Ksurf, a state-of-the-art variance-minimizing estimator method ideal for highly variable cloud data, enables optimal resource estimation under conditions of high cloud variability.
  This work evaluates the performance of Ksurf on estimation-based resource orchestration tasks involving highly variable workloads when employed as a contextual multi-armed bandit objective function model for cloud scenarios using Drone. Ksurf enables significantly lower latency variance of $41\%$ at p95 and $47\%$ at p99, demonstrates a $4\%$ reduction in CPU usage and 7 MB reduction in master node memory usage on Kubernetes, resulting in a $7\%$ cost savings in average worker pod count on VarBench Kubernetes benchmark.

</details>


### [11] [Lit Silicon: A Case Where Thermal Imbalance Couples Concurrent Execution in Multiple GPUs](https://arxiv.org/abs/2511.09861)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: 该论文揭示了多GPU节点中由热不平衡引发的“Lit Silicon效应”，即热诱导的拖后腿GPU会通过计算通信重叠（C3）机制拖慢整体性能，导致节点级性能波动和能效下降。作者提出了分析模型，并设计了轻量级的检测与缓解策略，在AMD MI300X系统上实现了最高6%的性能提升和4%的功耗降低。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心中GPU系统虽性能强大，但在节点和集群层面存在显著性能波动，尤其影响大语言模型等AI工作负载。作者发现这种波动与计算通信重叠（C3）技术密切相关，并进一步识别出热不平衡导致的“Lit Silicon效应”是根本原因之一。

Method: 作者首先分析单节点多GPU系统在LLM训练中的内核级性能变化，识别Lit Silicon效应；随后构建性能与功耗的解析模型；最后设计并评估三种节点级电源管理方案：GPU热设计功耗下的功耗优化、节点GPU功耗限制下的性能优化，以及利用CPU功耗转移（power sloshing）的性能优化。

Result: 在两个AMD Instinct™ MI300X GPU系统上，使用两种LLM训练框架进行实验，所提方法实现了最高6%的性能提升和4%的功耗降低，且方案易于部署，几乎无额外开销。

Conclusion: Lit Silicon效应是多GPU系统中一个被忽视但影响深远的性能瓶颈，源于热不平衡与C3机制的耦合。通过简单的节点级电源管理策略即可有效缓解该问题，为数据中心带来显著的性能与能效收益，具有高实用价值和可扩展性。

Abstract: GPU systems are increasingly powering modern datacenters at scale. Despite being highly performant, GPU systems suffer from performance variation at the node and cluster levels. Such performance variation significantly impacts both high-performance computing and artificial intelligence workloads, such as cutting-edge large language models (LLMs). We analyze the performance of a single-node multi-GPU system running LLM training, and observe that the kernel-level performance variation is highly correlated with concurrent computation communication (C3), a technique to overlap computation and communication across GPUs for performance gains. We then take a further step to reason that thermally induced straggling coupling with C3 impacts performance variation, coined as the Lit Silicon effect. Lit Silicon describes that in a multi-GPU node, thermal imbalance across GPUs introduces node-level straggler GPUs, which in turn slow down the leader GPUs. Lit Silicon leads to node-level performance variation and inefficiency, impacting the entire datacenter from the bottom up. We propose analytical performance and power models for Lit Silicon, to understand the potential system-level gains. We further design simple detection and mitigation techniques to effectively address the Lit Silicon problem, and evaluate three different power management solutions, including power optimization under GPU thermal design power, performance optimization under node-level GPU power capping, and performance optimization under node-level CPU power sloshing. We conduct experiments on two workloads on two AMD InstinctTM MI300X GPU systems under two LLM training frameworks, and observe up to 6% performance and 4% power improvements, potentially saving hundreds of millions of dollars in datacenters. Our solution is almost free lunch and can be effortlessly adopted in datacenters as a new node-level power management layer.

</details>


### [12] [MoFa: A Unified Performance Modeling Framework for LLM Pretraining](https://arxiv.org/abs/2511.09837)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Shangchao Su,Ziqing Yin,Zhiyan Cui,Hongfeng Sun,Baoguo He,Yueqiang Chen,Liang Dong,Xiyuan Li,Lingbin Wang,Lijun Ma,Qiang Huang,Ting Liu,Chong Wang,Can Wei*

Main category: cs.DC

TL;DR: 本文提出了MoFa，一种结合多维优化特征与容错机制的大模型预训练性能建模框架，能高精度预测性能并指导系统设计。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型参数规模激增，需在大规模设备集群上进行分布式预训练，但混合并行策略组合空间庞大，传统调优方法成本高昂，现有性能建模方法未能全面考虑主流优化技术和容错机制（如检查点恢复）带来的开销。

Method: 提出MoFa框架，包含增强的成本模型以准确刻画关键优化效果，并基于历史集群可靠性数据构建容错模型；同时开发了基于MoFa的调优系统，用于探索不同场景下的最优预训练性能和潜在瓶颈。

Result: 大量实验表明，MoFa在多种场景下均能实现高精度性能预测，并系统揭示了不同配置下影响预训练性能的关键因素。

Conclusion: MoFa为大语言模型预训练系统的性能建模与部署提供了有效的先验指导，显著提升了调优效率与系统设计的科学性。

Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.

</details>


### [13] [Optimizing CPU Cache Utilization in Cloud VMs with Accurate Cache Abstraction](https://arxiv.org/abs/2511.09956)
*Mani Tofigh,Edward Guo,Weiwei Jia,Xiaoning Ding,Jianchen Shan*

Main category: cs.DC

TL;DR: 该论文指出云虚拟机中缓存优化常因缺乏对缓存资源的可见性和控制而失效，并提出名为CacheX的新方案，通过驱逐集在虚拟机内部探测细粒度缓存抽象，无需硬件或虚拟机监控器支持，进而实现LLC争用感知任务调度和虚拟颜色感知页面缓存管理，显著提升公有云虚拟机中的缓存利用率。


<details>
  <summary>Details</summary>
Motivation: 在公有云环境中，虚拟机无法获知CPU缓存的分配细节，也无法通过页面放置策略影响缓存使用，导致传统基于缓存的优化方法效果不佳。

Method: 提出CacheX系统，利用驱逐集（eviction sets）在虚拟机内探测精确且细粒度的缓存抽象，无需依赖硬件或Hypervisor支持，并在此基础上实现两种新技术：末级缓存（LLC）争用感知的任务调度和虚拟颜色感知的页面缓存管理。

Result: 在x86 Linux内核中实现并评估CacheX，结果表明其能有效提升多种工作负载在公有云虚拟机中的缓存利用率。

Conclusion: CacheX为云虚拟机提供了一种可行的缓存感知机制，在不修改底层硬件或虚拟化平台的前提下，显著增强了缓存资源的利用效率。

Abstract: This paper shows that cache-based optimizations are often ineffective in cloud virtual machines (VMs) due to limited visibility into and control over provisioned caches. In public clouds, CPU caches can be partitioned or shared among VMs, but a VM is unaware of cache provisioning details. Moreover, a VM cannot influence cache usage via page placement policies, as memory-to-cache mappings are hidden. The paper proposes a novel solution, CacheX, which probes accurate and fine-grained cache abstraction within VMs using eviction sets without requiring hardware or hypervisor support, and showcases the utility of the probed information with two new techniques: LLC contention-aware task scheduling and virtual color-aware page cache management. Our evaluation of CacheX's implementation in x86 Linux kernel demonstrates that it can effectively improve cache utilization for various workloads in public cloud VMs.

</details>


### [14] [Selection of Supervised Learning-based Sparse Matrix Reordering Algorithms](https://arxiv.org/abs/2511.10180)
*Tao Tang,Youfu Jiang,Yingbo Cui,Jianbin Fang,Peng Zhang,Lin Peng,Chun Huang*

Main category: cs.DC

TL;DR: 本文提出了一种基于监督学习的模型，用于自动选择适合不同稀疏矩阵结构的重排序算法，在佛罗里达稀疏矩阵数据集上实验表明，相比仅使用AMD算法，平均求解时间减少55.37%，加速比达1.45。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏矩阵重排序算法的选择依赖暴力搜索或经验知识，难以适应多样化的稀疏矩阵结构，缺乏智能化和自适应能力。

Method: 构建一个监督学习模型，学习稀疏矩阵特征与常用重排序算法之间的关联，从而实现对最优重排序算法的自动预测。

Result: 在佛罗里达稀疏矩阵数据集上的实验显示，该模型能准确预测最优重排序算法，相比仅使用AMD算法，平均求解时间减少55.37%，平均加速比为1.45。

Conclusion: 基于监督学习的稀疏矩阵重排序算法选择方法能显著提升求解效率，具备良好的自适应性和实用性。

Abstract: Sparse matrix ordering is a vital optimization technique often employed for solving large-scale sparse matrices. Its goal is to minimize the matrix bandwidth by reorganizing its rows and columns, thus enhancing efficiency. Conventional methods for algorithm selection usually depend on brute-force search or empirical knowledge, lacking the ability to adjust to diverse sparse matrix structures.As a result, we have introduced a supervised learning-based model for choosing sparse matrix reordering algorithms. This model grasps the correlation between matrix characteristics and commonly utilized reordering algorithms, facilitating the automated and intelligent selection of the suitable sparse matrix reordering algorithm. Experiments conducted on the Florida sparse matrix dataset reveal that our model can accurately predict the optimal reordering algorithm for various matrices, leading to a 55.37% reduction in solution time compared to solely using the AMD reordering algorithm, with an average speedup ratio of 1.45.

</details>


### [15] [Workload Schedulers -- Genesis, Algorithms and Differences](https://arxiv.org/abs/2511.10258)
*Leszek Sliwko,Vladimir Getov*

Main category: cs.DC

TL;DR: 本文提出了一种对现代工作负载调度器进行分类的新方法，描述了操作系统进程调度器、集群系统作业调度器和大数据调度器三类调度器的演进与特征，并总结了它们之间的异同及设计策略上的共性。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解和组织现代调度器的发展脉络，有必要对不同类型的工作负载调度器进行系统性分类和比较。

Method: 通过描述和分析三类调度器（操作系统进程调度器、集群作业调度器和大数据调度器）的历史演进、算法使用及其功能特性，进行分类与对比。

Result: 明确了三类调度器在发展过程中的差异与联系，揭示了其调度策略设计上的共同关注点。

Conclusion: 尽管应用场景不同，本地与分布式系统中的调度策略在设计重点上存在显著相似性。

Abstract: This paper presents a novel approach to categorization of modern workload schedulers. We provide descriptions of three classes of schedulers: Operating Systems Process Schedulers, Cluster Systems Jobs Schedulers and Big Data Schedulers. We describe their evolution from early adoptions to modern implementations, considering both the use and features of algorithms. In summary, we discuss differences between all presented classes of schedulers and discuss their chronological development. In conclusion we highlight similarities in the focus of scheduling strategies design, applicable to both local and distributed systems.

</details>


### [16] [Scalable Synthesis of distributed LLM workloads through Symbolic Tensor Graphs](https://arxiv.org/abs/2511.10480)
*Changhai Man,Joongun Park,Hanjiang Wu,Huan Xu,Srinivas Sridharan,Tushar Krishna*

Main category: cs.DC

TL;DR: 提出了STAGE框架，用于合成高保真度的大语言模型（LLM）执行轨迹，支持多种并行策略和大规模系统配置探索。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖真实系统采集执行轨迹，但大规模基础设施访问受限，且难以适配未来更大规模的系统配置，因此需要一种可扩展、高保真的建模机制。

Method: 开发了Symbolic Tensor grAph GEnerator（STAGE）框架，通过合成方式生成包含计算、内存和通信细节的高保真LLM执行轨迹，并支持多种并行化策略。

Result: STAGE成功合成了覆盖超过32K GPU的高保真LLM执行轨迹，在张量级别上准确反映计算、内存与通信行为。

Conclusion: STAGE为分布式机器学习系统研究提供了可扩展、灵活且高保真的建模工具，将公开发布以促进相关研究。

Abstract: Optimizing the performance of large language models (LLMs) on large-scale AI training and inference systems requires a scalable and expressive mechanism to model distributed workload execution. Such modeling is essential for pre-deployment system-level optimizations (e.g., parallelization strategies) and design-space explorations. While recent efforts have proposed collecting execution traces from real systems, access to large-scale infrastructure remains limited to major cloud providers. Moreover, traces obtained from existing platforms cannot be easily adapted to study future larger-scale system configurations. We introduce Symbolic Tensor grAph GEnerator(STAGE), a framework that synthesizes high-fidelity execution traces to accurately model LLM workloads. STAGE supports a comprehensive set of parallelization strategies, allowing users to systematically explore a wide spectrum of LLM architectures and system configurations. STAGE demonstrates its scalability by synthesizing high-fidelity LLM traces spanning over 32K GPUs, while preserving tensor-level accuracy in compute, memory, and communication. STAGE will be publicly available to facilitate further research in distributed machine learning systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [History-Aware Trajectory k-Anonymization Using an FPGA-Based Hardware Accelerator for Real-Time Location Services](https://arxiv.org/abs/2511.09688)
*Hiroshi Nakano,Hiroaki Nishi*

Main category: cs.AR

TL;DR: 本文提出了一种结合历史轨迹信息的k-匿名化方法，并在FPGA上实现了支持该方法的硬件架构，在保证实时性的同时提升了匿名数据的行为真实性和道路保留效果。


<details>
  <summary>Details</summary>
Motivation: 先前基于最短路径的轨迹匿名化方法无法准确反映用户真实出行行为，导致匿名后数据效用降低；为提升匿名数据的行为保真度，需引入历史轨迹信息。

Method: 提出一种历史感知的轨迹k-匿名化方法，设计了集成并行历史轨迹搜索与最短路径查找的FPGA硬件架构，并采用定制定点计数模块对历史数据贡献进行加权。

Result: FPGA实现达到每秒6000条以上记录的吞吐量，相比仅使用最短路径的方法，数据保留率提升最多1.2%，并更有效地保留了主干道路。

Conclusion: 该方法在满足LBS严格延迟约束的前提下，实现了兼顾隐私保护与行为准确性的高保真轨迹匿名化，是实时轨迹隐私保护的重要进展。

Abstract: Our previous work established the feasibility of FPGA-based real-time trajectory anonymization, a critical task for protecting user privacy in modern location-based services (LBS). However, that pioneering approach relied exclusively on shortest-path computations, which can fail to capture re- alistic travel behavior and thus reduce the utility of the anonymized data. To address this limitation, this paper introduces a novel, history-aware trajectory k-anonymization methodology and presents an advanced FPGA-based hardware architecture to implement it. Our proposed architecture uniquely integrates par- allel history-based trajectory searches with conventional shortest- path finding, using a custom fixed-point counting module to ac- curately weigh contributions from historical data. This approach enables the system to prioritize behaviorally common routes over geometrically shorter but less-traveled paths. The FPGA implementation demonstrates that our new architecture achieves a real-time throughput of over 6,000 records/s, improves data retention by up to 1.2% compared to our previous shortest-path- only design, and preserves major arterial roads more effectively. These results signify a key advancement, enabling high-fidelity, history-aware anonymization that preserves both privacy and behavioral accuracy under the strict latency constraints of LBS.

</details>


### [18] [AssertMiner: Module-Level Spec Generation and Assertion Mining using Static Analysis Guided LLMs](https://arxiv.org/abs/2511.10007)
*Hongqin Lyu,Yonghao Wang,Jiaxin Zhou,Zhiteng Chao,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: AssertMiner is a module-level assertion generation framework that uses static AST-derived information to guide LLMs in generating detailed, high-quality assertions, improving verification coverage and error detection over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing assertion generation methods focus mainly on top-level specifications and neglect micro-architectural implementation details where design errors are more common, leading to insufficient verification coverage.

Method: AssertMiner leverages abstract syntax tree (AST) analysis to extract structural information—including module call graphs, I/O tables, and dataflow graphs—and uses this to guide large language models (LLMs) in generating module-level assertions.

Result: AssertMiner outperforms prior methods like AssertLLM and Spec2Assertion in generating high-quality module-level assertions and enhances structural coverage and error detection when combined with them.

Conclusion: By focusing on module-level assertion generation guided by static structural information, AssertMiner enables more comprehensive and efficient hardware verification.

Abstract: Assertion-based verification (ABV) is a key approach to checking whether a logic design complies with its architectural specifications. Existing assertion generation methods based on design specifications typically produce only top-level assertions, overlooking verification needs on the implementation details in the modules at the micro-architectural level, where design errors occur more frequently. To address this limitation, we present AssertMiner, a module-level assertion generation framework that leverages static information generated from abstract syntax tree (AST) to assist LLMs in mining assertions. Specifically, it performs AST-based structural extraction to derive the module call graph, I/O table, and dataflow graph, guiding the LLM to generate module-level specifications and mine module-level assertions. Our evaluation demonstrates that AssertMiner outperforms existing methods such as AssertLLM and Spec2Assertion in generating high-quality assertions for modules. When integrated with these methods, AssertMiner can enhance the structural coverage and significantly improve the error detection capability, enabling a more comprehensive and efficient verification process.

</details>


### [19] [Beamspace Equalization for mmWave Massive MIMO: Algorithms and VLSI Implementations](https://arxiv.org/abs/2511.10563)
*Seyed Hadi Mirfarshbafan,Christoph Studer*

Main category: cs.AR

TL;DR: 本文研究了面向毫米波大规模MIMO系统的波束域数据检测算法与VLSI架构，提出了一种复杂稀疏自适应均衡器（CSPADE），在22nm工艺下实现显著的功耗、能效和面积优势，并达到现有检测器中最高的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 大规模多用户MIMO与毫米波通信虽为未来无线系统的关键技术，但其基带处理硬件成本与功耗过高；利用毫米波信道稀疏性进行波束域处理可有效降低复杂度。

Method: 提出新的波束域数据检测算法CSPADE，并设计对应的全并行与基于MAC的串行VLSI架构，在22nm FDSOI工艺下进行实现与评估。

Result: 全并行CSPADE架构相比天线域均衡器最多节省54%功耗，且吞吐量为现有大规模MIMO检测器中最高；基于MAC的串行架构则进一步将功耗降低达66%。

Conclusion: 所提出的CSPADE算法及其VLSI架构在功耗、能效、面积和吞吐量方面均优于现有方案，为毫米波大规模MIMO系统提供了高效可行的基带处理解决方案。

Abstract: Massive multiuser multiple-input multiple-output (MIMO) and millimeter-wave (mmWave) communication are key physical layer technologies in future wireless systems. Their deployment, however, is expected to incur excessive baseband processing hardware cost and power consumption. Beamspace processing leverages the channel sparsity at mmWave frequencies to reduce baseband processing complexity. In this paper, we review existing beamspace data detection algorithms and propose new algorithms as well as corresponding VLSI architectures that reduce data detection power. We present VLSI implementation results for the proposed architectures in a 22nm FDSOI process. Our results demonstrate that a fully-parallelized implementation of the proposed complex sparsity-adaptive equalizer (CSPADE) achieves up to 54% power savings compared to antenna-domain equalization. Furthermore, our fully-parallelized designs achieve the highest reported throughput among existing massive MIMO data detectors, while achieving better energy and area efficiency. We also present a sequential multiply-accumulate (MAC)-based architecture for CSPADE, which enables even higher power savings, i.e., up to 66%, compared to a MAC-based antenna-domain equalizer.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [20] [Localized kernel gradient correction for SPH simulations of water wave propagation](https://arxiv.org/abs/2511.10064)
*Lennart Justin Schulze,Vito Zago,Giuseppe Bilotta,Robert Anthony Dalrymple*

Main category: cs.CE

TL;DR: 本文提出一种局部高阶SPH方法，仅在水波特定区域应用核梯度修正，以减少计算开销并改善水波模拟中的数值耗散问题。


<details>
  <summary>Details</summary>
Motivation: 基础SPH模型在模拟水波传播时存在过大的数值耗散，而现有高阶方法（如核梯度修正）虽能改善但计算成本高。

Method: 基于水波力学原理，识别需高阶处理的粒子区域，在该局部区域应用核梯度修正，并对自由面处的核梯度修正问题提出改进措施。

Result: 在驻波水池和长波水槽中的行进波模拟中，该方法取得了令人满意的结果，尤其在深水波情形下显著节省计算量。

Conclusion: 局部应用高阶SPH方法可在保证模拟精度的同时大幅降低计算成本，是一种高效且实用的水波模拟策略。

Abstract: Basic Smoothed Particle Hydrodynamics (SPH) models exhibit excessive, numerical dissipation in the simulation of water wave propagation. This can be remedied using higher-order approaches such as kernel gradient correction, which introduce additional computational effort. The present work demonstrates, that the higher-order scheme is only required in a limited part of the water wave in order to obtain satisfying results. The criterion for distinguishing particles in need of special treatment from those that do not is motivated by water wave mechanics. Especially for deep water waves, the approach potentially spares large amounts of computational effort. The present paper also proposes a remedy for issues of the kernel gradient correction occurring at the free surface. Satisfying results for the proposed approach are shown for a standing wave in a basin and a progressive wave train in a long wave tank.

</details>


### [21] [Phase field modelling of cracking and capacity fade in core-shell cathode particles for lithium-ion batteries](https://arxiv.org/abs/2511.10355)
*Y. Tu,B. Wu,E. Martínez-Pañeda*

Main category: cs.CE

TL;DR: 本文提出了一种耦合化学-力学-损伤的相场计算框架，用于预测核壳结构电极颗粒在锂离子电池中的失效行为，揭示了表面裂纹和界面脱粘的形成机制及其对容量衰减的影响。


<details>
  <summary>Details</summary>
Motivation: 核壳结构虽被视为提升锂离子电池性能的有效策略，但实验发现其在单次充放电后即出现壳层开裂与核壳界面脱粘等机械失效问题，亟需深入理解其失效机理以指导材料设计。

Method: 采用完全耦合的化学-力学-损伤相场模型，同时模拟体相断裂与界面脱粘，对典型核壳体系（NMC811@NMC532）进行案例研究，量化颗粒开裂与容量衰减。

Result: 研究发现：核中锂浓度显著高于壳时引发表面裂纹；壳层膨胀导致界面附近环向应力集中，迅速引发脱粘并阻碍锂离子传输，单次放电即可造成超10%容量损失；大颗粒中拉伸区可诱发裂纹分叉，导致颗粒碎裂。

Conclusion: 所开发的计算框架不仅揭示了核壳颗粒的退化机制，还可用于设计性能更优、寿命更长的电极材料。

Abstract: Core-shell electrode particles are a promising morphology control strategy for high-performance lithium-ion batteries. However, experimental observations reveal that these structures remain prone to mechanical failure, with shell fractures and core-shell debonding occurring after a single charge. In this work, we present a novel, comprehensive computational framework to predict and gain insight into the failure of core-shell morphologies and the associated degradation in battery performance. The fully coupled chemo-mechano-damage model presented captures the interplay between mechanical damage and electrochemical behaviours, enabling the quantification of particle cracking and capacity fade. Both bulk material fracture and interface debonding are captured by utilising the phase field method. We quantify the severity of particle cracking and capacity loss through case studies on a representative core-shell system (NMC811@NMC532). The results bring valuable insights into cracking patterns, underlying mechanisms, and their impact on capacity loss. Surface cracks are found to initiate when a significantly higher lithium concentration accumulates in the core compared to the shell. Interfacial debonding is shown to arise from localised hoop stresses near the core-shell interface, due to greater shell expansion. This debonding develops rapidly, impedes lithium-ion transport, and can lead to more than 10\% capacity loss after a single discharge. Furthermore, larger particles may experience crack branching driven by extensive tensile zones, potentially fragmenting the entire particle. The framework developed can not only bring new insight into the degradation mechanisms of core-shell particles but also be used to design electrode materials with improved performance and extended lifetime.

</details>
