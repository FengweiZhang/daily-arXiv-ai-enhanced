<div id=toc></div>

# Table of Contents

- [cs.CE](#cs.CE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 10]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [1] [Multivariate Sensitivity Analysis of Electric Machine Efficiency Maps and Profiles Under Design Uncertainty](https://arxiv.org/abs/2511.17099)
*Aylar Partovizadeh,Sebastian Schöps,Dimitrios Loukrezis*

Main category: cs.CE

TL;DR: 本文提出使用多变量全局敏感性分析来评估不确定电机设计参数对效率图和效率曲线的影响，相比传统的逐点Sobol'方法，该方法能为每个参数提供单一的敏感性指标，从而整体评估参数重要性，并用于简化模型。


<details>
  <summary>Details</summary>
Motivation: 传统基于方差的敏感性分析（如Sobol'方法）通常逐点进行，难以全面反映参数在整个效率图或效率曲线上的综合影响；因此需要一种能够提供整体敏感性评估的方法以指导模型简化。

Method: 采用多变量全局敏感性分析方法，结合蒙特卡洛抽样和多项式混沌展开，对不同保真度的永磁同步电机模型进行参数敏感性评估，并基于结果固定非关键参数以简化模型。

Result: 多变量敏感性分析成功识别出关键与非关键设计参数；简化后的模型在保留预测精度的同时显著降低了计算成本，验证了该方法在模型简化中的有效性。

Conclusion: 多变量全局敏感性分析能够有效指导电机模型的简化，在保证不确定性估计准确性的同时提升计算效率，优于传统的逐点敏感性分析方法。

Abstract: This work proposes the use of multivariate global sensitivity analysis for assessing the impact of uncertain electric machine design parameters on efficiency maps and profiles. Contrary to the common approach of applying variance-based (Sobol') sensitivity analysis elementwise, multivariate sensitivity analysis provides a single sensitivity index per parameter, thus allowing for a holistic estimation of parameter importance over the full efficiency map or profile. Its benefits are demonstrated on permanent magnet synchronous machine models of different fidelity. Computations based on Monte Carlo sampling and polynomial chaos expansions are compared in terms of computational cost. The sensitivity analysis results are subsequently used to simplify the models, by fixing non-influential parameters to their nominal values and allowing random variations only for influential parameters. Uncertainty estimates obtained with the full and reduced models confirm the validity of model simplification guided by multivariate sensitivity analysis.

</details>


### [2] [Towards Generative Design Using Optimal Transport for Shape Exploration and Solution Field Interpolation](https://arxiv.org/abs/2511.17111)
*Sergio Torregrosa,David Munoz,Hector Navarro,Charbel Farhat,Francisco Chinesta*

Main category: cs.CE

TL;DR: 本文提出了一种基于最优传输（OT）的统一结构保持框架，用于生成式设计（GD），通过高斯泼溅和Wasserstein重心实现复杂几何形状及其物理场在演化设计空间中的插值，克服了现有方法在数据依赖、计算成本和网格适应性方面的局限。


<details>
  <summary>Details</summary>
Motivation: 当前生成式设计方法面临三大挑战：AI方法依赖大量数据且泛化能力弱；拓扑优化计算昂贵且难以扩展到多物理场问题；针对演化几何的模型降阶技术尚不成熟。因此亟需一种能同时处理几何与物理场演化、不依赖固定网格的新框架。

Method: 提出一种基于最优传输（OT）的结构保持框架，利用高斯泼溅构建连续、无网格的解表示，并通过Wasserstein重心实现几何形状的平滑、“质量”守恒的融合，从而在非匹配网格和大形变条件下插值正标量场。

Result: 该框架能在任意演化几何间高效插值物理场，无需相同网格拓扑或维度，并能保留应力集中等局部物理特征，避免人为平滑。初步实验展示了其在效率、适应性和物理保真度方面的优势，并拓展至有符号场和矢量场。

Conclusion: 所提出的最优传输框架为生成式设计提供了一种通用、高效且物理一致的基础方法，有望支撑未来基于基础模型的生成式设计工作流。

Abstract: Generative Design (GD) combines artificial intelligence (AI), physics-based modeling, and multi-objective optimization to autonomously explore and refine engineering designs. Despite its promise in aerospace, automotive, and other high-performance applications, current GD methods face critical challenges: AI approaches require large datasets and often struggle to generalize; topology optimization is computationally intensive and difficult to extend to multiphysics problems; and model order reduction for evolving geometries remains underdeveloped. To address these challenges, we introduce a unified, structure-preserving framework for GD based on optimal transport (OT), enabling simultaneous interpolation of complex geometries and their associated physical solution fields across evolving design spaces, even with non-matching meshes and substantial shape changes. This capability leverages Gaussian splatting to provide a continuous, mesh-independent representation of the solution and Wasserstein barycenters to enable smooth, mathematically ''mass''-preserving blending of geometries, offering a major advance over surrogate models tied to static meshes. Our framework efficiently interpolates positive scalar fields across arbitrarily shaped, evolving geometries without requiring identical mesh topology or dimensionality. OT also naturally preserves localized physical features -- such as stress concentrations or sharp gradients -- by conserving the spatial distribution of quantities, interpreted as ''mass'' in a mathematical sense, rather than averaging them, avoiding artificial smoothing. Preliminary extensions to signed and vector fields are presented. Representative test cases demonstrate enhanced efficiency, adaptability, and physical fidelity, establishing a foundation for future foundation-model-powered generative design workflows.

</details>


### [3] [Randomness as Reference: Benchmark Metric for Optimization in Engineering](https://arxiv.org/abs/2511.17226)
*Stefan Ivić,Siniša Družeta,Luka Grbčić*

Main category: cs.CE

TL;DR: 本文提出了一套包含231个源自工程设计与仿真的连续无约束优化问题的新基准测试集，并引入一种基于随机采样的新性能度量方法，用于对20种优化算法进行系统评估，结果揭示了传统人工测试集的局限性，并为算法的实际应用提供了指导。


<details>
  <summary>Details</summary>
Motivation: 现有广泛使用的人工测试集难以反映真实工程优化问题的多样性与复杂性，导致算法评估与实际应用之间存在差距。

Method: 构建包含231个工程来源的连续无约束优化问题的基准测试集，提出一种基于随机采样统计参考的新性能指标，对20种确定性和随机优化算法进行数百次独立运行的系统评估。

Result: 仅有少数算法在工程类问题上表现优异，而一些常用元启发式算法效率显著下降；研究还分析了算法特性并提供了实用应用指南。

Conclusion: 所提出的测试集与性能指标共同构成一个透明、可复现且贴近实际的评估平台，有助于弥合现有基准测试与真实工程应用之间的差距。

Abstract: Benchmarking optimization algorithms is fundamental for the advancement of computational intelligence. However, widely adopted artificial test suites exhibit limited correspondence with the diversity and complexity of real-world engineering optimization tasks. This paper presents a new benchmark suite comprising 231 bounded, continuous, unconstrained optimization problems, the majority derived from engineering design and simulation scenarios, including computational fluid dynamics and finite element analysis models. In conjunction with this suite, a novel performance metric is introduced, which employs random sampling as a statistical reference, providing nonlinear normalization of objective values and enabling unbiased comparison of algorithmic efficiency across heterogeneous problems. Using this framework, 20 deterministic and stochastic optimization methods were systematically evaluated through hundreds of independent runs per problem, ensuring statistical robustness. The results indicate that only a few of the tested optimization methods consistently achieve excellent performance, while several commonly used metaheuristics exhibit severe efficiency loss on engineering-type problems, emphasizing the limitations of conventional benchmarks. Furthermore, the conducted tests are used for analyzing various features of the optimization methods, providing practical guidelines for their application. The proposed test suite and metric together offer a transparent, reproducible, and practically relevant platform for evaluating and comparing optimization methods, thereby narrowing the gap between the available benchmark tests and realistic engineering applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [4] [RAG-Driven Data Quality Governance for Enterprise ERP Systems](https://arxiv.org/abs/2511.16700)
*Sedat Bin Vedat,Enes Kutay Yarkan,Meftun Akarsu,Recep Kaan Karaman,Arda Sar,Çağrı Çelikbilek,Savaş Saygılı*

Main category: cs.DB

TL;DR: 本文提出一个端到端的AI驱动数据治理框架，结合自动化清洗与大语言模型（GPT-4o）生成多语言SQL查询，在管理24万员工记录的企业ERP系统中显著提升查询效率、准确性和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 企业ERP系统在多语言环境下由HR部门分散手动录入员工数据，导致严重的数据质量问题，亟需一种可扩展、高准确率且支持自然语言交互的数据治理方案。

Method: 系统包含两个集成阶段：1）多阶段数据清洗管道，执行翻译标准化、拼写校正和实体去重；2）基于GPT-4o的检索增强生成框架，利用LangChain、FAISS向量搜索和500+示例的少样本学习，将土耳其语、俄语和英语的自然语言问题转为合规SQL查询。

Result: 在2,847条生产查询中达到92.5%查询有效性、95.1%模式合规性和90.7%语义准确性；查询响应时间从2.3天降至5秒内，系统可用性达99.2%；相比GPT-3.5，GPT-4o降低46%延迟和68%成本；用户满意度为4.3/5.0。

Conclusion: 该模块化架构为企业级AI原生数据治理提供了可复现、高效且高可用的解决方案，验证了其在大规模真实场景中的可行性与价值。

Abstract: Enterprise ERP systems managing hundreds of thousands of employee records face critical data quality challenges when human resources departments perform decentralized manual entry across multiple languages. We present an end-to-end pipeline combining automated data cleaning with LLM-driven SQL query generation, deployed on a production system managing 240,000 employee records over six months.
  The system operates in two integrated stages: a multi-stage cleaning pipeline that performs translation normalization, spelling correction, and entity deduplication during periodic synchronization from Microsoft SQL Server to PostgreSQL; and a retrieval-augmented generation framework powered by GPT-4o that translates natural-language questions in Turkish, Russian, and English into validated SQL queries. The query engine employs LangChain orchestration, FAISS vector similarity search, and few-shot learning with 500+ validated examples.
  Our evaluation demonstrates 92.5% query validity, 95.1% schema compliance, and 90.7\% semantic accuracy on 2,847 production queries. The system reduces query turnaround time from 2.3 days to under 5 seconds while maintaining 99.2% uptime, with GPT-4o achieving 46% lower latency and 68% cost reduction versus GPT-3.5. This modular architecture provides a reproducible framework for AI-native enterprise data governance, demonstrating real-world viability at enterprise scale with 4.3/5.0 user satisfaction.

</details>


### [5] [LinkML: An Open Data Modeling Framework](https://arxiv.org/abs/2511.16935)
*Sierra A. T. Moxon,Harold Solbrig,Nomi L. Harris,Patrick Kalita,Mark A. Miller,Sujay Patil,Kevin Schaper,Chris Bizon,J. Harry Caufield,Silvano Cirujano Cuesta,Corey Cox,Frank Dekervel,Damion M. Dooley,William D. Duncan,Tim Fliss,Sarah Gehrke,Adam S. L. Graefe,Harshad Hegde,AJ Ireland,Julius O. B. Jacobsen,Madan Krishnamurthy,Carlo Kroll,David Linke,Ryan Ly,Nicolas Matentzoglu,James A. Overton,Jonny L. Saunders,Deepak R. Unni,Gaurav Vaidya,Wouter-Michiel A. M. Vierdag,LinkML Community Contributors,Oliver Ruebel,Christopher G. Chute,Matthew H. Brush,Melissa A. Haendel,Christopher J. Mungall*

Main category: cs.DB

TL;DR: LinkML 是一个开放的数据建模框架，用于简化数据的定义、验证与共享，支持多种数据结构并促进跨学科协作，有助于实现 FAIR 数据标准。


<details>
  <summary>Details</summary>
Motivation: 科研数据常以非结构化或非标准化形式存储，导致互操作性差、难以整合与复用，亟需统一的数据建模方法。

Method: 采用 LinkML（Linked Data Modeling Language）框架，通过其与技术架构无关的语法描述数据模式、类和关系，支持从简单到复杂的数据模型，并可与其他 LinkML 模式集成。

Result: LinkML 已在生物、化学、生物医学、金融等多个领域得到应用，有效减少了数据模型的异构性和一次性使用问题，提升了数据标准化和语义共享能力。

Conclusion: LinkML 提供了一种可靠且易于协作的数据建模范式，能够使隐式模型显式化、可计算化，并在数据源头实现标准化，推动 FAIR 原则落地。

Abstract: Scientific research relies on well-structured, standardized data; however, much of it is stored in formats such as free-text lab notebooks, non-standardized spreadsheets, or data repositories. This lack of structure challenges interoperability, making data integration, validation, and reuse difficult. LinkML (Linked Data Modeling Language) is an open framework that simplifies the process of authoring, validating, and sharing data. LinkML can describe a range of data structures, from flat, list-based models to complex, interrelated, and normalized models that utilize polymorphism and compound inheritance. It offers an approachable syntax that is not tied to any one technical architecture and can be integrated seamlessly with many existing frameworks. The LinkML syntax provides a standard way to describe schemas, classes, and relationships, allowing modelers to build well-defined, stable, and optionally ontology-aligned data structures. Once defined, LinkML schemas may be imported into other LinkML schemas. These key features make LinkML an accessible platform for interdisciplinary collaboration and a reliable way to define and share data semantics.
  LinkML helps reduce heterogeneity, complexity, and the proliferation of single-use data models while simultaneously enabling compliance with FAIR data standards. LinkML has seen increasing adoption in various fields, including biology, chemistry, biomedicine, microbiome research, finance, electrical engineering, transportation, and commercial software development. In short, LinkML makes implicit models explicitly computable and allows data to be standardized at its origin. LinkML documentation and code are available at linkml.io.

</details>


### [6] [Anomaly Pattern-guided Transaction Bug Testing in Relational Databases](https://arxiv.org/abs/2511.17377)
*Huicong Xu,Shuang Liu,Xianyu Zhu,Qiyu Zhuang,Wei Lu,Xiaoyong Du*

Main category: cs.DB

TL;DR: 本文提出了一种基于异常模式引导的测试方法APTrans，用于检测关系型数据库管理系统（RDBMS）在不同隔离级别下的事务处理缺陷，成功在MySQL、MariaDB和OceanBase中发现了13个新漏洞。


<details>
  <summary>Details</summary>
Motivation: 并发事务处理是RDBMS的核心能力，但针对不同隔离级别的事务行为测试面临两大挑战：一是难以自动生成能有效触发事务逻辑缺陷的测试用例；二是随机生成事务的正确执行结果通常未知，导致难以检测逻辑异常。

Method: 作者提出一种异常模式引导的测试方法：首先通过预定义的异常模式指导测试用例生成，提高暴露事务缺陷的可能性；其次采用包含显式错误检测和隐式错误检测的两阶段检测流程来识别事务执行中的错误。

Result: 实现的工具APTrans在MySQL、MariaDB和OceanBase三个主流RDBMS上共发现13个此前未知的事务相关漏洞，其中11个已被开发团队确认。

Conclusion: 该研究证明了基于异常模式引导的测试方法在发现RDBMS事务处理缺陷方面的有效性，为提升数据库系统可靠性提供了实用工具和方法。

Abstract: Concurrent transaction processing is a fundamental capability of Relational Database Management Systems (RDBMSs), widely utilized in applications requiring high levels of parallel user interaction, such as banking systems, e-commerce platforms, and telecommunications infrastructure. Isolation levels offer a configurable mechanism to manage the interaction between concurrent transactions, enabling varying degrees of consistency and performance trade-offs. These isolation guarantees are supported by all major RDBMSs. However, testing transaction behavior under different isolation levels remains a significant challenge due to two primary reasons. First, automatically generating test transactions that can effectively expose bugs in transaction handling logic is non-trivial, as such bugs are typically triggered under specific transactional constraints. Second, detecting logic anomalies in transaction outcomes is difficult because the correct execution results are often unknown for randomly generated transactions. To address these challenges, we propose an anomaly pattern-guided testing approach for uncovering transaction bugs in RDBMSs. Our solution tackles the first challenge by introducing a test case generation technique guided by predefined anomaly patterns, which increases the likelihood of exposing transactional bugs. For the second challenge, we present a two-phase detection process, involving explicit error detection and implicit error detection, to identify bugs in transaction execution. We have implemented our approach in a tool, APTrans, and evaluated it on three widely-used RDBMSs: MySQL, MariaDB, and OceanBase. APTrans successfully identified 13 previously unknown transaction-related bugs, 11 of which have been confirmed by the respective development teams.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [7] [One Walk is All You Need: Data-Efficient 3D RF Scene Reconstruction with Human Movements](https://arxiv.org/abs/2511.16966)
*Yiheng Bian,Zechen Li,Lanqing Yang,Hao Pan,Yezhou Wang,Longyuan Ge,Jeffery Wu,Ruiheng Liu,Yongjian Fu,Yichao chen,Guangtao xue*

Main category: cs.NI

TL;DR: 本文提出一种新范式，利用单次短暂的人类行走视频，通过复合3D高斯泼溅（3DGS）框架，高效、高质量地重建被遮挡的静态3D辐射场场景，将人体运动视为有用信号而非噪声，在仅60秒数据下SSIM达0.96，优于现有SOTA方法12%。


<details>
  <summary>Details</summary>
Motivation: 传统3D辐射场重建需大量静态测量，将人体运动视为噪声，导致数据采集繁琐；本文旨在突破这一瓶颈，探索利用非结构化人体运动作为信息源，实现快速、数据高效且高保真的遮挡场景重建。

Method: 设计基于复合3D高斯泼溅（3DGS）的因子分解框架，从原始射频流中联合建模人体动态效应与静态场景几何，仅用单次60秒随意行走数据进行训练。

Result: 模型在仅60秒人类行走数据下重建完整静态场景，SSIM达到0.96，比当前最先进的重度采样方法性能提升12%。

Conclusion: 将人体运动转化为有效信号可显著降低数据采集成本，为实时、无先验的3D射频环境建图提供新路径。

Abstract: Reconstructing 3D Radiance Field (RF) scenes through opaque obstacles is a long-standing goal, yet it is fundamentally constrained by a laborious data acquisition process requiring thousands of static measurements, which treats human motion as noise to be filtered. This work introduces a new paradigm with a core objective: to perform fast, data-efficient, and high-fidelity RF reconstruction of occluded 3D static scenes, using only a single, brief human walk. We argue that this unstructured motion is not noise, but is in fact an information-rich signal available for reconstruction. To achieve this, we design a factorization framework based on composite 3D Gaussian Splatting (3DGS) that learns to model the dynamic effects of human motion from the persistent static scene geometry within a raw RF stream. Trained on just a single 60-second casual walk, our model reconstructs the full static scene with a Structural Similarity Index (SSIM) of 0.96, remarkably outperforming heavily-sampled state-of-the-art (SOTA) by 12%. By transforming the human movements into its valuable signals, our method eliminates the data acquisition bottleneck and paves the way for on-the-fly 3D RF mapping of unseen environments.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [MicroMoE: Fine-Grained Load Balancing for Mixture-of-Experts with Token Scheduling](https://arxiv.org/abs/2511.16947)
*Chenqi Zhao,Wenfei Wu,Linhai Song,Yuchen Xu*

Main category: cs.DC

TL;DR: 本文提出了MicroEP并行策略和基于其的MicroMoE系统，通过高效的跨GPU令牌调度，在每个微批次中实现细粒度负载均衡，显著提升MoE模型训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts（MoE）模型虽能降低计算资源消耗，但其动态特性导致专家间负载不均衡，严重影响训练效率；现有方法或牺牲模型精度，或引入额外系统开销，难以实现细粒度负载均衡。

Method: 提出MicroEP并行策略，通过在GPU之间高效调度token，实现在每个微批次中的最优负载均衡；并在此基础上构建了分布式MoE训练系统MicroMoE。

Result: 实验表明，MicroMoE相比当前最先进的系统，端到端训练吞吐量最高提升47.6%，并在GPU间几乎始终实现最优负载均衡。

Conclusion: MicroEP和MicroMoE有效解决了MoE训练中的细粒度负载均衡问题，在不牺牲模型精度的前提下显著提升了训练效率。

Abstract: Mixture-of-Experts (MoE) has emerged as a promising approach to scale up deep learning models due to its significant reduction in computational resources. However, the dynamic nature of MoE leads to load imbalance among experts, severely impacting training efficiency. While previous research has attempted to address the load balancing challenge, existing solutions either compromise model accuracy or introduce additional system overhead. As a result, they fail to achieve fine-grained load balancing, which is crucial to optimizing training efficiency.
  We propose MicroEP, a novel parallelization strategy to achieve fine-grained load balancing in MoE systems. MicroEP is capable of achieving optimal load balancing in every micro-batch through efficient token scheduling across GPUs. Furthermore, we propose MicroMoE, an efficient distributed MoE training system with MicroEP's load balancing capabilities. Our experimental results demonstrate that MicroMoE improves the end-to-end training throughput by up to 47.6% compared with the state-of-the-art system, and almost consistently achieves optimal load balance among GPUs.

</details>


### [9] [Modeling Anomaly Detection in Cloud Services: Analysis of the Properties that Impact Latency and Resource Consumption](https://arxiv.org/abs/2511.17119)
*Gabriel Job Antunes Grabher,Fumio Machida,Thomas Ropars*

Main category: cs.DC

TL;DR: 本文研究了云服务中性能异常检测器的特性（如精确率、召回率和检测频率）如何影响性能与成本之间的权衡，发现高频检测时高精确率足够，而低频检测时高召回率更为关键。


<details>
  <summary>Details</summary>
Motivation: 在云服务中，性能异常检测对维持性能目标至关重要，但检测器会出错，且其触发的扩缩容操作会增加资源消耗。因此，有必要理解哪些检测器特性对优化性能与成本之间的权衡最为关键。

Method: 作者使用随机奖励网（Stochastic Reward Nets）对由性能异常检测器监控的云服务进行建模，并分析检测器的精确率、召回率和检测频率对服务平均延迟和资源消耗的影响。

Result: 研究结果表明，并非总是需要同时实现高精确率和高召回率：当检测频率高时，仅高精确率即可获得良好的性能-成本权衡；而当检测频率低时，高召回率则更为重要。

Conclusion: 性能异常检测器的设计应根据其运行频率来权衡精确率与召回率，以实现最优的性能与成本平衡。

Abstract: Detecting and resolving performance anomalies in Cloud services is crucial for maintaining desired performance objectives. Scaling actions triggered by an anomaly detector help achieve target latency at the cost of extra resource consumption. However, performance anomaly detectors make mistakes. This paper studies which characteristics of performance anomaly detection are important to optimize the trade-off between performance and cost. Using Stochastic Reward Nets, we model a Cloud service monitored by a performance anomaly detector. Using our model, we study the impact of detector characteristics, namely precision, recall and inspection frequency, on the average latency and resource consumption of the monitored service. Our results show that achieving a high precision and a high recall is not always necessary. If detection can be run frequently, a high precision is enough to obtain a good performance-to-cost trade-off, but if the detector is run infrequently, recall becomes the most important.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [10] [Multi-Agent Code Verification with Compound Vulnerability Detection](https://arxiv.org/abs/2511.16708)
*Shreshth Rajan*

Main category: cs.SE

TL;DR: CodeX-Verify 是一个多智能体系统，通过四个专用智能体协同检测代码中的不同类型的错误，在不执行测试的情况下以更快速度达到与现有最佳方法相当的 bug 检测率（76.1%），并揭示多漏洞组合带来的风险被严重低估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型生成的代码存在大量缺陷和安全漏洞，而现有检测工具覆盖率有限且误报率高，亟需更高效、准确的静态验证方法。

Method: 构建包含四个专业化智能体的多智能体系统 CodeX-Verify，每个智能体针对不同类型的 bug；通过数学证明和实验验证多智能体组合在低相关性（p = 0.05–0.25）下能显著提升检测效果；在99个带标签样本和300个真实补丁上评估性能。

Result: 系统检测率达76.1%，优于单一智能体（最高提升39.7个百分点），最佳双智能体组合达79.3%；单样本分析耗时低于200ms；同时发现多漏洞共存时风险呈指数级增长（如SQL注入+凭证泄露风险达300，远超传统模型预测的20）。

Conclusion: 多智能体协同验证能有效提升 LLM 生成代码的 bug 检测效率与准确性，且具备实际部署可行性；研究还强调需重新评估多漏洞交互带来的安全风险。

Abstract: LLMs generate buggy code: 29.6% of SWE-bench "solved" patches fail, 62% of BaxBench solutions have vulnerabilities, and existing tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns finds more bugs than any single agent when the agents look for different problems, confirmed by measuring agent correlation of p = 0.05--0.25. We also show that multiple vulnerabilities in the same code create exponentially more risk than previously thought--SQL injection plus exposed credentials creates 15x more danger (risk 300 vs. 20) than traditional models predict. Testing on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method while running faster and without test execution. We tested 15 different agent combinations and found that using multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with gains of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4. The best two-agent combination reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in under 200ms per sample, making this practical for production use.

</details>


### [11] [Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair](https://arxiv.org/abs/2511.16858)
*Toufique Ahmed,Jatin Ganhotra,Avraham Shinnar,Martin Hirzel*

Main category: cs.SE

TL;DR: 本文研究了在当前基于大语言模型的自动化程序修复中，测试过拟合问题是否仍然存在，使用SWE-bench仓库级任务进行实验分析。


<details>
  <summary>Details</summary>
Motivation: 以往研究表明，自动化程序修复容易产生仅在已知测试上通过但在隐藏测试上失败的修复代码（即测试过拟合）。随着大语言模型的发展，有必要重新评估该问题是否依然显著。

Method: 作者通过在SWE-bench的仓库级任务上进行实验，评估当前自动化程序修复方法在测试过拟合方面的表现。

Result: 实验结果表明，尽管使用了大语言模型，测试过拟合问题在当前的自动化程序修复中仍然普遍存在。

Conclusion: 测试过拟合仍是自动化程序修复中的关键挑战，即使在大语言模型时代也需进一步研究和改进。

Abstract: Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.

</details>


### [12] [MOOT: a Repository of Many Multi-Objective Optimization Tasks](https://arxiv.org/abs/2511.16882)
*Tim Menzies,Tao Chen,Yulong Ye,Kishan Kumar Ganguly,Amirali Rayegan,Srinath Srinivasan,Andre Lustosa*

Main category: cs.SE

TL;DR: 本文介绍了MOOT，一个包含120多个多目标优化任务的开源仓库，旨在支持软件工程中权衡决策的研究。


<details>
  <summary>Details</summary>
Motivation: 软件工程师在开发过程中常需在相互冲突的目标之间做出权衡（如速度与成本、安全与可用性等），但现有研究和工业实践缺乏有效工具来探索这些权衡，限制了最优决策的实现。

Method: 作者构建并公开了一个名为MOOT的多目标优化任务仓库，整合了来自近期软件工程研究的多样化任务，涵盖软件配置、云调优、项目健康等多个领域，并采用MIT许可证开放获取。

Result: MOOT目前已收录120多项任务，为研究者提供了丰富的数据资源，能够支撑大量关于多目标权衡的新研究问题。

Conclusion: MOOT填补了软件工程多目标优化研究中的工具空白，有望推动该领域更深入和广泛的研究。

Abstract: Software engineers must make decisions that trade off competing goals (faster vs. cheaper, secure vs. usable, accurate vs. interpretable, etc.). Despite MSR's proven techniques for exploring such goals, researchers still struggle with these trade-offs. Similarly, industrial practitioners deliver sub-optimal products since they lack the tools needed to explore these trade-offs.
  To enable more research in this important area, we introduce MOOT, a repository of multi-objective optimization tasks taken from recent SE research papers. MOOT's tasks cover software configuration, cloud tuning, project health, process modeling, hyperparameter optimization, and more. Located at github.com/timm/moot, MOOT's current 120+ tasks are freely available under an MIT license (and we invite community contributions). As shown here, this data enables dozens of novel research questions.

</details>


### [13] [ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting](https://arxiv.org/abs/2511.17027)
*Zhijie Chen,Xiang Chen,Ziming Li,Jiacheng Xue,Chaoyang Gao*

Main category: cs.SE

TL;DR: 本文提出了一种结合检索增强生成（RAG）与思维链（CoT）提示的新框架ReVul-CoT，用于提升大语言模型在软件漏洞评估（SVA）中的表现。该方法通过从本地知识库动态检索权威漏洞信息，并引导模型进行逐步推理，在12,070个漏洞数据集上显著优于现有SVA方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在软件漏洞评估中存在两个主要问题：一是缺乏领域特定知识，二是依赖浅层模式匹配而非深度上下文推理，难以准确理解复杂代码语义及其安全影响。

Method: 提出ReVul-CoT框架，整合检索增强生成（RAG）与思维链（CoT）提示。RAG模块从包含NVD、CWE等权威来源的本地知识库中动态检索相关信息；基于DeepSeek-V3.1模型，利用CoT提示引导模型对可利用性、影响范围等因素进行逐步推理。

Result: 在包含12,070个漏洞的数据集上，ReVul-CoT在MCC指标上比当前最优基线提升16.50%-42.26%，在Accuracy、F1-score和MCC上分别提升10.43%、15.86%和16.50%。消融实验验证了动态检索、知识融合与CoT推理的有效性。

Conclusion: 结合RAG与CoT提示能显著提升大语言模型在软件漏洞评估中的性能，为未来研究提供了有效方向。

Abstract: Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.

</details>


### [14] [SlsReuse: LLM-Powered Serverless Function Reuse](https://arxiv.org/abs/2511.17262)
*Jinfeng Wen,Yuehan Sun*

Main category: cs.SE

TL;DR: 本文提出了SlsReuse，首个基于大语言模型（LLM）的无服务器函数复用框架，通过构建可复用函数库、利用提示工程学习异构函数的语义增强表示，并结合意图感知发现与多级剪枝策略，在任务查询匹配中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算虽降低了运维负担，但对新手开发者而言，从头开发函数需适应多样化的平台特定编程风格，过程耗时且易错。现有函数推荐方法难以弥合任务描述与异构函数实现之间的语义鸿沟，亟需专门针对无服务器场景的推荐方案。

Method: SlsReuse首先构建可复用函数仓库作为知识基础；其次，通过少样本提示工程，利用LLM学习异构函数的统一语义增强表示，涵盖代码意图、目标平台、编程语言和云服务等信息；最后，针对自然语言任务查询，采用意图感知发现、多级剪枝与相似度匹配进行函数推荐。

Result: 在包含110个任务查询的数据集上评估，基于ChatGPT-4o的SlsReuse达到91.20%的Recall@10，比当前最优基线高出24.53个百分点。

Conclusion: SlsReuse有效解决了无服务器函数复用中的语义对齐问题，显著提升了函数推荐的准确率，为开发者提供了高效、精准的函数复用支持。

Abstract: Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.
  This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.

</details>


### [15] [Detecting Performance-Relevant Changes in Configurable Software Systems](https://arxiv.org/abs/2511.17271)
*Sebastian Böhm,Florian Sattler,Norbert Siegmund,Sven Apel*

Main category: cs.SE

TL;DR: ConfFLARE is a method that reduces performance testing costs by identifying performance-relevant features through data-flow analysis, enabling targeted configuration testing and significantly cutting down the number of configurations needed while effectively detecting performance regressions.


<details>
  <summary>Details</summary>
Motivation: Repeated full performance profiling across all configurations after every code change is too costly, especially in configurable software systems; existing sampling methods risk missing regressions that affect only a few configurations.

Method: ConfFLARE analyzes data-flow interactions with performance-relevant code to determine which software features are involved, then selects only the configurations involving those features for performance testing.

Result: In experiments on synthetic and real-world systems, ConfFLARE detected nearly all performance regressions, identified relevant features in all but two cases, and reduced required configurations by 79% (synthetic) and 70% (real-world), saving substantial testing time.

Conclusion: ConfFLARE offers an effective and efficient alternative to full or sampled performance profiling by focusing testing efforts on configurations most likely affected by code changes, thereby maintaining regression detection accuracy while drastically reducing measurement cost.

Abstract: Performance is a volatile property of a software system and frequent performance profiling is required to keep the knowledge about a software system's performance behavior up to date. Repeating all performance measurements after every revision is a cost-intensive task, especially in the presence of configurability, where one has to measure multiple configurations to obtain a comprehensive picture. Configuration sampling is a common approach to control the measurement cost. However, it cannot guarantee completeness and might miss performance regressions, especially if they only affect few configurations. As an alternative to solve the cost reduction problem, we present ConfFLARE: ConfFLARE estimates whether a change potentially impacts performance by identifying data-flow interactions with performance-relevant code and extracts which software features participate in such interactions. Based on these features, we can select a subset of relevant configurations to focus performance profiling efforts on. In a study conducted on both, synthetic and real-world software systems, ConfFLARE correctly detects performance regressions in almost all cases and identifies relevant features in all but two cases, reducing the number of configurations to be tested on average by $79\%$ for synthetic and by $70\%$ for real-world regression scenarios saving hours of performance testing time.

</details>


### [16] [Framework Matters: Energy Efficiency of UI Automation Testing Frameworks](https://arxiv.org/abs/2511.17303)
*Timmie M. R. Lagermann,Kristina Sophia Carter,Su Mei Gwen Ho,Luís Cruz,Kerstin Eder,Maja H. Kirkeby*

Main category: cs.SE

TL;DR: 该研究评估了四个Web UI自动化测试框架在执行常见用户操作时的能耗表现，发现不同框架对同一操作的能耗差异可达六倍，并指出Puppeteer在多数操作中最为节能，而Selenium在刷新和滚动操作中表现最佳，Nightwatch整体能效最差。研究建议为开发者提供能耗透明度以支持节能测试设计。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏关于UI自动化测试框架能耗特性的系统性研究，开发者难以在测试设计中做出节能决策。本文旨在通过量化不同框架执行相同UI操作的能耗差异，为构建能源感知的测试策略提供依据。

Method: 在受控的客户端-服务器环境中，使用外部功率计对四种主流UI自动化测试框架（Puppeteer、Selenium、Nightwatch等）执行七类常见UI操作（如点击、复选框、拖拽、文本输入等）各35次，测量并比较其每次操作的能耗。

Result: 同一UI操作在不同框架下的能耗差异显著，最高可达六倍。Puppeteer在左/右/双击、复选框和文本输入操作中最节能；Selenium在刷新和滚动操作中表现最优；Nightwatch整体能效最低。

Conclusion: UI自动化测试框架的能耗存在显著且可预测的差异，向开发者提供此类能耗透明度有助于其针对特定UI操作选择更节能的测试框架，从而实现能源感知的测试设计。

Abstract: We examine per action energy consumption across four web user interface (UI) automation testing frameworks to determine whether consistent tendencies can guide energy-aware test design. Using a controlled client-server setup with external power metering, we repeat each UI action (refresh, click variants, checkbox, drag&drop, input-text, scroll) 35 times. Across each of the actions, energy costs vary by both framework and action. Puppeteer is the most efficient for left-click, right-click, double-click, checkbox, and input-text; Selenium is the most efficient for refresh and scroll; Nightwatch is generally the least energy efficient. The energy cost of performing the same action varied by up to a factor of six depending on the framework. This indicates that providing transparency of energy consumption for UI automation testing frameworks allows developers to make informed, energy-aware decisions when testing a specific UI action.

</details>


### [17] [Agentic Program Verification](https://arxiv.org/abs/2511.17330)
*Haoxin Tu,Huan Zhao,Yahui Song,Mehtab Zafar,Ruijie Meng,Abhik Roychoudhury*

Main category: cs.SE

TL;DR: 本文提出了首个用于程序验证的大语言模型（LLM）智能体 AutoRocq，它通过与 Rocq 定理证明器的迭代交互实现无需大量训练的自动证明生成，并在 SV-COMP 和 Linux 内核模块上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型自动生成代码日益普及，如何对这些代码进行自动验证成为关键问题。虽然通用数学推理可用于程序推理，但程序具有更强的结构性和上下文丰富性，因此需要专门的 AI 智能体来实现可信的自动化编程。

Method: AutoRocq 采用一个 LLM 智能体，通过与 Rocq（原 Coq）定理证明器的迭代协作，在无需大量训练数据的情况下动态学习并优化证明过程。该方法依赖于智能体与定理证明器之间的反馈循环，以自主构建并通过 Rocq 验证的证明推导。

Result: 在 SV-COMP 基准测试和 Linux 内核模块上的实验表明，AutoRocq 在自动化程序验证方面表现出良好的效果。

Conclusion: AutoRocq 展示了将 LLM 智能体与定理证明器结合用于程序验证的可行性，未来可与 AI 编码智能体集成，形成“生成—验证”闭环，推动可信自动化编程的发展。

Abstract: Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.
  In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.
  Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.

</details>


### [18] [Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software](https://arxiv.org/abs/2511.17368)
*Eric L. Melin,Ahmed Musa Awon,Nasir U. Eisty,Neil A. Ernst,Shurui Zhou*

Main category: cs.SE

TL;DR: 本研究发现科学软件中的自认技术债（SATD）显著高于通用软件，并通过微调Transformer模型实现了更优的SATD识别效果，揭示了SATD对科研软件质量和科学有效性的独特影响。


<details>
  <summary>Details</summary>
Motivation: 科学软件中普遍存在自认技术债（SATD），可能威胁科研结果的准确性与可复现性，但SATD在该领域的特性与影响尚未被系统研究。

Method: 分析27个科学与通用开源仓库中的代码注释，比较两类软件中SATD的分布差异；在67,066条标注数据上微调并评估10种不同规模（1亿至70亿参数）的Transformer模型以识别SATD。

Result: 科学软件中的科学债务和SATD分别高出通用软件9.25倍和4.93倍；所提出的最佳模型在SATD识别任务上优于现有方法。

Conclusion: 科学软件中的SATD具有独特性且影响深远，需针对性管理策略以保障科研软件质量与科学发现的可靠性。

Abstract: Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.

</details>


### [19] [CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval](https://arxiv.org/abs/2511.17417)
*Soroush Javdan,Pragash Krishnamoorthy,Olga Baysal*

Main category: cs.SE

TL;DR: 本文提出CREST方法，通过为故障报告（TR）中的不同观察标准训练专用检索模型，并集成其输出，从而提升检索效果、校准能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 电信行业对高效排错流程的需求日益增长，而现有TR数据复杂且量大，单一检索模型难以有效处理多种故障标准，影响软件维护效率。

Method: 提出CREST框架，采用针对不同TR字段（如故障描述、日志等）的专用检索模型，在初始检索和重排序两阶段流程中分别建模，并融合各模型输出以提升整体性能。

Result: 在爱立信内部TR数据子集上的实验表明，CREST在关键评估指标上显著优于单一模型方法，验证了多标准建模的有效性。

Conclusion: 针对不同TR标准使用专用模型能显著提升检索系统的准确性与可解释性，有助于加快故障定位和软件维护。

Abstract: The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \textbf{CREST} (\textbf{C}riteria-specific \textbf{R}etrieval via \textbf{E}nsemble of \textbf{S}pecialized \textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [NX-CGRA: A Programmable Hardware Accelerator for Core Transformer Algorithms on Edge Devices](https://arxiv.org/abs/2511.17235)
*Rohit Prasad*

Main category: cs.AR

TL;DR: NX-CGRA is a programmable CGRA-based hardware accelerator that efficiently supports diverse transformer inference workloads at the edge by balancing performance, energy efficiency, and flexibility.


<details>
  <summary>Details</summary>
Motivation: Edge deployment of transformers demands hardware that can handle diverse and complex workloads while meeting strict power and area constraints; existing fixed-function accelerators lack the needed flexibility.

Method: The paper proposes NX-CGRA, a coarse-grained reconfigurable array architecture with software-driven programmability to support both linear and non-linear transformer operations.

Result: Evaluation on real-world transformer benchmarks shows NX-CGRA achieves high efficiency and favorable energy-area tradeoffs across various operation types.

Conclusion: NX-CGRA demonstrates strong potential as a scalable and adaptable hardware solution for efficient edge-based transformer inference under resource constraints.

Abstract: The increasing diversity and complexity of transformer workloads at the edge present significant challenges in balancing performance, energy efficiency, and architectural flexibility. This paper introduces NX-CGRA, a programmable hardware accelerator designed to support a range of transformer inference algorithms, including both linear and non-linear functions. Unlike fixed-function accelerators optimized for narrow use cases, NX-CGRA employs a coarse-grained reconfigurable array (CGRA) architecture with software-driven programmability, enabling efficient execution across varied kernel patterns. The architecture is evaluated using representative benchmarks derived from real-world transformer models, demonstrating high overall efficiency and favorable energy-area tradeoffs across different classes of operations. These results indicate the potential of NX-CGRA as a scalable and adaptable hardware solution for edge transformer deployment under constrained power and silicon budgets.

</details>


### [21] [DISCA: A Digital In-memory Stochastic Computing Architecture Using A Compressed Bent-Pyramid Format](https://arxiv.org/abs/2511.17265)
*Shady Agwa,Yikang Shen,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: 本文提出了一种新型数字存内随机计算架构DISCA，通过压缩的准随机Bent-Pyramid数据格式，在保持数字系统可扩展性与可靠性的同时，显著提升矩阵乘法任务的能效。


<details>
  <summary>Details</summary>
Motivation: 传统冯·诺依曼架构受限于“内存墙”和摩尔定律终结，难以满足边缘AI应用（如机器人、无人机）对高效能、低功耗硬件的需求；现有模拟和数字存内计算方案又因设计局限无法充分发挥优势。

Method: 提出DISCA架构，采用压缩版的准随机Bent-Pyramid数据格式，结合数字存内随机计算，在180nm CMOS工艺下进行后布局建模验证。

Result: 在500 MHz频率下，DISCA实现每比特3.59 TOPS/W的能效，相比同类架构在矩阵乘法任务中能效提升数个数量级。

Conclusion: DISCA兼具模拟计算的简洁性和数字系统的可扩展性与可靠性，是面向边缘AI高效能计算的有前景解决方案。

Abstract: Nowadays, we are witnessing an Artificial Intelligence revolution that dominates the technology landscape in various application domains, such as healthcare, robotics, automotive, security, and defense. Massive-scale AI models, which mimic the human brain's functionality, typically feature millions and even billions of parameters through data-intensive matrix multiplication tasks. While conventional Von-Neumann architectures struggle with the memory wall and the end of Moore's Law, these AI applications are migrating rapidly towards the edge, such as in robotics and unmanned aerial vehicles for surveillance, thereby adding more constraints to the hardware budget of AI architectures at the edge. Although in-memory computing has been proposed as a promising solution for the memory wall, both analog and digital in-memory computing architectures suffer from substantial degradation of the proposed benefits due to various design limitations. We propose a new digital in-memory stochastic computing architecture, DISCA, utilizing a compressed version of the quasi-stochastic Bent-Pyramid data format. DISCA inherits the same computational simplicity of analog computing, while preserving the same scalability, productivity, and reliability of digital systems. Post-layout modeling results of DISCA show an energy efficiency of 3.59 TOPS/W per bit at 500 MHz using a commercial 180nm CMOS technology. Therefore, DISCA significantly improves the energy efficiency for matrix multiplication workloads by orders of magnitude if scaled and compared to its counterpart architectures.

</details>
