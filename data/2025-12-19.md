<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 1]
- [cs.SE](#cs.SE) [Total: 11]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.CE](#cs.CE) [Total: 1]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Acoustic RIS for Massive Spatial Multiplexing: Unleashing Degrees of Freedom and Capacity in Underwater Communications](https://arxiv.org/abs/2512.16470)
*Longfei Zhao,Jingbo Tan,Jintao Wang,Ian F Akyildiz,Zhi Sun*

Main category: cs.NI

TL;DR: 本文提出利用声学可重构智能表面（aRIS）增强水下声学MIMO系统的空间自由度和信道容量，通过建立海洋环境下的自由度-信道耦合模型，确定最优部署位置（Light-Point），并设计一种具备主动同时透射与反射（ASTAR）能力的aRIS架构，结合无人水下航行器（UUV）与声强梯度感知实现自适应波束跟踪，在浅海和深海场景中分别提升信道容量达265%和170%。


<details>
  <summary>Details</summary>
Motivation: 传统水下声学MIMO系统受限于阵列分辨率不足，导致角度模糊和空间自由度有限，难以满足高速海洋数据传输需求。现有方法未能有效利用环境特性来增强信道容量。

Method: 提出基于声学可重构智能表面（aRIS）的新架构：首先建立适用于海洋环境的自由度-信道耦合模型，推导空间秩增强条件；其次解析确定最优部署位置“Light-Point”；最后设计具备独立波束控制能力的ASTAR aRIS，并集成UUV与声强梯度传感实现自适应波束跟踪。

Result: 仿真结果表明，所提联合aRIS部署与波束成形框架在浅海和深海环境中分别实现高达265%和170%的水下声学信道容量提升。

Conclusion: 通过引入aRIS并优化其部署与控制策略，可显著提升水下声学通信系统的空间自由度与信道容量，为高吞吐量海洋通信提供新路径。

Abstract: Underwater acoustic (UWA) communications are essential for high-speed marine data transmission but remain severely constrained by limited bandwidth, significant propagation loss, and sparse multipath structures. Conventional underwater acoustic multiple-input multiple-output (MIMO) systems primarily utilize spatial diversity but suffer from limited array resolution, causing angular ambiguity and insufficient spatial degrees of freedom (DoFs). This paper addresses these limitations through acoustic Reconfigurable Intelligent Surfaces (aRIS) to actively generate orthogonally distinguishable virtual paths, significantly enhancing spatial DoFs and channel capacity. An ocean-specific DoF-channel coupling model is established, explicitly deriving conditions for spatial rank enhancement. Subsequently, the optimal geometric locus, termed the Light-Point, is analytically identified, where deploying a single aRIS maximizes DoFs by introducing two and three additional resolvable paths in deep-sea and shallow-sea environments, respectively. Furthermore, an active simultaneous transmitting and reflecting (ASTAR) aRIS architecture with independent beam control and adaptive beam-tracking mechanism integrating unmanned underwater vehicles (UUVs) and acoustic intensity gradient sensing is proposed. Extensive simulations validate the proposed joint aRIS deployment and beamforming framework, demonstrating substantial UWA channel capacity improvements-up to 265% and 170% in shallow-sea and deep-sea scenarios, respectively.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [XBIDetective: Leveraging Vision Language Models for Identifying Cross-Browser Visual Inconsistencies](https://arxiv.org/abs/2512.15804)
*Balreet Grewal,James Graham,Jeff Muizelaar,Jan Honza Odvarko,Suhaib Mujahid,Marco Castelluccio,Cor-Paul Bezemer*

Main category: cs.SE

TL;DR: 本文提出XBIDetective工具，利用视觉语言模型（VLM）自动检测跨浏览器不一致性（XBI），在1,052个网站上评估表明，微调后的VLM可实现79%的XBI识别准确率，并能有效识别动态元素和广告。


<details>
  <summary>Details</summary>
Motivation: 浏览器渲染错误通常由特定条件触发，仅在少数网站上显现，难以被开发者发现。跨浏览器不一致性（XBI）可作为检测此类错误的重要线索，但现有基于视觉或DOM的方法在处理动态和交互式内容时效果有限。

Method: 开发了XBIDetective工具，自动在Firefox和Chrome中截取网页截图，并使用视觉语言模型（VLM）分析这些截图以识别XBI；评估了现成和微调后的VLM在1,052个网站上的表现。

Result: 使用微调VLM时，XBIDetective在XBI识别、动态元素检测和广告识别方面分别达到79%、84%和85%的准确率。

Conclusion: XBIDetective展示了VLM在自动化检测跨浏览器渲染差异方面的潜力，适用于回归测试、大规模网站监控和XBI缺陷报告的快速分类等实际应用场景。

Abstract: Browser rendering bugs can be challenging to detect for browser developers, as they may be triggered by very specific conditions that are exhibited on only a very small subset of websites. Cross-browser inconsistencies (XBIs), variations in how a website is interpreted and displayed on different browsers, can be helpful guides to detect such rendering bugs. Although visual and Document Object Model (DOM)-based analysis techniques exist for detecting XBIs, they often struggle with dynamic and interactive elements. In this study, we discuss our industry experience with using vision language models (VLMs) to identify XBIs. We present the XBIDetective tool which automatically captures screenshots of a website in Mozilla Firefox and Google Chrome, and analyzes them with a VLM for XBIs. We evaluate XBIDetective's performance with an off-the-shelf and a fine-tuned VLM on 1,052 websites. We show that XBIDetective can identify cross-browser discrepancies with 79% accuracy and detect dynamic elements and advertisements with 84% and 85% accuracy, respectively, when using the fine-tuned VLM. We discuss important lessons learned, and we present several potential practical use cases for XBIDetective, including automated regression testing, large-scale monitoring of websites, and rapid triaging of XBI bug reports.

</details>


### [3] [CodeMem: Architecting Reproducible Agents via Dynamic MCP and Procedural Memory](https://arxiv.org/abs/2512.15813)
*Nishant Gaurav,Adit Akarsh,Tejas Ravishankar,Manoj Bajaj*

Main category: cs.SE

TL;DR: CodeMem introduces a procedural memory architecture using code to enable deterministic, reusable agentic workflows, addressing the probabilistic instability of current tool-using AI agents.


<details>
  <summary>Details</summary>
Motivation: Current AI agents struggle with repetitive tasks due to limited action space, context inefficiency, and probabilistic instability; existing solutions improve action space and context but fail to ensure consistent behavior across runs.

Method: The paper proposes CodeMem, an architecture that implements procedural memory through executable code, allowing agents to store and reuse deterministic workflows for reliable task execution.

Result: CodeMem enables the creation and execution of reusable, deterministic agentic workflows, overcoming the inconsistency caused by the probabilistic nature of LLMs.

Conclusion: By integrating procedural memory via code, CodeMem provides a path toward reliable and efficient AI agents capable of handling repetitive tasks with deterministic consistency.

Abstract: Current tool-using AI agents suffer from limited action space, context inefficiency, and probabilistic instability that makes them unsuitable for handling repetitive tasks which are otherwise reliably and efficiently tackled by agentic workflows built on platforms like n8n and Zapier. Earlier works like CodeAct, DynaSaur, Code Mode have tried to tackle the first two issues by using the whole Python language as its action space: The number of tools that the agent can call becomes infinite. Python code blocks can execute complex actions into a single step and print only relevant results which helps in keeping the context lean. However, the probabilistic instability issue still remains, as for the same task in the same environment, the agent can follow different trajectories due to the probabilistic nature of LLMs. Therefore, we need procedural memory for consistency and reliability. This paper proposes CodeMem, an architecture to implement procedural memory via code which can be used to build and run reusable agentic workflows with deterministic reliability.

</details>


### [4] [OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering](https://arxiv.org/abs/2512.15979)
*Mia Mohammad Imran,Tarannum Shaila Zaman*

Main category: cs.SE

TL;DR: 本文提出将大语言模型（LLM）用于软件工程中的标注任务应视为一种测量过程，并提出了一个名为OLAF的概念框架，以提升此类标注的可靠性、可复现性与透明度。


<details>
  <summary>Details</summary>
Motivation: 当前在实证软件工程中使用LLM进行标注的研究缺乏对可靠性、校准和漂移等关键指标的标准化衡量，且常忽略必要的配置细节，导致结果难以复现和验证。

Method: 提出一个名为“基于LLM的标注操作化框架（OLAF）”的概念性框架，整合可靠性、校准、漂移、共识、聚合与透明度等核心要素。

Result: 该框架尚未通过实证验证，但为未来研究提供了结构化的方法论指导，旨在促进更透明和可复现的LLM标注实践。

Conclusion: LLM驱动的标注应被视作一种测量过程，需采用严谨的方法论；OLAF框架有望推动软件工程领域在此方向上的方法论讨论与实证研究。

Abstract: Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \textit{reliability, calibration, drift, consensus, aggregation}, and \textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.

</details>


### [5] [Embedding Software Intent: Lightweight Java Module Recovery](https://arxiv.org/abs/2512.15980)
*Yirui He,Yuqi Huai,Xingyu Chen,Joshua Garcia*

Main category: cs.SE

TL;DR: 本文提出ClassLAR，一种基于类名和语言模型的轻量级架构恢复方法，用于将单体Java项目高效、准确地重构为JPMS模块。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模扩大，仅依赖代码级抽象已不现实；而现有架构恢复技术在将单体Java项目迁移到Java平台模块系统（JPMS）时效果不佳，难以有效恢复模块结构。

Method: ClassLAR利用全限定类名，结合语言模型从包名和类名中提取语义信息，同时捕捉结构与功能意图，从而实现轻量高效的Java模块恢复。

Result: 在20个主流Java项目上的评估表明，ClassLAR在架构相似性指标上优于现有最先进方法，且执行速度提升3.99至10.50倍。

Conclusion: ClassLAR是一种高效且准确的架构恢复方法，能有效支持将单体Java系统重构为符合JPMS规范的模块化架构。

Abstract: As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.

</details>


### [6] [LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)](https://arxiv.org/abs/2512.16070)
*Xin Wang,Zhenhao Li,Zishuo Ding*

Main category: cs.SE

TL;DR: 本文提出并评估了LLM4Perf框架，通过实证研究表明大语言模型（LLMs）可作为高效的多目标性能建模采样器，在多数场景下优于传统方法，并揭示其成功源于配置空间剪枝与反馈驱动策略优化的双重能力。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统的性能高度依赖于复杂的配置选项，而现有采样方法在多目标优化和利用文档语义信息方面存在不足。受大语言模型（LLMs）近期成功的启发，作者探索LLMs是否能有效用于多目标性能建模中的采样任务。

Method: 作者设计并实现了名为LLM4Perf的基于反馈的框架，对LLM引导的采样过程在四个真实高可配系统上进行了系统性评估，分析其在不同组件选择和超参数设置下的表现，并对比传统基线方法。

Result: 实验表明，LLM4Perf在112个评估场景中约68.8%（77个）取得最佳性能；其配置空间剪枝能力还能提升基线方法在448个案例中约91.5%（410个）的表现。

Conclusion: 本研究为LLMs在性能工程中的有效性提供了有力证据，并深入揭示了其成功机制，包括配置空间剪枝与反馈驱动策略优化，为未来研究和实践提供了具体指导。

Abstract: The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.

</details>


### [7] [Analysis of Design Patterns and Benchmark Practices in Apache Kafka Event-Streaming Systems](https://arxiv.org/abs/2512.16146)
*Muzeeb Mohammad*

Main category: cs.SE

TL;DR: 本文系统综述了2015至2025年间42篇关于Apache Kafka的同行评审研究，提炼出九种常见设计模式，并分析其使用趋势、领域部署及基准测试方法，指出当前研究在配置披露与可复现性方面的不足，提出统一分类法和决策启发式以指导实践。


<details>
  <summary>Details</summary>
Motivation: 尽管Apache Kafka被广泛采用，但关于其可复用架构设计模式和可复现基准测试方法的研究分散于学术与工业文献中，缺乏系统整合，限制了跨研究比较与实际复现。

Method: 对2015至2025年间发表的42篇同行评审文献进行结构化综述，识别并分类Kafka设计模式，分析其共现趋势、领域应用及基准测试实践（包括TPCx-Kafka、Yahoo Streaming Benchmark等），并评估其配置透明度与实验严谨性。

Result: 识别出九种高频Kafka设计模式（如日志压缩、CQRS总线、精确一次管道等），发现现有研究在配置披露、评估严谨性和可复现性方面存在显著不一致，阻碍了成果的横向比较与工程落地。

Conclusion: 通过构建统一的模式分类体系、基准矩阵和决策启发规则，本研究为构建高性能、容错且可复现的Kafka事件流系统提供了实用指导，呼吁社区加强实验透明度与标准化。

Abstract: Apache Kafka has become a foundational platform for high throughput event streaming, enabling real time analytics, financial transaction processing, industrial telemetry, and large scale data driven systems. Despite its maturity and widespread adoption, consolidated research on reusable architectural design patterns and reproducible benchmarking methodologies remains fragmented across academic and industrial publications. This paper presents a structured synthesis of forty two peer reviewed studies published between 2015 and 2025, identifying nine recurring Kafka design patterns including log compaction, CQRS bus, exactly once pipelines, change data capture, stream table joins, saga orchestration, tiered storage, multi tenant topics, and event sourcing replay. The analysis examines co usage trends, domain specific deployments, and empirical benchmarking practices using standard suites such as TPCx Kafka and the Yahoo Streaming Benchmark, as well as custom workloads. The study highlights significant inconsistencies in configuration disclosure, evaluation rigor, and reproducibility that limit cross study comparison and practical replication. By providing a unified taxonomy, pattern benchmark matrix, and actionable decision heuristics, this work offers practical guidance for architects and researchers designing reproducible, high performance, and fault tolerant Kafka based event streaming systems.

</details>


### [8] [Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls](https://arxiv.org/abs/2512.16272)
*Ora Nova Fandina,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Wesam Ibraheem,Rami Katan,Alice Podolsky*

Main category: cs.SE

TL;DR: 本文研究了大语言模型作为代码生成评判者（LaaJ）在COBOL现代化任务中的局限性，发现其常忽略领域关键错误；为此提出一种轻量级分析检查器，通过向LaaJ提示中动态注入分析提示（hints），显著提升错误检测率（从45%提升至94%）和解释质量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为代码评判者在可扩展性方面具有优势，但在特定领域（如COBOL代码现代化）中常忽略关键错误，影响其在关键评估任务中的可靠性，因此需要深入理解并弥补其评估盲点。

Method: 作者基于专家知识构建了一个COBOL领域错误的初步分类法，并据此开发了一个轻量级分析检查器，用于检测30多种领域特定问题；随后将检查器的输出作为“分析提示”动态注入到LaaJ的提示中，形成LaaJ+Hints混合方法。

Result: 在包含100个程序的测试集上，单独使用LaaJ仅能检测约45%的错误，而结合分析提示后，最佳配置下错误检测覆盖率提升至94%，且生成的解释更丰富准确。

Conclusion: 将分析工具与大语言模型结合形成的混合方法能显著提升在实际部署管道中对领域特定代码错误的评估可靠性，证明了分析-LLM协同的有效性。

Abstract: Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.
  To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.
  Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.

</details>


### [9] [Using a Sledgehammer to Crack a Nut? Revisiting Automated Compiler Fault Isolation](https://arxiv.org/abs/2512.16335)
*Yibiao Yang,Qingyang Li,Maolin Sun,Jiangchang Wu,Yuming Zhou*

Main category: cs.SE

TL;DR: 该研究将基于Bug引入提交（BIC）的策略（Basic）与主流频谱故障定位（SBFL）技术在编译器故障定位任务中进行对比，发现Basic在Top-1和Top-5指标上表现相当甚至更优。


<details>
  <summary>Details</summary>
Motivation: 现有SBFL技术在编译器故障定位中的有效性尚未与实践中广泛采用的BIC策略进行比较，存在研究与实践脱节的问题。

Method: 提出名为Basic的BIC策略：通过二分查找定位引入故障的提交，并将该提交中修改的所有文件视为可疑故障文件；在包含60个GCC和60个LLVM缺陷的基准上与SBFL方法进行严格对比。

Result: Basic在关键的Top-1和Top-5排名指标上表现与先进SBFL技术相当，甚至更优。

Conclusion: 建议未来编译器故障隔离研究将Basic作为基线方法，以更贴近实际开发场景评估新方法的有效性。

Abstract: Background: Compilers are fundamental to software development, translating high-level source code into executable software systems. Faults in compilers can have severe consequences and thus effective localization and resolution of compiler bugs are crucial. Problem: In practice, developers often examine version history to identify and investigate bug-inducing commit (BIC) for fixing bugs. However, while numerous sophisticated Spectrum-Based Fault Localization (SBFL) techniques have been proposed for compiler fault isolation, their effectiveness has not been evaluated against the BIC-based strategies widely adopted in practice. Objective: This study aims to bridge this gap by directly comparing a BIC-based strategy, Basic, with representative SBFL techniques in the context of compiler fault localization. The BIC-based strategy closely aligns with common developer practices, as it directly identifies the BIC and treats the files modified in that commit as faulty candidates. Method: The Basic identifies the most recent good release and earliest bad release, and then employs a binary search to pinpoint the bug-inducing commit. All files modified in the identified commit are flagged as potentially faulty. We rigorously compare Basic against SBFL-based techniques using a benchmark consisting of 60 GCC bugs and 60 LLVM bugs. Result: Our analysis reveals that Basic performs comparably to, and in many cases outperforms, state-of-the-art SBFL-based techniques, particularly on the critical Top-1 and Top-5 ranking metrics. Conclusion: This study provides new insights into the practical effectiveness of SBFL-based techniques in real-world compiler debugging scenarios. We recommend that future research adopt Basic as a baseline when developing and evaluating new compiler fault isolation methods.

</details>


### [10] [An Empirical Study of the Realism of Mutants in Deep Learning](https://arxiv.org/abs/2512.16741)
*Zaheed Ahmed,Philip Makedonski,Jens Grabowski*

Main category: cs.SE

TL;DR: 本研究首次对深度学习中预训练和后训练突变方法的现实性进行了实证比较，发现预训练突变更接近真实故障，但计算成本高，需改进后训练方法。


<details>
  <summary>Details</summary>
Motivation: 传统软件中的突变分析已被广泛用于评估测试质量，其在深度学习中的应用虽已扩展至故障定位、修复、数据生成和模型鲁棒性评估等多个任务，但“突变体行为与真实故障相似”这一核心假设在深度学习中尚未得到充分验证。

Method: 提出一个统计框架，利用CleanML、DeepFD、DeepLocalize和defect4ML等公开的真实缺陷数据集，量化预训练与后训练突变方法与真实故障之间的耦合强度和行为相似性；使用代表两类方法的前沿工具生成突变体。

Result: 预训练突变体在耦合强度和行为相似性方面均显著优于后训练突变体，表明其具有更高的现实性；但预训练突变的高昂计算成本凸显了开发更高效后训练算子的必要性。

Conclusion: 预训练突变在模拟真实故障方面更具现实性，但因其计算开销大，未来应致力于设计能匹配甚至超越其现实性的高效后训练突变算子。

Abstract: Mutation analysis is a well-established technique for assessing test quality in the traditional software development paradigm by injecting artificial faults into programs. Its application to deep learning (DL) has expanded beyond classical testing to support tasks such as fault localization, repair, data generation, and model robustness evaluation. The core assumption is that mutants behave similarly to real faults, an assumption well established in traditional software systems but largely unverified for DL.
  This study presents the first empirical comparison of pre-training and post-training mutation approaches in DL with respect to realism. We introduce a statistical framework to quantify their coupling strength and behavioral similarity to real faults using publicly available bugs datasets: CleanML, DeepFD, DeepLocalize, and defect4ML. Mutants are generated using state-of-the-art tools representing both approaches.
  Results show that pre-training mutants exhibit consistently stronger coupling and higher behavioral similarity to real faults than post-training mutants, indicating greater realism. However, the substantial computational cost of pre-training mutation underscores the need for more effective post-training operators that match or exceed the realism demonstrated by pre-training mutants.

</details>


### [11] [Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse](https://arxiv.org/abs/2512.16790)
*Aaron Imani,Mohammad Moshirpour,Iftekhar Ahmed*

Main category: cs.SE

TL;DR: 本文首次从概念层面研究大语言模型（LLM）在软件工程任务中对代码注释的内部表征，发现注释被模型视为可区分的潜在概念，并且激活或抑制这些概念会显著影响不同任务的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管注释是非功能性代码元素，但大语言模型常依赖它们执行软件工程任务；然而，这种依赖在模型内部如何体现及其对性能的影响尚不清楚。

Method: 使用概念激活向量（CAV）分析LLM在代码补全、翻译和优化三个任务中对不同类型注释（如Javadoc、行内、多行）的内部表征，并通过在嵌入空间中系统性激活/去激活这些概念来评估其对性能的影响；此外，还对10个不同SE任务中注释概念的激活程度进行了对照实验。

Result: LLM能将注释内化为可区分的潜在概念；激活/去激活这些概念会导致任务性能发生显著变化（-90%至+67%）；在多个任务中，代码摘要最强烈地激活注释概念，而代码补全对其最不敏感。

Conclusion: 该研究揭示了LLM在软件工程任务中对注释的内部机制，为未来构建基于内部概念操作而非仅依赖表面输入的SE工具和模型提供了新方向。

Abstract: While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.

</details>


### [12] [Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework](https://arxiv.org/abs/2512.16816)
*Alessandra Parziale,Gianmario Voria,Valeria Pontillo,Gemma Catolino,Andrea De Lucia,Fabio Palomba*

Main category: cs.SE

TL;DR: 本文提出CAFFE框架，用于更有效地测试大语言模型中的反事实公平性，相比现有方法能更全面、可靠地检测不公平行为。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在软件系统中的广泛应用，其公平性问题日益突出；现有基于变形测试的方法在检测公平性缺陷方面存在局限，因此需要一种更结构化、意图感知的测试方法。

Method: 提出CAFFE（Counterfactual Assessment Framework for Fairness Evaluation）框架，通过明确定义提示意图、对话上下文、输入变体、公平性阈值和测试环境等组件构建公平性测试用例，自动生​​成针对性测试数据，并利用语义相似度指标评估模型响应。

Result: 在三种不同架构的大语言模型上的实验表明，CAFFE比现有变形测试方法具有更广的偏见覆盖范围和更高的不公平行为检测可靠性。

Conclusion: CAFFE提供了一种系统化、可操作的反事实公平性测试方法，显著优于传统变形测试，在提升大语言模型公平性评估方面具有实用价值。

Abstract: Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [13] [Trustworthy and Controllable Professional Knowledge Utilization in Large Language Models with TEE-GPU Execution](https://arxiv.org/abs/2512.16238)
*Yifeng Cai,Zhida An,Yuhan Meng,Houqian Liu,Pengli Wang,Yao Guo,Ding Li*

Main category: cs.OS

TL;DR: 本文提出了PKUS系统，将专业知识作为独立、可分离的组件，在可信执行环境（TEE）中以紧凑适配器形式运行，从而在保障数据提供方权益的同时实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型服务依赖高质量的专业知识，但知识提供方因收益低、风险高（如版权和隐私责任）而不愿贡献数据；现有方法无法在保护提供方权益的同时实现可控、可信的知识使用。

Method: 提出PKUS系统，将专业知​​识编码为紧凑适配器，在硬件级可信执行环境（TEE）中执行；结合硬件根生命周期协议、适配器剪枝、多提供方聚合与分拆执行调度等技术。

Result: 在SST-2、MNLI和SQuAD数据集上，使用GPT-2 Large和Llama-3.2-1B模型，PKUS在保持与全量微调和普通LoRA相当的准确率和F1分数的同时，实现了最低单请求延迟，比纯CPU TEE推理和简单CPU-GPU协同执行快8.1–11.9倍。

Conclusion: PKUS通过将专业知识模块化并在TEE中安全执行，有效平衡了知识提供方的权益保障与模型服务的高效性，为专业数据参与LLM生态提供了可行路径。

Abstract: Future improvements in large language model (LLM) services increasingly hinge on access to high-value professional knowledge rather than more generic web data. However, the data providers of this knowledge face a skewed tradeoff between income and risk: they receive little share of downstream value yet retain copyright and privacy liability, making them reluctant to contribute their assets to LLM services. Existing techniques do not offer a trustworthy and controllable way to use professional knowledge, because they keep providers in the dark and combine knowledge parameters with the underlying LLM backbone.
  In this paper, we present PKUS, the Professional Knowledge Utilization System, which treats professional knowledge as a first-class, separable artifact. PKUS keeps the backbone model on GPUs and encodes each provider's contribution as a compact adapter that executes only inside an attested Trusted Execution Environment (TEE). A hardware-rooted lifecycle protocol, adapter pruning, multi-provider aggregation, and split-execution scheduling together make this design practical at serving time. On SST-2, MNLI, and SQuAD with GPT-2 Large and Llama-3.2-1B, PKUS preserves model utility, matching the accuracy and F1 of full fine-tuning and plain LoRA, while achieving the lowest per-request latency with 8.1-11.9x speedup over CPU-only TEE inference and naive CPU-GPU co-execution.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [14] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: LOG.io 是一种面向无服务器可扩展架构的分布式数据管道系统，支持精确回滚恢复和细粒度数据血缘追踪，在存在慢节点且事件吞吐适中时，其恢复性能优于 ABS 协议；同时，其数据血缘捕获开销低于 1.5%。


<details>
  <summary>Details</summary>
Motivation: 现有分布式数据流系统在容错和数据血缘追踪方面存在局限，尤其在无服务器、高度并行和动态扩展场景下难以兼顾正确性、效率与通用性。因此，作者提出 LOG.io 以解决这些问题。

Method: LOG.io 采用基于日志的回滚恢复协议，支持通用编程模型（包括非确定性算子、外部系统交互和任意自定义代码），实现非阻塞式故障恢复，并允许运行时动态扩缩容。同时，它以事件级别粒度捕获任意两个算子间的数据血缘。

Result: 实验表明：在存在慢节点且事件吞吐适中（如每100ms一个事件）时，LOG.io 在正常处理阶段与 ABS 性能相当，在恢复阶段优于 ABS；在其他情况下 ABS 更优，但此时 LOG.io 可通过数据并行显著降低开销，而 ABS 无法进一步优化。此外，LOG.io 的数据血缘捕获开销始终低于 1.5%。

Conclusion: LOG.io 在保证通用性和细粒度数据血缘的同时，提供了高效的容错机制，特别适用于存在慢节点或需要动态扩展的无服务器数据管道场景，具有实用价值和部署潜力。

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [15] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: 本文提出Multipath Memory Access（MMA），首次实现GPU与主机内存之间的高效多路径数据传输，显著提升带宽并降低大语言模型服务中的首token延迟和模型切换延迟。


<details>
  <summary>Details</summary>
Motivation: PCIe带宽限制已成为大语言模型性能的关键瓶颈，尤其是在前缀缓存获取和模型切换等场景中。尽管服务器内部GPU与主机内存之间理论上可支持多路径传输，但现有异构协议（如PCIe和NVLink）将带宽限制为单条PCIe链路，导致服务器内部带宽未被充分利用。

Method: 提出Multipath Memory Access（MMA）方案，通过动态库注入实现无缝部署，无需修改应用代码即可启用多路径数据传输，从而突破单PCIe链路的带宽限制。

Result: 在测试平台上，MMA将GPU与主机内存间的数据传输峰值带宽提升至245 GB/s，相较原生单路径提升4.62倍；端到端评估显示，LLM服务的首token时间（TTFT）降低1.14x–2.38x，vLLM睡眠模式下的模型切换延迟减少1.12x–2.48x。

Conclusion: MMA有效解决了PCIe带宽瓶颈问题，显著提升了大语言模型在实际部署中的性能，且具备良好的兼容性和易用性。

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [16] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: 该论文通过分析81个开源无服务器系统的问题报告，提出了冷启动问题的分类法，并开发了SCABENCH基准和INITSCOPE分析框架，显著提升了冷启动诊断的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算中的冷启动延迟是主要性能瓶颈，现有研究多将其视为黑盒优化问题，缺乏对开发者可见的设计层面的理解与支持。

Method: 作者从81个经过裁定的开源无服务器系统问题报告中归纳出初始化反模式、修复策略和诊断挑战的分类体系，并在此基础上构建了可复现的基准SCABENCH和轻量级分析框架INITSCOPE，用于关联加载代码与执行代码。

Result: 在SCABENCH上，INITSCOPE相比现有工具将定位准确率最高提升40%，诊断工作量减少64%；开发者研究表明其能提高任务准确率并加快诊断速度。

Conclusion: 该研究推动了基于实证、关注性能的无服务器冷启动缓解设计实践，并公开了研究工具以促进后续工作。

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [17] [An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs](https://arxiv.org/abs/2512.16099)
*Hsu-Tzu Ting,Jerry Chou,Ming-Hung Chen,I-Hsin Chung*

Main category: cs.DC

TL;DR: 该论文提出了一种面向NVIDIA MIG（多实例GPU）的在线调度框架，通过条件负载均衡、动态分区和作业迁移，有效缓解了MIG环境下的资源争用与碎片化问题，显著提升了系统效率，最多可将任务完成时间（makespan）缩短35%。


<details>
  <summary>Details</summary>
Motivation: MIG虽提供硬件级强隔离的GPU共享能力，但仍面临PCIe带宽等共享资源引起的争用问题，以及由MIG配置限制和刚性放置策略导致的独特碎片化问题，尤其在作业动态到达与退出时更为严重。

Method: 提出一个在线调度框架，整合条件负载均衡、动态分区调整和作业迁移机制，以动态优化作业放置、减少资源争用，并重组GPU分配以缓解内部与外部碎片。

Result: 实验表明，所提方法能显著提升系统效率，在综合应用所有技术时，任务完成时间最多可减少35%。

Conclusion: 该工作有效解决了MIG环境下资源争用与碎片化难题，为高效利用MIG提供了实用的调度方案，显著提升了整体系统性能。

Abstract: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.

</details>


### [18] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: 针对大规模DP+EP架构下LLM服务中因即时调度导致的内部排队和并行气泡问题，提出交错批处理调度（SBS）与负载感知全局分配策略，在H800集群上部署Deepseek-V3时显著降低TTFT 30%-40%并提升吞吐量15%-20%。


<details>
  <summary>Details</summary>
Motivation: 在P/D分离、大规模DP+EP架构的LLM服务中，传统即时请求分发会引发严重的引擎内排队和并行化气泡，从而显著恶化首Token生成时间（TTFT）。

Method: 提出交错批处理调度（SBS），通过有意缓冲请求以形成最优执行批次，消除内部排队气泡；同时利用缓冲带来的调度窗口，设计负载感知的全局分配策略，在Prefill和Decode阶段均衡DP单元间的计算负载。

Result: 在生产级H800集群上部署Deepseek-V3，相比当前最先进的即时调度方法，TTFT降低30%-40%，吞吐量提升15%-20%。

Conclusion: 交错批处理调度与负载感知分配能有效解决DP+EP架构下的调度瓶颈，在不牺牲吞吐的前提下显著优化延迟指标。

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [19] [Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks](https://arxiv.org/abs/2512.16136)
*Zhisheng Hu,Pengfei Zuo,Junliang Hu,Yizou Chen,Yingjia Wang,Ming-Chang Yang*

Main category: cs.DC

TL;DR: Lotus 是一种面向解耦内存（DM）架构的可扩展分布式事务系统，通过将锁与数据分离并将锁操作卸载到计算节点（CNs），解决了现有系统中内存节点 RDMA 网卡成为性能瓶颈的问题，从而显著提升吞吐量并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于 DM 的分布式事务系统在内存节点（MNs）上使用大量单边 RDMA 原子操作实现锁机制，导致 MN 的 RDMA 网卡成为性能瓶颈，限制了系统可扩展性。

Method: Lotus 将锁与数据解耦，所有锁操作在计算节点（CNs）上执行；采用应用感知的锁管理机制，利用 OLTP 工作负载的局部性对锁进行分片并保持负载均衡；引入“先锁后事务”协议，在事务执行初期完成加锁并提前中止冲突事务；并通过无锁重建的恢复机制处理 CN 故障，将锁视为临时状态以实现轻量级恢复。

Result: 实验表明，Lotus 相比当前最先进的 DM 事务系统，事务吞吐量最高提升 2.1 倍，延迟最多降低 49.4%。

Conclusion: 通过锁解耦、应用感知锁管理和轻量级故障恢复机制，Lotus 有效消除了 DM 架构下内存节点的 RDMA 瓶颈，显著提升了分布式事务系统的可扩展性和性能。

Abstract: Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.
  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.

</details>


### [20] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: 本文介绍了一个面向科研人工智能工作负载的联邦计算平台，支持机器学习全生命周期，并强调可复现性、透明访问和易用性。


<details>
  <summary>Details</summary>
Motivation: 为科研领域的人工智能应用提供一个可复现、透明且易于使用的联邦计算平台，以整合分散的电子基础设施资源并降低外部社区的采用门槛。

Method: 构建一个联邦计算平台，通过统一的服务目录提供涵盖模型开发、训练和部署的完整机器学习生命周期支持，并集成GPU资源、标注工具、实验追踪、联邦学习、模型与数据集管理等功能。

Result: 该平台实现了对分布式e-基础设施的一致透明访问，支持多种部署模式，具备良好的可定制性和生态系统集成能力。

Conclusion: 所提出的平台有效支持科研AI工作流，提升了可追溯性、可复现性及跨社区协作能力。

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [21] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: 本文提出了一种新颖的CPU-GPU协同推理框架，通过在GPU上引入专家缓存机制，减少MoE模型推理时的数据传输开销，并利用CPU多线程优化处理缓存未命中情况，从而提升消费级硬件上的单请求推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）计算需求高，难以在消费级硬件上部署；尽管混合专家（MoE）模型通过选择性激活参数子集提高了效率，但其内存需求仍超出典型消费级GPU容量。传统CPU-GPU权重卸载方法因频繁数据传输带来高延迟，限制了推理性能。

Method: 提出一种CPU-GPU协同推理框架：在GPU上缓存常用专家以减少数据传输，利用缓存命中加速推理；对于缓存未命中的专家计算，则卸载至CPU执行，并借助CPU多线程优化提高效率。

Result: 实验评估表明，该框架在消费级系统上显著提升了单请求推理性能，有效减少了数据传输并充分利用了CPU与GPU的协同计算能力。

Conclusion: CPU-GPU协同推理结合专家缓存机制是一种高效可行的方案，能够显著提升MoE模型在资源受限设备上的推理效率，为消费级硬件部署大模型提供了新思路。

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [22] [Workload Characterization for Branch Predictability](https://arxiv.org/abs/2512.15827)
*FNU Vikas,Paul Gratz,Daniel Jiménez*

Main category: cs.AR

TL;DR: 本文提出了一种面向分支预测的工作负载表征方法，定义了两个新指标——分支工作集大小和分支可预测性，用于分析现代分支预测器（如TAGE和感知机）的准确率与工作负载特性之间的关系。


<details>
  <summary>Details</summary>
Motivation: 分支预测是提升指令级并行性（ILP）的关键技术，其准确性直接影响性能与能耗。现有研究缺乏对工作负载本身如何影响现代分支预测器准确性的系统性刻画。

Method: 作者提出一种基于工作负载的分支预测表征方法，定义“分支工作集”为最常出现的分支上下文（包含分支地址、全局历史和局部历史的三元组），并据此计算每个trace的分支工作集大小和可预测性。通过对2,451个工作负载trace进行分类（7类工作集大小、9类可预测性），分析其与现代分支预测器误预测率的相关性。

Result: 分支工作集大小和可预测性与TAGE和感知机等现代分支预测器的误预测率高度相关；研究将大量trace归类，并揭示了哪些工作负载类别更受现代预测器青睐，以及预测准确性的来源。

Conclusion: 所提出的两个指标能有效刻画工作负载对分支预测准确率的影响，为未来分支预测器设计和工作负载优化提供了新的分析维度和洞察。

Abstract: Conditional branch prediction predicts the likely direction of a conditional branch instruction to support ILP extraction. Branch prediction is a pattern recognition problem that learns mappings between a context to the branch outcome. An accurate predictor reduces the number of instructions executed on the wrong path resulting in an improvement of performance and energy consumption. In this paper, we present a workload characterization methodology for branch prediction. We propose two new workload-driven branch prediction accuracy identifiers -- branch working set size and branch predictability. These parameters are highly correlated with misprediction rates of modern branch prediction schemes (e.g. TAGE and perceptron). We define the branch working set of a trace as a group of most frequently occurring branch contexts, i.e. the 3-part tuple of branch address, and associated global and local history. We analyze the branch working set's size and predictability on a per-trace basis to study its relationship with a modern branch predictor's accuracy. We have characterized 2,451 workload traces into seven branch working set size and nine predictability categories after analyzing their branch behavior. We present further insights into the source of prediction accuracy and favored workload categories for modern branch predictors.

</details>


### [23] [Full System Architecture Modeling for Wearable Egocentric Contextual AI](https://arxiv.org/abs/2512.16045)
*Vincent T. Lee,Tanfer Alan,Sung Kim,Ecenur Ustun,Amr Suleiman,Ajit Krisshna,Tim Balbekov,Armin Alaghi,Richard Newcombe*

Main category: cs.AR

TL;DR: 本文提出了首个面向情境感知AI的可穿戴系统（Aria2）的完整架构，并通过系统建模与设计空间探索，揭示了在功耗受限条件下需以全系统视角进行协同优化的关键设计原则。


<details>
  <summary>Details</summary>
Motivation: 下一代以人为中心的计算需要始终在线、具备空间感知能力的可穿戴设备，以捕捉自我中心视觉和功能原语，构建用户个人情境，并结合生成式AI实现强大的情境AI助手。然而，由于系统复杂性和严格的功耗限制，设计此类可穿戴系统极具挑战。

Method: 作者对一个完整的可穿戴情境AI系统（Aria2）进行了系统架构建模和设计空间探索，分析各组件对整体功耗的影响，并从全系统视角评估长期设计决策与功耗优化策略。

Result: 研究发现，没有任何单一组件或类别在系统功耗中占据绝对主导地位，因此必须在全系统上下文中进行设计和优化，以避免因其他瓶颈（即“功耗版Amdahl定律”）而受限。

Conclusion: 实现全天候可穿戴情境AI系统需要跨组件协同优化，未来的设计应基于完整的系统模型，综合考虑不断变化的瓶颈因素，以有效应对功耗与性能的平衡挑战。

Abstract: The next generation of human-oriented computing will require always-on, spatially-aware wearable devices to capture egocentric vision and functional primitives (e.g., Where am I? What am I looking at?, etc.). These devices will sense an egocentric view of the world around us to observe all human- relevant signals across space and time to construct and maintain a user's personal context. This personal context, combined with advanced generative AI, will unlock a powerful new generation of contextual AI personal assistants and applications. However, designing a wearable system to support contextual AI is a daunting task because of the system's complexity and stringent power constraints due to weight and battery restrictions. To understand how to guide design for such systems, this work provides the first complete system architecture view of one such wearable contextual AI system (Aria2), along with the lessons we have learned through the system modeling and design space exploration process. We show that an end-to-end full system model view of such systems is vitally important, as no single component or category overwhelmingly dominates system power. This means long-range design decisions and power optimizations need to be made in the full system context to avoid running into limits caused by other system bottlenecks (i.e., Amdahl's law as applied to power) or as bottlenecks change. Finally, we reflect on lessons and insights for the road ahead, which will be important toward eventually enabling all-day, wearable, contextual AI systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [24] [DP-Bench: A Benchmark for Evaluating Data Product Creation Systems](https://arxiv.org/abs/2512.15798)
*Faisal Chowdhury,Sola Shirai,Sarthak Dash,Nandana Mihindukulasooriya,Horst Samulowitz*

Main category: cs.DB

TL;DR: 本文提出了首个用于评估自动数据产品生成的基准DP-Bench，并提供了基于大语言模型（LLM）的基线方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏用于评估自动创建数据产品的基准，尽管工业界已有大量手动或半自动构建数据产品的实践。

Method: 利用现有的ELT（Extract-Load-Transform）和Text-to-SQL基准构建DP-Bench，并提出多种基于大语言模型的方法作为自动数据产品生成的基线。

Result: 成功构建了DP-Bench基准，并在Hugging Face上公开发布数据集及补充材料，为后续研究提供基础。

Conclusion: DP-Bench填补了自动数据产品生成评估领域的空白，为该方向的研究和开发提供了重要资源。

Abstract: A data product is created with the intention of solving a specific problem, addressing a specific business usecase or meeting a particular need, going beyond just serving data as a raw asset. Data products enable end users to gain greater insights about their data. Since it was first introduced over a decade ago, there has been considerable work, especially in industry, to create data products manually or semi-automatically. However, there exists hardly any benchmark to evaluate automatic data product creation. In this work, we present a benchmark, first of its kind, for this task. We call it DP-Bench. We describe how this benchmark was created by taking advantage of existing work in ELT (Extract-Load-Transform) and Text-to-SQL benchmarks. We also propose a number of LLM based approaches that can be considered as baselines for generating data products automatically. We make the DP-Bench and supplementary materials available in https://huggingface.co/datasets/ibm-research/dp-bench .

</details>


### [25] [Implementing a Scalable, Redeployable and Multitiered Repository for FAIR and Secure Scientific Data Sharing: The BIG-MAP Archive](https://arxiv.org/abs/2512.15815)
*Valeria Granata,Francois Liot,Xing Wang,Steen Lysgaard,Ivano E. Castelli,Tejs Vegge,Nicola Marzari,Giovanni Pizzi*

Main category: cs.DB

TL;DR: BIG-MAP Archive is a cloud-based, private repository built on InvenioRDM to enable secure, scalable, and permission-controlled data sharing within large research consortia like BATTERY 2030+.


<details>
  <summary>Details</summary>
Motivation: Large research consortia face organizational and technical challenges in data sharing, including the need for secure access, scalability, user-friendly interfaces, and fine-grained access control—motivating the development of a tailored repository solution.

Method: The authors designed and implemented the BIG-MAP Archive, a cloud-based disciplinary repository based on InvenioRDM, featuring customizable access controls, community-based permissions, and standardized data upload workflows.

Result: The BIG-MAP Archive successfully supports secure and flexible data sharing within consortia such as BATTERY 2030+, with fine-grained permissions, structured metadata, and readiness for eventual open publication; it is also redeployable for other initiatives like MaterialsCommons4.eu and RAISE.

Conclusion: The BIG-MAP Archive provides a robust, adaptable platform that addresses key data-sharing challenges in large consortia by combining security, usability, and scalability, while ensuring compliance with future open-data requirements.

Abstract: Data sharing in large consortia, such as research collaborations or industry partnerships, requires addressing both organizational and technical challenges. A common platform is essential to promote collaboration, facilitate exchange of findings, and ensure secure access to sensitive data. Key technical challenges include creating a scalable architecture, a user-friendly interface, and robust security and access control. The BIG-MAP Archive is a cloud-based, disciplinary, private repository designed to address these challenges. Built on InvenioRDM, it leverages platform functionalities to meet consortium-specific needs, providing a tailored solution compared to general repositories. Access can be restricted to members of specific communities or open to the entire consortium, such as the BATTERY 2030+, a consortium accelerating advanced battery technologies. Uploaded data and metadata are controlled via fine grained permissions, allowing access to individual project members or the full initiative. The formalized upload process ensures data are formatted and ready for publication in open repositories when needed. This paper reviews the repository's key features, showing how the BIG-MAP Archive enables secure, controlled data sharing within large consortia. It ensures data confidentiality while supporting flexible, permissions-based access and can be easily redeployed for other consortia, including MaterialsCommons4.eu and RAISE (Resource for AI Science in Europe).

</details>


### [26] [Scaling Text2SQL via LLM-efficient Schema Filtering with Functional Dependency Graph Rerankers](https://arxiv.org/abs/2512.16083)
*Thanh Dat Hoang,Thanh Tam Nguyen,Thanh Trung Huynh,Hongzhi Yin,Quoc Viet Hung Nguyen*

Main category: cs.DB

TL;DR: 本文提出了一种名为\toolname的高效开源框架，用于在大规模数据库中对Text2SQL任务进行模式过滤，通过结合查询感知的列排序、基于图结构的列重排序以及保留连通性的子模式选择策略，在保持低延迟的同时显著提升了精度和召回率。


<details>
  <summary>Details</summary>
Motivation: 现有Text2SQL方法在处理真实世界的大规模数据库（如Spider 2.0）时，因超出大语言模型上下文限制而失效；当前缓解策略要么成本高，要么忽略列间结构关系。

Method: \toolname框架包含三个步骤：(i) 使用融合值与元数据的查询感知LLM编码器对列进行排序；(ii) 利用轻量图变换器基于函数依赖对互连列进行重排序；(iii) 采用斯坦纳树启发式算法选择保持连通性的子模式。

Result: 在真实数据集上的实验表明，\toolname相比CodeS、SchemaExP、Qwen重排器和嵌入检索方法，实现了接近完美的召回率和更高的精度，同时中位延迟低于1秒，并可扩展至包含23,000+列的模式。

Conclusion: \toolname是一种高效、可扩展且开源的Text2SQL模式过滤框架，能有效解决大规模数据库中的上下文限制问题，在性能和效率上均优于现有方法。

Abstract: Most modern Text2SQL systems prompt large language models (LLMs) with entire schemas -- mostly column information -- alongside the user's question. While effective on small databases, this approach fails on real-world schemas that exceed LLM context limits, even for commercial models. The recent Spider 2.0 benchmark exemplifies this with hundreds of tables and tens of thousands of columns, where existing systems often break. Current mitigations either rely on costly multi-step prompting pipelines or filter columns by ranking them against user's question independently, ignoring inter-column structure. To scale existing systems, we introduce \toolname, an open-source, LLM-efficient schema filtering framework that compacts Text2SQL prompts by (i) ranking columns with a query-aware LLM encoder enriched with values and metadata, (ii) reranking inter-connected columns via a lightweight graph transformer over functional dependencies, and (iii) selecting a connectivity-preserving sub-schema with a Steiner-tree heuristic. Experiments on real datasets show that \toolname achieves near-perfect recall and higher precision than CodeS, SchemaExP, Qwen rerankers, and embedding retrievers, while maintaining sub-second median latency and scaling to schemas with 23,000+ columns. Our source code is available at https://github.com/thanhdath/grast-sql.

</details>


### [27] [ModelTables: A Corpus of Tables about Models](https://arxiv.org/abs/2512.16106)
*Zhengyuan Dong,Victor Zhong,Renée J. Miller*

Main category: cs.DB

TL;DR: ModelTables 是一个包含90K张表格的大规模基准数据集，源自Hugging Face模型卡、GitHub README和相关论文，旨在支持对AI模型性能与配置表格的语义检索研究。该基准通过三种互补信号构建多源真值，并在表格搜索任务中评估多种检索方法，发现现有方法仍有较大提升空间。


<details>
  <summary>Details</summary>
Motivation: 当前文本检索方法忽视了模型文档中结构化表格所蕴含的语义信息；同时缺乏针对AI模型相关表格的大规模基准用于开发和评估语义检索、结构化比较等技术。

Method: 从Hugging Face模型卡、GitHub README和引用论文中提取表格并关联其上下文，构建包含60K模型和90K表格的数据集；利用论文引用、模型卡链接/继承关系、共享训练数据集三种信号构建多源真值；在表格搜索任务中评估传统数据湖操作符（可联合、可连接、关键词）和信息检索基线（稠密、稀疏、混合检索）。

Result: 基于联合的语义表格检索整体P@1为54.8%；基于表格的稠密检索达到66.5% P@1；元数据混合检索为54.1%。结果表明现有方法在捕捉表格间语义关联方面仍有明显不足。

Conclusion: ModelTables 是首个面向AI模型结构化知识的大规模表格基准，为语义检索、结构化比较和知识组织提供了基础资源与实证依据，推动更精准的模型表格发现与利用。

Abstract: We present ModelTables, a benchmark of tables in Model Lakes that captures the structured semantics of performance and configuration tables often overlooked by text only retrieval. The corpus is built from Hugging Face model cards, GitHub READMEs, and referenced papers, linking each table to its surrounding model and publication context. Compared with open data lake tables, model tables are smaller yet exhibit denser inter table relationships, reflecting tightly coupled model and benchmark evolution. The current release covers over 60K models and 90K tables. To evaluate model and table relatedness, we construct a multi source ground truth using three complementary signals: (1) paper citation links, (2) explicit model card links and inheritance, and (3) shared training datasets. We present one extensive empirical use case for the benchmark which is table search. We compare canonical Data Lake search operators (unionable, joinable, keyword) and Information Retrieval baselines (dense, sparse, hybrid retrieval) on this benchmark. Union based semantic table retrieval attains 54.8 % P@1 overall (54.6 % on citation, 31.3 % on inheritance, 30.6 % on shared dataset signals); table based dense retrieval reaches 66.5 % P@1, and metadata hybrid retrieval achieves 54.1 %. This evaluation indicates clear room for developing better table search methods. By releasing ModelTables and its creation protocol, we provide the first large scale benchmark of structured data describing AI model. Our use case of table discovery in Model Lakes, provides intuition and evidence for developing more accurate semantic retrieval, structured comparison, and principled organization of structured model knowledge. Source code, data, and other artifacts have been made available at https://github.com/RJMillerLab/ModelTables.

</details>


### [28] [Multi-granularity Spatiotemporal Flow Patterns](https://arxiv.org/abs/2512.16255)
*Chrysanthi Kosyfaki,Nikos Mamoulis,Reynold Cheng,Ben Kai*

Main category: cs.DB

TL;DR: 本文研究如何在不同时空粒度下发现乘客移动中的重要趋势，提出ODT（Origin-Destination-Time）模式及其高效挖掘算法，并通过优化、变体模式和近似方法提升效率，在真实数据集上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在交通等领域，通过聚合乘客出行数据（如区域间流动人数）可揭示移动行为趋势；然而，如何在多粒度时空下高效识别有意义的流动模式仍具挑战。

Method: 提出ODT模式定义，设计自底向上的枚举算法，并引入多种优化策略以减少搜索空间和计算开销；同时提出约束模式、Top-k模式等变体，并开发基于生成-测试的近似方法以快速识别特定规模的ODT模式。

Result: 在三个真实数据集上的实验表明，所提方法在效率和有效性方面表现良好，并成功挖掘出具有实际意义的ODT流动模式。

Conclusion: 该工作为多粒度时空流动数据分析提供了系统的方法框架，能有效支持交通等领域中的趋势发现与决策支持。

Abstract: Analyzing flow of objects or data at different granularities of space and time can unveil interesting insights or trends. For example, transportation companies, by aggregating passenger travel data (e.g., counting passengers traveling from one region to another), can analyze movement behavior. In this paper, we study the problem of finding important trends in passenger movements between regions at different granularities. We define Origin (O), Destination (D), and Time (T ) patterns (ODT patterns) and propose a bottom-up algorithm that enumerates them. We suggest and employ optimizations that greatly reduce the search space and the computational cost of pattern enumeration. We also propose pattern variants (constrained patterns and top-k patterns) that could be useful to differ- ent applications scenarios. Finally, we propose an approximate solution that fast identifies ODT patterns of specific sizes, following a generate-and-test approach. We evaluate the efficiency and effectiveness of our methods on three real datasets and showcase interesting ODT flow patterns in them.

</details>


### [29] [Subset Sampling over Joins](https://arxiv.org/abs/2512.16321)
*Aryan Esmailpour,Xiao Hu,Jinchao Huang,Stavros Sintos*

Main category: cs.DB

TL;DR: 本文研究了在关系连接结果上进行子集采样（即每个连接结果以一定概率独立被选中）的问题，提出了针对无环连接的高效静态索引、一次性采样算法和动态索引方法，避免了显式生成所有连接结果，在时间和空间复杂度上接近最优。


<details>
  <summary>Details</summary>
Motivation: 现代应用（如关系数据上的机器学习）常需对由关系连接隐式定义的集合进行子集采样，而传统方法因连接结果可能指数级大于输入数据而不适用，因此需要更高效的采样技术。

Method: 提出三种新算法：(1) 用于多次独立采样的静态索引；(2) 用于单次采样的一次性算法；(3) 支持元组插入并维持采样能力的动态索引。这些方法适用于由可分解函数（如乘积、求和等）从输入元组权重导出采样概率的通用场景。

Result: 所提算法在无环连接上实现了接近最优的时间与空间复杂度，显著优于需物化全部连接结果的朴素方法。

Conclusion: 该工作首次为连接上的子集采样问题提供了高效解决方案，适用于大规模关系数据分析中的近似计算需求。

Abstract: Subset sampling (also known as Poisson sampling), where the decision to include any specific element in the sample is made independently of all others, is a fundamental primitive in data analytics, enabling efficient approximation by processing representative subsets rather than massive datasets. While sampling from explicit lists is well-understood, modern applications -- such as machine learning over relational data -- often require sampling from a set defined implicitly by a relational join. In this paper, we study the problem of \emph{subset sampling over joins}: drawing a random subset from the join results, where each join result is included independently with some probability. We address the general setting where the probability is derived from input tuple weights via decomposable functions (e.g., product, sum, min, max). Since the join size can be exponentially larger than the input, the naive approach of materializing all join results to perform subset sampling is computationally infeasible. We propose the first efficient algorithms for subset sampling over acyclic joins: (1) a \emph{static index} for generating multiple (independent) subset samples over joins; (2) a \emph{one-shot} algorithm for generating a single subset sample over joins; (3) a \emph{dynamic index} that can support tuple insertions, while maintaining a one-shot sample or generating multiple (independent) samples. Our techniques achieve near-optimal time and space complexity with respect to the input size and the expected sample size.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [30] [Pressure-robust enriched Galerkin finite element methods for coupled Navier-Stokes and heat equations](https://arxiv.org/abs/2512.16716)
*Sanjeeb Poudel,Sanghyun Lee,Lin Mu*

Main category: cs.CE

TL;DR: 本文提出了一种压力鲁棒的富集Galerkin（EG）有限元方法，用于求解Boussinesq近似下的不可压缩Navier-Stokes方程与热传导方程耦合系统。该方法在速度空间中结合连续Lagrange元与单元内不连续向量增强项，采用分片常数压力空间，并通过速度重构算子实现压力鲁棒性。数值实验表明，该方法在高Rayleigh数和高度扭曲网格下仍保持稳定、准确且高效。


<details>
  <summary>Details</summary>
Motivation: 传统有限元方法在处理不可压缩流动时可能因压力-速度耦合不良而导致精度下降，尤其在复杂几何或高Rayleigh数热对流问题中表现不佳。因此，需要一种兼具灵活性、准确性与压力鲁棒性的数值方法。

Method: 提出一种压力鲁棒的富集Galerkin（EG）有限元方法：速度空间由连续Lagrange基函数与每个单元内的不连续向量增强项构成，压力采用分片常数空间；引入基于Arbogast-Correa混合有限元的速度重构算子，将离散速度场映射为无散且H(div)-协调的场；非线性系统采用Picard迭代及Anderson加速策略求解。

Result: 数值实验表明，所提方法在高度扭曲的四边形网格上仍保持稳定性和精度；Anderson加速显著提升高Rayleigh数流动的收敛效率；在多个基准问题和应用驱动测试中验证了方法在复杂几何中模拟耦合流动与传热问题的潜力。

Conclusion: 压力鲁棒的EG方法是一种灵活、准确且高效的数值工具，适用于Boussinesq近似下复杂几何中的耦合流动与热传输问题，尤其在高Rayleigh数和非结构化网格条件下具有显著优势。

Abstract: We propose a pressure-robust enriched Galerkin (EG) finite element method for the incompressible Navier-Stokes and heat equations in the Boussinesq regime. For the Navier-Stokes equations, the EG formulation combines continuous Lagrange elements with a discontinuous enrichment vector per element in the velocity space and a piecewise constant pressure space, and it can be implemented efficiently within standard finite element frameworks. To enforce pressure robustness, we construct velocity reconstruction operators that map the discrete EG velocity field into exactly divergence-free, H(div)-conforming fields. In particular, we develop reconstructions based on Arbogast-Correa (AC) mixed finite element spaces on quadrilateral meshes and demonstrate that the resulting schemes remain stable and accurate even on highly distorted grids. The nonlinearity of the coupled Navier-Stokes-Boussinesq system is treated with several iterative strategies, including Picard iterations and Anderson-accelerated iterations; our numerical study shows that Anderson acceleration yields robust and efficient convergence for high Rayleigh number flows within the proposed framework. The performance of the method is assessed on a set of benchmark problems and application-driven test cases. These numerical experiments highlight the potential of pressure-robust EG methods as flexible and accurate tools for coupled flow and heat transport in complex geometries.

</details>
