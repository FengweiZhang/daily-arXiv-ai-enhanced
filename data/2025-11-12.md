<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 5]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 7]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Post Processing Graphical User Interface for Heat Flow Visualization](https://arxiv.org/abs/2511.07709)
*Lars Olt,Luis Diego Fonseca Flores,Ian Mckinley*

Main category: cs.SE

TL;DR: 本文介绍了一个基于MATLAB和C++开发的图形用户界面（GUI），利用Thermal Desktop的API（OpenTD）和自定义解析器，高效读取并可视化热模型中的温度、导热和子模型数据，显著提升热工程师分析效率。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏有效工具从Thermal Desktop中提取和可视化热流相关指标，阻碍了热工程师快速分析结果。

Method: 开发了一个结合MATLAB与C++的GUI，通过调用Thermal Desktop的OpenTD API，并利用其压缩解算结果（CSR）文件的副作用，高效加载温度、导热及子模型指标。

Result: 该方法将模型节点、导体与子模型ID关联的运行时间缩短了多个数量级。

Conclusion: 该GUI显著提升了Thermal Desktop热分析结果的处理效率，但作者也指出了当前数据读取方法的不足，并对GUI未来发展及OpenTD后续版本提出了建议。

Abstract: Thermal Desktop (TD) is an industry-standard thermal analysis tool used to create and analyze thermal models for landers, rovers, spacecraft, and instrument payloads. Currently, limited software exists to extract and visualize metrics relevant to heat flow within TD, impeding thermal engineers from analyzing their results quickly. This paper discusses a graphical user interface (GUI) built in MATLAB and C++ which uses TDs application programming interface (API), OpenTD, and a custom parser to address this void. Specifically, we present a method for efficiently loading temperature, conductance, and submodel metrics using a side effect of TDs Compressed Solution Results (CSR) files. This approach can reduce the runtime for correlating model nodes and conductors with submodel IDs by orders of magnitude. Lastly, we reflect on the shortcomings of this method for reading data, consider the future of the GUI, and provide recommendations for subsequent OpenTD releases.

</details>


### [2] [A Small Leak Sinks All: Exploring the Transferable Vulnerability of Source Code Models](https://arxiv.org/abs/2511.08127)
*Weiye Li,Wenyi Tang*

Main category: cs.SE

TL;DR: 本文系统研究了传统源代码模型（SCMs）与大语言模型用于代码（LLM4Code）之间的可迁移脆弱性，提出了一种无需访问下游分类器的通用对抗样本生成方法HABITAT，并揭示了影响跨模型攻击成功率的关键因素。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对源代码模型（包括传统SCMs和LLM4Code）中可迁移脆弱性的深入探索，且多数方法依赖于对下游分类器的访问，难以在实际场景中应用。因此，亟需一种不依赖目标模型信息、适用于现代开发平台中广泛使用的LLM4Code的通用对抗攻击方法。

Method: 作者提出了HABITAT框架，包含定制化的扰动插入机制和分层强化学习结构，能够在无需访问下游分类器的情况下自适应选择最优扰动，生成针对SCMs和LLM4Code的有效对抗样本。

Result: 实验表明，基于传统SCMs生成的对抗样本对LLM4Code的攻击成功率最高可达64%，比当前最先进方法高出15%以上；同时揭示了传统SCMs与LLM4Code之间存在内在脆弱性关联及其影响因素。

Conclusion: 该研究揭示了源代码模型中可迁移脆弱性的本质，为未来构建更鲁棒的AI驱动软件生态系统提供了关键防御方向。

Abstract: Source Code Model learn the proper embeddings from source codes, demonstrating significant success in various software engineering or security tasks. The recent explosive development of LLM extends the family of SCMs,bringing LLMs for code that revolutionize development workflows. Investigating different kinds of SCM vulnerability is the cornerstone for the security and trustworthiness of AI-powered software ecosystems, however, the fundamental one, transferable vulnerability, remains critically underexplored. Existing studies neither offer practical ways, i.e. require access to the downstream classifier of SCMs, to produce effective adversarial samples for adversarial defense, nor give heed to the widely used LLM4Code in modern software development platforms and cloud-based integrated development environments. Therefore, this work systematically studies the intrinsic vulnerability transferability of both traditional SCMs and LLM4Code, and proposes a victim-agnostic approach to generate practical adversarial samples. We design HABITAT, consisting of a tailored perturbation-inserting mechanism and a hierarchical Reinforcement Learning framework that adaptively selects optimal perturbations without requiring any access to the downstream classifier of SCMs. Furthermore, an intrinsic transferability analysis of SCM vulnerabilities is conducted, revealing the potential vulnerability correlation between traditional SCMs and LLM4Code, together with fundamental factors that govern the success rate of victim-agnostic transfer attacks. These findings of SCM vulnerabilities underscore the critical focal points for developing robust defenses in the future. Experimental evaluation demonstrates that our constructed adversarial examples crafted based on traditional SCMs achieve up to 64% success rates against LLM4Code, surpassing the state-of-the-art by over 15%.

</details>


### [3] [OWLAPY: A Pythonic Framework for OWL Ontology Engineering](https://arxiv.org/abs/2511.08232)
*Alkid Baci,Luke Friedrichs,Caglar Demir,Axel-Cyrille Ngonga Ngomo*

Main category: cs.SE

TL;DR: OWLAPY 是一个功能全面的 Python 框架，用于 OWL 本体工程，支持本体的创建、修改、序列化，并集成本地与外部推理机，兼容多种表达格式，还支持利用大语言模型从自然语言生成本体。


<details>
  <summary>Details</summary>
Motivation: 为满足用户在 Python 环境中进行高级 OWL 本体工程的需求，特别是从 Java 环境迁移的用户，提供一个灵活、易用且功能完整的工具框架。

Method: 开发并实现 OWLAPY 框架，集成原生 Python 推理器和外部 Java 推理器，支持多种本体组件实现及 OWL 表达式与描述逻辑、Manchester 语法、SPARQL 等格式之间的转换，并允许自定义基于大语言模型的本体生成流程。

Result: OWLAPY 成功实现了对 OWL 2 本体的全面支持，具备良好的灵活性和扩展性，已在 GitHub 和 PyPI 上发布并获得超过 5 万次下载。

Conclusion: OWLAPY 为本体工程师提供了一个强大、灵活且经过充分测试的 Python 工具，适用于各种高级本体工程任务，尤其适合希望从 Java 迁移至 Python 的用户。

Abstract: In this paper, we introduce OWLAPY, a comprehensive Python framework for OWL ontology engineering. OWLAPY streamlines the creation, modification, and serialization of OWL 2 ontologies. It uniquely integrates native Python-based reasoners with support for external Java reasoners, offering flexibility for users. OWLAPY facilitates multiple implementations of core ontology components and provides robust conversion capabilities between OWL class expressions and formats such as Description Logics, Manchester Syntax, and SPARQL. It also allows users to define custom workflows to leverage large language models (LLMs) in ontology generation from natural language text. OWLAPY serves as a well-tested software framework for users seeking a flexible Python library for advanced ontology engineering, including those transitioning from Java-based environments. The project is publicly available on GitHub at https://github.com/dice-group/owlapy and on the Python Package Index (PyPI) at https://pypi.org/project/owlapy/ , with over 50,000 downloads at the time of writing.

</details>


### [4] [Designing LLM-based Multi-Agent Systems for Software Engineering Tasks: Quality Attributes, Design Patterns and Rationale](https://arxiv.org/abs/2511.08475)
*Yangxiao Cai,Ruiyin Li,Peng Liang,Mojtaba Shahin,Zengyang Li*

Main category: cs.SE

TL;DR: 本文系统研究了面向软件工程任务的基于大语言模型的多智能体系统（LLM-based MASs）的设计，分析了其关注的质量属性、常用设计模式及设计动机，并提出了相关设计启示。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对LLM-based MASs在软件工程中系统性设计研究，尤其是对其关注的质量属性、采用的设计模式及设计动机的理解不足。

Method: 作者收集了94篇关于LLM-based MASs应用于软件工程任务的论文，通过内容分析识别其中关注的质量属性、使用的设计模式以及设计背后的动机。

Result: 研究发现：(1) 代码生成是最常见的应用任务；(2) 功能适用性是最受关注的质量属性；(3) 基于角色的协作是最常用的设计模式；(4) 提升生成代码质量是最主要的设计动机。

Conclusion: 该研究为LLM-based MASs在软件工程任务中的设计提供了实证依据和实践启示，有助于指导未来系统的设计与优化。

Abstract: As the complexity of Software Engineering (SE) tasks continues to escalate, Multi-Agent Systems (MASs) have emerged as a focal point of research and practice due to their autonomy and scalability. Furthermore, through leveraging the reasoning and planning capabilities of Large Language Models (LLMs), the application of LLM-based MASs in the field of SE is garnering increasing attention. However, there is no dedicated study that systematically explores the design of LLM-based MASs, including the Quality Attributes (QAs) on which the designers mainly focus, the design patterns used by the designers, and the rationale guiding the design of LLM-based MASs for SE tasks. To this end, we conducted a study to identify the QAs that LLM-based MASs for SE tasks focus on, the design patterns used in the MASs, and the design rationale for the MASs. We collected 94 papers on LLM-based MASs for SE tasks as the source. Our study shows that: (1) Code Generation is the most common SE task solved by LLM-based MASs among ten identified SE tasks, (2) Functional Suitability is the QA on which designers of LLM-based MASs pay the most attention, (3) Role-Based Cooperation is the design pattern most frequently employed among 16 patterns used to construct LLM-based MASs, and (4) Improving the Quality of Generated Code is the most common rationale behind the design of LLM-based MASs. Based on the study results, we presented the implications for the design of LLM-based MASs to support SE tasks.

</details>


### [5] [Can Large Language Models Simulate Symbolic Execution Output Like KLEE?](https://arxiv.org/abs/2511.08530)
*Rong Feng,Vanisha Gupta,Vivek Patel,Viroopaksh Reddy Ernampati,Suman Saha*

Main category: cs.SE

TL;DR: 本文探索了使用大语言模型（如GPT-4o）模拟符号执行工具KLEE输出的可行性，重点在于识别程序中最受约束的路径。在100个C程序数据集上的实验表明，GPT-4o生成类似KLEE输出并识别最复杂路径的准确率约为20%，虽不高，但揭示了当前LLM在模拟符号执行方面的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 符号执行工具（如KLEE）在处理具有大量分支路径的程序时计算开销巨大，难以扩展到大型复杂代码。因此，作者希望探索是否可以利用大语言模型（如GPT-4o）来模拟KLEE的部分功能，从而节省时间和资源，特别是在识别包含最多符号条件的“最受约束路径”方面。

Method: 作者构建了一个包含100个C程序的数据集，利用GPT-4o尝试预测KLEE的输出，并识别每个程序中符号条件最多的“最受约束路径”，以此评估LLM模拟符号执行的能力。

Result: 实验结果显示，GPT-4o在生成类似KLEE的输出以及识别最受约束路径方面的准确率约为20%。

Conclusion: 尽管准确率不高，但该初步研究表明当前的大语言模型在模拟符号执行方面具备一定潜力，同时也明确了其现有能力的边界。

Abstract: Symbolic execution helps check programs by exploring different paths based on symbolic inputs. Tools like KLEE are commonly used because they can automatically detect bugs and create test cases. But one of KLEE's biggest issues is how slow it can get when programs have lots of branching paths-it often becomes too resource-heavy to run on large or complex code. In this project, we wanted to see if a large language model like GPT-4o could simulate the kinds of outputs that KLEE generates. The idea was to explore whether LLMs could one day replace parts of symbolic execution to save time and resources.
  One specific goal was to have GPT-4o identify the most constrained path in a program, this is the execution path with the most symbolic conditions. These paths are especially important because they often represent edge cases that are harder to test and more likely to contain deep bugs. However, figuring this out usually requires fully running KLEE, which can be expensive. So, we tested whether GPT-4o could predict the KLEE outputs and the most complex path using a dataset of 100 C programs. Our results showed about 20% accuracy in generating KLEE-like outputs and identifying the most constrained path. While not highly accurate, this early work helps show what current LLMs can and can't do when it comes to simulating symbolic execution.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [6] [A Large-Scale Dataset and Reproducible Framework for RF Fingerprinting on IEEE 802.11g Same-Model Devices](https://arxiv.org/abs/2511.07770)
*Zewei Guo,Zhen Jia,JinXiao Zhu,Wenhao Huang,Yin Chen*

Main category: cs.NI

TL;DR: 本文提出了一个大规模同型号设备的射频（RF）指纹数据集及配套的开源可复现实验框架，包含123个相同商用IEEE 802.11g设备的3542万原始I/Q样本和185万RF特征，并基于随机森林算法实现了89.06%的设备识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有射频指纹数据集受限于设备数量少和型号异构，难以有效训练和公平评估机器学习模型，尤其在区分同型号设备方面存在挑战。

Method: 构建包含123个相同型号IEEE 802.11g设备的大规模数据集，采集前导码中的原始I/Q样本并提取RF特征；开发完全开源、可复现的实验框架；采用基于随机森林的算法进行设备识别。

Result: 在所提数据集上实现了89.06%的设备识别准确率，并通过大量实验验证了所提取特征之间的关联性。

Conclusion: 该研究通过提供大规模同型号设备数据集与可复现框架，显著推动了射频指纹识别领域在模型训练与评估方面的标准化与可靠性。

Abstract: Radio frequency (RF) fingerprinting exploits hardware imperfections for device identification, but distinguishing between same-model devices remains challenging due to their minimal hardware variations. Existing datasets for RF fingerprinting are constrained by small device scales and heterogeneous models, which hinders robust training and fair evaluation for machine learning models. To address this gap, we introduce a large-scale dataset of same-model devices along with a fully reproducible, open-source experimental framework. The dataset is built using 123 identical commercial IEEE 802.11g devices and contains 35.42 million raw I/Q samples from the preambles and corresponding 1.85 million RF features. The open-source framework further ensures full reproducibility from data collection to final evaluation. Within this framework, a Random Forest-based algorithm is proposed to achieve 89.06% identification accuracy on this dataset. Extensive experimental evaluations further confirm the relationships between the extracted features.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [7] [ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework](https://arxiv.org/abs/2511.07886)
*Dechuang Chen,Sibo Wang,Qintian Guo*

Main category: cs.DB

TL;DR: ACGraph 是一种面向 SSD 且内存受限环境的新型异步图处理系统，通过动态块优先级调度、异步工作列表、计算与 I/O 流水线以及混合存储格式，显著提升了运行速度和 I/O 效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于严格同步迭代执行的外存图处理系统存在严重的 I/O 低效问题，包括读取与计算放大以及同步等待造成的磁盘利用率低下，难以应对大规模图数据处理需求。

Method: ACGraph 引入动态块中心优先级调度器、在线异步工作列表以减少冗余磁盘访问，采用计算与异步 I/O 流水线执行模型，并结合优化的混合存储格式加速低度顶点访问。

Result: 在 BFS、WCC、PPR、PR 和 k-core 等典型图算法上，ACGraph 在运行时间和 I/O 效率方面均显著优于当前最先进的外存图处理系统。

Conclusion: ACGraph 有效解决了现有外存图处理系统中的 I/O 与同步瓶颈，为内存受限的大规模图计算提供了高效可行的新方案。

Abstract: Graphs are a ubiquitous data structure in diverse domains such as machine learning, social networks, and data mining. As real-world graphs continue to grow beyond the memory capacity of single machines, out-of-core graph processing systems have emerged as a viable solution. Yet, existing systems that rely on strictly synchronous, iteration-by-iteration execution incur significant overheads. In particular, their scheduling mechanisms lead to I/O inefficiencies, stemming from read and work amplification, and induce costly synchronization stalls hindering sustained disk utilization. To overcome these limitations, we present {\em ACGraph}, a novel asynchronous graph processing system optimized for SSD-based environments with constrained memory resources. ACGraph employs a dynamic, block-centric priority scheduler that adjusts in real time based on workload, along with an online asynchronous worklist that minimizes redundant disk accesses by efficiently reusing active blocks in memory. Moreover, ACGraph unifies asynchronous I/O with computation in a pipelined execution model that maintains sustained I/O activation, and leverages a highly optimized hybrid storage format to expedite access to low-degree vertices. We implement popular graph algorithms, such as Breadth-First Search (BFS), Weakly Connected Components (WCC), personalized PageRank (PPR), PageRank (PR), and $k$-core on ACGraph and demonstrate that ACGraph substantially outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [Synera: Synergistic LLM Serving across Device and Cloud at Scale](https://arxiv.org/abs/2511.07423)
*Genglin Wang,Liekang Zeng,Bufang Yang,Kaiwei Liu,Guoliang Xing,Chumin Sun,Li Zhou,Jie Sun,Zhenyu Yan*

Main category: cs.DC

TL;DR: 本文提出Synera，一种设备-云协同的大语言模型（LLM）服务系统，通过高效的小语言模型（SLM）与LLM协同机制，在保持延迟性能的同时显著提升生成质量并降低云服务成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在移动端部署面临生成质量下降和延迟增加的问题；纯云端卸载受通信瓶颈限制，纯端侧小模型则因资源受限牺牲生成质量。

Method: Synera引入三种针对性设计：通信高效的有选择性卸载、无停滞并行推理和可扩展的云端批处理，以优化设备-云协同推理中的卸载决策、流水线停顿和批处理瓶颈。

Result: 实验表明，Synera相比现有基线在延迟相当的情况下，生成质量提升1.20–5.47倍，并比传统云服务节省8.2%–16.5%的云服务成本。

Conclusion: Synera有效结合设备与云端优势，通过协同推理机制在移动端实现高质量、低延迟且成本效益更优的LLM服务。

Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.

</details>


### [9] [Enhancing reliability in AI inference services: An empirical study on real production incidents](https://arxiv.org/abs/2511.07424)
*Bhala Ranganathan,Mickey Zhang,Kai Wu*

Main category: cs.DC

TL;DR: 本文基于一年的运营经验，对156起高严重性大语言模型（LLM）推理事故进行了系统性分析，提出了一个具有高标注一致性的分类方法，识别出主要故障模式（如推理引擎故障和超时），并总结了多种缓解策略（如自动检测、流量调度、节点再平衡等），最终提供了一个供从业者采用的检查清单，以提升大规模LLM推理服务的可靠性与成本效益。


<details>
  <summary>Details</summary>
Motivation: 超大规模大语言模型推理对云系统提出极高要求，短暂故障也可能造成重大用户和业务影响。为理解并缓解这些风险，亟需基于实际运维经验的系统性分析。

Method: 作者基于一年的运维实践构建了一套事故分类法和分析方法，在156起高严重性事故上验证其有效性，并对2025年4月至6月的数据进行聚焦定量研究；使用Cohen's Kappa评估标注一致性，并结合案例提炼缓解措施和优化策略。

Result: 该方法实现了高标注一致性（Cohen's K ≈ 0.89）；发现约60%的事故源于推理引擎故障，其中约40%为超时；约74%的事故可被自动检测，28%需热修复；其余多通过流量路由、节点再平衡或扩容策略缓解；所提分类法还指导了连接活性检测、GPU容量感知路由和端点隔离等针对性策略，有效降低事故影响并加速恢复。

Conclusion: 系统性、基于实证的LLM推理运维分析能显著提升大规模服务的可靠性与成本效率，作者提供的实践导向检查清单有助于业界复用该方法论与自动化机会。

Abstract: Hyperscale large language model (LLM) inference places extraordinary demands on cloud systems, where even brief failures can translate into significant user and business impact. To better understand and mitigate these risks, we present one of the first provider-internal, practice-based analysis of LLM inference incidents. We developed a taxonomy and methodology grounded in a year of operational experience, validating it on 156 high-severity incidents, and conducted a focused quantitative study of Apr-Jun 2025 to ensure recency and relevance. Our approach achieves high labeling consistency (Cohen's K ~0.89), identifies dominant failure modes (in our dataset ~60% inference engine failures, within that category ~40% timeouts), and surfaces mitigation levers (~74% auto-detected; ~28% required hotfix). Beyond hotfixes, many incidents were mitigated via traffic routing, node rebalancing, or capacity increase policies, indicating further automation opportunities. We also show how the taxonomy guided targeted strategies such as connection liveness, GPU capacity-aware routing, and per-endpoint isolation and reduced incident impact and accelerated recovery. Finally, we contribute a practitioner-oriented adoption checklist that enables others to replicate our taxonomy, analysis, and automation opportunities in their own systems. This study demonstrates how systematic, empirically grounded analysis of inference operations can drive more reliable and cost-efficient LLM serving at scale.

</details>


### [10] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: 本文对启用MCP（Model Context Protocol）的大语言模型交互进行了测量分析，揭示了能力、性能与成本之间的权衡，并提出了提升效率和降低成本的优化策略。


<details>
  <summary>Details</summary>
Motivation: MCP虽能增强大语言模型与外部工具交互的能力，但其引入的大量上下文信息显著增加了token使用量，进而推高了经济成本和计算负担。

Method: 通过基于测量的分析方法，评估不同大语言模型和MCP配置对token效率、金钱成本、任务完成时间和成功率等关键指标的影响。

Result: 研究发现并量化了MCP在提升功能的同时带来的成本与性能开销，并验证了并行工具调用和任务中止机制等优化手段的有效性。

Conclusion: 为构建更高效、稳健且经济的MCP驱动工作流提供了实证依据和实用建议。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [11] [UniFormer: Unified and Efficient Transformer for Reasoning Across General and Custom Computing](https://arxiv.org/abs/2511.08135)
*Zhuoheng Ran,Chong Wu,Renjie Xu,Maolin Che,Hong Yan*

Main category: cs.DC

TL;DR: 本文提出了UniFormer，一种面向通用计算和定制计算平台的统一高效Transformer架构，在GPU上实现SOTA精度与延迟，并在FPGA上展现出良好适应性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在通用计算平台（如GPU）与定制计算平台（如FPGA、ASIC）之间部署时，因并行计算范式差异导致模型迁移困难，常需在复杂度、效率或精度上做出妥协，且跨平台优化原则研究不足。

Method: 提出UniFormer架构，通过增强并行性和计算-存储融合，统一适配通用与定制计算平台。

Result: UniFormer在GPU上达到SOTA的准确率与延迟表现，同时在FPGA上展现出强适应性。

Conclusion: UniFormer是首个联合考虑通用与定制计算架构的高效Transformer工作，为跨平台部署提供了新思路。

Abstract: The success of neural networks such as convolutional neural networks (CNNs) has been largely attributed to their effective and widespread deployment on customised computing platforms, including field-programmable gate arrays (FPGAs) and application-specific integrated circuits (ASICs). In the current era, Transformer-based architectures underpin the majority of state-of-the-art (SOTA) larger models that are also increasingly deployed on customised computing hardware for low-power and real-time applications. However, the fundamentally different parallel computation paradigms between general-purpose and customised computing often lead to compromises in model transfer and deployability, which typically come at the cost of complexity, efficiency or accuracy. Moreover, many cross-platform optimisation principles have also remained underexplored in existing studies. This paper introduces UniFormer, a unified and efficient Transformer architecture for both general-purpose and customised computing platforms. By enabling higher parallelism and compute-storage fusion, UniFormer achieves state-of-the-art (SOTA) accuracy and latency on GPUs while exhibiting strong adaptability on FPGAs. To the best of our knowledge, this paper is the first efficient Transformer work that jointly considers both general-purpose and customised computing architectures.

</details>


### [12] [ProbSelect: Stochastic Client Selection for GPU-Accelerated Compute Devices in the 3D Continuum](https://arxiv.org/abs/2511.08147)
*Andrija Stanisic,Stefan Nastic*

Main category: cs.DC

TL;DR: 本文提出ProbSelect，一种无需历史数据或持续监控、基于分析建模与概率预测的客户端选择方法，用于在GPU加速设备上提升联邦学习系统的服务等级目标（SLO）合规性并减少计算浪费。


<details>
  <summary>Details</summary>
Motivation: 在由边缘、云和空间设备构成的动态3D连续体中，传统联邦学习的客户端选择方法依赖历史数据和持续监控，在频繁变化的操作条件下变得不切实际；同时现有方法多基于CPU计算，无法有效应对广泛存在的GPU加速训练场景。

Method: 提出ProbSelect方法，通过分析建模和概率预测，在用户定义的服务等级目标（SLO）约束下进行客户端选择，适用于GPU加速设备，且无需历史数据或持续监控。

Result: 在多种GPU架构和工作负载上的实验表明，ProbSelect相比基线方法平均提升13.77%的SLO合规率，并减少72.5%的计算浪费。

Conclusion: ProbSelect有效解决了动态3D连续体中GPU加速联邦学习的客户端选择难题，在不依赖历史数据的前提下显著提升了性能与资源利用效率。

Abstract: Integration of edge, cloud and space devices into a unified 3D continuum imposes significant challenges for client selection in federated learning systems. Traditional approaches rely on continuous monitoring and historical data collection, which becomes impractical in dynamic environments where satellites and mobile devices frequently change operational conditions. Furthermore, existing solutions primarily consider CPU-based computation, failing to capture complex characteristics of GPU-accelerated training that is prevalent across the 3D continuum. This paper introduces ProbSelect, a novel approach utilizing analytical modeling and probabilistic forecasting for client selection on GPU-accelerated devices, without requiring historical data or continuous monitoring. We model client selection within user-defined SLOs. Extensive evaluation across diverse GPU architectures and workloads demonstrates that ProbSelect improves SLO compliance by 13.77% on average while achieving 72.5% computational waste reduction compared to baseline approaches.

</details>


### [13] [Gathering in Vertex- and Edge-Transitive Graphs without Multiplicity Detection under Round Robin](https://arxiv.org/abs/2511.08222)
*Serafino Cicerone,Alessia Di Fonso,Gabriele Di Stefano,Alfredo Navarra*

Main category: cs.DC

TL;DR: 本文研究了在受限条件下（如无法检测多重占据、初始配置允许多重占据）的图上群体机器人聚集问题，针对无限网格和超立方体两种顶点与边传递图提出了时间最优的聚集算法，并指出可能不存在适用于所有可解情形的通用算法。


<details>
  <summary>Details</summary>
Motivation: 经典群体机器人聚集问题通常假设机器人能感知其他机器人的数量或初始无多重占据，而本文旨在探索更“敌对”的设定下（如无法检测多重占据、初始存在多重占据）是否仍能实现聚集，特别是在顶点与边传递图上。

Method: 作者在OBLOT模型下，采用轮转调度机制（round-robin），分别针对无限网格和超立方体这两种特定的顶点-边传递图，设计了两个利用图结构特性的分布式聚集算法。

Result: 提出了两个时间最优的聚集算法，分别适用于无限网格和超立方体；同时证明了一些基本的不可能性结果，并推测不存在适用于所有可解情形的通用算法。

Conclusion: 在所设定的强限制条件下，虽然对特定拓扑结构可以设计出高效聚集算法，但由于算法高度依赖底层图的结构性质，因此很可能不存在适用于所有可解情况的通用解法。

Abstract: In the field of swarm robotics, one of the most studied problem is Gathering. It asks for a distributed algorithm that brings the robots to a common location, not known in advance. We consider the case of robots constrained to move along the edges of a graph under the well-known OBLOT model. Gathering is then accomplished once all the robots occupy a same vertex. Differently from classical settings, we assume: i) the initial configuration may contain multiplicities, i.e. more than one robot may occupy the same vertex; ii) robots cannot detect multiplicities; iii) robots move along the edges of vertex- and edge-transitive graphs, i.e. graphs where all the vertices (and the edges, resp.) belong to a same class of equivalence. To balance somehow such a `hostile' setting, as a scheduler for the activation of the robots, we consider the round-robin, where robots are cyclically activated one at a time.
  We provide some basic impossibility results and we design two different algorithms approaching the Gathering for robots moving on two specific topologies belonging to edge- and vertex-transitive graphs: infinite grids and hypercubes. The two algorithms are both time-optimal and heavily exploit the properties of the underlying topologies. Because of this, we conjecture that no general algorithm can exist for all the solvable cases.

</details>


### [14] [Priority Matters: Optimising Kubernetes Clusters Usage with Constraint-Based Pod Packing](https://arxiv.org/abs/2511.08373)
*Henrik Daniel Christensen,Saverio Giallorenzo,Jacopo Mauro*

Main category: cs.DC

TL;DR: 本文提出了一种基于约束规划的Kubernetes调度插件，作为默认调度器的后备机制，在默认调度失败时优化Pod分配。实验表明，在1秒和10秒调度窗口内，该方法分别在44%和73%以上的场景中优于默认调度器，并能在约19%的场景中验证默认调度结果已是最优。


<details>
  <summary>Details</summary>
Motivation: Kubernetes默认调度器使用轻量级启发式算法，可能导致次优的Pod放置和资源碎片化，从而无法部署本可运行的Pod。为解决这一问题，作者希望引入更精确的优化方法以提升资源利用率和高优先级Pod的调度成功率。

Method: 作者采用约束规划方法，利用OR-Tools约束求解器实现了一个Kubernetes调度插件。该插件在默认调度器无法分配Pod时介入，尝试寻找满足所有优先级和资源需求的最优Pod分配方案。

Result: 在小到中型集群上的实验显示：在1秒调度窗口内，该方法在超过44%的可实现场景中比默认调度器放置更多高优先级Pod；在10秒窗口内，这一比例提升至73%以上。此外，在约19%的场景中，该方法能证明默认调度器的结果已是最优。

Conclusion: 将约束规划作为Kubernetes调度的后备机制，能够在合理时间内显著提升调度质量，尤其在默认调度器失败的情况下有效减少资源碎片并提高高优先级工作负载的部署成功率。

Abstract: Distributed applications employ Kubernetes for scalable, fault-tolerant deployments over computer clusters, where application components run in groups of containers called pods. The scheduler, at the heart of Kubernetes' architecture, determines the placement of pods given their priority and resource requirements on cluster nodes. To quickly allocate pods, the scheduler uses lightweight heuristics that can lead to suboptimal placements and resource fragmentation, preventing allocations of otherwise deployable pods on the available nodes.
  We propose the usage of constraint programming to find the optimal allocation of pods satisfying all their priorities and resource requests. Implementation-wise, our solution comes as a plug-in to the default scheduler that operates as a fallback mechanism when some pods cannot be allocated. Using the OR-Tools constraint solver, our experiments on small-to-mid-sized clusters indicate that, within a 1-second scheduling window, our approach places more higher-priority pods than the default scheduler (possibly demonstrating allocation optimality) in over 44\% of realisable allocation scenarios where the default scheduler fails, while certifying that the default scheduler's placement is already optimal in over 19\% of scenarios. With a 10-second window, our approach improves placements in over 73\% and still certifies that the default scheduler's placement is already optimal in over 19\% of scenarios.

</details>
