<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 20]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DB](#cs.DB) [Total: 5]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CE](#cs.CE) [Total: 3]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [A High-level Synthesis Toolchain for the Julia Language](https://arxiv.org/abs/2512.15679)
*Benedict Short,Ian McInerney,John Wickerson*

Main category: cs.SE

TL;DR: 本文提出了一种基于MLIR的编译器工具链，可将Julia语言编写的计算内核自动编译为SystemVerilog，无需额外指令或语言定制，从而解决FPGA专用加速器开发中的“双语言问题”。


<details>
  <summary>Details</summary>
Motivation: 随着Exascale计算和数据驱动方法的发展，算法计算需求激增，促使人们将计算任务卸载到GPU、TPU等通用硬件加速器，并重新关注使用FPGA设计专用加速器。然而，当前专用加速器的开发存在“双语言问题”：算法通常用高级语言开发，而内核却需用完全不同抽象层次和专业知识的语言实现。

Method: 作者构建了一个基于MLIR的编译器工具链，支持将Julia语言编写的内核自动编译为与厂商无关的SystemVerilog RTL代码；该工具链支持动态与静态调度，直接集成AXI4-Stream协议以连接片上/片外内存子系统，且无需用户添加任何编译指令或修改语言。

Result: 该原型工具链成功合成了多个信号处理/数学基准测试，在真实FPGA设备上运行频率达100MHz，吞吐量达到仅支持C/C++等低级语言的先进工具链的59.71%至82.6%。

Conclusion: 该工具链使领域专家能够像平常一样用Julia编写计算内核，并无缝将其部署到FPGA上，无需额外注解或代码修改，有效弥合了算法开发与硬件实现之间的鸿沟。

Abstract: With the push towards Exascale computing and data-driven methods, problem sizes have increased dramatically, increasing the computational requirements of the underlying algorithms. This has led to a push to offload computations to general purpose hardware accelerators such as GPUs and TPUs, and a renewed interest in designing problem-specific accelerators using FPGAs. However, the development process of these problem-specific accelerators currently suffers from the "two-language problem": algorithms are developed in one (usually higher-level) language, but the kernels are implemented in another language at a completely different level of abstraction and requiring fundamentally different expertise. To address this problem, we propose a new MLIR-based compiler toolchain that unifies the development process by automatically compiling kernels written in the Julia programming language into SystemVerilog without the need for any additional directives or language customisations. Our toolchain supports both dynamic and static scheduling, directly integrates with the AXI4-Stream protocol to interface with subsystems like on- and off-chip memory, and generates vendor-agnostic RTL. This prototype toolchain is able to synthesize a set of signal processing/mathematical benchmarks that can operate at 100MHz on real FPGA devices, achieving between 59.71% and 82.6% of the throughput of designs generated by state-of-the-art toolchains that only compile from low-level languages like C or C++. Overall, this toolchain allows domain experts to write compute kernels in Julia as they normally would, and then retarget them to an FPGA without additional pragmas or modifications.

</details>


### [2] [How Deep Does Your Dependency Tree Go? An Empirical Study of Dependency Amplification Across 10 Package Ecosystems](https://arxiv.org/abs/2512.14739)
*Jahidul Arafat*

Main category: cs.SE

TL;DR: 该研究对10个主流软件包生态系统的500个项目进行了实证分析，发现Maven的依赖放大效应（平均24.70倍）显著高于其他生态系统，而CocoaPods等则几乎无放大；不同生态系统间的放大程度存在显著差异，应采取针对性的安全策略。


<details>
  <summary>Details</summary>
Motivation: 依赖放大（即传递依赖与直接依赖之比）对软件供应链安全有重大影响，但目前缺乏跨生态系统的规模化比较研究。

Method: 对包括Maven、npm、PyPI等在内的10个主流生态系统中的500个项目进行实证分析，计算各生态系统的平均依赖放大倍数，并进行成对比较和效应量评估。

Result: Maven的平均放大倍数为24.70，远高于Go Modules（4.48）、npm（4.32）和CocoaPods（0.32）；在45组两两比较中有22组存在显著差异；28%的Maven项目放大倍数超过10倍，表明其为系统性现象；差异归因于各生态系统的依赖解析机制、标准库完备性及平台限制等设计选择。

Conclusion: 应针对不同生态系统的依赖放大特性制定差异化安全策略：对Maven环境进行系统性审计，对npm和RubyGems实施定向异常检测，而对放大可控的生态系统（如Cargo、CocoaPods等）可延续现有做法。

Abstract: Modern software development relies on package ecosystems where a single declared dependency can pull in many additional transitive packages. This dependency amplification, defined as the ratio of transitive to direct dependencies, has major implications for software supply chain security, yet amplification patterns across ecosystems have not been compared at scale. We present an empirical study of 500 projects across ten major ecosystems, including Maven Central for Java, npm Registry for JavaScript, crates io for Rust, PyPI for Python, NuGet Gallery for dot NET, RubyGems for Ruby, Go Modules for Go, Packagist for PHP, CocoaPods for Swift and Objective C, and Pub for Dart. Our analysis shows that Maven exhibits mean amplification of 24.70 times, compared to 4.48 times for Go Modules, 4.32 times for npm, and 0.32 times for CocoaPods. We find significant differences with large effect sizes in 22 of 45 pairwise comparisons, challenging the assumption that npm has the highest amplification due to its many small purpose packages. We observe that 28 percent of Maven projects exceed 10 times amplification, indicating a systematic pattern rather than isolated outliers, compared to 14 percent for RubyGems, 12 percent for npm, and zero percent for Cargo, PyPI, Packagist, CocoaPods, and Pub. We attribute these differences to ecosystem design choices such as dependency resolution behavior, standard library completeness, and platform constraints. Our findings suggest adopting ecosystem specific security strategies, including systematic auditing for Maven environments, targeted outlier detection for npm and RubyGems, and continuation of current practices for ecosystems with controlled amplification. We provide a full replication package with data and analysis scripts.

</details>


### [3] [VDMN: A Graphical Notation for Modelling Value Driver Trees](https://arxiv.org/abs/2512.14740)
*Benjamin Matthies*

Main category: cs.SE

TL;DR: 本文提出了价值驱动建模符号（VDMN），用于系统化地构建价值驱动树（VDT），并通过案例研究和专家访谈验证了其有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 尽管价值驱动树（VDT）在管理决策中应用日益广泛，但缺乏系统化的建模指南，阻碍了其一致性和标准化发展。

Method: 开发了一种名为价值驱动建模符号（VDMN）的图形化建模语言，包含语义构造和直观图形语法，并通过两个案例研究和专家访谈进行评估。

Result: VDMN能够支持一致且易于理解的价值驱动树建模，获得了专家认可。

Conclusion: VDMN为价值驱动树的系统化与标准化建模提供了重要基础，有助于提升其在实践中的应用效果。

Abstract: Value Driver Trees (VDTs) are conceptual models used to illustrate and analyse the causal relationships between key performance indicators and business outcomes, thereby supporting managerial decision-making and value-based management. Despite their increasing application, there are still no systematic guidelines for the modelling of such conceptual models. To fill this gap, this study introduces the Value Driver Modelling Notation (VDMN), a graphical notation developed to systematically guide VDT modelling. This notation includes a comprehensive set of semantic constructs and an intuitive graphical syntax. To evaluate its practical utility, the VDMN was applied in two case studies and assessed through expert interviews. The results show that the notation supports a consistent and comprehensible modelling of VDTs. The VDMN thus represents a significant step towards the systematisation and standardisation of VDT modelling.

</details>


### [4] [Revisiting the Reliability of Language Models in Instruction-Following](https://arxiv.org/abs/2512.14754)
*Jianshuo Dong,Yutong Zhang,Yan Liu,Zhenyu Zhong,Tao Wei,Chao Zhang,Han Qiu*

Main category: cs.SE

TL;DR: 本文提出“细微差异导向的可靠性”概念，通过构建新指标 reliable@k 和增强数据集 IFEval++，发现当前大语言模型在面对语义相近但表述细微不同的用户指令时表现显著下降，最高降幅达61.8%，并探讨了三种改进方法。


<details>
  <summary>Details</summary>
Motivation: 尽管先进大语言模型在IFEval等基准上指令遵循准确率接近上限，但在真实场景中面对用户多样的措辞、上下文和任务表达时，其可靠性仍存疑。因此，作者旨在研究模型在语义相近但具细微差异的“同类提示”下的行为一致性。

Method: 作者提出新指标 reliable@k，设计自动化数据增强流程生成高质量“同类提示”，构建扩展评测集 IFEval++，并在20个闭源与26个开源大模型上系统评估其细微差异导向的可靠性。

Result: 实验表明，当前模型在此类可靠性方面存在严重不足，细微提示变化可导致性能最多下降61.8%；同时，论文对问题特征进行刻画，并探索了三种潜在改进策略。

Conclusion: 细微差异导向的可靠性是实现更可靠、可信大语言模型的关键且尚未充分探索的方向，需引起重视。

Abstract: Advanced LLMs have achieved near-ceiling instruction-following accuracy on benchmarks such as IFEval. However, these impressive scores do not necessarily translate to reliable services in real-world use, where users often vary their phrasing, contextual framing, and task formulations. In this paper, we study nuance-oriented reliability: whether models exhibit consistent competence across cousin prompts that convey analogous user intents but with subtle nuances. To quantify this, we introduce a new metric, reliable@k, and develop an automated pipeline that generates high-quality cousin prompts via data augmentation. Building upon this, we construct IFEval++ for systematic evaluation. Across 20 proprietary and 26 open-source LLMs, we find that current models exhibit substantial insufficiency in nuance-oriented reliability -- their performance can drop by up to 61.8% with nuanced prompt modifications. What's more, we characterize it and explore three potential improvement recipes. Our findings highlight nuance-oriented reliability as a crucial yet underexplored next step toward more dependable and trustworthy LLM behavior. Our code and benchmark are accessible: https://github.com/jianshuod/IFEval-pp.

</details>


### [5] [Examining Software Developers' Needs for Privacy Enforcing Techniques: A survey](https://arxiv.org/abs/2512.14756)
*Ioanna Theophilou,Georgia M. Kapitsaki*

Main category: cs.SE

TL;DR: 该研究通过一项涉及68名开发者的调查，揭示了在数据隐私法规（如GDPR和CCPA/CPRA）背景下，开发者对自动化隐私合规工具的迫切需求，并发现隐私经验越丰富的开发者对相关工具的关注度越高。


<details>
  <summary>Details</summary>
Motivation: 随着GDPR和CCPA/CPRA等数据隐私法规的实施，软件系统必须满足合规要求。然而，由于合规涉及复杂的法律知识，开发者在集成相关功能时面临困难。现有研究主要关注开发者对隐私原则的理解和行业采用的隐私技术，但尚未系统探讨开发者在合规过程中新兴的实际需求，尤其是对自动化工具（如生成式AI）的需求。

Method: 开展了一项针对68名开发者的调查，以识别其在隐私法律合规方面的具体需求，并分析影响这些需求的因素（如隐私经验）。

Result: 大多数开发者表达了对更多自动化工具的需求；同时，具备更多隐私经验的从业者对隐私工具表现出更高的关注度。

Conclusion: 研究结果有助于从业者更好地将开发活动与隐私合规要求对齐，并凸显了开发隐私辅助工具的紧迫性。

Abstract: Data privacy legislation, such as GDPR and CCPA/CPRA, has rendered data privacy law compliance a requirement of all software systems. Developers need to implement various kinds of functionalities to cover law needs, including user rights and law principles. As data compliance is tightly coupled with legal knowledge, it is not always easy to perform such integrations in software systems. Prior studies have focused on developers' understanding of privacy principles, such as Privacy by Design, and have examined privacy techniques used in the software industry. Nevertheless, emerging developer needs that can assist in privacy law compliance have not been examined but are useful in understanding what development automation tools, such as Generative AI, need to cover to make the compliance process more straightforward and seamless within the development process. In this work, we present a survey that examines the above needs with the participation of 68 developers, while we have examined which factors affect practitioners' needs. Most developers express a need for more automated tools, while privacy experience increases practitioners' concerns for privacy tools. Our results can assist practitioners in better positioning their development activities within privacy law compliance and point to an urgent need for privacy facilitators.

</details>


### [6] [CAPE: Capability Achievement via Policy Execution](https://arxiv.org/abs/2512.14761)
*David Ball*

Main category: cs.SE

TL;DR: 本文提出“能力工程”（Capability Engineering）方法，通过CAPE协议将需求转化为可执行规范，并在训练中确保模型默认满足这些规范，显著降低违规率、成本和开发周期。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏表达和强制执行需求的机制，预训练和后训练阶段均无法保证模型在部署中可靠满足具体、上下文相关的约束，导致高性能模型在实际应用中频繁失败。

Method: 引入CAPE（Capability Achievement via Policy Execution）协议，实施“指定→验证→修正→训练”循环；基于上下文客观性和验证保真度随模型规模提升两个实证发现，采用可复用的规范替代逐样本标注。

Result: 在六个领域共109,500个样本上，CAPE相比DPO将违规率降低81%（标准差<0.3%），成本降低5–20倍，开发周期从数月缩短至数周。

Conclusion: 能力工程为AI系统提供了一种系统化方法，通过可执行规范确保模型满足实际需求；作者开源CAPE协议及相关工具，并推出CapabilityBench以推动从智能评测向能力评测的范式转变。

Abstract: Modern AI systems lack a way to express and enforce requirements. Pre-training produces intelligence, and post-training optimizes preferences, but neither guarantees that models reliably satisfy explicit, context-dependent constraints. This missing abstraction explains why highly intelligent models routinely fail in deployment despite strong benchmark performance.
  We introduce Capability Engineering, the systematic practice of converting requirements into executable specifications and training models to satisfy them by default. We operationalize this practice through CAPE (Capability Achievement via Policy Execution), a protocol implementing a Specify -> Verify -> Correct -> Train loop.
  CAPE is grounded in two empirical findings: (1) contextual objectivity, where properties appearing subjective become objective once context is fixed (inter-annotator agreement rises from kappa = 0.42 to kappa = 0.98), and (2) verification-fidelity scaling, where verification accuracy improves with model scale (r = 0.94), unlike preference agreement which plateaus at 30 to 50 percent disagreement regardless of compute. Across 109,500 examples in six domains, CAPE reduces violation rates by 81 percent relative to DPO (standard deviation less than 0.3 percent). By replacing per-example annotation with reusable specifications, CAPE reduces costs by 5 to 20 times and shortens timelines from months to weeks.
  We release the CAPE protocol, PredicateGraph schema, CPL specification language, and policy packs under Apache 2.0. We also launch CapabilityBench, a public registry of model evaluations against community-contributed policies, shifting evaluation from intelligence benchmarks toward capability measurement.

</details>


### [7] [Workflows vs Agents for Code Translation](https://arxiv.org/abs/2512.14762)
*Henry Gray,Tom Yotam,Octavian Udrea*

Main category: cs.SE

TL;DR: 本文比较了两种基于大语言模型（LLM）的 MATLAB 到 HDL 语法修复方法：一种是结构化的专家设计流程，另一种是使用 Model Context Protocol（MCP）的自主智能体方法。实验表明，智能体方法在中小型模型上更有效地修复语法错误，显著提升下游仿真成功率。


<details>
  <summary>Details</summary>
Motivation: 将 MATLAB 等高级语言算法转译为硬件描述语言（HDL）对 FPGA/ASIC 部署至关重要，但 LLM 因 HDL 训练数据有限，端到端转译易出错，因此需研究有效的语法修复策略。

Method: 作者对比两种 LLM 驱动的语法修复方法：固定流程的结构化方法与基于 MCP 协议、可动态选择工具的智能体方法。实验在 42 个 MATLAB 信号处理函数上进行，评估三种不同规模模型的表现。

Result: 智能体方法在初始语法错误修复上更有效，尤其在中小型模型上显著提升仿真可达率（最高提升超 20 个百分点）。其优势可能源于短提示、激进上下文管理和条件化工具调用。在 235B 大模型上收益较小，且朴素 RAG 变体表现最佳。

Conclusion: 合理设计的智能体框架能有效弥补中小型 LLM 的能力局限，在 MATLAB 到 HDL 转译的语法修复阶段具有显著优势。

Abstract: Translating algorithms from high-level languages like MATLAB to hardware description languages (HDLs) is a resource-intensive but necessary step for deployment on FPGAs and ASICs. While large language models (LLMs) offer a path to automation, their limited training on HDL code makes end-to-end transpilation brittle and prone to syntax errors. We compare two LLM-driven methods for syntax repair in a MATLAB-to-HDL pipeline: a structured, expert-designed flow that follows a fixed sequence of operations, and a more autonomous agentic approach that uses the Model Context Protocol (MCP) \cite{anthropic2024mcp} to dynamically select its own tools. We study 42 MATLAB signal-processing functions and isolate the syntax-repair stage. Across three model scales, the agentic approach is more effective at resolving initial syntax errors, unblocking a greater number of candidates to proceed through the pipeline. This upstream improvement yields measurable downstream improvements, most notably on mid-sized models, where it increases the simulation reach rate by over 20 percentage points. We hypothesize the gains come from short prompts, aggressive context management, and conditional tool use. Conditional retrieval helps at 8B and 30B; at 235B final-success gains are small and a naive RAG variant attains the highest final success. Our findings suggest that these agentic frameworks, when properly designed, are most effective at compensating for the capacity limits of small and mid-sized models.

</details>


### [8] [Let the Barbarians In: How AI Can Accelerate Systems Performance Research](https://arxiv.org/abs/2512.14806)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Shubham Agarwal,Mert Cemri,Bowen Wang,Alexander Krentsel,Tian Xia,Jongseok Park,Shuo Yang,Jeff Chen,Lakshya Agrawal,Ashwin Naren,Shulu Li,Ruiying Ma,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.SE

TL;DR: 本文提出了一种名为AI驱动的系统研究（ADRS）的新范式，通过生成、评估和优化的迭代循环，利用AI自动生成并验证系统设计方案；在十个案例研究中，ADRS生成的方案可媲美甚至超越人类设计，并总结了有效应用ADRS的最佳实践与未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统系统研究依赖人工设计与调优，而随着AI的发展，若能结合可靠的验证机制，有望自动化发现高性能系统解决方案。系统性能研究尤其适合此范式，因其天然具备可验证性。

Method: 采用AI驱动的迭代研究框架ADRS，结合多个开源实例（如OpenEvolve、GEPA、ShinkaEvolve），在真实系统或模拟器中实现候选方案，并通过预定义工作负载进行评估与优化。

Result: 在包括多区域云调度、混合专家负载均衡、基于大语言模型的SQL生成等十个案例中，ADRS生成的方案达到或超越当前人类最优设计。

Conclusion: 尽管尚无适用于所有系统研究领域的通用ADRS方法，但本文通过实证分析提出了有效使用ADRS的最佳实践，并指出未来研究应更聚焦于问题建模与战略监督。

Abstract: Artificial Intelligence (AI) is beginning to transform the research process by automating the discovery of new solutions. This shift depends on the availability of reliable verifiers, which AI-driven approaches require to validate candidate solutions. Research focused on improving systems performance is especially well-suited to this paradigm because system performance problems naturally admit such verifiers: candidates can be implemented in real systems or simulators and evaluated against predefined workloads. We term this iterative cycle of generation, evaluation, and refinement AI-Driven Research for Systems (ADRS). Using several open-source ADRS instances (i.e., OpenEvolve, GEPA, and ShinkaEvolve), we demonstrate across ten case studies (e.g., multi-region cloud scheduling, mixture-of-experts load balancing, LLM-based SQL, transaction scheduling) that ADRS-generated solutions can match or even outperform human state-of-the-art designs. Based on these findings, we outline best practices (e.g., level of prompt specification, amount of feedback, robust evaluation) for effectively using ADRS, and we discuss future research directions and their implications. Although we do not yet have a universal recipe for applying ADRS across all of systems research, we hope our preliminary findings, together with the challenges we identify, offer meaningful guidance for future work as researcher effort shifts increasingly toward problem formulation and strategic oversight. Note: This paper is an extension of our prior work [14]. It adds extensive evaluation across multiple ADRS frameworks and provides deeper analysis and insights into best practices.

</details>


### [9] [Evaluating Code Reasoning Abilities of Large Language Models Under Real-World Settings](https://arxiv.org/abs/2512.14917)
*Changshu Liu,Alireza Ghazanfari,Yang Chen,Reyhaneh Jabbarvand*

Main category: cs.SE

TL;DR: 本文提出了RE2-Bench，一个包含1,101个代码推理问题的新基准，其中195个来自真实项目，旨在更真实地评估大语言模型（LLM）在复杂代码场景下的推理能力。该基准利用静态与动态程序分析处理复杂类型，并基于九种可解释的代码复杂度指标将问题划分为“简单”和“困难”两类。实验表明，现有LLM在困难问题上性能显著下降，说明以往评估高估了其真实推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有代码推理基准多使用简单程序，无法反映真实代码中的复杂依赖、API调用、嵌套结构和非基本数据类型，导致对LLM推理能力的评估过于乐观，缺乏现实意义。

Method: 提出RE2-Bench基准，结合静态与动态程序分析自动处理真实代码中的复杂类型；通过九种可解释的代码复杂度指标，采用多数投票机制将问题划分为“简单”和“困难”两类；在输入预测和输出预测任务上评估六种主流LLM。

Result: LLM在RE2-Bench的困难问题上表现明显下降：输入预测准确率下降51.50%，输出预测下降42.15%，揭示了现有评估方法对模型能力的高估。

Conclusion: RE2-Bench提供了一个更贴近现实的代码推理评估框架，揭示了当前LLM在处理复杂代码时的局限性，为未来模型改进和评估提供了可靠基准。

Abstract: Code reasoning tasks are becoming prevalent in large language model (LLM) assessments. Existing benchmarks involve simple programs, failing to represent real-world complexities such as inter- or intra-procedural dependencies, core or third-party API calls, highly nested constructs, and non-primitive complex types. Evaluating LLMs under such a simplistic setting poses a significant threat to assumptions about their generalizability in practice. To enable a more realistic evaluation of code reasoning, this paper proposes RE2-Bench, a benchmark of 1,101 reasoning problems, including 195 drawn from mature real-world projects. RE2-Bench leverages static and dynamic program analysis to automatically serialize and deserialize compound, complex, and custom types in real-world code, going far beyond the primitive-only settings used in prior work.
  A key feature of RE2-Bench is categorizing each reasoning problem as Easy or Hard via a principled majority-vote mechanism over nine interpretable code complexity metrics, resulting in two well-separated and semantically meaningful difficulty categories suitable for precise calibration of LLM reasoning ability. A comprehensive evaluation of six general-purpose and reasoning-oriented LLMs on two widely used code reasoning tasks -- input prediction and output prediction -- using RE2-Bench reveals a significant performance drop from Easy to Hard problems (51.50\% for input prediction and 42.15\% for output prediction), confirming that prior evaluations substantially overestimate the reasoning capabilities of LLMs.

</details>


### [10] [Imitation Game: Reproducing Deep Learning Bugs Leveraging an Intelligent Agent](https://arxiv.org/abs/2512.14990)
*Mehil B Shah,Mohammad Masudur Rahman,Foutse Khomh*

Main category: cs.SE

TL;DR: 本文提出 RepGen，一种基于大语言模型的自动化方法，用于高效复现深度学习中的 Bug，在真实数据集上达到 80.19% 的复现成功率，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 深度学习应用广泛但存在大量 Bug，而由于其非确定性和软硬件环境耦合，手动复现 Bug 极其困难，成功率仅约 3%。

Method: RepGen 通过构建学习增强上下文、制定复现计划，并采用迭代的生成-验证-优化机制，利用大语言模型自动生成可复现 Bug 的代码。

Result: 在 106 个真实 DL Bug 上评估，RepGen 实现 80.19% 的复现率，比最先进方法提升 19.81%；用户研究表明其提升成功率 23.35%，减少 56.8% 复现时间并降低认知负荷。

Conclusion: RepGen 显著提高了深度学习 Bug 的自动化复现能力，为开发人员提供高效、低负担的调试支持。

Abstract: Despite their wide adoption in various domains (e.g., healthcare, finance, software engineering), Deep Learning (DL)-based applications suffer from many bugs, failures, and vulnerabilities. Reproducing these bugs is essential for their resolution, but it is extremely challenging due to the inherent nondeterminism of DL models and their tight coupling with hardware and software environments. According to recent studies, only about 3% of DL bugs can be reliably reproduced using manual approaches. To address these challenges, we present RepGen, a novel, automated, and intelligent approach for reproducing deep learning bugs. RepGen constructs a learning-enhanced context from a project, develops a comprehensive plan for bug reproduction, employs an iterative generate-validate-refine mechanism, and thus generates such code using an LLM that reproduces the bug at hand. We evaluate RepGen on 106 real-world deep learning bugs and achieve a reproduction rate of 80.19%, a 19.81% improvement over the state-of-the-art measure. A developer study involving 27 participants shows that RepGen improves the success rate of DL bug reproduction by 23.35%, reduces the time to reproduce by 56.8%, and lowers participants' cognitive load.

</details>


### [11] [Toxicity Ahead: Forecasting Conversational Derailment on GitHub](https://arxiv.org/abs/2512.15031)
*Mia Mohammad Imran,Robert Zita,Rahat Rizvi Rahman,Preetha Chatterjee,Kostadin Damevski*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的两阶段提示框架，用于预测GitHub开源社区中的对话脱轨（毒性行为），通过生成“对话动态摘要”并评估脱轨可能性，在多个模型和数据集上取得了优于传统NLP方法的F1分数，支持可解释、主动式的社区管理。


<details>
  <summary>Details</summary>
Motivation: 开源软件社区中的毒性互动会降低贡献者参与度并威胁项目可持续性，而现有主动干预策略多依赖人工，成本高昂。因此，亟需可扩展的自动化方法来提前识别潜在有害对话。

Method: 作者构建了一个包含159个有毒和207个无毒GitHub讨论线程的数据集，并提出一种基于Least-to-Most（LtM）提示的两步LLM框架：首先生成“对话动态摘要”（SCDs），再利用这些摘要预测对话脱轨的可能性。

Result: 在Qwen和Llama模型上，该方法在阈值为0.3时分别达到0.901和0.852的F1分数；在外部验证集（308个线程）上最高F1达0.797，优于现有NLP基线。

Conclusion: 结构化的LLM提示策略能有效实现对开源社区对话脱轨的早期、可解释预测，为自动化、主动式内容审核提供了可行路径。

Abstract: Toxic interactions in Open Source Software (OSS) communities reduce contributor engagement and threaten project sustainability. Preventing such toxicity before it emerges requires a clear understanding of how harmful conversations unfold. However, most proactive moderation strategies are manual, requiring significant time and effort from community maintainers. To support more scalable approaches, we curate a dataset of 159 derailed toxic threads and 207 non-toxic threads from GitHub discussions. Our analysis reveals that toxicity can be forecast by tension triggers, sentiment shifts, and specific conversational patterns.
  We present a novel Large Language Model (LLM)-based framework for predicting conversational derailment on GitHub using a two-step prompting pipeline. First, we generate \textit{Summaries of Conversation Dynamics} (SCDs) via Least-to-Most (LtM) prompting; then we use these summaries to estimate the \textit{likelihood of derailment}. Evaluated on Qwen and Llama models, our LtM strategy achieves F1-scores of 0.901 and 0.852, respectively, at a decision threshold of 0.3, outperforming established NLP baselines on conversation derailment. External validation on a dataset of 308 GitHub issue threads (65 toxic, 243 non-toxic) yields an F1-score up to 0.797. Our findings demonstrate the effectiveness of structured LLM prompting for early detection of conversational derailment in OSS, enabling proactive and explainable moderation.

</details>


### [12] [Aligning Academia with Industry: An Empirical Study of Industrial Needs and Academic Capabilities in AI-Driven Software Engineering](https://arxiv.org/abs/2512.15148)
*Hang Yu,Yuzhou Lai,Li Zhang,Xiaoli Lian,Fang Liu,Yanrui Dong,Ting Zhang,Zhi Jin,David Lo*

Main category: cs.SE

TL;DR: 本文通过系统分析2022–2025年FSE、ASE和ICSE三大顶会的1,367篇论文，并结合对17家企业的282份问卷调查，对比学术研究与工业实践在六大软件工程主题上的差异，揭示了当前研究在需求与架构、可靠性、可解释性、输入假设、评估方法及伦理等方面与工业需求脱节的问题，并提出七项关键启示。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型推动了软件工程领域的学术进展，但这些成果是否契合工业界实际需求尚不明确。为弥合学术研究与工业实践之间的差距，有必要系统评估当前研究的工业相关性并识别被忽视的关键挑战。

Method: 作者首先对2022至2025年间FSE、ASE和ICSE三大会议的1,367篇论文进行系统性内容分析，识别研究主题、常用基准、工业相关性和开源情况；随后对17家组织开展了包含282份有效回复的实证问卷调查，聚焦程序分析、自动测试、代码生成/补全、问题修复、预训练代码模型和依赖管理六大主题。

Result: 研究发现学术界在多个方面与工业界存在显著脱节，包括对软件需求与架构关注不足、智能方法的可靠性与可解释性欠缺、学术研究中对输入条件的不切实际假设、评估指标与实际场景不符，以及伦理问题被忽视等，并据此提炼出七项关键启示。

Conclusion: 该研究呼吁学术界重新聚焦于那些具有重要工业价值但尚未充分探索的问题，以提升未来软件工程研究的实际影响力和应用价值。

Abstract: The rapid advancement of large language models (LLMs) is fundamentally reshaping software engineering (SE), driving a paradigm shift in both academic research and industrial practice. While top-tier SE venues continue to show sustained or emerging focus on areas like automated testing and program repair, with researchers worldwide reporting continuous performance gains, the alignment of these academic advances with real industrial needs remains unclear. To bridge this gap, we first conduct a systematic analysis of 1,367 papers published in FSE, ASE, and ICSE between 2022 and 2025, identifying key research topics, commonly used benchmarks, industrial relevance, and open-source availability. We then carry out an empirical survey across 17 organizations, collecting 282 responses on six prominent topics, i.e., program analysis, automated testing, code generation/completion, issue resolution, pre-trained code models, and dependency management, through structured questionnaires. By contrasting academic capabilities with industrial feedback, we derive seven critical implications, highlighting under-addressed challenges in software requirements and architecture, the reliability and explainability of intelligent SE approaches, input assumptions in academic research, practical evaluation tensions, and ethical considerations. This study aims to refocus academic attention on these important yet under-explored problems and to guide future SE research toward greater industrial impact.

</details>


### [13] [Automating Execution and Verification of BPMN+DMN Business Processes](https://arxiv.org/abs/2512.15214)
*Giuseppe Della Penna,Igor Melatti*

Main category: cs.SE

TL;DR: 本文提出了一种名为BDTransTest的工具，用于将BPMN+DMN业务流程自动转换为Java程序，并生成和执行测试计划以检测语义错误，同时评估测试覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 当前主流BPMN+DMN建模框架仅支持语法错误检测，缺乏对语义（行为）错误的验证手段，且专有工具的模拟机制不透明，迫使设计者手动运行单次执行来排查故障。

Method: 设计并实现BDTransTest工具，包括：(i) 将BPMN+DMN流程转换为Java程序；(ii) 自动生成并执行测试计划，必要时由设计者澄清输入域；(iii) 分析测试计划在流程节点和边上的覆盖率。

Result: 在文献中的多个BPMN+DMN流程上进行了实验评估，验证了所提方法的有效性。

Conclusion: BDTransTest显著提升了BPMN+DMN流程的语义验证能力，弥补了现有工具在行为正确性验证方面的不足。

Abstract: The increasing and widespread use of BPMN business processes, also embodying DMN tables, requires tools and methodologies to verify their correctness. However, most commonly used frameworks to build BPMN+DMN models only allow designers to detect syntactical errors, thus ignoring semantic (behavioural) faults. This forces business processes designers to manually run single executions of their BPMN+DMN processes using proprietary tools in order to detect failures. Furthermore, how proprietary tools translate a BPMN+DMN process to a computer simulation is left unspecified. In this paper, we advance this state of the art by designing a tool, named BDTransTest providing: i) a translation from a BPMN + DMN process B to a Java program P ; ii) the synthesis and execution of a testing plan for B, that may require the business designer to disambiguate some input domain; iii) the analysis of the coverage achieved by the testing plan in terms of nodes and edges of B. Finally, we provide an experimental evaluation of our methodology on BPMN+DMN processes from the literature.

</details>


### [14] [Heterogeneous Model Alignment in Digital Twin](https://arxiv.org/abs/2512.15281)
*Faima Abbasi,Jean-Sébastien Sottet,Cedric Pruski*

Main category: cs.SE

TL;DR: 本文提出了一种面向多层模型驱动数字孪生的异构模型对齐方法，通过自适应一致性机制与大语言模型验证流程，实现跨抽象层语义一致、结构保真的自动对齐，提升可扩展性并减少人工干预。


<details>
  <summary>Details</summary>
Motivation: 多层模型驱动的数字孪生系统中，不同抽象层间的异构模型易产生语义不匹配、不一致和同步问题；现有依赖静态映射和人工更新的方法缺乏灵活性且易出错，难以保障数据完整性。

Method: 提出一个包含灵活性机制的框架：(i) 自适应一致性机制，将元模型与演化中的模型动态关联；(ii) 基于大语言模型（LLM）的对齐验证流程，利用领域知识确保元模型在整个数字孪生生命周期中的结构保真与概念一致性。

Result: 该方法能自动发现语义对应关系、显著减少手动映射，并在多种模型类型间提升可扩展性；通过空气质量用例及OAEI多个赛道测试用例验证了其有效性。

Conclusion: 所提方法有效解决了多层数字孪生中异构模型对齐难题，增强了语义一致性、自动化程度与系统可扩展性，为复杂系统的建模与优化提供了可靠支持。

Abstract: Digital twin (DT) technology integrates heterogeneous data and models, along with semantic technologies to create multi-layered digital representation of physical systems. DTs enable monitoring, simulation, prediction, and optimization to enhance decision making and operational efficiency. A key challenge in multi-layered, model-driven DTs is aligning heterogeneous models across abstraction layers, which can lead to semantic mismatches, inconsistencies, and synchronization issues. Existing methods, relying on static mappings and manual updates, are often inflexible, error-prone, and risk compromising data integrity. To address these limitations, we present a heterogeneous model alignment approach for multi-layered, model-driven DTs. The framework incorporates a flexibility mechanism that allows metamodels to adapt and interconnect seamlessly while maintaining semantic coherence across abstraction layers. It integrates: (i) adaptive conformance mechanisms that link metamodels with evolving models and (ii) a large language model (LLM) validated alignment process that grounds metamodels in domain knowledge, ensuring structural fidelity and conceptual consistency throughout the DT lifecycle. This approach automates semantic correspondences discovery, minimizes manual mapping, and enhances scalability across diverse model types. We illustrate the approach using air quality use case and validate its performance using different test cases from Ontology Alignment Evaluation Initiative (OAEI) tracks.

</details>


### [15] [Can AI Generate more Comprehensive Test Scenarios? Review on Automated Driving Systems Test Scenario Generation Methods](https://arxiv.org/abs/2512.15422)
*Ji Zhou,Yongqi Zhao,Yixian Hu,Hexuan Li,Zhengguo Gu,Nan Xu,Arno Eichberger*

Main category: cs.SE

TL;DR: 本文系统综述了2015–2025年间自动驾驶系统（ADS）场景测试的研究进展，重点分析2023–2025年基于AI与多模态技术的新方法，并提出包含多模态扩展的分类体系、伦理与安全检查清单及ODD覆盖图，以推动ADS的安全验证与部署。


<details>
  <summary>Details</summary>
Motivation: 传统大规模道路测试成本高、耗时长，而现有综述对近年场景测试的方法和技术进展覆盖不全，亟需系统性梳理与整合。

Method: 通过全面检索2015–2025年间文献，筛选出31篇主研究和10篇综述，重点对2023–2025年采用生成模型（如大语言模型、GAN、扩散模型、强化学习）的场景生成框架进行方法论综合与对比评估。

Result: 识别出三大研究缺口：缺乏标准化评估指标、伦理与人为因素整合不足、多模态及ODD特定场景覆盖有限；并据此提出改进的分类法、伦理安全检查清单和ODD场景覆盖图。

Conclusion: 本综述为学术界提供方法论清晰度，为工业界提供实践指导，有助于实现可复现的评估并加速高等级ADS的安全落地。

Abstract: Ensuring the safety and reliability of Automated Driving Systems (ADS) remains a critical challenge, as traditional verification methods such as large-scale on-road testing are prohibitively costly and time-consuming.To address this,scenario-based testing has emerged as a scalable and efficient alternative,yet existing surveys provide only partial coverage of recent methodological and technological advances.This review systematically analyzes 31 primary studies,and 10 surveys identified through a comprehensive search spanning 2015~2025;however,the in-depth methodological synthesis and comparative evaluation focus primarily on recent frameworks(2023~2025),reflecting the surge of Artificial Intelligent(AI)-assisted and multimodal approaches in this period.Traditional approaches rely on expert knowledge,ontologies,and naturalistic driving or accident data,while recent developments leverage generative models,including large language models,generative adversarial networks,diffusion models,and reinforcement learning frameworks,to synthesize diverse and safety-critical scenarios.Our synthesis identifies three persistent gaps:the absence of standardized evaluation metrics,limited integration of ethical and human factors,and insufficient coverage of multimodal and Operational Design Domain (ODD)-specific scenarios.To address these challenges,this review contributes a refined taxonomy that incorporates multimodal extensions,an ethical and safety checklist for responsible scenario design,and an ODD coverage map with a scenario-difficulty schema to enable transparent benchmarking.Collectively,these contributions provide methodological clarity for researchers and practical guidance for industry,supporting reproducible evaluation and accelerating the safe deployment of higher-level ADS.

</details>


### [16] [Insecure Ingredients? Exploring Dependency Update Patterns of Bundled JavaScript Packages on the Web](https://arxiv.org/abs/2512.15447)
*Ben Swierzy,Marc Ohm,Michael Meier*

Main category: cs.SE

TL;DR: 本文提出了一种名为Aletheia的新方法，用于从JavaScript打包文件中识别软件包版本，从而更准确地分析现代Web应用的依赖更新行为。研究发现，打包引入的依赖比CDN引入的更新更快，漏洞版本更少，但及时更新主要由少数大型厂商推动。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法全面、大规模地分析现代Web应用中JavaScript依赖包的使用和更新情况，尤其是在面对打包后的代码时，导致对实际生产环境中漏洞版本流行程度的理解存在盲区。

Method: 提出Aletheia，一种与具体软件包无关的方法，通过借鉴抄袭检测领域的算法，对JavaScript打包文件进行剖析，以识别其中包含的软件包及其版本。

Result: Aletheia在实践中显著优于现有方法。通过对Tranco排名前10万网站的爬取发现，5%-20%的网站在16周内会更新其依赖；打包引入的依赖比CDN引入的更新快得多，所含已知漏洞版本最多可减少10倍。但及时更新主要由少数大型厂商驱动。

Conclusion: 虽然打包依赖的更新速度更快、安全性更高，但整个生态系统的健康状况可能被少数积极更新的大厂商所掩盖，仅靠量化指标无法全面反映真实的安全态势。

Abstract: Reusable software components, typically distributed as packages, are a central paradigm of modern software development. The JavaScript ecosystem serves as a prime example, offering millions of packages with their use being promoted as idiomatic. However, download statistics on npm raise security concerns as they indicate a high popularity of vulnerable package versions while their real prevalence on production websites remains unknown. Package version detection mechanisms fill this gap by extracting utilized packages and versions from observed artifacts on the web. Prior research focuses on mechanisms for either hand-selected popular packages in bundles or for single-file resources utilizing the global namespace. This does not allow for a thorough analysis of modern web applications' dependency update behavior at scale. In this work, we improve upon this by presenting Aletheia, a package-agnostic method which dissects JavaScript bundles to identify package versions through algorithms originating from the field of plagiarism detection. We show that this method clearly outperforms the existing approaches in practical settings. Furthermore, we crawl the Tranco top 100,000 domains to reveal that 5% - 20% of domains update their dependencies within 16 weeks. Surprisingly, from a longitudinal perspective, bundled packages are updated significantly faster than their CDN-included counterparts, with consequently up to 10 times fewer known vulnerable package versions included. Still, we observe indicators that few widespread vendors seem to be a major driving force behind timely updates, implying that quantitative measures are not painting a complete picture.

</details>


### [17] [A Container-based Approach For Proactive Asset Administration Shell Digital Twins](https://arxiv.org/abs/2512.15452)
*Carsten Ellwein,Jingxi Zhang,Andreas Wortmann,Antony Ayman Alfy Meckhael*

Main category: cs.SE

TL;DR: 本文提出一种基于子模型的架构，将可执行的容器化服务动态集成到资产管理壳（AAS）中，使其从静态数据模型转变为支持运行时动态交互与适应的主动接口。


<details>
  <summary>Details</summary>
Motivation: 现有AAS方法仅作为静态信息模型，缺乏对动态服务集成和系统自适应的支持，尤其缺少对可执行行为（如容器化服务）的整合机制。

Method: 通过扩展现有子模型，嵌入行为定义和触发条件，构建模块化事件驱动架构，实现基于AAS的容器化服务动态部署与交互。

Result: 在三轴铣床案例中验证了所提方法，展示了AAS能够动态调用和执行增值服务，实现从被动数字表示到主动服务接口的转变。

Conclusion: 该研究为AAS引入了动态服务能力，为未来数字孪生环境中的人工智能驱动自适应和系统级智能奠定了基础。

Abstract: In manufacturing, digital twins, realized as Asset Administration Shells (AAS), have emerged as a prevalent practice. These digital replicas, often utilized as structured repositories of asset-related data, facilitate interoperability across diverse systems. However, extant approaches treat the AAS as a static information model, lacking support for dynamic service integration and system adaptation. The existing body of literature has not yet thoroughly explored the potential for integrating executable behavior, particularly in the form of containerized services, into or from the AAS. This integration could serve to enable proactive functionality. In this paper, we propose a submodel-based architecture that introduces a structured service notion to the AAS, enabling services to dynamically interact with and adapt AAS instances at runtime. This concept is implemented through the extension of a submodel with behavioral definitions, resulting in a modular event-driven architecture capable of deploying containerized services based on embedded trigger conditions. The approach is illustrated through a case study on a 3-axis milling machine. Our contribution enables the AAS to serve not only as a passive digital representation but also as an active interface for executing added-value services.%, thereby laying the foundation for future AI-driven adaptation and system-level intelligence in digital twin environments.

</details>


### [18] [On Assessing the Relevance of Code Reviews Authored by Generative Models](https://arxiv.org/abs/2512.15466)
*Robert Heumüller,Frank Ortmeier*

Main category: cs.SE

TL;DR: 本文提出一种基于多主观排序的新评估方法，用于评价大语言模型（如ChatGPT）在代码审查中的表现，并发现其生成的评论质量显著优于人类评论。


<details>
  <summary>Details</summary>
Motivation: 现有代码审查生成评估方法存在不足：自动比对单一标准答案无法反映人类观点的多样性，而主观“有用性”评估又过于模糊。因此需要更可靠、有意义的评估方式。

Method: 构建包含280个独立代码审查请求及其评论的数据集，邀请多位人类评审员对ChatGPT生成的评论与CodeReview StackExchange上的高赞人类评论进行质量排序。

Result: ChatGPT生成的评论在多主观排序中显著优于人类评论，甚至超过StackExchange上被采纳的答案。

Conclusion: 所提出的多主观排序方法能更有效地评估生成式AI在代码审查中的性能，同时提醒人们注意在审查流程中无限制使用AI可能带来的风险。

Abstract: The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of "usefulness", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.

</details>


### [19] [How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?](https://arxiv.org/abs/2512.15468)
*Hua Yang,Alejandro Velasco,Thanh Le-Cong,Md Nazmul Haque,Bowen Xu,Denys Poshyvanyk*

Main category: cs.SE

TL;DR: 本文研究了语义等价的代码变换是否能规避成员推断（MI）检测，发现变量重命名等变换可显著削弱MI效果，暴露了大语言模型训练中许可证合规性检测的漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码领域依赖大量公开和私有代码数据，引发知识产权合规问题；现有成员推断技术可能因语义等价的代码变换而失效，需系统评估其鲁棒性。

Method: 系统评估多种语义等价代码变换规则对成员推断检测的影响，通过实验测量模型准确率变化，并结合因果分析验证变量重命名对MI检测的干扰作用。

Result: 单个变换规则最多仅使模型准确率下降1.5%，表明变换后数据可用于微调；其中RenameVariable规则使MI成功率降低10.19%，且多变换组合未进一步削弱MI效果。

Conclusion: 语义等价的代码变换（尤其是变量重命名）可有效规避成员推断检测，揭示当前大模型代码训练中许可证合规机制存在严重漏洞。

Abstract: The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.
  In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.

</details>


### [20] [WuppieFuzz: Coverage-Guided, Stateful REST API Fuzzing](https://arxiv.org/abs/2512.15554)
*Thomas Rooijakkers,Anne Nijsten,Cristian Daniele,Erieke Weitenberg,Ringo Groenewegen,Arthur Melissen*

Main category: cs.SE

TL;DR: 本文提出了WuppieFuzz，一个基于LibAFL的开源REST API模糊测试工具，支持白盒、灰盒和黑盒测试，利用OpenAPI规范自动生成初始请求序列，并结合REST专用变异策略与覆盖率引导机制，有效探索被测系统的复杂状态，同时提供多种报告以辅助漏洞修复。


<details>
  <summary>Details</summary>
Motivation: 由于REST API广泛用于业务系统且暴露大量端点，存在安全风险，因此需要高效自动化测试方法。传统模糊测试通常需大量人工干预，且难以有效覆盖复杂状态路径，故本文旨在开发一种能自动构建测试用例、减少人工负担并提升覆盖率的REST API模糊测试工具。

Method: WuppieFuzz基于LibAFL构建，利用OpenAPI规范生成初始请求序列，通过REST特定及LibAFL提供的变异器对请求进行变异，并以代码覆盖率作为反馈指导后续请求序列的选择，实现白盒、灰盒和黑盒模糊测试。该工具还自动完成测试桩（harness）生成，并提供多种漏洞报告形式。

Result: 在Petstore API上的实验表明，WuppieFuzz的白盒方法具有良好的鲁棒性，不同能量调度策略对测试效果有显著影响，且随时间推移，端点覆盖率和代码覆盖率均有效提升，验证了该方法的有效性。

Conclusion: WuppieFuzz是一种高效、自动化的REST API模糊测试工具，能够显著减少人工干预，提升测试覆盖率，并有效发现潜在安全漏洞，适用于现代Web服务的安全保障。

Abstract: Many business processes currently depend on web services, often using REST APIs for communication. REST APIs expose web service functionality through endpoints, allowing easy client interaction over the Internet. To reduce the security risk resulting from exposed endpoints, thorough testing is desired. Due to the generally vast number of endpoints, automated testing techniques, like fuzzing, are of interest.
  This paper introduces WuppieFuzz, an open-source REST API fuzzer built on LibAFL, supporting white-box, grey-box and black-box fuzzing. Using an OpenAPI specification, it can generate an initial input corpus consisting of sequences of requests. These are mutated with REST-specific and LibAFL-provided mutators to explore different code paths in the software under test. Guided by the measured coverage, WuppieFuzz then selects which request sequences to send next to reach complex states in the software under test. In this process, it automates harness creation to reduce manual efforts often required in fuzzing. Different kinds of reporting are provided by the fuzzer to help fixing bugs.
  We evaluated our tool on the Petstore API to assess the robustness of the white-box approach and the effectiveness of different power schedules. We further monitored endpoint and code coverage over time to measure the efficacy of the approach.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [21] [GenAI-enabled Residual Motion Estimation for Energy-Efficient Semantic Video Communication](https://arxiv.org/abs/2512.15481)
*Shavbo Salehi,Pedro Enrique Iturria-Rivera,Medhat Elsayed,Majid Bavand,Yigit Ozcan,Melike Erol-Kantarci*

Main category: cs.NI

TL;DR: 本文提出了一种可预测性感知与熵自适应的神经运动估计方法（PENME），通过智能选择运动建模策略和选择性扩散精炼，在显著降低视频传输延迟、数据量和功耗的同时，提升语义通信质量。


<details>
  <summary>Details</summary>
Motivation: 传统香农范式在视频传输中效率低下，因其关注精确表示而非语义内容，导致高带宽、高功耗和高延迟。语义通信通过传输“意义”可有效缓解这些问题，但现有方法在动态视频场景下面临建模灵活性与资源效率的挑战。

Method: PENME 方法根据每帧的运动强度、全局一致性、峰值锐度、异质性和残差误差等五项指标，动态选择残差运动提取模型（CNN、ViT 或光流）；接收端利用运动补偿更新重建帧，并仅对低可预测性或大残差帧使用 LCM-4 扩散模型进行精炼；同时结合信道状态与残差信息自适应分配无线资源块。

Result: 在 Vimeo90K 数据集上的实验表明，PENME 相比传统通信、混合及自适应码率语义通信方法，延迟降低 40%、传输数据减少 90%、吞吐量提升 35%；在语义质量指标上，PSNR 提升约 40%，MS-SSIM 提高约 19%，LPIPS 降低近 35%。

Conclusion: PENME 通过融合可预测性感知的运动建模、选择性扩散精炼与信道感知的资源分配，在保证高质量语义还原的同时，大幅提升了视频语义通信的效率与性能，为未来高效视频传输提供了有效方案。

Abstract: Semantic communication addresses the limitations of the Shannon paradigm by focusing on transmitting meaning rather than exact representations, thereby reducing unnecessary resource consumption. This is particularly beneficial for video, which dominates network traffic and demands high bandwidth and power, making semantic approaches ideal for conserving resources while maintaining quality. In this paper, we propose a Predictability-aware and Entropy-adaptive Neural Motion Estimation (PENME) method to address challenges related to high latency, high bitrate, and power consumption in video transmission. PENME makes per-frame decisions to select a residual motion extraction model, convolutional neural network, vision transformer, or optical flow, using a five-step policy based on motion strength, global motion consistency, peak sharpness, heterogeneity, and residual error. The residual motions are then transmitted to the receiver, where the frames are reconstructed via motion-compensated updates. Next, a selective diffusion-based refinement, the Latent Consistency Model (LCM-4), is applied on frames that trigger refinement due to low predictability or large residuals, while predictable frames skip refinement. PENME also allocates radio resource blocks with awareness of residual motion and channel state, reducing power consumption and bandwidth usage while maintaining high semantic similarity. Our simulation results on the Vimeo90K dataset demonstrate that the proposed PENME method handles various types of video, outperforming traditional communication, hybrid, and adaptive bitrate semantic communication techniques, achieving 40% lower latency, 90% less transmitted data, and 35% higher throughput. For semantic communication metrics, PENME improves PSNR by about 40%, increases MS-SSIM by roughly 19%, and reduces LPIPS by nearly 35%, compared with the baseline methods.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [22] [MS-Index: Fast Top-k Subsequence Search for Multivariate Time Series under Euclidean Distance](https://arxiv.org/abs/2512.14723)
*Jens E. d'Hondt,Teun Kortekaas,Odysseas Papapetrou,Themis Palpanas*

Main category: cs.DB

TL;DR: 本文提出了MS-Index，一种支持在多变量时间序列中按需选择查询通道的精确子序列搜索算法，在34个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 实际应用中，多变量时间序列（MTS）包含多个通道，但并非所有通道都与特定查询相关。例如飞机传感器虽采集大量组件数据，但仅部分通道与特定故障或飞行动作相关。因此需要一种能在查询时动态指定相关通道、高效执行子序列相似性搜索的方法。

Method: 提出名为Multivariate Subsequence Index (MS-Index)的新算法，用于在欧氏距离下对多变量时间序列进行最近邻子序列搜索，并支持查询时任意选择相关通道。该算法是精确的，且查询性能随查询通道数呈次线性扩展。

Result: 在34个数据集上的实验表明，MS-Index在原始和归一化子序列搜索任务中，比当前最先进方法快1到2个数量级。

Conclusion: MS-Index有效解决了多变量时间序列中按需通道选择的子序列搜索问题，具备高效率与精确性，显著优于现有技术。

Abstract: Modern applications frequently collect and analyze temporal data in the form of multivariate time series (MTS) -- time series that contain multiple channels. A common task in this context is subsequence search, which involves identifying all MTS that contain subsequences highly similar to a query time series. In practical scenarios, not all channels of an MTS are relevant to every query. For instance, airplane sensors may gather data on a plethora of components and subsystems, but only a few of these are relevant to a specific query, such as identifying the cause of a malfunctioning landing gear, or a specific flight maneuver. Consequently, the relevant query channels are often specified at query time. In this work, we introduce the Multivariate Subsequence Index (MS-Index), a novel algorithm for nearest neighbor MTS subsequence search under Euclidean distance that supports ad-hoc selection of query channels. The algorithm is exact and demonstrates query performance that scales sublinearly to the number of query channels. We examine the properties of \name with a thorough experimental evaluation over 34 datasets, and show that it outperforms the state-of-the-art one to two orders of magnitude for both raw and normalized subsequences.

</details>


### [23] [Extracting node comparison insights for the interactive exploration of property graphs](https://arxiv.org/abs/2512.15157)
*Cristina Aguiar,Jacques Chabin,Alexandre Chanson,Mirian Halfeld-Ferrari,Nicolas Hiot,Nicolas Labroche,Patrick Marcel,Verónika Peralta,Felipe Vasconcelos*

Main category: cs.DB

TL;DR: 本文提出了一种在属性图中自动提取节点比较的方法，通过设计基于节点上下文的比较指标，并利用启发式算法对节点进行分组，以支持交互式探索性分析。


<details>
  <summary>Details</summary>
Motivation: 现有研究长期关注图中节点的重要性评分（如中心性），但尚未解决基于属性对属性图中的节点进行比较的问题。为支持用户对属性图的交互式探索分析，亟需一种自动提取节点间有意义且非显而易见比较的方法。

Method: 首先利用待比较节点的上下文设计比较指标，然后形式化定义利用这些指标对节点进行分组的问题，以确保所提取的比较既显著又非平凡；接着提出了多种启发式算法来求解该问题。

Result: 在真实属性图数据库上的实验表明，简单的启发式算法可在几分钟内获得有用洞察，而更耗时的启发式算法则能产生更高质量的洞察。

Conclusion: 所提出的方法有效支持了属性图中节点的自动比较与探索性分析，不同复杂度的启发式策略可满足对效率与结果质量的不同需求。

Abstract: While scoring nodes in graphs to understand their importance (e.g., in terms of centrality) has been investigated for decades, comparing nodes in property graphs based on their properties has not, to our knowledge, yet been addressed. In this paper, we propose an approach to automatically extract comparison of nodes in property graphs, to support the interactive exploratory analysis of said graphs. We first present a way of devising comparison indicators using the context of nodes to be compared. Then, we formally define the problem of using these indicators to group the nodes so that the comparisons extracted are both significant and not straightforward. We propose various heuristics for solving this problem. Our tests on real property graph databases show that simple heuristics can be used to obtain insights within minutes while slower heuristics are needed to obtain insights of higher quality.

</details>


### [24] [Graph Pattern-based Association Rules Evaluated Under No-repeated-anything Semantics in the Graph Transactional Setting](https://arxiv.org/abs/2512.15308)
*Basil Ell*

Main category: cs.DB

TL;DR: 本文提出了面向有向标记多重图（如RDF图）的图模式关联规则（GPARs），支持图生成与图合理性评估任务，并在无重复语义下定义了概率空间及多种关联度量，拓展并统一了现有相关形式化方法。


<details>
  <summary>Details</summary>
Motivation: 现有图关联规则形式化方法（如图函数依赖、路径关联规则等）在表达能力和语义覆盖上存在局限，难以有效利用图的拓扑结构进行生成或评估任务，因此需要一种更通用且能兼顾图结构特性的关联规则框架。

Method: 提出图模式关联规则（GPARs）框架，在“无重复任何元素”（no-repeated-anything）语义下对图模式进行评估；构建概率空间，推导置信度、提升度、杠杆率和确信度等指标，并分析其与传统项集关联规则中对应指标的关系及性质保持条件。

Result: GPARs 能够超越现有多种图与关系型关联规则形式，在考虑图拓扑结构的同时支持生成与评估任务；所定义的概率指标在特定条件下保留了经典指标的关键性质。

Conclusion: GPARs 提供了一个更强大、灵活且理论严谨的图关联规则框架，适用于复杂图数据的挖掘与推理，为知识图谱等应用中的规则发现奠定了基础。

Abstract: We introduce graph pattern-based association rules (GPARs) for directed labeled multigraphs such as RDF graphs. GPARs support both generative tasks, where a graph is extended, and evaluative tasks, where the plausibility of a graph is assessed. The framework goes beyond related formalisms such as graph functional dependencies, graph entity dependencies, relational association rules, graph association rules, multi-relation and path association rules, and Horn rules. Given a collection of graphs, we evaluate graph patterns under no-repeated-anything semantics, which allows the topology of a graph to be taken into account more effectively. We define a probability space and derive confidence, lift, leverage, and conviction in a probabilistic setting. We further analyze how these metrics relate to their classical itemset-based counterparts and identify conditions under which their characteristic properties are preserved.

</details>


### [25] [Revisiting Task-Oriented Dataset Search in the Era of Large Language Models: Challenges, Benchmark, and Solution](https://arxiv.org/abs/2512.15363)
*Zixin Wei,Yucan Guo,Jinyang Li,Xiaolin Han,Xiaolong Jin,Chenhao Ma*

Main category: cs.DB

TL;DR: 本文提出了KATS系统，一种端到端的任务导向型数据集搜索方法，通过构建任务-数据集知识图谱和混合查询引擎，显著提升了从科学文献中检索相关数据集的效果与效率，并发布了配套评测基准CS-TDS。


<details>
  <summary>Details</summary>
Motivation: 在数据驱动研究中，研究人员常需根据高层任务描述查找合适的数据集，但现有搜索系统因用户意图模糊、任务与数据集映射缺失、实体歧义等问题难以有效支持该需求。

Method: KATS包含离线知识库构建和在线查询处理两部分：离线阶段采用多智能体协作框架自动构建可动态更新的任务-数据集知识图谱，并引入语义机制解决任务与数据集的实体消歧；在线阶段采用结合向量检索与图排序的混合查询引擎进行高效精准检索。同时构建了CS-TDS评测基准用于系统评估。

Result: 在CS-TDS基准上的实验表明，KATS在效果和效率上均显著优于当前最先进的检索增强生成框架。

Conclusion: KATS为任务导向型数据集发现提供了一个高效、鲁棒的解决方案，并通过新基准推动该领域标准化评估的发展。

Abstract: The search for suitable datasets is the critical "first step" in data-driven research, but it remains a great challenge. Researchers often need to search for datasets based on high-level task descriptions. However, existing search systems struggle with this task due to ambiguous user intent, task-to-dataset mapping and benchmark gaps, and entity ambiguity. To address these challenges, we introduce KATS, a novel end-to-end system for task-oriented dataset search from unstructured scientific literature. KATS consists of two key components, i.e., offline knowledge base construction and online query processing. The sophisticated offline pipeline automatically constructs a high-quality, dynamically updatable task-dataset knowledge graph by employing a collaborative multi-agent framework for information extraction, thereby filling the task-to-dataset mapping gap. To further address the challenge of entity ambiguity, a unique semantic-based mechanism is used for task entity linking and dataset entity resolution. For online retrieval, KATS utilizes a specialized hybrid query engine that combines vector search with graph-based ranking to generate highly relevant results. Additionally, we introduce CS-TDS, a tailored benchmark suite for evaluating task-oriented dataset search systems, addressing the critical gap in standardized evaluation. Experiments on our benchmark suite show that KATS significantly outperforms state-of-the-art retrieval-augmented generation frameworks in both effectiveness and efficiency, providing a robust blueprint for the next generation of dataset discovery systems.

</details>


### [26] [ArcBERT: An LLM-based Search Engine for Exploring Integrated Multi-Omics Metadata](https://arxiv.org/abs/2512.15365)
*Gajendra Doniparthi,Shashank Balu Pandhare,Stefan Deßloch,Timo Mühlhaus*

Main category: cs.DB

TL;DR: ArcBERT 是一个基于大语言模型的系统，用于在科研数据管理中通过自然语言查询实现语义化、结构感知的元数据探索。


<details>
  <summary>Details</summary>
Motivation: 传统科研数据管理中的搜索应用依赖关键词查询，无法理解自然语言；而领域专用的大语言模型在自然语言处理任务中日益普及，促使开发更智能的元数据检索系统。

Method: 提出 ArcBERT 系统，利用在领域内容上训练的大语言模型，结合语义匹配与对元数据结构和层次的理解，支持自然语言查询。

Result: ArcBERT 能有效处理多样化的用户查询模式，优于依赖关键词的传统搜索方法。

Conclusion: ArcBERT 展示了大语言模型在科研数据管理中提升元数据发现与探索能力的潜力，尤其在理解自然语言和元数据结构方面具有优势。

Abstract: Traditional search applications within Research Data Management (RDM) ecosystems are crucial in helping users discover and explore the structured metadata from the research datasets. Typically, text search engines require users to submit keyword-based queries rather than using natural language. However, using Large Language Models (LLMs) trained on domain-specific content for specialized natural language processing (NLP) tasks is becoming increasingly common. We present ArcBERT, an LLM-based system designed for integrated metadata exploration. ArcBERT understands natural language queries and relies on semantic matching, unlike traditional search applications. Notably, ArcBERT also understands the structure and hierarchies within the metadata, enabling it to handle diverse user querying patterns effectively.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [27] [LLMQ: Efficient Lower-Precision Pretraining for Consumer GPUs](https://arxiv.org/abs/2512.15306)
*Erik Schultheis,Dan Alistarh*

Main category: cs.DC

TL;DR: LLMQ 是一个针对中等规模语言模型（3B-32B 参数）在消费级 GPU 上进行高效训练的端到端 CUDA/C++ 实现，通过一系列优化技术克服了显存和通信瓶颈，在单张 16GB 显卡上即可训练 7B 模型，并在 4 张 RTX 4090 上训练 32B 模型，性能媲美昂贵的云端系统。


<details>
  <summary>Details</summary>
Motivation: 消费级 GPU 显存有限、通信速度慢，难以支持中等规模语言模型的训练，亟需一种高效且经济的解决方案。

Method: 采用激活值检查点、卸载（offloading）和基于复制引擎的集合通信等优化策略，在标准 8-bit 训练流程下实现高效训练。

Result: 在单张 16GB 中端游戏显卡上成功训练 7B 模型，在配备 4 张 RTX 4090 的工作站上训练 32B 模型，FLOP 利用率达约 50%，效率可与高端云 GPU 系统相媲美。

Conclusion: LLMQ 证明了在低成本消费级硬件上高效训练中等规模语言模型的可行性，显著降低了大模型训练的门槛。

Abstract: We present LLMQ, an end-to-end CUDA/C++ implementation for medium-sized language-model training, e.g. 3B to 32B parameters, on affordable, commodity GPUs. These devices are characterized by low memory availability and slow communication compared to datacentre-grade GPUs. Consequently, we showcase a range of optimizations that target these bottlenecks, including activation checkpointing, offloading, and copy-engine based collectives. LLMQ is able to train or fine-tune a 7B model on a single 16GB mid-range gaming card, or a 32B model on a workstation equipped with 4 RTX 4090s. This is achieved while executing a standard 8-bit training pipeline, without additional algorithmic approximations, and maintaining FLOP utilization of around 50%. The efficiency of LLMQ rivals that of production-scale systems on much more expensive cloud-grade GPUs.

</details>


### [28] [Optimizing Bloom Filters for Modern GPU Architectures](https://arxiv.org/abs/2512.15595)
*Daniel Jünger,Kevin Kristensen,Yunsong Wang,Xiangyao Yu,Bertil Schmidt*

Main category: cs.DC

TL;DR: 本文针对GPU平台优化布隆过滤器，通过向量化、线程协作和计算延迟三个维度的设计探索，在保持高精度的同时显著提升吞吐量，在B200 GPU上实现接近理论极限的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管布隆过滤器在多种应用中广泛使用，且已有大量针对CPU的优化研究，但面向GPU的高效设计仍不充分。GPU具备大规模线程并行性和高带宽内存，适合加速布隆过滤器操作，因此有必要系统探索其在GPU上的优化空间。

Method: 作者从向量化、线程协作和计算延迟三个维度对GPU上的布隆过滤器进行优化设计，并分析不同参数配置下硬件行为与性能之间的关系。

Result: 所提方法在相同错误率下，批量查询和构建分别比现有最先进方法快11.35倍和15.4倍，在多种配置下达到B200 GPU实际速度极限的92%以上，同时兼顾高吞吐与高精度。

Conclusion: 通过系统性的GPU优化策略，本文成功打破了布隆过滤器在速度与精度之间的传统权衡，为高性能近似成员查询提供了高效可行的实现方案，并将开源其实现。

Abstract: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.
  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [29] [FAME: FPGA Acceleration of Secure Matrix Multiplication with Homomorphic Encryption](https://arxiv.org/abs/2512.15515)
*Zhihan Xu,Rajgopal Kannan,Viktor K. Prasanna*

Main category: cs.AR

TL;DR: 本文提出FAME，一种面向同态加密矩阵乘法（HE MM）的高效FPGA加速器，通过优化片上内存使用和减少片外访存，在Alveo U280上实现比CPU方案平均221倍的加速。


<details>
  <summary>Details</summary>
Motivation: 同态加密（HE）虽能保障云环境下数据隐私，但其高昂计算开销（尤其是矩阵乘法）阻碍了实际应用，亟需高效硬件加速方案。

Method: 作者构建成本模型分析片上内存需求，设计新型同态线性变换（HLT）数据通路以支持细粒度数据复用，并基于此开发首个专用于HE MM的FPGA加速器FAME，支持任意矩阵形状和多种HE参数配置。

Result: 在Alveo U280 FPGA上的实验表明，FAME相比当前最先进的CPU实现平均提速221倍，展现出良好的可扩展性和对大规模连续HE MM及实际工作负载的适用性。

Conclusion: 通过优化内存访问与数据复用，FAME显著提升了HE MM的效率，为隐私保护机器学习等应用提供了实用且可扩展的硬件加速方案。

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data, addressing privacy concerns in cloud computing. However, the high computational cost of HE operations, particularly matrix multiplication (MM), remains a major barrier to its practical deployment. Accelerating homomorphic encrypted MM (HE MM) is therefore crucial for applications such as privacy-preserving machine learning.
  In this paper, we present a bandwidth-efficient FPGA implementation of HE MM. We first develop a cost model to evaluate the on-chip memory requirements for a given set of HE parameters and input matrix sizes. Our analysis shows that optimizing on-chip memory usage is critical for scalable and efficient HE MM. To this end, we design a novel datapath for Homomorphic Linear Transformation (HLT), the primary bottleneck in HE MM. The proposed datapath significantly reduces off-chip memory traffic and on-chip memory demand by enabling fine-grained data reuse. Leveraging this datapath, we introduce FAME, the first FPGA-based accelerator specifically tailored for HE MM. FAME supports arbitrary matrix shapes and is configurable across a wide range of HE parameter sets. We implement FAME on an Alveo U280 FPGA and evaluate its performance across diverse matrix sizes and shapes. Experimental results show that FAME achieves an average speedup of 221x over state-of-the-art CPU-based implementations, demonstrating its scalability and practicality for large-scale consecutive HE MM and real-world workloads.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [30] [HD-Prot: A Protein Language Model for Joint Sequence-Structure Modeling with Continuous Structure Tokens](https://arxiv.org/abs/2512.15133)
*Yi Zhou,Haohao Qu,Yunqing Liu,Shanru Lin,Le Song,Wenqi Fan*

Main category: cs.CE

TL;DR: 本文提出了一种混合扩散蛋白语言模型 HD-Prot，通过在离散蛋白语言模型上引入连续结构隐变量，实现序列与结构的联合建模，在多项任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白语言模型（pLMs）在整合连续结构信息时通常需将结构离散化，导致细粒度信息丢失，限制了多模态 pLM 的性能。作者旨在避免这种信息损失，探索将连续结构嵌入 pLM 的有效方式。

Method: 提出 HD-Prot 模型，在离散 pLM 之上嵌入一个连续值的扩散头，利用统一的吸收扩散过程建模序列（离散）和结构（连续）之间的跨模态依赖，并分别通过分类预测和连续扩散估计每 token 的分布。

Result: HD-Prot 在无条件序列-结构共生成、基序支架设计、蛋白结构预测和逆折叠等任务中取得具有竞争力的结果，性能媲美当前最先进的多模态 pLM，且在有限计算资源下完成开发。

Conclusion: 研究表明，在统一语言模型架构中同时估计分类与连续分布是可行的，为多模态蛋白语言模型提供了一条有前景的新路径。

Abstract: Proteins inherently possess a consistent sequence-structure duality. The abundance of protein sequence data, which can be readily represented as discrete tokens, has driven fruitful developments in protein language models (pLMs). A key remaining challenge, however, is how to effectively integrate continuous structural knowledge into pLMs. Current methods often discretize protein structures to accommodate the language modeling framework, which inevitably results in the loss of fine-grained information and limits the performance potential of multimodal pLMs. In this paper, we argue that such concerns can be circumvented: a sequence-based pLM can be extended to incorporate the structure modality through continuous tokens, i.e., high-fidelity protein structure latents that avoid vector quantization. Specifically, we propose a hybrid diffusion protein language model, HD-Prot, which embeds a continuous-valued diffusion head atop a discrete pLM, enabling seamless operation with both discrete and continuous tokens for joint sequence-structure modeling. It captures inter-token dependencies across modalities through a unified absorbing diffusion process, and estimates per-token distributions via categorical prediction for sequences and continuous diffusion for structures. Extensive empirical results show that HD-Prot achieves competitive performance in unconditional sequence-structure co-generation, motif-scaffolding, protein structure prediction, and inverse folding tasks, performing on par with state-of-the-art multimodal pLMs despite being developed under limited computational resources. It highlights the viability of simultaneously estimating categorical and continuous distributions within a unified language model architecture, offering a promising alternative direction for multimodal pLMs.

</details>


### [31] [A Data-Enhanced Agent-Based Model for Simulating 3D Cancer Spheroid Growth: Integrating Metabolism and Mechanics](https://arxiv.org/abs/2512.15361)
*Pedro Garcia-Gomez,Paula Guerrero-Lopez,Silvia Hervas-Raluy,Jose Manuel Garcia-Aznar*

Main category: cs.CE

TL;DR: 本文提出了一种整合肿瘤代谢与力学的三维多智能体计算模型，用于模拟癌细胞球体的生长，能够定性和定量复现实验结果，并揭示相同条件下癌细胞行为的多样性及其对球体尺寸变异的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 传统癌症研究过于侧重基因层面，忽视了肿瘤微环境中力学与代谢机制的复杂相互作用；目前对这些多因素机制之间的关联理解不足，亟需整合性建模方法加以探索。

Method: 构建了一个基于智能体的计算模型（ABM），将肿瘤代谢与力学机制统一于三维癌细胞球体生长的模拟中，并通过参数校准和调整以适应不同细胞系和行为。

Result: 模型成功复现了实验室中癌细胞球体生长的实验数据，揭示了在相同条件下癌细胞可能呈现的不同动态行为，并展示了通过参数调节适配多种细胞系的能力。

Conclusion: 整合代谢与力学的建模方法在癌症研究中具有高度潜力和灵活性，不仅可辅助体外实验，还可作为独立工具从物理现实中推导结论。

Abstract: Cancer research has shifted from a purely gene-centric view to a more holistic understanding that recognizes the critical role of the tumour microenvironment, where mechanics and metabolism are key drivers of disease progression. However, the intricate interplay between these multifactorial mechanisms remains poorly understood. To address this gap, we present an agent-based computational model (ABM) that integrates tumour metabolism and mechanics to study 3D cancer spheroid growth. Our approach unifies the metabolism and mechanical aspects of tumour development within an integral model for cancer spheroid formation and growth. In addition to that, we performed a computational calibration of the parameters and tested the model versatility to reproduce different cellular behaviours. Our model reproduced qualitatively and quantitatively the experimental results of spheroid growth obtained in the lab and also allowed to discern different dynamics that cancer cells can present under the same conditions, providing insight into the potential factors contributing to the variability in the size of spheroids. Furthermore, it also showed its adaptability to reproduce diferent cell lines and behaviours by tuning its parameters. This study highlights the significant potential and versatility of integrative modelling approaches in the field of cancer research, not only as a tool to complement in vitro studies, but also as independent tools to derive conclusions from the physical reality.

</details>


### [32] [Nonparametric Stochastic Subspaces via the Bootstrap for Characterizing Model Error](https://arxiv.org/abs/2512.15624)
*Akash Yadav,Ruda Zhang*

Main category: cs.CE

TL;DR: 本文提出了一种基于自助法的随机子空间模型，用于在随机降阶建模框架中表征模型误差，以更可靠地量化工程中的前向不确定性。


<details>
  <summary>Details</summary>
Motivation: 工程中前向不确定性量化需同时考虑偶然性和认知性不确定性，而现有方法难以有效区分和表征由参数和模型形式不确定性引起的认知效应，导致模型可信度和预测可靠性受限。

Method: 基于状态向量快照矩阵，利用经验数据分布构建主子空间的抽样分布，从而建立一种假设自由、仅含一个超参数、可自动满足线性约束（如边界条件）且易于实现的随机子空间模型。

Result: 在计算力学和结构动力学的数值算例中，所提方法相比现有方法能更有效地刻画模型误差。

Conclusion: 该方法通过结合经验数据分布与子空间建模，在无需强假设的前提下提升了模型误差的表征能力，为工程不确定性量化提供了实用且高效的工具。

Abstract: Reliable forward uncertainty quantification in engineering requires methods that account for aleatory and epistemic uncertainties. In many applications, epistemic effects arising from uncertain parameters and model form dominate prediction error and strongly influence engineering decisions. Because distinguishing and representing each source separately is often infeasible, their combined effect is typically analyzed using a unified model-error framework. Model error directly affects model credibility and predictive reliability; yet its characterization remains challenging. To address this need, we introduce a bootstrap-based stochastic subspace model for characterizing model error in the stochastic reduced-order modeling framework. Given a snapshot matrix of state vectors, the method leverages the empirical data distribution to induce a sampling distribution over principal subspaces for reduced order modeling. The resulting stochastic model enables improved characterization of model error in computational mechanics compared with existing approaches. The method offers several advantages: (1) it is assumption-free and leverages the empirical data distribution; (2) it enforces linear constraints (such as boundary conditions) by construction; (3) it requires only one hyperparameter, significantly simplifying the training process; and (4) its algorithm is straightforward to implement. We evaluate the method's performance against existing approaches using numerical examples in computational mechanics and structural dynamics.

</details>
