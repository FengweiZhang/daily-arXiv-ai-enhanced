{"id": "2602.20471", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20471", "abs": "https://arxiv.org/abs/2602.20471", "authors": ["Da Chen", "Guangyu Hu", "Kaihong Xu", "Kaichao Liang", "Songjiang Li", "Wei Yang", "XiangYu Wen", "Mingxuan Yuan"], "title": "SegSEM: Enabling and Enhancing SAM2 for SEM Contour Extraction", "comment": "4 pages, 6 figures, accpeted by ISCAS 2026", "summary": "Extracting high-fidelity 2D contours from Scanning Electron Microscope (SEM) images is critical for calibrating Optical Proximity Correction (OPC) models. While foundation models like Segment Anything 2 (SAM2) are promising, adapting them to specialized domains with scarce annotated data is a major challenge. This paper presents a case study on adapting SAM2 for SEM contour extraction in a few-shot setting. We propose SegSEM, a framework built on two principles: a data-efficient fine-tuning strategy that adapts by selectively training only the model's encoders, and a robust hybrid architecture integrating a traditional algorithm as a confidence-aware fallback. Using a small dataset of 60 production images, our experiments demonstrate this methodology's viability. The primary contribution is a methodology for leveraging foundation models in data-constrained industrial applications."}
{"id": "2602.20748", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.20748", "abs": "https://arxiv.org/abs/2602.20748", "authors": ["Sungwoo Park", "Seohyeon Kim", "Min-Soo Kim"], "title": "cuRPQ: A High-Performance GPU-Based Framework for Processing Regular and Conjunctive Regular Path Queries", "comment": "Accepted at SIGMOD 2026. 16 pages, 18 figures", "summary": "Regular path queries (RPQs) are fundamental for path-constrained reachability analysis, and more complex variants such as conjunctive regular path queries (CRPQs) are increasingly used in graph analytics. Evaluating these queries is computationally expensive, but to the best of our knowledge, no prior work has explored GPU acceleration. In this paper, we propose cuRPQ, a high-performance GPU-optimized framework for processing RPQs and CRPQs. cuRPQ addresses the key GPU challenges through a novel traversal algorithm, an efficient visited-set management scheme, and a concurrent exploration-materialization strategy. Extensive experiments show that cuRPQ outperforms state-of-the-art methods by orders of magnitude, without out-of-memory errors."}
{"id": "2602.20515", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20515", "abs": "https://arxiv.org/abs/2602.20515", "authors": ["Rakshith Jayanth", "Viktor Prasanna"], "title": "FAST-Prefill: FPGA Accelerated Sparse Attention for Long Context LLM Prefill", "comment": null, "summary": "In long-context large language model (LLM) inference, the prefill stage dominates computation due to self-attention over the complete input context. Sparse attention significantly reduces self-attention computation by limiting each token's interactions to a subset of tokens. The attention sparsity pattern varies across input prompts, and within a prompt, each attention head can follow a distinct pattern. This makes attention sparsity dynamic. The requirement of generating the sparsity pattern, combined with limited data reuse in attention, shifts the prefill compute to being memory-bound. This, in addition to the huge energy requirements for long-context inference on GPU, motivates FPGAs as good candidates for accelerating dynamic long-context inference.\n  To tackle these challenges, we propose FAST-Prefill, the first FPGA accelerator for long-context prefill-stage inference with dynamic sparse attention. To efficiently generate sparse indices, we propose a \\textit{fused pipeline unit with a memory-aware execution order} to reduce large tensors and irregular memory accesses. To reduce off-chip memory traffic for accessing the KV cache, we utilize the memory hierarchy to design a \\textit{liveness-driven, dual-tier cache}. For high-throughput matrix multiplication, we design a \\textit{hybrid Matrix Processing Unit (MPU)} with DSPs and bit-plane decomposition using LUTs. We implement FAST-Prefill on Alveo U280 and evaluate it on the Llama and Qwen models (batch size = 1) for context lengths ranging from 4K to 128K tokens. We demonstrate an average speedup of up to 2.5$\\times$ in TTFT and 4.5$\\times$ improvement in energy efficiency over GPU implementation on Nvidia A5000 GPU."}
{"id": "2602.20952", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2602.20952", "abs": "https://arxiv.org/abs/2602.20952", "authors": ["Zhen Lv", "Cong Cao", "Hongwei Huo", "Jiangtao Cui", "Yanguo Peng", "Hui Li", "Yingfan Liu"], "title": "RISK: Efficiently processing rich spatial-keyword queries on encrypted geo-textual data", "comment": "15 pages, 10 figures, IEEE ICDE", "summary": "Symmetric searchable encryption (SSE) for geo-textual data has attracted significant attention. However, existing schemes rely on task-specific, incompatible indices for isolated specific secure queries (e.g., range or k-nearest neighbor spatial-keyword queries), limiting practicality due to prohibitive multi-index overhead. To address this, we propose RISK, a model for rich spatial-keyword queries on encrypted geo-textual data. In a textual-first-then-spatial manner, RISK is built on a novel k-nearest neighbor quadtree (kQ-tree) that embeds representative and regional nearest neighbors, with the kQ-tree further encrypted using standard cryptographic tools (e.g., keyed hash functions and symmetric encryption). Overall, RISK seamlessly supports both secure range and k-nearest neighbor queries, is provably secure under IND-CKA2 model, and extensible to multi-party scenarios and dynamic updates. Experiments on three real-world and one synthetic datasets show that RISK outperforms state-of-the-art methods by at least 0.5 and 4 orders of magnitude in response time for 1% range queries and 10-nearest neighbor queries, respectively."}
{"id": "2602.20662", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20662", "abs": "https://arxiv.org/abs/2602.20662", "authors": ["Hongyi Guan", "Yijia Zhang", "Wenqiang Wang", "Yizhao Gao", "Shijie Cao", "Chen Zhang", "Ningyi Xu"], "title": "TOM: A Ternary Read-only Memory Accelerator for LLM-powered Edge Intelligence", "comment": "13 pages", "summary": "The deployment of Large Language Models (LLMs) for real-time intelligence on edge devices is rapidly growing. However, conventional hardware architectures face a fundamental memory wall challenge, where limited on-device memory capacity and bandwidth severely constrain the size of deployable models and their inference speed, while also limiting on-device adaptation. To address this challenge, we propose TOM, a hybrid ROM-SRAM accelerator co-designed with ternary quantization, which balances extreme density with on-device tunability. TOM exploits the synergy between ternary quantization and ROM to achieve extreme memory density and bandwidth, while preserving flexibility through a hybrid ROM-SRAM architecture designed for QLoRA-based tunability. Specifically, we introduce: (1) a sparsity-aware ROM architecture that synthesizes ternary weights as standard-cell logic, eliminating area overhead from zero-valued bits; (2) a distributed processing architecture that co-locates high-density ROM banks with flexible SRAM-based QLoRA adapters and compute units; and (3) a workload-aware dynamic power gating scheme that exploits the logic-based nature of ROM to power down inactive banks, minimizing dynamic energy consumption. TOM achieves an inference throughput of 3,306 TPS using BitNet-2B model, demonstrating its effectiveness in delivering real-time, energy-efficient edge intelligence."}
{"id": "2602.20802", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.20802", "abs": "https://arxiv.org/abs/2602.20802", "authors": ["Philippos Papaphilippou"], "title": "LUTstructions: Self-loading FPGA-based Reconfigurable Instructions", "comment": null, "summary": "General-purpose processors feature a limited number of instructions based on an instruction set. They can be numerous, such as with vector extensions that include hundreds or thousands of instructions, but this comes at a cost; they are often unable to express arbitrary tasks efficiently. This paper explores the concept of having reconfigurable instructions by incorporating reconfigurable areas in a softcore. It follows a relatively-recently proposed computer architecture concept for seamlessly loading instruction implementation-carrying bitstreams from main memory. The resulting softcore is entirely evaluated on an FPGA, essentially having an FPGA-on-an-FPGA for the instruction implementations, with no notable operating frequency overhead. This is achieved with a custom FPGA architecture called LUTstruction, which is tailored towards low-latency for custom instructions and wide reconfiguration, as well as a soft implementation for the purposes of architectural exploration."}
{"id": "2602.20493", "categories": ["cs.NI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.20493", "abs": "https://arxiv.org/abs/2602.20493", "authors": ["Xiaohang Nie", "Zihan Guo", "Youliang Chen", "Yuanjian Zhou", "Weinan Zhang"], "title": "AWCP: A Workspace Delegation Protocol for Deep-Engagement Collaboration across Remote Agents", "comment": "16 pages, 7 figure, tech report of Agent Workspace Collaboration Protocol", "summary": "The rapid evolution of Large Language Model (LLM)-based autonomous agents is reshaping the digital landscape toward an emerging Agentic Web, where increasingly specialized agents must collaborate to accomplish complex tasks. However, existing collaboration paradigms are constrained to message passing, leaving execution environments as isolated silos. This creates a context gap: agents cannot directly manipulate files or invoke tools in a peer's environment, and must instead resort to costly, error-prone environment reconstruction. We introduce the Agent Workspace Collaboration Protocol (AWCP), which bridges this gap through temporary workspace delegation inspired by the Unix philosophy that everything is a file. AWCP decouples a lightweight control plane from pluggable transport mechanisms, allowing a Delegator to project its workspace to a remote Executor, who then operates on the shared files directly with unmodified local toolchains. We provide a fully open-source reference implementation with MCP tool integration and validate the protocol through live demonstrations of asymmetric collaboration, where agents with complementary capabilities cooperate through delegated workspaces. By establishing the missing workspace layer in the agentic protocol stack, AWCP paves the way for a universally interoperable agent ecosystem in which collaboration transcends message boundaries. The protocol and reference implementation are publicly available at https://github.com/SII-Holos/awcp."}
{"id": "2602.20206", "categories": ["cs.SE", "cs.AI", "cs.CY", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.20206", "abs": "https://arxiv.org/abs/2602.20206", "authors": ["Sreecharan Sankaranarayanan"], "title": "Mitigating \"Epistemic Debt\" in Generative AI-Scaffolded Novice Programming using Metacognitive Scripts", "comment": null, "summary": "The democratization of Large Language Models (LLMs) has given rise to ``Vibe Coding,\" a workflow where novice programmers prioritize semantic intent over syntactic implementation. While this lowers barriers to entry, we hypothesize that without pedagogical guardrails, it is fundamentally misaligned with cognitive skill acquisition. Drawing on the distinction between Cognitive Offloading and Cognitive Outsourcing, we argue that unrestricted AI encourages novices to outsource the Intrinsic Cognitive Load required for schema formation, rather than merely offloading Extraneous Load. This accumulation of ``Epistemic Debt\" creates ``Fragile Experts\" whose high functional utility masks critically low corrective competence.\n  To quantify and mitigate this debt, we conducted a between-subjects experiment (N=78) using a custom Cursor IDE plugin backed by Claude 3.5 Sonnet. Participants represented \"AI-Native\" learners across three conditions: Manual (Control), Unrestricted AI (Outsourcing), and Scaffolded AI (Offloading). The Scaffolded condition utilized a novel ``Explanation Gate,\" leveraging a real-time LLM-as-a-Judge framework to enforce a ``Teach-Back\" protocol before generated code could be integrated.\n  Results reveal a ``Collapse of Competence\": while Unrestricted AI users matched the productivity of the Scaffolded group (p < .001 vs. Manual), they suffered a 77% failure rate in a subsequent AI-Blackout maintenance task, compared to only 39% in the Scaffolded group. Qualitative analysis suggests that successful vibe coders naturally engage in self-scaffolding, treating the AI as a consultant rather than a contractor. We discuss the implications for the maintainability of AI-generated software and propose that future learning systems must enforce Metacognitive Friction to prevent the mass production of unmaintainable code."}
{"id": "2602.20826", "categories": ["cs.OS"], "pdf": "https://arxiv.org/pdf/2602.20826", "abs": "https://arxiv.org/abs/2602.20826", "authors": ["Yuanhai Zhang", "Songyang He", "Ruizhe Gou", "Mingyue Cui", "Boyang Li", "Shuai Zhao", "Kai Huang"], "title": "Exploiting Dependency and Parallelism: Real-Time Scheduling and Analysis for GPU Tasks", "comment": null, "summary": "With the rapid advancement of Artificial Intelligence, the Graphics Processing Unit (GPU) has become increasingly essential across a growing number of safety-critical application domains. Applying a GPU is indispensable for parallel computing; however, the complex data dependencies and resource contention across kernels within a GPU task may unpredictably delay its execution time. To address these problems, this paper presents a scheduling and analysis method for Directed Acyclic Graph (DAG)-structured GPU tasks. Given a DAG representation, the proposed scheduling scales the kernel-level parallelism and establishes inter-kernel dependencies to provide a reduced and predictable DAG response time. The corresponding timing analysis yields a safe yet nonpessimistic makespan bound without any assumption on kernel priorities. The proposed method is implemented using the standard CUDA API, requiring no additional software or hardware support. Experimental results under synthetic and real-world benchmarks demonstrate that the proposed approach effectively reduces the worst-case makespan and measured task execution time compared to the existing methods up to 32.8% and 21.3%, respectively."}
{"id": "2602.20489", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.20489", "abs": "https://arxiv.org/abs/2602.20489", "authors": ["Minseop Kim", "Jaeeun Kwon", "Hanbyeol Park", "Kikun Park", "Taekhyun Park", "Hyerim Bae"], "title": "Application of Large Language Models for Container Throughput Forecasting: Incorporating Contextual Information in Port Logistics", "comment": null, "summary": "Recent advancements in generative artificial intelligence (AI) have demonstrated its substantial potential in various fields. However, its application in port logistics remains underexplored. Ports are complex operational environments where diverse types of contextual information coexist, making them a promising domain for the implementation of generative AI and highlighting the urgency of related research. In this study, we applied a large language model (LLM)-a leading generative AI technique-to forecast container throughput, which is a critical challenge in port logistics. To this end, we adopted a state-of-the-art LLM approach and proposed a novel prompt structure designed to incorporate the contextual characteristics of port operations. Extensive experiments confirm the superiority of our method, showing that the proposed approach outperforms competitive benchmark models. Furthermore, additional experiments revealed that LLMs can effectively learn and utilize multiple layers of contextual information for inference in port logistics. Based on these findings, we explore the key constraints affecting LLM adoption in this domain and outline future research directions aimed at addressing them. Accordingly, we offer both technical and practical insights to support the effective deployment of generative AI in port logistics."}
{"id": "2602.20724", "categories": ["cs.NI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.20724", "abs": "https://arxiv.org/abs/2602.20724", "authors": ["Siya Chen", "Chee Wei Tan", "H. Vincent Poor"], "title": "Deep Reinforcement Learning Based Block Coordinate Descent for Downlink Weighted Sum-rate Maximization on AI-Native Wireless Networks", "comment": "This paper has been accepted by IEEE Transactions on Wireless Communications in 2025", "summary": "This paper introduces a deep reinforcement learning-based block coordinate descent (DRL-based BCD) algorithm to address the nonconvex weighted sum-rate maximization (WSRM) problem with a total power constraint. Firstly, we present an efficient block coordinate descent (BCD) method to solve the problem. We then integrate deep reinforcement learning (DRL) techniques into the BCD method and propose the DRL-based BCD algorithm. This approach combines the data-driven learning capability of machine learning techniques with the navigational and decision-making characteristics of the optimization-theoretic-based BCD method. This combination significantly improves the algorithm's performance by reducing its sensitivity to initial points and mitigating the risk of entrapment in local optima. The primary advantages of the proposed DRL-based BCD algorithm lie in its ability to adhere to the constraints of the WSRM problem and significantly enhance accuracy, potentially achieving the exact optimal solution. Moreover, unlike many pure machine-learning approaches, the DRL-based BCD algorithm capitalizes on the underlying theoretical analysis of the WSRM problem's structure. This enables it to be easily trained and computationally efficient while maintaining a level of interpretability. Through numerical experiments, the DRL-based BCD algorithm demonstrates substantial advantages in effectiveness, efficiency, robustness, and interpretability for maximizing sum rates, which also provides valuable potential for designing resource-constrained AI-native wireless optimization strategies in next-generation wireless networks."}
{"id": "2602.20213", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.20213", "abs": "https://arxiv.org/abs/2602.20213", "authors": ["Jingwei Shi", "Xinxiang Yin", "Jing Huang", "Jinman Zhao", "Shengyu Tao"], "title": "CodeHacker: Automated Test Case Generation for Detecting Vulnerabilities in Competitive Programming Solutions", "comment": null, "summary": "The evaluation of Large Language Models (LLMs) for code generation relies heavily on the quality and robustness of test cases. However, existing benchmarks often lack coverage for subtle corner cases, allowing incorrect solutions to pass. To bridge this gap, we propose CodeHacker, an automated agent framework dedicated to generating targeted adversarial test cases that expose latent vulnerabilities in program submissions. Mimicking the hack mechanism in competitive programming, CodeHacker employs a multi-strategy approach, including stress testing, anti-hash attacks, and logic-specific targeting to break specific code submissions. To ensure the validity and reliability of these attacks, we introduce a Calibration Phase, where the agent iteratively refines its own Validator and Checker via self-generated adversarial probes before evaluating contestant code.Experiments demonstrate that CodeHacker significantly improves the True Negative Rate (TNR) of existing datasets, effectively filtering out incorrect solutions that were previously accepted. Furthermore, generated adversarial cases prove to be superior training data, boosting the performance of RL-trained models on benchmarks like LiveCodeBench."}
{"id": "2602.20540", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.20540", "abs": "https://arxiv.org/abs/2602.20540", "authors": ["Minseop Kim", "Takhyeong Kim", "Taekhyun Park", "Hanbyeol Park", "Hyerim Bae"], "title": "Generative AI and Machine Learning Collaboration for Container Dwell Time Prediction via Data Standardization", "comment": null, "summary": "Import container dwell time (ICDT) prediction is a key task for improving productivity in container terminals, as accurate predictions enable the reduction of container re-handling operations by yard cranes. Achieving this objective requires accurately predicting the dwell time of individual containers. However, the primary determinants of dwell time-owner information and cargo information-are recorded as unstructured text, which limits their effective use in machine learning models. This study addresses this limitation by proposing a collaborative framework that integrates generative artificial intelligence (Gen AI) with machine learning. The proposed framework employs Gen AI to standardize unstructured information into standard international codes, with dynamic re-prediction triggered by electronic data interchange state updates, enabling the machine learning model to predict ICDT accurately. Extensive experiments conducted on real container terminal data demonstrate that the proposed methodology achieves a 13.88% improvement in mean absolute error compared to conventional models that do not utilize standardized information. Furthermore, applying the improved predictions to container stacking strategies achieves up to 14.68% reduction in the number of relocations, thereby empirically validating the potential of Gen AI to enhance productivity in container terminal operations. Overall, this study provides both technical and methodological insights into the adoption of Gen AI in port logistics and its effectiveness."}
{"id": "2602.20924", "categories": ["cs.NI", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20924", "abs": "https://arxiv.org/abs/2602.20924", "authors": ["Alagappan Ramanathan", "Eunju Kang", "Dongsu Han", "Sangeetha Abdu Jyothi"], "title": "Airavat: An Agentic Framework for Internet Measurement", "comment": null, "summary": "Internet measurement faces twin challenges: complex analyses require expert-level orchestration of tools, yet even syntactically correct implementations can have methodological flaws and can be difficult to verify. Democratizing measurement capabilities thus demands automating both workflow generation and verification against methodological standards established through decades of research.\n  We present Airavat, the first agentic framework for Internet measurement workflow generation with systematic verification and validation. Airavat coordinates a set of agents mirroring expert reasoning: three agents handle problem decomposition, solution design, and code implementation, with assistance from a registry of existing tools. Two specialized engines ensure methodological correctness: a Verification Engine evaluates workflows against a knowledge graph encoding five decades of measurement research, while a Validation Engine identifies appropriate validation techniques grounded in established methodologies. Through four Internet measurement case studies, we demonstrate that Airavat (i) generates workflows matching expert-level solutions, (ii) makes sound architectural decisions, (iii) addresses novel problems without ground truth, and (iv) identifies methodological flaws missed by standard execution-based testing."}
{"id": "2602.20284", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20284", "abs": "https://arxiv.org/abs/2602.20284", "authors": ["Han Fu", "Andreas Ermedahl", "Sigrid Eldh", "Kristian Wiklund", "Philipp Haller", "Cyrille Artho"], "title": "PhantomRun: Auto Repair of Compilation Errors in Embedded Open Source Software", "comment": "13 pages, 5 figures, Mining Software Repositories 2026 (MSR 2026) , Rio de Janeiro, Brazil, 13-14 April 2026", "summary": "Continuous Integration (CI) pipelines for embedded software sometimes fail during compilation, consuming significant developer time for debugging. We study four major open-source embedded system projects, spanning over 4000 build failures from the project's CI runs. We find that hardware dependencies account for the majority of compilation failures, followed by syntax errors and build-script issues. Most repairs need relatively small changes, making automated repair potentially suitable as long as the diverse setups and lack of test data can be handled.\n  In this paper, we present PhantomRun, an automated framework that leverages large language models (LLMs) to generate and validate fixes for CI compilation failures. The framework addresses the challenge of diverse build infrastructures and tool chains across embedded system projects by providing an adaptation layer for GitHub Actions and GitLab CI and four different build systems. PhantomRun utilizes build logs, source code, historical fixes, and compiler error messages to synthesize fixes using LLMs. Our evaluations show that PhantomRun successfully repairs up to 45% of CI compilation failures across the targeted projects, demonstrating the viability of LLM-based repairs for embedded-system CI pipelines."}
{"id": "2602.20609", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.20609", "abs": "https://arxiv.org/abs/2602.20609", "authors": ["Zhenhua Zheng", "Lu Zhang", "Junhong Zou", "Shitong Liu", "Zhen Lei", "Xiangyu Zhu", "Zhiyong Liu"], "title": "GA-Field: Geometry-Aware Vehicle Aerodynamic Field Prediction", "comment": null, "summary": "Accurate aerodynamic field prediction is crucial for vehicle drag evaluation, but the computational cost of high-fidelity CFD hinders its use in iterative design workflows. While learning-based methods enable fast and scalable inference, accurately aerodynamic fields modeling remains challenging, as it demands capturing both long-range geometric effects and fine-scale flow structures. Existing approaches typically encode geometry only once at the input and formulate prediction as a one-shot mapping, which often leads to diluted global shape awareness and insufficient resolution of sharp local flow variations. To address these issues, we propose GA-Field, a Geometry-Aware Field prediction network that introduces two complementary design components: (i) a global geometry injection mechanism that repeatedly conditions the network on a compact 3D geometry embedding at multiple stages to preserve long-range geometric consistency, and (ii) a coarse-to-fine field refinement strategy to recover sharp local aerodynamic details. GA-Field achieves new state-of-the-art performance on ShapeNet-Car and the large-scale DrivAerNet++ benchmark for surface pressure, wall shear stress, and 3D velocity prediction tasks, while exhibiting strong out-of-distribution generalization across different vehicle categories."}
{"id": "2602.20341", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20341", "abs": "https://arxiv.org/abs/2602.20341", "authors": ["Ignacio Amores-Sesar", "Mirza Ahad Baig", "Seth Gilbert", "Ray Neiheiser", "Michelle X. Yeo"], "title": "The Tragedy of Chain Commons", "comment": null, "summary": "Byzantine Fault Tolerant (BFT) consensus forms the foundation of many modern blockchains striving for both high throughput and low latency. A growing bottleneck is transaction execution and validation on the critical path of consensus, which has led to modular decoupled designs that separate ordering from execution: Consensus orders only metadata, while transactions are executed and validated concurrently. While this approach improves performance, it can leave invalid transactions in the ledger, increasing storage costs and enabling new forms of strategic behavior. We present the first systematic study of this setting, providing a formal framework to reason about the interaction between consensus and execution. Using this framework, we show that the decoupled design enables a previously unidentified attack, which we term gaslighting. We prove a fundamental trade-off between resilience to this attack and resource capacity utilization, where both are impossible to achieve deterministically in the decoupled model. To address this trade-off, we discuss an intermediate model for leader-based protocols that is robust to gaslighting attacks while achieving high throughput and low latency."}
{"id": "2602.20292", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.20292", "abs": "https://arxiv.org/abs/2602.20292", "authors": ["Sebastian Lobentanzer"], "title": "Quantifying the Expectation-Realisation Gap for Agentic AI Systems", "comment": "9 pages, no figures", "summary": "Agentic AI systems are deployed with expectations of substantial productivity gains, yet rigorous empirical evidence reveals systematic discrepancies between pre-deployment expectations and post-deployment outcomes. We review controlled trials and independent validations across software engineering, clinical documentation, and clinical decision support to quantify this expectation-realisation gap. In software development, experienced developers expected a 24% speedup from AI tools but were slowed by 19% -- a 43 percentage-point calibration error. In clinical documentation, vendor claims of multi-minute time savings contrast with measured reductions of less than one minute per note, and one widely deployed tool showed no statistically significant effect. In clinical decision support, externally validated performance falls substantially below developer-reported metrics. These shortfalls are driven by workflow integration friction, verification burden, measurement construct mismatches, and systematic heterogeneity in treatment effects. The evidence motivates structured planning frameworks that require explicit, quantified benefit expectations with human oversight costs factored in."}
{"id": "2602.20928", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2602.20928", "abs": "https://arxiv.org/abs/2602.20928", "authors": ["Odysseas Vlachopoulos", "Niklas Luther", "Andrej Ceglar", "Andrea Toreti", "Elena Xoplaki"], "title": "Surrogate impact modelling for crop yield assessment", "comment": null, "summary": "This study presents the Surrogate Engine for Crop Simulations (SECS) a group of deep-learning models that emulate the process-based ECroPS model using only daily maximum and minimum temperature and precipitation. In this study we emulate grain maize and spring barley. Trained on ERA5-forced ECroPS simulations, SECS reproduces crop growth dynamics and harvest timing with high fidelity. Critically, SECS extremely reduces computational costs enabling ensemble-scale inference suitable for operational pipelines. When driven by seasonal data, SECS captures the interannual and spatial patterns of crop stress across Europe and aligns with independent monitoring, supporting its use as a probabilistic Areas of Concern indicator for early warning. Under CMIP6 SSP3-7.0 and SSP5-8.5 scenarios, SECS consistently identifies the Mediterranean basin as a persistent hotspot of yield risk through mid-century, with central-northern Europe showing mixed signals. These results demonstrate that a streamlined, data-efficient emulator can provide robust seasonal-to-climate risk assessments at continental scale."}
{"id": "2602.20444", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20444", "abs": "https://arxiv.org/abs/2602.20444", "authors": ["Paul Borrill"], "title": "Circumventing the FLP Impossibility Result with Open Atomic Ethernet", "comment": "12 pages, 3 figures, 1 table", "summary": "The Fischer--Lynch--Paterson (FLP) impossibility result is widely regarded as one of the most fundamental negative results in distributed computing: no deterministic protocol can guarantee consensus in an asynchronous system with even one faulty process. For forty years, the field has treated this as an immovable constraint, designing around it with randomized protocols, failure detectors, and weakened consistency models. This essay argues that FLP is not a law of physics but a theorem about a particular system model -- and that Open Atomic Ethernet (OAE) circumvents it by rejecting the asynchronous model at its foundation. We introduce the term bisynchronous to describe OAE's key property: bounded-time bilateral resolution in which both parties reach common knowledge of outcome at every round boundary -- a strictly stronger guarantee than synchrony alone. By constructing a bisynchronous, swap-based protocol at Layer 2, OAE sidesteps the load-bearing assumptions of FLP's asynchronous model, achieving deterministic atomic coordination without violating any impossibility result."}
{"id": "2602.20334", "categories": ["cs.SE", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.20334", "abs": "https://arxiv.org/abs/2602.20334", "authors": ["Chengjie Lu", "Jiahui Wu", "Shaukat Ali", "Malaika Din Hashmi", "Sebastian Mathias Thomle Mason", "Francois Picard", "Mikkel Labori Olsen", "Thomas Peyrucain"], "title": "UAMTERS: Uncertainty-Aware Mutation Analysis for DL-enabled Robotic Software", "comment": "23 pages, 6 figures, 7 tables", "summary": "Self-adaptive robots adjust their behaviors in response to unpredictable environmental changes. These robots often incorporate deep learning (DL) components into their software to support functionality such as perception, decision-making, and control, enhancing autonomy and self-adaptability. However, the inherent uncertainty of DL-enabled software makes it challenging to ensure its dependability in dynamic environments. Consequently, test generation techniques have been developed to test robot software, and classical mutation analysis injects faults into the software to assess the test suite's effectiveness in detecting the resulting failures. However, there is a lack of mutation analysis techniques to assess the effectiveness under the uncertainty inherent to DL-enabled software. To this end, we propose UAMTERS, an uncertainty-aware mutation analysis framework that introduces uncertainty-aware mutation operators to explicitly inject stochastic uncertainty into DL-enabled robotic software, simulating uncertainty in its behavior. We further propose mutation score metrics to quantify a test suite's ability to detect failures under varying levels of uncertainty. We evaluate UAMTERS across three robotic case studies, demonstrating that UAMTERS more effectively distinguishes test suite quality and captures uncertainty-induced failures in DL-enabled software."}
{"id": "2602.20450", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.20450", "abs": "https://arxiv.org/abs/2602.20450", "authors": ["Nihal Balivada", "Shrey Gupta", "Shashank Shreedhar Bhatt", "Suyash Gupta"], "title": "Heterogeneity-Aware Client Selection Methodology For Efficient Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables a distributed client-server architecture where multiple clients collaboratively train a global Machine Learning (ML) model without sharing sensitive local data. However, FL often results in lower accuracy than traditional ML algorithms due to statistical heterogeneity across clients. Prior works attempt to address this by using model updates, such as loss and bias, from client models to select participants that can improve the global model's accuracy. However, these updates neither accurately represent a client's heterogeneity nor are their selection methods deterministic. We mitigate these limitations by introducing Terraform, a novel client selection methodology that uses gradient updates and a deterministic selection algorithm to select heterogeneous clients for retraining. This bi-pronged approach allows Terraform to achieve up to 47 percent higher accuracy over prior works. We further demonstrate its efficiency through comprehensive ablation studies and training time analyses, providing strong justification for the robustness of Terraform."}
{"id": "2602.20478", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20478", "abs": "https://arxiv.org/abs/2602.20478", "authors": ["Aristidis Vasilopoulos"], "title": "Codified Context: Infrastructure for AI Agents in a Complex Codebase", "comment": "9 pages, 4 figures, companion repository: https://github.com/arisvas4/codified-context-infrastructure, code DOI: 10.5281/zenodo.18746623", "summary": "LLM-based agentic coding assistants lack persistent memory: they lose coherence across sessions, forget project conventions, and repeat known mistakes. Recent studies characterize how developers configure agents through manifest files, but an open challenge remains how to scale such configurations for large, multi-agent projects. This paper presents a three-component codified context infrastructure developed during construction of a 108,000-line C# distributed system: (1) a hot-memory constitution encoding conventions, retrieval hooks, and orchestration protocols; (2) 19 specialized domain-expert agents; and (3) a cold-memory knowledge base of 34 on-demand specification documents. Quantitative metrics on infrastructure growth and interaction patterns across 283 development sessions are reported alongside four observational case studies illustrating how codified context propagates across sessions to prevent failures and maintain consistency. The framework is published as an open-source companion repository."}
{"id": "2602.20561", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20561", "abs": "https://arxiv.org/abs/2602.20561", "authors": ["Sana Taghipour Anvar", "David Kaeli"], "title": "A Granularity Characterization of Task Scheduling Effectiveness", "comment": null, "summary": "Task-based runtime systems provide flexible load balancing and portability for parallel scientific applications, but their strong scaling is highly sensitive to task granularity. As parallelism increases, scheduling overhead may transition from negligible to dominant, leading to rapid drops in performance for some algorithms, while remaining negligible for others. Although such effects are widely observed empirically, there is a general lack of understanding how algorithmic structure impacts whether dynamic scheduling is always beneficial. In this work, we introduce a granularity characterization framework that directly links scheduling overhead growth to task-graph dependency topology. We show that dependency structure, rather than problem size alone, governs how overhead scales with parallelism. Based on this observation, we characterize execution behavior using a simple granularity measure that indicates when scheduling overhead can be amortized by parallel computation and when scheduling overhead dominates performance. Through experimental evaluation on representative parallel workloads with diverse dependency patterns, we demonstrate that the proposed characterization explains both gradual and abrupt strong-scaling breakdowns observed in practice. We further show that overhead models derived from dependency topology accurately predict strong-scaling limits and enable a practical runtime decision rule for selecting dynamic or static execution without requiring exhaustive strong-scaling studies or extensive offline tuning."}
{"id": "2602.20598", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20598", "abs": "https://arxiv.org/abs/2602.20598", "authors": ["Shoma Ansai", "Masaki Waga"], "title": "A Case Study on Runtime Verification of a Continuous Deployment Process", "comment": "Presented at the Runtime Verification Case-Studies Workshop 2025 (RVCase'25), with no formal proceedings", "summary": "We report our experience in applying runtime monitoring to a FluxCD-based continuous deployment (CD) process. Our target system consists of GitHub Actions, GitHub Container Registry (GHCR), FluxCD, and an application running on Kubernetes. We monitored its logs using SyMon. In our setting, we regard a deployment update as detected when FluxCD's polling log resolves the latest image tag. Through the case study, we found that FluxCD did not always detect a new image within five minutes after it was pushed to GHCR, whereas it always did so within ten minutes in the collected logs. Moreover, our results show that SyMon is fast enough for near-real-time monitoring in our setting."}
{"id": "2602.20656", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.20656", "abs": "https://arxiv.org/abs/2602.20656", "authors": ["Guanbin Xu", "ZhenGuo Xu", "Yuzhe Li", "Youhui Bai", "Ping Gong", "Chaoyi Ruan", "Cheng Li"], "title": "Lagom: Unleashing the Power of Communication and Computation Overlapping for Distributed LLM Training", "comment": "6 pages, 8 figures", "summary": "Overlapping communication with computation is crucial for distributed large-model training, yet optimizing it - especially when computation becomes the bottleneck-remains challenging. We present Lagom, a system that co-tunes communication parameters to balance resource usage between computation and communication. By introducing a unified cost model and a priority-based search algorithm, Lagom reduces optimization complexity from exponential to linear. Evaluations on high- and low-bandwidth GPU clusters show that Lagom achieves 1.07-1.33x and 1.03-1.27x speedup over NCCL and AutoCCL across diverse models and parallelizations."}
{"id": "2602.20610", "categories": ["cs.SE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.20610", "abs": "https://arxiv.org/abs/2602.20610", "authors": ["Cuong Chi Le", "Minh V. T Pham", "Tung Vu Duy", "Cuong Duc Van", "Huy N. Phan", "Hoang N. Phan", "Tien N. Nguyen"], "title": "SpecMind: Cognitively Inspired, Interactive Multi-Turn Framework for Postcondition Inference", "comment": null, "summary": "Specifications are vital for ensuring program correctness, yet writing them manually remains challenging and time-intensive. Recent large language model (LLM)-based methods have shown successes in generating specifications such as postconditions, but existing single-pass prompting often yields inaccurate results. In this paper, we present SpecMind, a novel framework for postcondition generation that treats LLMs as interactive and exploratory reasoners rather than one-shot generators. SpecMind employs feedback-driven multi-turn prompting approaches, enabling the model to iteratively refine candidate postconditions by incorporating implicit and explicit correctness feedback, while autonomously deciding when to stop. This process fosters deeper code comprehension and improves alignment with true program behavior via exploratory attempts. Our empirical evaluation shows that SpecMind significantly outperforms state-of-the-art approaches in both accuracy and completeness of generated postconditions."}
{"id": "2602.20887", "categories": ["cs.DC", "cs.CG"], "pdf": "https://arxiv.org/pdf/2602.20887", "abs": "https://arxiv.org/abs/2602.20887", "authors": ["David Knapp", "Johannes Albrecht Holke", "Thomas Spenke", "Carsten Burstedde"], "title": "A Morton-Type Space-Filling Curve for Pyramid Subdivision and Hybrid Adaptive Mesh Refinement", "comment": null, "summary": "The forest-of-refinement-trees approach allows for dynamic adaptive mesh refinement (AMR) at negligible cost. While originally developed for quadrilateral and hexahedral elements, previous work established the theory and algorithms for unstructured meshes of simplicial and prismatic elements. To harness the full potential of tree-based AMR for three-dimensional mixed-element meshes, this paper introduces the pyramid as a new functional element type; its primary purpose is to connect tetrahedral and hexahedral elements without hanging edges.We present a well-defined space-filling curve (SFC) for the pyramid and detail how the unique challenges on the element and forest level associated with the pyramidal refinement are resolved. We propose the necessary functional design and generalize the fundamental global parallel algorithms for refinement, coarsening, partitioning, and face ghost exchange to fully support this new element. Our demonstrations confirm the efficiency and scalability of this complete, hybrid-element dynamic AMR framework."}
{"id": "2602.20644", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20644", "abs": "https://arxiv.org/abs/2602.20644", "authors": ["Fida Khandaker Safa", "Yupeng Jiang", "Xi Zheng"], "title": "An LLM-driven Scenario Generation Pipeline Using an Extended Scenic DSL for Autonomous Driving Safety Validation", "comment": null, "summary": "Real-world crash reports, which combine textual summaries and sketches, are valuable for scenario-based testing of autonomous driving systems (ADS). However, current methods cannot effectively translate this multimodal data into precise, executable simulation scenarios, hindering the scalability of ADS safety validation. In this work, we propose a scalable and verifiable pipeline that uses a large language model (GPT-4o mini) and a probabilistic intermediate representation (an Extended Scenic domain-specific language) to automatically extract semantic scenario configurations from crash reports and generate corresponding simulation-ready scenarios. Unlike earlier approaches such as ScenicNL and LCTGen (which generate scenarios directly from text) or TARGET (which uses deterministic mappings from traffic rules), our method introduces an intermediate Scenic DSL layer to separate high-level semantic understanding from low-level scenario rendering, reducing errors and capturing real-world variability. We evaluated the pipeline on cases from the NHTSA CIREN database. The results show high accuracy in knowledge extraction: 100% correctness for environmental and road network attributes, and 97% and 98% for oracle and actor trajectories, respectively, compared to human-derived ground truth. We executed the generated scenarios in the CARLA simulator using the Autoware driving stack, and they consistently triggered the intended traffic-rule violations (such as opposite-lane crossing and red-light running) across 2,000 scenario variations. These findings demonstrate that the proposed pipeline provides a legally grounded, scalable, and verifiable approach to ADS safety validation."}
{"id": "2602.21022", "categories": ["cs.DC", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.21022", "abs": "https://arxiv.org/abs/2602.21022", "authors": ["Antonio Cruciani", "Avinandan Das", "Massimo Equi", "Henrik Lievonen", "Diep Luong-Le", "Augusto Modanese", "Jukka Suomela"], "title": "Is a LOCAL algorithm computable?", "comment": "33 pages, 1 figure", "summary": "Common definitions of the \"standard\" LOCAL model tend to be sloppy and even self-contradictory on one point: do the nodes update their state using an arbitrary function or a computable function? So far, this distinction has been safe to neglect, since problems where it matters seem contrived and quite different from e.g. typical local graph problems studied in this context.\n  We show that this question matters even for locally checkable labeling problems (LCLs), perhaps the most widely studied family of problems in the context of the LOCAL model. Furthermore, we show that assumptions about computability are directly connected to another aspect already recognized as highly relevant: whether we have any knowledge of $n$, the size of the graph. Concretely, we show that there is an LCL problem $Π$ with the following properties:\n  1. $Π$ can be solved in $O(\\log n)$ rounds if the \\textsf{LOCAL} model is uncomputable.\n  2. $Π$ can be solved in $O(\\log n)$ rounds in the computable model if we know any upper bound on $n$.\n  3. $Π$ requires $Ω(\\sqrt{n})$ rounds in the computable model if we do not know anything about $n$.\n  We also show that the connection between computability and knowledge of $n$ holds in general: for any LCL problem $Π$, if you have any bound on $n$, then $Π$ has the same round complexity in the computable and uncomputable models."}
{"id": "2602.20684", "categories": ["cs.SE", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.20684", "abs": "https://arxiv.org/abs/2602.20684", "authors": ["Christopher Koch", "Joshua Andreas Wellbrock"], "title": "Agile V: A Compliance-Ready Framework for AI-Augmented Engineering -- From Concept to Audit-Ready Delivery", "comment": "9 pages, 2 figures", "summary": "Current AI-assisted engineering workflows lack a built-in mechanism to maintain task-level verification and regulatory traceability at machine-speed delivery. Agile V addresses this gap by embedding independent verification and audit artifact generation into each task cycle. The framework merges Agile iteration with V-Model verification into a continuous Infinity Loop, deploying specialized AI agents for requirements, design, build, test, and compliance, governed by mandatory human approval gates. We evaluate three hypotheses: (H1) audit-ready artifacts emerge as a by-product of development, (H2) 100% requirement-level verification is achievable with independent test generation, and (H3) verified increments can be delivered with single-digit human interactions per cycle. A feasibility case study on a Hardware-in-the-Loop system (about 500 LOC, 8 requirements, 54 tests) supports all three hypotheses: audit-ready documentation was generated automatically (H1), 100% requirement-level pass rate was achieved (H2), and only 6 prompts per cycle were required (H3), yielding an estimated 10-50x cost reduction versus a COCOMO II baseline (sensitivity range from pessimistic to optimistic assumptions). We invite independent replication to validate generalizability."}
{"id": "2602.21140", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21140", "abs": "https://arxiv.org/abs/2602.21140", "authors": ["Haley Li", "Xinglu Wang", "Cong Feng", "Chunxu Zuo", "Yanan Wang", "Hei Lo", "Yufei Cui", "Bingji Wang", "Duo Cui", "Shuming Jing", "Yizhou Shan", "Ying Xiong", "Jiannan Wang", "Yong Zhang", "Zhenan Fan"], "title": "ReviveMoE: Fast Recovery for Hardware Failures in Large-Scale MoE LLM Inference Deployments", "comment": "21 pages, 6 figures", "summary": "As LLM deployments scale over more hardware, the probability of a single failure in a system increases significantly, and cloud operators must consider robust countermeasures to handle these inevitable failures. A common recovery approach is to simply restart the LLM serving instance; however, this is costly in model-as-a-service (MaaS) inference settings, where reloading model weights and recompiling computation graphs can introduce significant delays to incoming requests. We propose ReviveMoE, a method for rapid failure recovery in large-scale LLM deployments without restarting the serving instance. ReviveMoE is designed to support both the traditional LLM architecture, which collocates MoE and attention on the same hardware, and the disaggregated architectures, which separate MoE from attention. Integrated into Huawei Cloud's MaaS, ReviveMoE is built on top of Huawei's xDeepServe serving platform and the XCCL communications library."}
{"id": "2602.20717", "categories": ["cs.SE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.20717", "abs": "https://arxiv.org/abs/2602.20717", "authors": ["Xiting Liu", "Yuetong Liu", "Yitong Zhang", "Jia Li", "Shi-Min Hu"], "title": "PackMonitor: Enabling Zero Package Hallucinations Through Decoding-Time Monitoring", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly integrated into software development workflows, their trustworthiness has become a critical concern. However, in dependency recommendation scenarios, the reliability of LLMs is undermined by widespread package hallucinations, where models often recommend hallucinated packages. Recent studies have proposed a range of approaches to mitigate this issue. Nevertheless, existing approaches typically merely reduce hallucination rates rather than eliminate them, leaving persistent software security risks.\n  In this work, we argue that package hallucinations are theoretically preventable based on the key insight that package validity is decidable through finite and enumerable authoritative package lists. Building on this, we propose PackMonitor, the first approach capable of fundamentally eliminating package hallucinations by continuously monitoring the model's decoding process and intervening when necessary. To implement this in practice, PackMonitor addresses three key challenges: (1) determining when to trigger intervention via a Context-Aware Parser that continuously monitors model outputs and selectively activates intervening only during installation command generation; (2) resolving how to intervene by employing a Package-Name Intervenor that strictly limits the decoding space to an authoritative package list; and (3) ensuring monitoring efficiency through a DFA-Caching Mechanism that enables scalability to millions of packages with negligible overhead. Extensive experiments on five widely used LLMs demonstrate that PackMonitor is a training-free, plug-and-play solution that consistently reduces package hallucination rates to zero while maintaining low-latency inference and preserving original model capabilities."}
{"id": "2602.21144", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.21144", "abs": "https://arxiv.org/abs/2602.21144", "authors": ["Anurag Dutt", "Nimit Shah", "Hazem Masarani", "Anshul Gandhi"], "title": "Scaling State-Space Models on Multiple GPUs with Tensor Parallelism", "comment": "Submitted to 46th IEEE International Conference on Distributed Computing Systems (ICDCS 2026)", "summary": "Selective state space models (SSMs) have rapidly become a compelling backbone for large language models, especially for long-context workloads. Yet in deployment, their inference performance is often bounded by the memory capacity, bandwidth, and latency limits of a single GPU, making multi-GPU execution increasingly necessary. Although tensor parallelism (TP) is widely used to scale Transformer inference, applying it to selective SSM blocks is non-trivial because the SSM mixer couples large projections with a sequence-wise recurrent state update and local mixing whose efficiency depends on preserving locality and avoiding synchronization in the critical path.\n  This paper presents a communication-efficient TP design for selective SSM inference that addresses three practical engineering challenges: enabling TTFT improvements via an SSM state cache across prefill and decode, partitioning the mixer's packed parameter tensor so that recurrent updates remain local while minimizing communication, and reducing TP aggregation overhead with quantized AllReduce. We evaluate on three representative SSM-based LLMs spanning pure-SSM and hybrid architectures - Mamba, Falcon-Mamba, and Zamba - on NVIDIA A6000 and A100 clusters. Our experiments show substantial throughput gains from tensor-parallel SSM inference, improving batch-request throughput by ~1.6-2.1x on 2 GPUs and ~2.6-4.0x on 4 GPUs for Mamba, with the largest benefits at long context lengths, and achieving a further ~10-18% throughput improvement from quantized all-reduce by lowering synchronization bandwidth overhead."}
{"id": "2602.20799", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20799", "abs": "https://arxiv.org/abs/2602.20799", "authors": ["Guangsheng Ou", "Qiming Zhang", "Sirong Chen", "Anji Li", "Dong Xu", "Tiancheng Luo", "Dekun Dai", "Cuiyun Gao", "Long Wang", "Jun Zhou", "Mingwei Liu", "Zibin Zheng"], "title": "Unseen-Codebases-Domain Data Synthesis and Training Based on Code Graphs", "comment": null, "summary": "In the context of newly release software frameworks, large language models (LLMs) often exhibit poor performance and a high rate of hallucination, as they are not exposed to such environments during training. Although inference-time augmentation techniques such as retrieval-augmented generation (RAG) can partially mitigate hallucinations, knowledge injection through prompting alone is insufficient to enable models to fully understand the intrinsic relationships among different components of a codebase, or to reason about the correct compositions and apply. Although explicit knowledge injection can be achieved through post-training, compared with public code domains, unseen codebases typically provide only source code and lack large volumes of high-quality, usage-oriented code that can be directly leveraged as training data. Consequently, existing data synthesis approaches are insufficient to adequately capture unseen codebases usage scenarios when restricted to source code alone. To address these challenges, we propose UCD-Training, a two-stage training framework for reasoning-aware data synthesis grounded in a code graph constructed from unseen codebases. UCD-Training first parses the source code to build a code graph, then conducts dependency-preserving continued pretraining (CPT) using file-level dependency data, followed by graph-grounded supervised fine-tuning (SFT) on three types of synthesized data augmented with explicit reasoning traces: (1) single-hop relation reasoning data, (2) compositional API reasoning data, and (3) codebase utilization data. We further introduce a new benchmark, UnseenCodeBench, for code generation on unseen codebases and conduct comprehensive experiments across multiple codebases."}
{"id": "2602.21182", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.21182", "abs": "https://arxiv.org/abs/2602.21182", "authors": ["Paul Borrill"], "title": "Circumventing the CAP Theorem with Open Atomic Ethernet", "comment": "23 pages, 14 figures", "summary": "The CAP theorem is routinely treated as a systems law: under network partition, a replicated service must sacrifice either consistency or availability. The theorem is correct within its standard asynchronous network model, but operational practice depends on where partition-like phenomena become observable and on how lower layers discard or preserve semantic information about message fate. This paper argues that Open Atomic Ethernet (OAE) shifts the engineering regime in which CAP tradeoffs become application-visible by (i) replacing fire-and-forget link semantics with bounded-time bilateral reconciliation of endpoint state -- the property we call bisynchrony -- and (ii) avoiding Clos funnel points via an octavalent mesh in which each node can act as the root of a locally repaired spanning tree. The result is not the elimination of hard graph cuts, but a drastic reduction in the frequency and duration of application-visible \"soft partitions\" by detecting and healing dominant fabric faults within hundreds of nanoseconds. We connect this view to Brewer's original CAP framing, the formalization by Gilbert and Lynch, the CAL theorem of Lee et al., which replaces binary partition tolerance with a quantitative measure of apparent latency, and Abadi's PACELC extension."}
{"id": "2602.20979", "categories": ["cs.SE", "cs.AI", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.20979", "abs": "https://arxiv.org/abs/2602.20979", "authors": ["Mark Marron"], "title": "Toward an Agentic Infused Software Ecosystem", "comment": null, "summary": "Fully leveraging the capabilities of AI agents in software development requires a rethinking of the software ecosystem itself. To this end, this paper outlines the creation of an Agentic Infused Software Ecosystem (AISE), that rests on three pillars. The first, of course, is the AI agents themselves, which in the past 5 years have moved from simple code completion and toward sophisticated independent development tasks, a trend which will only continue. The second pillar is the programming language and APIs (or tools) that these agents use to accomplish tasks, and increasingly, serve as the communication substrate that humans and AI agents interact and collaborate through. The final pillar is the runtime environment and ecosystem that agents operate within, and which provide the capabilities that programmatic agents use to interface with (and effect actions in) the external world. To realize the vision of AISE, all three pillars must be advanced in a holistic manner, and critically, in a manner that is synergistic for AI agents as they exist today, those that will exist in the future, and for the human developers that work alongside them."}
{"id": "2602.21026", "categories": ["cs.SE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2602.21026", "abs": "https://arxiv.org/abs/2602.21026", "authors": ["David Heddle"], "title": "A Modular Multi-Document Framework for Scientific Visualization and Simulation in Java", "comment": "10 pages, 5 figures. Includes optional 3D extension module and integrated plotting subsystem. Source code available on GitHub", "summary": "This paper presents the design and implementation of a modular multi-document interface (MDI) framework for scientific visualization and simulation in the Java Virtual Machine (JVM) ecosystem. The framework emphasizes architectural separation between visualization layers, simulation engines, and optional hardware-accelerated 3D rendering. 3D functionality is isolated into a separate module to prevent unnecessary dependency coupling in 2D-only applications. We describe the core abstractions, threading model, simulation integration strategy, and dependency isolation approach. A case study involving a real-time 3D gas expansion simulation integrated with synchronized 2D entropy plotting demonstrates architectural cohesion. The framework is publicly available via Maven Central and targets long-lived scientific and engineering desktop applications."}
{"id": "2602.21037", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21037", "abs": "https://arxiv.org/abs/2602.21037", "authors": ["Bruno Guindani", "Matteo Camilli", "Livia Lestingi", "Marcello M. Bersani"], "title": "Automated Detection and Mitigation of Dependability Failures in Healthcare Scenarios through Digital Twins", "comment": "Submitted to IEEE Transactions on Reliability", "summary": "Medical Cyber-Physical Systems (CPSs) integrating Patients, Devices, and healthcare personnel (Physicians) form safety-critical PDP triads whose dependability is challenged by system heterogeneity and uncertainty in human and physiological behavior. While existing clinical decision support systems support clinical practice, there remains a need for proactive, reliability-oriented methodologies capable of identifying and mitigating failure scenarios before patient safety is compromised. This paper presents M-GENGAR, a methodology based on a closed-loop Digital Twin (DT) paradigm for dependability assurance of medical CPSs. The approach combines Stochastic Hybrid Automata modeling, data-driven learning of patient dynamics, and Statistical Model Checking with an offline critical scenario detection phase that integrates model-space exploration and diversity analysis to systematically identify and classify scenarios violating expert-defined dependability requirements. M-GENGAR also supports the automated synthesis of mitigation strategies, enabling runtime feedback and control within the DT loop. We evaluate M-GENGAR on a representative use case study involving a pulmonary ventilator. Results show that, in 87.5% of the evaluated scenarios, strategies synthesized through formal game-theoretic analysis stabilize patient vital metrics at least as effectively as human decision-making, while maintaining relevant metrics 20% closer to nominal healthy values on average."}
{"id": "2602.21074", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2602.21074", "abs": "https://arxiv.org/abs/2602.21074", "authors": ["Ana Díaz-Muñoz", "José A. Cruz-Lemus", "Moisés Rodríguez", "Maria Teresa Baldassarre", "Mario Piattini"], "title": "Validation of an analyzability model for quantum software: a family of experiments", "comment": "42 pages, 5 figures, 12 tables. This is the Author Accepted Manuscript (AAM) of the article published in Empirical Software Engineering (2026). The final published version is available at https://doi.org/10.1007/s10664-026-10825-3", "summary": "The analyzability of hybrid software, which integrates both classical and quantum components, is a key factor in ensuring its maintainability and industrial adoption. This article presents the empirical validation, through a family of experiments, of the quantum component of a previously proposed hybrid software analyzability model based on the ISO/IEC 25010 standard. The experimental series consists of four studies involving participants with diverse profiles in both academic and professional settings. In these experiments, the model's ability to effectively measure the analyzability of quantum algorithms is assessed, and the relationship between the analyzability levels computed by the model and the participant's perceptions of the complexity of these algorithms is examined. The results indicate that the proposed model effectively distinguishes between quantum software components with varying levels of analyzability and aligns with human perception, reinforcing its validity in quantum computing."}
{"id": "2602.20924", "categories": ["cs.NI", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.20924", "abs": "https://arxiv.org/abs/2602.20924", "authors": ["Alagappan Ramanathan", "Eunju Kang", "Dongsu Han", "Sangeetha Abdu Jyothi"], "title": "Airavat: An Agentic Framework for Internet Measurement", "comment": null, "summary": "Internet measurement faces twin challenges: complex analyses require expert-level orchestration of tools, yet even syntactically correct implementations can have methodological flaws and can be difficult to verify. Democratizing measurement capabilities thus demands automating both workflow generation and verification against methodological standards established through decades of research.\n  We present Airavat, the first agentic framework for Internet measurement workflow generation with systematic verification and validation. Airavat coordinates a set of agents mirroring expert reasoning: three agents handle problem decomposition, solution design, and code implementation, with assistance from a registry of existing tools. Two specialized engines ensure methodological correctness: a Verification Engine evaluates workflows against a knowledge graph encoding five decades of measurement research, while a Validation Engine identifies appropriate validation techniques grounded in established methodologies. Through four Internet measurement case studies, we demonstrate that Airavat (i) generates workflows matching expert-level solutions, (ii) makes sound architectural decisions, (iii) addresses novel problems without ground truth, and (iv) identifies methodological flaws missed by standard execution-based testing."}
