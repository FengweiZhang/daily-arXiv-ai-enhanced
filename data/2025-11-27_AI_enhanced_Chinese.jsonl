{"id": "2511.21449", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2511.21449", "abs": "https://arxiv.org/abs/2511.21449", "authors": ["Steffen Tillmann", "Felipe A. Gonz\u00e1lez", "Stefanie Elgeti"], "title": "Numerical Optimization of Nozzle Shapes for Fused Deposition Modeling", "comment": null, "summary": "Purpose: In fused deposition modeling (FDM), the nozzle plays a critical role in enabling high printing speeds while maintaining precision. Despite its importance, most applications still rely on standard nozzle designs. This work investigates the influence of nozzle geometry on pressure loss inside the nozzle, a key factor in high-speed printing performance. Design/methodology/approach: We focus on optimizing the nozzle shape to minimize the pressure loss and establish a framework that allows both sim- ple angle-based optimization and more advanced spline-based parametrization. To model the polymer melt flow, we compare two constitutive descriptions commonly employed in the literature: a temperature-dependent, shear-thinning viscous model and an isothermal viscoelastic model. Findings: For the viscous model, the optimal half-opening angle exhibits a strong dependence on the feeding rate, with higher rates favoring half-opening angles near 30\u00b0, whereas lower rates are more efficient at larger angles. In con- trast, the viscoelastic model predicts a weaker dependence of the optimal angle on the feeding rate. For both models, spline-based parametrization yields only marginal improvements over angle optimization in terms of reducing pressure loss. Originality/value: This paper presents a comparative study of FDM nozzle shape optimization using different simulation models. We introduce a flexible optimization framework that accommodates both simple and advanced geomet- ric parametrizations. The results highlight the impact of model choice on the optimal nozzle geometry and provide support for improving nozzle design in high-speed printing applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7194\u878d\u6c89\u79ef\u6210\u578b\uff08FDM\uff09\u4e2d\u55b7\u5634\u51e0\u4f55\u5f62\u72b6\u5bf9\u5185\u90e8\u538b\u964d\u7684\u5f71\u54cd\uff0c\u6bd4\u8f83\u4e86\u7c98\u6027\u4e0e\u7c98\u5f39\u6027\u6d41\u53d8\u6a21\u578b\u5728\u55b7\u5634\u4f18\u5316\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u517c\u987e\u7b80\u5355\u89d2\u5ea6\u4e0e\u6837\u6761\u53c2\u6570\u5316\u7684\u4f18\u5316\u6846\u67b6\u3002", "motivation": "\u5728\u9ad8\u901fFDM\u6253\u5370\u4e2d\uff0c\u55b7\u5634\u51e0\u4f55\u5f62\u72b6\u5bf9\u538b\u964d\u6709\u663e\u8457\u5f71\u54cd\uff0c\u800c\u73b0\u6709\u5e94\u7528\u591a\u91c7\u7528\u6807\u51c6\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u9488\u5bf9\u9ad8\u901f\u6027\u80fd\u7684\u7cfb\u7edf\u4f18\u5316\u7814\u7a76\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u6d41\u53d8\u6a21\u578b\uff08\u6e29\u5ea6\u4f9d\u8d56\u526a\u5207\u7a00\u5316\u7c98\u6027\u6a21\u578b\u548c\u7b49\u6e29\u7c98\u5f39\u6027\u6a21\u578b\uff09\uff0c\u5206\u522b\u7ed3\u5408\u57fa\u4e8e\u534a\u9525\u89d2\u548c\u6837\u6761\u66f2\u7ebf\u7684\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5bf9\u55b7\u5634\u5f62\u72b6\u8fdb\u884c\u4f18\u5316\u4ee5\u6700\u5c0f\u5316\u538b\u964d\u3002", "result": "\u7c98\u6027\u6a21\u578b\u4e0b\u6700\u4f18\u534a\u9525\u89d2\u968f\u8fdb\u6599\u901f\u7387\u663e\u8457\u53d8\u5316\uff08\u9ad8\u901f\u65f6\u7ea630\u00b0\u66f4\u4f18\uff09\uff0c\u800c\u7c98\u5f39\u6027\u6a21\u578b\u4e0b\u8be5\u4f9d\u8d56\u6027\u8f83\u5f31\uff1b\u6837\u6761\u53c2\u6570\u5316\u76f8\u6bd4\u89d2\u5ea6\u4f18\u5316\u4ec5\u5e26\u6765\u5fae\u5c0f\u538b\u964d\u6539\u5584\u3002", "conclusion": "\u6a21\u578b\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6700\u4f18\u55b7\u5634\u51e0\u4f55\u5f62\u72b6\uff0c\u6240\u63d0\u51fa\u7684\u4f18\u5316\u6846\u67b6\u4e3a\u9ad8\u901fFDM\u6253\u5370\u4e2d\u7684\u55b7\u5634\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2511.21235", "categories": ["cs.OS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.21235", "abs": "https://arxiv.org/abs/2511.21235", "authors": ["Daniel Berend", "Shlomi Dolev", "Sweta Kumari", "Dhruv Mishra", "Marina Kogan-Sadetsky", "Archit Somani"], "title": "DynamicAdaptiveClimb: Adaptive Cache Replacement with Dynamic Resizing", "comment": "19 pages, 11 figures, 3 tables, Patented", "summary": "Efficient cache management is critical for optimizing the system performance, and numerous caching mechanisms have been proposed, each exploring various insertion and eviction strategies. In this paper, we present AdaptiveClimb and its extension, DynamicAdaptiveClimb, two novel cache replacement policies that leverage lightweight, cache adaptation to outperform traditional approaches. Unlike classic Least Recently Used (LRU) and Incremental Rank Progress (CLIMB) policies, AdaptiveClimb dynamically adjusts the promotion distance (jump) of the cached objects based on recent hit and miss patterns, requiring only a single tunable parameter and no per-item statistics. This enables rapid adaptation to changing access distributions while maintaining low overhead. Building on this foundation, DynamicAdaptiveClimb further enhances adaptability by automatically tuning the cache size in response to workload demands. Our comprehensive evaluation across a diverse set of real-world traces, including 1067 traces from 6 different datasets, demonstrates that DynamicAdaptiveClimb consistently achieves substantial speedups and higher hit ratios compared to other state-of-the-art algorithms. In particular, our approach achieves up to a 29% improvement in hit ratio and a substantial reduction in miss penalties compared to the FIFO baseline. Furthermore, it outperforms the next-best contenders, AdaptiveClimb and SIEVE [43], by approximately 10% to 15%, especially in environments characterized by fluctuating working set sizes. These results highlight the effectiveness of our approach in delivering efficient performance, making it well-suited for modern, dynamic caching environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7f13\u5b58\u66ff\u6362\u7b56\u7565AdaptiveClimb\u548cDynamicAdaptiveClimb\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7f13\u5b58\u9879\u7684\u63d0\u5347\u8ddd\u79bb\u548c\u81ea\u52a8\u8c03\u8282\u7f13\u5b58\u5927\u5c0f\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7f13\u5b58\u66ff\u6362\u7b56\u7565\uff08\u5982LRU\u548cCLIMB\uff09\u96be\u4ee5\u5728\u52a8\u6001\u53d8\u5316\u7684\u8bbf\u95ee\u6a21\u5f0f\u4e0b\u9ad8\u6548\u9002\u5e94\uff0c\u4e14\u5f80\u5f80\u9700\u8981\u7ef4\u62a4\u590d\u6742\u7684\u6bcf\u9879\u7edf\u8ba1\u4fe1\u606f\u6216\u591a\u4e2a\u8c03\u4f18\u53c2\u6570\uff0c\u5bfc\u81f4\u5f00\u9500\u8f83\u9ad8\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u81ea\u9002\u5e94\u80fd\u529b\u5f3a\u7684\u7f13\u5b58\u7ba1\u7406\u673a\u5236\u3002", "method": "AdaptiveClimb\u6839\u636e\u6700\u8fd1\u7684\u547d\u4e2d/\u672a\u547d\u4e2d\u6a21\u5f0f\u52a8\u6001\u8c03\u6574\u7f13\u5b58\u5bf9\u8c61\u7684\u63d0\u5347\u8ddd\u79bb\uff0c\u4ec5\u9700\u4e00\u4e2a\u53ef\u8c03\u53c2\u6570\u4e14\u65e0\u9700\u6bcf\u9879\u7edf\u8ba1\uff1b\u5176\u6269\u5c55\u7248\u672cDynamicAdaptiveClimb\u8fdb\u4e00\u6b65\u5f15\u5165\u7f13\u5b58\u5927\u5c0f\u7684\u81ea\u52a8\u8c03\u8282\u673a\u5236\u4ee5\u5e94\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5316\u3002", "result": "\u5728\u6db5\u76d66\u4e2a\u6570\u636e\u96c6\u51711067\u6761\u771f\u5b9e\u8f68\u8ff9\u7684\u8bc4\u4f30\u4e2d\uff0cDynamicAdaptiveClimb\u76f8\u6bd4FIFO\u57fa\u51c6\u6700\u9ad8\u63d0\u534729%\u547d\u4e2d\u7387\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u672a\u547d\u4e2d\u60e9\u7f5a\uff1b\u76f8\u8f83\u6b21\u4f18\u65b9\u6cd5AdaptiveClimb\u548cSIEVE\uff0c\u6027\u80fd\u63d0\u5347\u7ea610%\u201315%\uff0c\u5c24\u5176\u5728\u5de5\u4f5c\u96c6\u5927\u5c0f\u6ce2\u52a8\u7684\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7f13\u5b58\u7b56\u7565\u5728\u4fdd\u6301\u4f4e\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u5728\u73b0\u4ee3\u52a8\u6001\u7f13\u5b58\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.20780", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20780", "abs": "https://arxiv.org/abs/2511.20780", "authors": ["Alison Silva", "Gustavo Callou"], "title": "Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures", "comment": null, "summary": "Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u968f\u673aPetri\u7f51\uff08SPN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u79c1\u6709\u4e91\u4e2dNextcloud\u6587\u4ef6\u670d\u52a1\u5668\u5728\u4e0d\u540c\u5197\u4f59\u7b56\u7565\u4e0b\u7684\u53ef\u7528\u6027\uff0c\u7ed3\u679c\u8868\u660e\u4e3b\u673a\u4e0e\u865a\u62df\u673a\u53cc\u91cd\u5197\u4f59\u80fd\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u53ef\u7528\u6027\u3002", "motivation": "\u968f\u7740\u4e91\u5b58\u50a8\u5e73\u53f0\u5728\u5b66\u672f\u548c\u5546\u4e1a\u73af\u5883\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u7ec4\u7ec7\u5bf9\u53ef\u9760\u6027\u7684\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u5c24\u5176\u662f\u5728\u5bfb\u6c42\u516c\u6709\u4e91\u66ff\u4ee3\u65b9\u6848\u65f6\u3002\u56e0\u6b64\uff0c\u8bc4\u4f30\u79c1\u6709\u4e91\u5b58\u50a8\u7cfb\u7edf\u7684\u53ef\u7528\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u968f\u673aPetri\u7f51\uff08SPNs\uff09\u5bf9\u8fd0\u884c\u5728Apache CloudStack\u79c1\u6709\u4e91\u4e0a\u7684Nextcloud\u6587\u4ef6\u670d\u52a1\u5668\u8fdb\u884c\u5efa\u6a21\uff0c\u5206\u6790\u56db\u79cd\u67b6\u6784\u914d\u7f6e\uff1a\u57fa\u7ebf\u3001\u4e3b\u673a\u7ea7\u5197\u4f59\u3001\u865a\u62df\u673a\uff08VM\uff09\u5197\u4f59\u4ee5\u53ca\u4e24\u8005\u7ed3\u5408\u3002", "result": "\u4e3b\u673a\u7ea7\u548c\u865a\u62df\u673a\u7ea7\u53cc\u91cd\u5197\u4f59\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u53ef\u7528\u6027\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u9884\u671f\u505c\u673a\u65f6\u95f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eSPN\u7684\u5efa\u6a21\u65b9\u6cd5\u53ef\u6709\u6548\u8bc4\u4f30\u79c1\u6709\u4e91\u73af\u5883\u4e2d\u7684\u7cfb\u7edf\u53ef\u7528\u6027\uff0c\u4e3a\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u548c\u5197\u4f59\u7b56\u7565\u9009\u62e9\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2511.21232", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21232", "abs": "https://arxiv.org/abs/2511.21232", "authors": ["Muhammed Yildirim", "Ozcan Ozturk"], "title": "RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI", "comment": "13 pages, 7 tables, 14 figures", "summary": "The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u786c\u4ef6\u52a0\u901f\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u878d\u5408\u50cf\u7d20\u7ea7\u6570\u636e\u6d41\u6d88\u9664\u4e2d\u95f4\u7f13\u51b2\u533a\u9700\u6c42\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u5e76\u63d0\u5347\u8fb9\u7f18AI\u4e2d\u8f7b\u91cf\u7ea7CNN\u7684\u6267\u884c\u6548\u7387\u3002", "motivation": "\u5728\u8fb9\u7f18AI\u548cTinyML\u5e94\u7528\u4e2d\uff0c\u73b0\u4ee3CNN\uff08\u5982MobileNetV2\uff09\u867d\u91c7\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f46\u5176\u591a\u9636\u6bb5\u9010\u5c42\u6267\u884c\u65b9\u5f0f\u5bfc\u81f4\u4e2d\u95f4\u7279\u5f81\u56fe\u9891\u7e41\u8bbf\u95ee\u7247\u4e0a\u6216\u7247\u5916\u5b58\u50a8\uff0c\u9020\u6210\u9ad8\u80fd\u8017\u4e0e\u5ef6\u8fdf\uff0c\u5f62\u6210\u201c\u5185\u5b58\u5899\u201d\u74f6\u9888\u3002", "method": "\u8bbe\u8ba1\u4e00\u79cd\u57fa\u4e8eRISC-V\u5904\u7406\u5668\u7684\u5b9a\u5236\u529f\u80fd\u5355\u5143\uff08CFU\uff09\uff0c\u91c7\u7528\u878d\u5408\u50cf\u7d20\u7ea7\u6570\u636e\u6d41\u67b6\u6784\uff0c\u5728\u7d27\u5bc6\u8026\u5408\u7684\u6d41\u6c34\u7ebf\u4e2d\u4e00\u6b21\u6027\u5b8c\u6210\u5355\u4e2a\u8f93\u51fa\u50cf\u7d20\u7684\u6240\u6709DSC\u9636\u6bb5\uff08\u6269\u5c55\u3001\u6df1\u5ea6\u5377\u79ef\u3001\u6295\u5f71\uff09\u8ba1\u7b97\uff0c\u65e0\u9700\u5199\u5165\u4e2d\u95f4\u7f13\u51b2\u533a\u3002", "result": "\u5728Xilinx Artix-7 FPGA\u4e0a\u5b9e\u73b0\u6700\u9ad859.3\u500d\u4e8eRISC-V\u8f6f\u4ef6\u57fa\u7ebf\u7684\u52a0\u901f\uff1bASIC\u7efc\u5408\u7ed3\u679c\u663e\u793a\u572828nm\u5de5\u827a\u4e0b\u9762\u79ef0.284 mm\u00b2\u3001\u529f\u8017910 mW\uff082 GHz\uff09\uff0c\u572840nm\u5de5\u827a\u4e0b\u9762\u79ef1.20 mm\u00b2\u3001\u529f\u8017233 mW\uff08300 MHz\uff09\uff1b\u6570\u636e\u79fb\u52a8\u51cf\u5c11\u9ad8\u8fbe87%\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9a8c\u8bc1\u4e86\u5728TinyML\u8d44\u6e90\u9650\u5236\u4e0b\u5b9e\u73b0\u96f6\u7f13\u51b2\u6570\u636e\u6d41\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u89e3\u51b3\u8fb9\u7f18AI\u52a0\u901f\u5668\u4e2d\u7684\u5185\u5b58\u5899\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b0\u7b56\u7565\u3002"}}
{"id": "2511.21346", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21346", "abs": "https://arxiv.org/abs/2511.21346", "authors": ["Mohamed Shahawy", "Julien de Castelnau", "Paolo Ienne"], "title": "Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration", "comment": null, "summary": "Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Bombyx\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u5c06OpenCilk\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u7c7bCilk-1\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u4ee5\u66f4\u9ad8\u6548\u5730\u5c06\u4efb\u52a1\u7ea7\u5e76\u884c\uff08TLP\uff09\u5e94\u7528\u6620\u5c04\u5230FPGA\u67b6\u6784\u4e0a\uff0c\u5e76\u652f\u6301\u591a\u79cd\u7f16\u8bd1\u76ee\u6807\u548c\u81ea\u52a8\u4f18\u5316\u3002", "motivation": "\u73b0\u6709OpenCilk\u7684\u4efb\u52a1\u6a21\u578b\u5728\u786c\u4ef6\u4e2d\u9700\u8981\u6602\u8d35\u7684\u4e0a\u4e0b\u6587\u5207\u6362\uff0c\u96be\u4ee5\u9ad8\u6548\u6620\u5c04\u5230FPGA\uff1b\u800cFPGA\u66f4\u9002\u5408\u663e\u5f0f\u5ef6\u7eed\u4f20\u9012\u98ce\u683c\u7684\u4efb\u52a1\u6a21\u578b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7f16\u8bd1\u65b9\u6cd5\u6765\u6865\u63a5CPU\u5bfc\u5411\u7684TLP\u7a0b\u5e8f\u4e0e\u7a7a\u95f4\u67b6\u6784\u3002", "method": "Bombyx\u5c06OpenCilk\u7a0b\u5e8f\u964d\u7ea7\u4e3a\u7c7bCilk-1\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u91c7\u7528\u663e\u5f0f\u5ef6\u7eed\u4f20\u9012\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e24\u4e2a\u7f16\u8bd1\u76ee\u6807\uff1a\u4e00\u662f\u517c\u5bb9OpenCilk\u8fd0\u884c\u65f6\u7684\u540e\u7aef\uff0c\u4e8c\u662f\u9762\u5411HLS\u5de5\u5177\uff08\u5982Vitis HLS\uff09\u7684\u53ef\u7efc\u5408\u5904\u7406\u5355\u5143\uff08PE\uff09\u751f\u6210\u5668\uff1b\u540c\u65f6\u5f15\u5165\u89e3\u8026\u8bbf\u95ee-\u6267\u884c\u4f18\u5316\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "result": "Bombyx\u80fd\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fdPE\uff0c\u6539\u5584\u5185\u5b58\u4e0e\u8ba1\u7b97\u7684\u91cd\u53e0\uff0c\u63d0\u9ad8\u6574\u4f53\u541e\u5410\u91cf\uff0c\u5e76\u6709\u6548\u652f\u6301\u5c06CPU\u5bfc\u5411\u7684TLP\u7a0b\u5e8f\u90e8\u7f72\u5230FPGA\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u7c7bCilk-1\u7684\u663e\u5f0f\u4efb\u52a1\u6a21\u578b\u548c\u7f16\u8bd1\u4f18\u5316\uff0cBombyx\u663e\u8457\u63d0\u5347\u4e86TLP\u7a0b\u5e8f\u5728FPGA\u4e0a\u7684\u6267\u884c\u6548\u7387\uff0c\u4e3a\u9ad8\u5c42\u7efc\u5408\u548c\u5f02\u6784\u7f16\u7a0b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7f16\u8bd1\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2511.21160", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.21160", "abs": "https://arxiv.org/abs/2511.21160", "authors": ["Wu Sai", "Xia Ruichen", "Yang Dingyu", "Wang Rui", "Lai Huihang", "Guan Jiarui", "Bai Jiameng", "Zhang Dongxiang", "Tang Xiu", "Xie Zhongle", "Lu Peng", "Chen Gang"], "title": "MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference", "comment": null, "summary": "The increasing demand for deep neural inference within database environments has driven the emergence of AI-native DBMSs. However, existing solutions either rely on model-centric designs requiring developers to manually select, configure, and maintain models, resulting in high development overhead, or adopt task-centric AutoML approaches with high computational costs and poor DBMS integration. We present MorphingDB, a task-centric AI-native DBMS that automates model storage, selection, and inference within PostgreSQL. To enable flexible, I/O-efficient storage of deep learning models, we first introduce specialized schemas and multi-dimensional tensor data types to support BLOB-based all-in-one and decoupled model storage. Then we design a transfer learning framework for model selection in two phases, which builds a transferability subspace via offline embedding of historical tasks and employs online projection through feature-aware mapping for real-time tasks. To further optimize inference throughput, we propose pre-embedding with vectoring sharing to eliminate redundant computations and DAG-based batch pipelines with cost-aware scheduling to minimize the inference time. Implemented as a PostgreSQL extension with LibTorch, MorphingDB outperforms AI-native DBMSs (EvaDB, Madlib, GaussML) and AutoML platforms (AutoGluon, AutoKeras, AutoSklearn) across nine public datasets, encompassing series, NLP, and image tasks. Our evaluation demonstrates a robust balance among accuracy, resource consumption, and time cost in model selection and significant gains in throughput and resource efficiency.", "AI": {"tldr": "MorphingDB \u662f\u4e00\u4e2a\u4efb\u52a1\u5bfc\u5411\u7684 AI \u539f\u751f\u6570\u636e\u5e93\u7cfb\u7edf\uff0c\u96c6\u6210\u4e8e PostgreSQL\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u7684\u6a21\u578b\u5b58\u50a8\u3001\u9009\u62e9\u4e0e\u63a8\u7406\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u541e\u5410\u91cf\u5e76\u964d\u4f4e\u8d44\u6e90\u5f00\u9500\u3002", "motivation": "\u73b0\u6709 AI \u539f\u751f\u6570\u636e\u5e93\u7cfb\u7edf\u8981\u4e48\u4f9d\u8d56\u6a21\u578b\u4e2d\u5fc3\u5316\u8bbe\u8ba1\uff0c\u5bfc\u81f4\u5f00\u53d1\u7ef4\u62a4\u6210\u672c\u9ad8\uff1b\u8981\u4e48\u91c7\u7528\u4efb\u52a1\u4e2d\u5fc3\u5316\u7684 AutoML \u65b9\u6cd5\uff0c\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u4e0e\u6570\u636e\u5e93\u96c6\u6210\u5dee\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u517c\u987e\u81ea\u52a8\u5316\u3001\u9ad8\u6548\u6027\u4e0e\u826f\u597d\u96c6\u6210\u5ea6\u7684\u65b0\u65b9\u6848\u3002", "method": "MorphingDB \u5f15\u5165\u4e13\u7528\u6a21\u5f0f\u548c\u591a\u7ef4\u5f20\u91cf\u6570\u636e\u7c7b\u578b\u652f\u6301\u7075\u6d3b\u9ad8\u6548\u7684\u6a21\u578b\u5b58\u50a8\uff1b\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u6a21\u578b\u9009\u62e9\uff08\u79bb\u7ebf\u6784\u5efa\u53ef\u8fc1\u79fb\u5b50\u7a7a\u95f4 + \u5728\u7ebf\u7279\u5f81\u611f\u77e5\u6620\u5c04\uff09\uff1b\u5e76\u901a\u8fc7\u9884\u5d4c\u5165\u5411\u91cf\u5171\u4eab\u548c\u57fa\u4e8e DAG \u7684\u6279\u5904\u7406\u6d41\u6c34\u7ebf\u4f18\u5316\u63a8\u7406\u541e\u5410\u3002", "result": "\u5728\u4e5d\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMorphingDB \u5728\u51c6\u786e\u6027\u3001\u8d44\u6e90\u6d88\u8017\u548c\u65f6\u95f4\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u5e76\u5728\u541e\u5410\u91cf\u548c\u8d44\u6e90\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709 AI \u539f\u751f DBMS \u548c AutoML \u5e73\u53f0\u3002", "conclusion": "MorphingDB \u6709\u6548\u89e3\u51b3\u4e86 AI \u539f\u751f\u6570\u636e\u5e93\u4e2d\u6a21\u578b\u7ba1\u7406\u4e0e\u63a8\u7406\u6548\u7387\u7684\u95ee\u9898\uff0c\u4e3a\u6570\u636e\u5e93\u5185\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u81ea\u52a8\u4e14\u6613\u4e8e\u96c6\u6210\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21451", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21451", "abs": "https://arxiv.org/abs/2511.21451", "authors": ["Flurin Arquint", "Oscar Casta\u00f1eda", "Gian Marti", "Christoph Studer"], "title": "A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm", "comment": "Presented at the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)", "summary": "We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5b9e\u73b0\u4e86\u6297\u5e72\u6270\u7684\u591a\u5929\u7ebf\u65f6\u95f4\u540c\u6b65ASIC\u82af\u7247\uff0c\u652f\u6301\u5355\u5929\u7ebf\u53d1\u5c04\u7aef\u4e0e16\u5929\u7ebf\u63a5\u6536\u7aef\u4e4b\u95f4\u7684\u540c\u6b65\uff0c\u5e76\u80fd\u62b5\u5fa1\u6700\u591a\u4e24\u6839\u5929\u7ebf\u7684\u667a\u80fd\u5e72\u6270\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u9488\u5bf9\u540c\u6b65\u4fe1\u53f7\u7684\u667a\u80fd\u5e72\u6270\u653b\u51fb\uff0c\u63d0\u5347\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u5728\u590d\u6742\u7535\u78c1\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u4f5c\u8005\u63d0\u51fa\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5929\u7ebf\u5904\u7406\u7684\u6297\u5e72\u6270\u65f6\u95f4\u540c\u6b65\u65b9\u6848\u3002", "method": "\u91c7\u752865 nm\u5de5\u827a\u8bbe\u8ba1\u5e76\u6d41\u7247\u4e86\u4e00\u6b3e\u4e13\u7528\u96c6\u6210\u7535\u8def\uff08ASIC\uff09\uff0c\u5b9e\u73b0\u4e86\u4e00\u79cd\u5229\u7528\u591a\u5929\u7ebf\u5904\u7406\u6280\u672f\u6765\u6291\u5236\u5e72\u6270\u7684\u540c\u6b65\u7b97\u6cd5\u3002\u8be5\u82af\u7247\u652f\u6301\u5355\u53d116\u6536\u7684\u5929\u7ebf\u914d\u7f6e\uff0c\u53ef\u5bf9\u6297\u6700\u591a\u4e24\u6839\u5929\u7ebf\u7684\u667a\u80fd\u5e72\u6270\u673a\u3002", "result": "\u6240\u5b9e\u73b0\u7684ASIC\u6838\u5fc3\u9762\u79ef\u4e3a2.87 mm\u00b2\uff0c\u529f\u8017\u4e3a310 mW\uff0c\u91c7\u6837\u7387\u8fbe1.28 MS/s\uff0c\u6210\u529f\u9a8c\u8bc1\u4e86\u8be5\u6297\u5e72\u6270\u540c\u6b65\u7b97\u6cd5\u7684\u786c\u4ef6\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u591a\u5929\u7ebf\u6297\u5e72\u6270\u65f6\u95f4\u540c\u6b65\u7b97\u6cd5\u5728ASIC\u4e0a\u7684\u6709\u6548\u5b9e\u73b0\uff0c\u4e3a\u9ad8\u5b89\u5168\u6027\u548c\u9ad8\u9c81\u68d2\u6027\u7684\u65e0\u7ebf\u540c\u6b65\u7cfb\u7edf\u63d0\u4f9b\u4e86\u786c\u4ef6\u57fa\u7840\u3002"}}
{"id": "2511.21307", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2511.21307", "abs": "https://arxiv.org/abs/2511.21307", "authors": ["Xinyi Zhang", "Liang Liang", "Anastasia Ailamaki", "Jianliang Xu"], "title": "HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads", "comment": "Accepted to SIGMOD 2026. This is the extended technical report", "summary": "Indexes are critical for efficient data retrieval and updates in modern databases. Recent advances in machine learning have led to the development of learned indexes, which model the cumulative distribution function of data to predict search positions and accelerate query processing. While learned indexes substantially outperform traditional structures for point lookups, they often suffer from high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads. To address these challenges, this paper proposes HIRE, a hybrid in-memory index structure designed to deliver efficient performance consistently. HIRE combines the structural and performance robustness of traditional indexes with the predictive power of model-based prediction to reduce search overhead while maintaining worst-case stability. Specifically, it employs (1) hybrid leaf nodes adaptive to varying data distributions and workloads, (2) model-accelerated internal nodes augmented by log-based updates for efficient updates, (3) a nonblocking, cost-driven recalibration mechanism for dynamic data, and (4) an inter-level optimized bulk-loading algorithm accounting for leaf and internal-node errors. Experimental results on multiple real-world datasets demonstrate that HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. Compared to state-of-the-art learned indexes and traditional indexes, HIRE achieves up to 41.7$\\times$ higher throughput under mixed workloads, reduces tail latency by up to 98% across varying scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIRE\u7684\u6df7\u5408\u5185\u5b58\u7d22\u5f15\u7ed3\u6784\uff0c\u7ed3\u5408\u4f20\u7edf\u7d22\u5f15\u7684\u9c81\u68d2\u6027\u4e0e\u5b66\u4e60\u578b\u7d22\u5f15\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5728\u70b9\u67e5\u3001\u8303\u56f4\u67e5\u8be2\u3001\u5c3e\u5ef6\u8fdf\u548c\u52a8\u6001\u6570\u636e\u66f4\u65b0\u7b49\u65b9\u9762\u5b9e\u73b0\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u578b\u7d22\u5f15\u5728\u70b9\u67e5\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5c3e\u5ef6\u8fdf\u3001\u8303\u56f4\u67e5\u8be2\u6027\u80fd\u4ee5\u53ca\u4e0d\u540c\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e9f\u9700\u4e00\u79cd\u517c\u987e\u6548\u7387\u4e0e\u9c81\u68d2\u6027\u7684\u65b0\u7d22\u5f15\u7ed3\u6784\u3002", "method": "HIRE\u91c7\u7528\u56db\u79cd\u5173\u952e\u6280\u672f\uff1a(1) \u81ea\u9002\u5e94\u6570\u636e\u5206\u5e03\u4e0e\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6df7\u5408\u53f6\u8282\u70b9\uff1b(2) \u7ed3\u5408\u6a21\u578b\u52a0\u901f\u4e0e\u65e5\u5fd7\u66f4\u65b0\u7684\u5185\u90e8\u8282\u70b9\uff1b(3) \u65e0\u963b\u585e\u3001\u6210\u672c\u9a71\u52a8\u7684\u52a8\u6001\u91cd\u6821\u51c6\u673a\u5236\uff1b(4) \u8003\u8651\u53f6\u8282\u70b9\u4e0e\u5185\u90e8\u8282\u70b9\u8bef\u5dee\u7684\u8de8\u5c42\u7ea7\u4f18\u5316\u6279\u91cf\u52a0\u8f7d\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHIRE\u5728\u8303\u56f4\u67e5\u8be2\u541e\u5410\u91cf\u3001\u5c3e\u5ef6\u8fdf\u548c\u6574\u4f53\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u578b\u7d22\u5f15\u548c\u4f20\u7edf\u7d22\u5f15\uff0c\u6df7\u5408\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u534741.7\u500d\uff0c\u5c3e\u5ef6\u8fdf\u6700\u591a\u964d\u4f4e98%\u3002", "conclusion": "HIRE\u901a\u8fc7\u878d\u5408\u4f20\u7edf\u7d22\u5f15\u7ed3\u6784\u7684\u7a33\u5b9a\u6027\u4e0e\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u5b66\u4e60\u578b\u7d22\u5f15\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u74f6\u9888\uff0c\u4e3a\u73b0\u4ee3\u6570\u636e\u5e93\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u7d22\u5f15\u65b9\u6848\u3002"}}
{"id": "2511.20982", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20982", "abs": "https://arxiv.org/abs/2511.20982", "authors": ["Junhan Liao", "Minxian Xu", "Wanyi Zheng", "Yan Wang", "Kejiang Ye", "Rajkumar Buyya", "Chengzhong Xu"], "title": "A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving", "comment": "14 pages", "summary": "To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.", "AI": {"tldr": "DOPD is a dynamic LLM inference system that optimizes the prefill-to-decoding (P/D) ratio in real time to address workload imbalance in disaggregated GPU architectures, significantly improving goodput and latency while maintaining high SLO compliance.", "motivation": "Contemporary LLM inference systems decouple prefill and decoding stages onto separate GPUs, but heterogeneous workloads cause producer-consumer imbalance between these stages, leading to inefficiencies and resource mismatches.", "method": "DOPD dynamically adjusts instance allocations for prefill and decoding based on real-time load monitoring and employs an effective request-scheduling policy. It also uses historical load data for proactive reconfiguration to maintain optimal P/D ratios.", "result": "DOPD improves system goodput by up to 1.5\u00d7, reduces P90 TTFT by up to 67.5%, and decreases P90 TPOT by up to 22.8% compared to vLLM and DistServe. It achieves over 99% SLO attainment with minimal additional resources.", "conclusion": "Dynamic adjustment of the prefill/decoding instance ratio based on real-time and historical workload data effectively balances resource utilization, enhances performance, and ensures high SLO compliance in disaggregated LLM inference systems."}}
{"id": "2511.21461", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21461", "abs": "https://arxiv.org/abs/2511.21461", "authors": ["Jonas Elmiger", "Fabian Stuber", "Oscar Casta\u00f1eda", "Gian Marti", "Christoph Studer"], "title": "A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection", "comment": "Presented at the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)", "summary": "We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\\times$ higher per-user throughput and 4.5$\\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u6b3e\u65b0\u578b\u5355\u8f93\u5165\u591a\u8f93\u51fa\uff08SIMO\uff09\u63a5\u6536\u673aASIC\uff0c\u91c7\u7528\u540d\u4e3aMAED\u7684\u7b97\u6cd5\uff0c\u572822 nm FD-SOI\u5de5\u827a\u4e0b\u5b9e\u73b0\u5e72\u6270\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u4e0e\u6570\u636e\u68c0\u6d4b\u7684\u4e00\u4f53\u5316\u5904\u7406\uff0c\u76f8\u8f83\u73b0\u6709\u6280\u672f\u5728\u541e\u5410\u91cf\u548c\u9762\u79ef\u6548\u7387\u65b9\u9762\u5206\u522b\u63d0\u53473\u500d\u548c4.5\u500d\u3002", "motivation": "\u73b0\u6709\u6297\u5e72\u6270\u63a5\u6536\u673a\u5728\u9762\u5bf9\u667a\u80fd\u5e72\u6270\u548c\u538b\u5236\u5f0f\u5e72\u6270\u65f6\uff0c\u901a\u5e38\u5c06\u5e72\u6270\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u4e0e\u6570\u636e\u68c0\u6d4b\u5206\u6b65\u5904\u7406\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002\u4e3a\u63d0\u5347\u5728\u5f3a\u5e72\u6270\u73af\u5883\u4e0b\u7684\u8bef\u7801\u7387\u6027\u80fd\u548c\u7cfb\u7edf\u6548\u7387\uff0c\u4e9f\u9700\u4e00\u79cd\u80fd\u8054\u5408\u4f18\u5316\u4e0a\u8ff0\u4efb\u52a1\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u5de5\u4f5c\u57fa\u4e8esiMultaneous mitigAtion, Estimation, and Detection\uff08MAED\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\uff0c\u7edf\u4e00\u5b9e\u73b0\u5e72\u6270\u4f30\u8ba1\u4e0e\u7a7a\u95f4\u6ee4\u6ce2\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u53ca\u6570\u636e\u68c0\u6d4b\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u4e8e\u4e13\u7528\u96c6\u6210\u7535\u8def\uff08ASIC\uff09\u4e2d\uff0c\u652f\u63018\u6839\u63a5\u6536\u5929\u7ebf\u3002", "result": "\u6240\u8bbe\u8ba1\u7684ASIC\u572822 nm FD-SOI\u5de5\u827a\u4e0b\u5b9e\u73b0\uff0c\u6838\u5fc3\u9762\u79ef\u4e3a0.32 mm\u00b2\uff0c\u529f\u8017223 mW\uff0c\u541e\u5410\u7387\u8fbe100 Mb/s\uff0c\u76f8\u8f83\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6297\u5e72\u6270\u68c0\u6d4b\u5668\uff0c\u6bcf\u7528\u6237\u541e\u5410\u91cf\u63d0\u53473\u500d\uff0c\u9762\u79ef\u6548\u7387\u63d0\u53474.5\u500d\uff0c\u5e76\u80fd\u6709\u6548\u5e94\u5bf9\u667a\u80fd\u5e72\u6270\u4e0e\u538b\u5236\u5f0f\u5e72\u6270\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u9996\u4e2a\u96c6\u5e72\u6270\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u4e0e\u6570\u636e\u68c0\u6d4b\u4e8e\u4e00\u4f53\u7684SIMO\u63a5\u6536\u673aASIC\uff0c\u9a8c\u8bc1\u4e86MAED\u7b97\u6cd5\u5728\u786c\u4ef6\u4e0a\u7684\u9ad8\u6548\u6027\u4e0e\u5b9e\u7528\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6297\u5e72\u6270\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u4e0e\u80fd\u6548\u3002"}}
{"id": "2511.21549", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21549", "abs": "https://arxiv.org/abs/2511.21549", "authors": ["Jason Yik", "Walter Gallego Gomez", "Andrew Cheng", "Benedetto Leto", "Alessandro Pierro", "Noah Pacik-Nelson", "Korneel Van den Berghe", "Vittorio Fra", "Andreea Danielescu", "Gianvito Urgese", "Vijay Janapa Reddi"], "title": "Modeling and Optimizing Performance Bottlenecks for Neuromorphic Accelerators", "comment": null, "summary": "Neuromorphic accelerators offer promising platforms for machine learning (ML) inference by leveraging event-driven, spatially-expanded architectures that naturally exploit unstructured sparsity through co-located memory and compute. However, their unique architectural characteristics create performance dynamics that differ fundamentally from conventional accelerators. Existing workload optimization approaches for neuromorphic accelerators rely on aggregate network-wide sparsity and operation counting, but the extent to which these metrics actually improve deployed performance remains unknown. This paper presents the first comprehensive performance bound and bottleneck analysis of neuromorphic accelerators, revealing the shortcomings of the conventional metrics and offering an understanding of what facets matter for workload performance. We present both theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators: Brainchip AKD1000, Synsense Speck, and Intel Loihi 2. From these, we establish three distinct accelerator bottleneck states, memory-bound, compute-bound, and traffic-bound, and identify which workload configuration features are likely to exhibit these bottleneck states. We synthesize all of our insights into the floorline performance model, a visual model that identifies performance bounds and informs how to optimize a given workload, based on its position on the model. Finally, we present an optimization methodology that combines sparsity-aware training with floorline-informed partitioning. Our methodology achieves substantial performance improvements at iso-accuracy: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u795e\u7ecf\u5f62\u6001\u52a0\u901f\u5668\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6027\u80fd\u8fb9\u754c\u4e0e\u74f6\u9888\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u201cfloorline\u6027\u80fd\u6a21\u578b\u201d\u4ee5\u6307\u5bfc\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5316\uff0c\u5e76\u7ed3\u5408\u7a00\u758f\u611f\u77e5\u8bad\u7ec3\u4e0e\u6a21\u578b\u9a71\u52a8\u7684\u5212\u5206\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4e86\u6700\u9ad83.86\u500d\u7684\u8fd0\u884c\u65f6\u95f4\u63d0\u5347\u548c3.38\u500d\u7684\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u795e\u7ecf\u5f62\u6001\u52a0\u901f\u5668\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5168\u7f51\u7a00\u758f\u5ea6\u548c\u64cd\u4f5c\u8ba1\u6570\u7b49\u6307\u6807\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u662f\u5426\u771f\u6b63\u63d0\u5347\u5b9e\u9645\u90e8\u7f72\u6027\u80fd\u5c1a\u4e0d\u6e05\u695a\u3002\u56e0\u6b64\uff0c\u4e9f\u9700\u6df1\u5165\u7406\u89e3\u5f71\u54cd\u795e\u7ecf\u5f62\u6001\u52a0\u901f\u5668\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5efa\u6a21\u4e0e\u5bf9\u4e09\u79cd\u771f\u5b9e\u795e\u7ecf\u5f62\u6001\u52a0\u901f\u5668\uff08Brainchip AKD1000\u3001Synsense Speck \u548c Intel Loihi 2\uff09\u7684\u5e7f\u6cdb\u5b9e\u8bc1\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u5185\u5b58\u53d7\u9650\u3001\u8ba1\u7b97\u53d7\u9650\u548c\u901a\u4fe1\u53d7\u9650\u4e09\u79cd\u74f6\u9888\u72b6\u6001\uff0c\u5e76\u636e\u6b64\u6784\u5efa\u4e86floorline\u6027\u80fd\u6a21\u578b\uff1b\u8fdb\u4e00\u6b65\u63d0\u51fa\u7ed3\u5408\u7a00\u758f\u611f\u77e5\u8bad\u7ec3\u4e0efloorline\u6307\u5bfc\u7684\u5212\u5206\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u8be5\u4f18\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u76f8\u540c\u7cbe\u5ea6\u4e0b\uff0c\u76f8\u6bd4\u4ee5\u5f80\u624b\u52a8\u8c03\u4f18\u914d\u7f6e\uff0c\u6700\u591a\u5b9e\u73b0\u4e863.86\u500d\u7684\u8fd0\u884c\u65f6\u95f4\u63d0\u5347\u548c3.38\u500d\u7684\u80fd\u8017\u964d\u4f4e\u3002", "conclusion": "\u4f20\u7edf\u57fa\u4e8e\u7a00\u758f\u5ea6\u7684\u4f18\u5316\u6307\u6807\u4e0d\u8db3\u4ee5\u51c6\u786e\u9884\u6d4b\u795e\u7ecf\u5f62\u6001\u52a0\u901f\u5668\u7684\u5b9e\u9645\u6027\u80fd\uff1b\u672c\u6587\u63d0\u51fa\u7684floorline\u6a21\u578b\u548c\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u74f6\u9888\u5e76\u663e\u8457\u63d0\u5347\u6027\u80fd\u4e0e\u80fd\u6548\u3002"}}
{"id": "2511.21413", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21413", "abs": "https://arxiv.org/abs/2511.21413", "authors": ["Tim Trappen", "Robert Ke\u00dfler", "Roland Pabel", "Viktor Achter", "Stefan Wesner"], "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM", "comment": "6 pages, 3 figures", "summary": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d85\u7ea7\u8ba1\u7b97\u673aRAMSES\u4e0a\u7ed3\u5408vLLM\u3001Slurm\u548cKubernetes\u7684\u67b6\u6784\uff0c\u4ee5\u9ad8\u6548\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u540c\u6b65\u3001\u7528\u6237\u4ea4\u4e92\u5f0fAI\u63a8\u7406\u8d1f\u8f7d\uff0c\u5e76\u5728\u9ad8\u5e76\u53d1\u8bf7\u6c42\u4e0b\u4ec5\u5f15\u5165\u7ea6500\u6beb\u79d2\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u9ad8\u6027\u80fd\u8ba1\u7b97\uff08HPC\uff09\u7684\u8fd0\u884c\u6a21\u5f0f\u96be\u4ee5\u6ee1\u8db3\u540c\u6b65\u3001\u9762\u5411\u7528\u6237\u7684\u52a8\u6001AI\u5e94\u7528\u8d1f\u8f7d\u7684\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u7b49\u6559\u80b2\u9886\u57df\u5bf9AI\u63a8\u7406\u9700\u6c42\u4e0d\u65ad\u589e\u957f\u7684\u80cc\u666f\u4e0b\uff0c\u4e9f\u9700\u5229\u7528\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u63d0\u51fa\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728\u8d85\u7ea7\u8ba1\u7b97\u673aRAMSES\u4e0a\u96c6\u6210vLLM\u3001Slurm\u548cKubernetes\uff0c\u6784\u5efa\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\u670d\u52a1\u7684\u65b0\u578b\u67b6\u6784\u3002", "result": "\u521d\u6b65\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728100\u3001500\u548c1000\u4e2a\u5e76\u53d1\u8bf7\u6c42\u4e0b\u5747\u80fd\u9ad8\u6548\u6269\u5c55\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4ec5\u589e\u52a0\u7ea6500\u6beb\u79d2\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u96c6\u6210\u67b6\u6784\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfHPC\u5728\u652f\u6301\u52a8\u6001AI\u63a8\u7406\u8d1f\u8f7d\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u5728\u73b0\u6709\u8d85\u7b97\u57fa\u7840\u8bbe\u65bd\u4e0a\u90e8\u7f72\u9ad8\u6548LLM\u670d\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.20813", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20813", "abs": "https://arxiv.org/abs/2511.20813", "authors": ["Simon Hacks"], "title": "Train While You Fight -- Technical Requirements for Advanced Distributed Learning Platforms", "comment": "17 pages, submitted to CAiSE - International Conference on Advanced information Systems Engineering 2026", "summary": "\"Train While You Fight\" (TWYF) advocates for continuous learning that occurs during operations, not just before or after. This paper examines the technical requirements that advanced distributed learning (ADL) platforms must meet to support TWYF, and how existing software engineering patterns can fulfill these requirements. Using a Design Science Research approach, we (i) derive challenges from PfPC/NATO documentation and recent practice, (ii) define solution objectives, and (iii) conduct a systematic mapping from challenges to proven patterns. We identify seven technical challenges: interoperability, resilience, multilingual support, data security and privacy, scalability, platform independence, and modularity. We illustrate the patterns with a national use case from the German armed forces.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u652f\u6301\u201c\u8fb9\u6218\u8fb9\u8bad\u201d\uff08TWYF\uff09\u7406\u5ff5\u7684\u5148\u8fdb\u5206\u5e03\u5f0f\u5b66\u4e60\uff08ADL\uff09\u5e73\u53f0\u6240\u9700\u6ee1\u8db3\u7684\u6280\u672f\u8981\u6c42\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff0c\u5c06\u6765\u81ea\u5317\u7ea6\u6587\u6863\u4e0e\u5b9e\u8df5\u4e2d\u7684\u4e03\u5927\u6280\u672f\u6311\u6218\u6620\u5c04\u5230\u5df2\u9a8c\u8bc1\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u5f0f\u4e0a\uff0c\u8f85\u4ee5\u5fb7\u56fd\u6b66\u88c5\u90e8\u961f\u7684\u56fd\u5bb6\u7528\u4f8b\u52a0\u4ee5\u8bf4\u660e\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u201c\u8fb9\u6218\u8fb9\u8bad\u201d\uff08TWYF\uff09\u6240\u5021\u5bfc\u7684\u5728\u4f5c\u6218\u8fc7\u7a0b\u4e2d\u6301\u7eed\u5b66\u4e60\uff0c\u9700\u6784\u5efa\u80fd\u591f\u652f\u6301\u8be5\u7406\u5ff5\u7684\u5148\u8fdb\u5206\u5e03\u5f0f\u5b66\u4e60\uff08ADL\uff09\u5e73\u53f0\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u660e\u786e\u5176\u5173\u952e\u6280\u672f\u9700\u6c42\u5e76\u5bfb\u627e\u53ef\u884c\u7684\u8f6f\u4ef6\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8bbe\u8ba1\u79d1\u5b66\u7814\u7a76\u65b9\u6cd5\uff1a\uff08i\uff09\u4ecePfPC/\u5317\u7ea6\u6587\u6863\u548c\u8fd1\u671f\u5b9e\u8df5\u4e2d\u63d0\u70bc\u6311\u6218\uff1b\uff08ii\uff09\u5b9a\u4e49\u89e3\u51b3\u65b9\u6848\u76ee\u6807\uff1b\uff08iii\uff09\u7cfb\u7edf\u5730\u5c06\u6311\u6218\u6620\u5c04\u5230\u5df2\u6709\u7684\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u5f0f\u3002", "result": "\u8bc6\u522b\u51fa\u4e03\u5927\u6280\u672f\u6311\u6218\uff1a\u4e92\u64cd\u4f5c\u6027\u3001\u5f39\u6027\u3001\u591a\u8bed\u8a00\u652f\u6301\u3001\u6570\u636e\u5b89\u5168\u4e0e\u9690\u79c1\u3001\u53ef\u6269\u5c55\u6027\u3001\u5e73\u53f0\u65e0\u5173\u6027\u548c\u6a21\u5757\u5316\uff0c\u5e76\u901a\u8fc7\u5fb7\u56fd\u6b66\u88c5\u90e8\u961f\u7684\u5b9e\u9645\u7528\u4f8b\u5c55\u793a\u4e86\u76f8\u5173\u8f6f\u4ef6\u6a21\u5f0f\u7684\u5e94\u7528\u3002", "conclusion": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u5f0f\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u652f\u6301TWYF\u7684ADL\u5e73\u53f0\u6240\u9762\u4e34\u7684\u5173\u952e\u6280\u672f\u6311\u6218\uff0c\u4e3a\u672a\u6765\u519b\u4e8b\u8bad\u7ec3\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2511.21431", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21431", "abs": "https://arxiv.org/abs/2511.21431", "authors": ["Lu Zhao", "Rong Shi", "Shaoqing Zhang", "Yueqiang Chen", "Baoguo He", "Hongfeng Sun", "Ziqing Yin", "Shangchao Su", "Zhiyan Cui", "Liang Dong", "Xiyuan Li", "Lingbin Wang", "Jianwei He", "Jiesong Ma", "Weikang Huang", "Jianglei Tong", "Dongdong Gao", "Jian Zhang", "Hong Tian"], "title": "MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training", "comment": null, "summary": "The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.", "AI": {"tldr": "MemFine \u662f\u4e00\u79cd\u9762\u5411\u5185\u5b58\u4f18\u5316\u7684\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u91cd\u8ba1\u7b97\u7b56\u7565\u6709\u6548\u7f13\u89e3\u5927\u89c4\u6a21 Mixture of Experts\uff08MoE\uff09\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5728\u6709\u9650\u663e\u5b58\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7a33\u5b9a\u7684\u8bad\u7ec3\u3002", "motivation": "\u5927\u89c4\u6a21 MoE \u6a21\u578b\u8bad\u7ec3\u56e0\u52a8\u6001 token \u8def\u7531\u5bfc\u81f4\u4e25\u91cd\u7684\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u5f15\u53d1 GPU \u5185\u5b58\u6ea2\u51fa\uff0c\u9650\u5236\u4e86\u6a21\u578b\u53ef\u6269\u5c55\u6027\uff1b\u73b0\u6709\u57fa\u4e8e\u4e13\u5bb6\u5bb9\u91cf\u9650\u5236\u7684\u8d1f\u8f7d\u5747\u8861\u65b9\u6cd5\u5728\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u4e0a\u4f1a\u635f\u5bb3\u6a21\u578b\u7cbe\u5ea6\u4e14\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa MemFine \u6846\u67b6\uff0c\u5c06 token \u5206\u914d\u4e0e\u4e13\u5bb6\u8ba1\u7b97\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5c0f\u5757\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u7406\u8bba\u5185\u5b58\u6a21\u578b\u52a8\u6001\u4f18\u5316\u7684\u5206\u5757\u91cd\u8ba1\u7b97\u7b56\u7565\uff0c\u5728\u5185\u5b58\u6548\u7387\u4e0e\u541e\u5410\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u4e8e\u5168\u91cd\u8ba1\u7b97\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cMemFine \u51cf\u5c11\u4e86 48.03% \u7684\u6fc0\u6d3b\u5185\u5b58\u5360\u7528\uff0c\u5e76\u63d0\u5347\u4e86 4.42% \u7684\u541e\u5410\u91cf\uff0c\u652f\u6301\u5728\u5185\u5b58\u53d7\u9650 GPU \u4e0a\u7a33\u5b9a\u8bad\u7ec3\u5927\u89c4\u6a21 MoE \u6a21\u578b\u3002", "conclusion": "MemFine \u6709\u6548\u89e3\u51b3\u4e86 MoE \u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u969c\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5185\u5b58\u6548\u7387\u548c\u8bad\u7ec3\u541e\u5410\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u6269\u5c55 MoE \u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.21535", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21535", "abs": "https://arxiv.org/abs/2511.21535", "authors": ["Morteza Sadeghi"], "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation", "comment": null, "summary": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u6570\u636e\u5197\u4f59\u63d0\u5347MLFMA\u4e2d\u8fd1\u573a\uff08P2P\uff09\u7b97\u5b50\u5728GPU\u4e0a\u7684\u7a7a\u95f4\u5c40\u90e8\u6027\uff0c\u4ece\u800c\u6539\u5584\u7f13\u5b58\u884c\u4e3a\u5e76\u5b9e\u73b0\u6700\u9ad87\u500d\u7684\u6838\u51fd\u6570\u52a0\u901f\uff1b\u5c3d\u7ba1\u6574\u4f53\u5e94\u7528\u52a0\u901f\u53d7\u9650\u4e8e\u6570\u636e\u91cd\u6784\u5f00\u9500\uff0c\u4f46\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u5c40\u90e8\u6027\u6307\u6807\u7684\u5206\u6790\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u6027\u80fd\u8d8b\u52bf\uff0c\u4e14\u8be5\u65b9\u6cd5\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u4ee3\u7801\u4e2d\u3002", "motivation": "MLFMA\u4e2d\u7684\u8fd1\u573a\uff08P2P\uff09\u7b97\u5b50\u5728GPU\u4e0a\u56e0\u5185\u5b58\u5c40\u90e8\u6027\u5dee\u800c\u6210\u4e3a\u6027\u80fd\u74f6\u9888\u3002", "method": "\u5f15\u5165\u6570\u636e\u5197\u4f59\u4ee5\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5206\u6563\u6027\uff0c\u63d0\u5347\u7a7a\u95f4\u5c40\u90e8\u6027\uff1b\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u6570\u636e\u91cf\u4e0e\u8bbf\u95ee\u5206\u6563\u5ea6\u7684\u5c40\u90e8\u6027\u6307\u6807\u5206\u6790\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u52a0\u901f\u8d8b\u52bf\uff0c\u65e0\u9700\u4f9d\u8d56\u786c\u4ef6\u6027\u80fd\u5206\u6790\u3002", "result": "\u5728\u4e24\u4e2aMLFMA\u5e94\u7528\uff08DBIM-MLFMA\u548cPhotoNs-2.0\uff09\u4e0a\u9a8c\u8bc1\uff0c\u6838\u51fd\u6570\u6700\u9ad8\u83b7\u5f977\u500d\u52a0\u901f\uff0c\u4f46\u7aef\u5230\u7aef\u5e94\u7528\u52a0\u901f\u4ec5\u8fbe1.04\u500d\uff1b\u5206\u6790\u6a21\u578b\u867d\u4e0d\u80fd\u7cbe\u786e\u9884\u6d4b\u7edd\u5bf9\u52a0\u901f\u6bd4\uff0c\u4f46\u80fd\u53ef\u9760\u53cd\u6620\u4e0d\u540c\u95ee\u9898\u89c4\u6a21\u548c\u5bc6\u5ea6\u4e0b\u7684\u6027\u80fd\u8d8b\u52bf\u3002", "conclusion": "\u6570\u636e\u5197\u4f59\u53ef\u6709\u6548\u63d0\u5347GPU\u4e0aP2P\u7b97\u5b50\u7684\u6027\u80fd\uff0c\u524d\u63d0\u662f\u5c40\u90e8\u6027\u6536\u76ca\u8d85\u8fc7\u6570\u636e\u79fb\u52a8\u5e26\u6765\u7684\u5f00\u9500\uff1b\u8be5\u6280\u672f\u6613\u4e8e\u5d4c\u5165\u73b0\u6709\u5b9e\u73b0\uff0c\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.20933", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.20933", "abs": "https://arxiv.org/abs/2511.20933", "authors": ["Mootez Saad", "Boqi Chen", "Jos\u00e9 Antonio Hern\u00e1ndez L\u00f3pez", "D\u00e1niel Varr\u00f3", "Tushar Sharma"], "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code", "comment": "18 figures", "summary": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08DeepSeek-R1\u7cfb\u5217\uff09\u5bf9\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5185\u805a\u6027\u548c\u8026\u5408\u6027\u6982\u5ff5\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u566a\u58f0\u5e72\u6270\u548c\u5f00\u653e\u5f0f\u4efb\u52a1\u4e2d\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff0c\u5c24\u5176\u5bf9\u8026\u5408\u6027\u7684\u7406\u89e3\u66f4\u4e3a\u8106\u5f31\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u5bf9\u6838\u5fc3\u8f6f\u4ef6\u8bbe\u8ba1\u6982\u5ff5\uff08\u5982\u5185\u805a\u4e0e\u8026\u5408\uff09\u7684\u7406\u89e3\u7a33\u5065\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u751f\u6210\u8bbe\u8ba1\u4e0d\u826f\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u5728\u4e0d\u540c\u5f15\u5bfc\u7a0b\u5ea6\uff08\u9a8c\u8bc1\u3001\u5f15\u5bfc\u3001\u5f00\u653e\u5f0f\u751f\u6210\uff09\u548c\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\uff08\u6ce8\u5165\u5e72\u6270\u5143\u7d20\uff09\u4e0b\uff0c\u6d4b\u8bd5DeepSeek-R1\u7cfb\u5217\u6a21\u578b\uff0814B\u300132B\u300170B\uff09\u5bf9\u5185\u805a\u6027\u548c\u8026\u5408\u6027\u7684\u8bc6\u522b\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u5206\u6790\u5176\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u6a21\u578b\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u5bf9\u4e24\u4e2a\u6982\u5ff5\u6709\u8f83\u597d\u7406\u89e3\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u8106\u5f31\u4e14\u4e0d\u5bf9\u79f0\uff1a\u8026\u5408\u6027\u63a8\u7406\u5728\u566a\u58f0\u548c\u5f00\u653e\u5f0f\u573a\u666f\u4e2dF1\u5206\u6570\u4e0b\u964d\u8d8550%\uff1b\u5185\u805a\u6027\u5728\u5f15\u5bfc\u4efb\u52a1\u4e2d\u5bf9\u5185\u90e8\u566a\u58f0\u9c81\u68d2\uff0c\u4f46\u65e0\u5f15\u5bfc\u65f6\u540c\u6837\u5931\u6548\u3002\u63a8\u7406\u8f68\u8ff9\u663e\u793a\u6a21\u578b\u5bf9\u8026\u5408\u91c7\u53d6\u201c\u8ba4\u77e5\u6377\u5f84\u201d\uff0c\u5bf9\u5185\u805a\u5219\u8fdb\u884c\u66f4\u8be6\u5c3d\u4f46\u4ecd\u5931\u8d25\u7684\u5206\u6790\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u5728\u8bc6\u522b\u8bbe\u8ba1\u7f3a\u9677\u65b9\u9762\u63d0\u4f9b\u53ef\u9760\u8f85\u52a9\uff0c\u4f46\u5728\u5608\u6742\u3001\u771f\u5b9e\u573a\u666f\u4e2d\u7f3a\u4e4f\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\uff0c\u51f8\u663e\u63d0\u5347\u5176\u7a0b\u5e8f\u7406\u89e3\u53ef\u6269\u5c55\u6027\u4e0e\u9c81\u68d2\u6027\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2511.20955", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.20955", "abs": "https://arxiv.org/abs/2511.20955", "authors": ["Sanchit Kaul", "Kevin Nhu", "Jason Eissayou", "Ivan Eser", "Victor Borup"], "title": "SpaceX: Exploring metrics with the SPACE model for developer productivity", "comment": "Code available at https://github.com/knhu/ECS260Project", "summary": "This empirical investigation elucidates the limitations of deterministic, unidimensional productivity heuristics by operationalizing the SPACE framework through extensive repository mining. Utilizing a dataset derived from open-source repositories, the study employs rigorous statistical methodologies including Generalized Linear Mixed Models (GLMM) and RoBERTa-based sentiment classification to synthesize a holistic, multi-faceted productivity metric. Analytical results reveal a statistically significant positive correlation between negative affective states and commit frequency, implying a cycle of iterative remediation driven by frustration. Furthermore, the investigation has demonstrated that analyzing the topology of contributor interactions yields superior fidelity in mapping collaborative dynamics compared to traditional volume-based metrics. Ultimately, this research posits a Composite Productivity Score (CPS) to address the heterogeneity of developer efficacy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6316\u6398\u5f00\u6e90\u4ed3\u5e93\u6570\u636e\uff0c\u7ed3\u5408\u7edf\u8ba1\u6a21\u578b\u4e0e\u60c5\u611f\u5206\u6790\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ef4\u5ea6\u7684\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u7efc\u5408\u8bc4\u4f30\u6307\u6807\uff08CPS\uff09\uff0c\u63ed\u793a\u8d1f\u9762\u60c5\u7eea\u4e0e\u63d0\u4ea4\u9891\u7387\u6b63\u76f8\u5173\uff0c\u5e76\u8bc1\u660e\u57fa\u4e8e\u534f\u4f5c\u62d3\u6251\u7684\u5206\u6790\u4f18\u4e8e\u4f20\u7edf\u6570\u91cf\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u751f\u4ea7\u529b\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u5355\u4e00\u3001\u786e\u5b9a\u6027\u7684\u542f\u53d1\u5f0f\u6307\u6807\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u5f00\u53d1\u8005\u5728\u590d\u6742\u534f\u4f5c\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u6548\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u6784\u5efa\u66f4\u5168\u9762\u3001\u591a\u7ef4\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90\u4ed3\u5e93\u6570\u636e\uff0c\u8fd0\u7528\u5e7f\u4e49\u7ebf\u6027\u6df7\u5408\u6a21\u578b\uff08GLMM\uff09\u548cRoBERTa\u60c5\u611f\u5206\u7c7b\u65b9\u6cd5\uff0c\u64cd\u4f5c\u5316SPACE\u6846\u67b6\uff0c\u6784\u5efa\u5305\u542b\u60c5\u611f\u72b6\u6001\u4e0e\u534f\u4f5c\u62d3\u6251\u7684\u590d\u5408\u751f\u4ea7\u529b\u8bc4\u5206\uff08CPS\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8d1f\u9762\u60c5\u7eea\u72b6\u6001\u4e0e\u4ee3\u7801\u63d0\u4ea4\u9891\u7387\u5448\u663e\u8457\u6b63\u76f8\u5173\uff0c\u4e14\u57fa\u4e8e\u8d21\u732e\u8005\u4e92\u52a8\u62d3\u6251\u7ed3\u6784\u7684\u5206\u6790\u6bd4\u4f20\u7edf\u57fa\u4e8e\u6570\u91cf\u7684\u6307\u6807\u66f4\u80fd\u51c6\u786e\u523b\u753b\u534f\u4f5c\u52a8\u6001\u3002", "conclusion": "\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u5e94\u901a\u8fc7\u591a\u7ef4\u5ea6\u3001\u60c5\u5883\u654f\u611f\u7684\u7efc\u5408\u6307\u6807\u6765\u8861\u91cf\uff0c\u6240\u63d0\u51fa\u7684CPS\u80fd\u66f4\u6709\u6548\u5730\u6355\u6349\u5f00\u53d1\u6548\u80fd\u7684\u5f02\u8d28\u6027\u3002"}}
{"id": "2511.21022", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21022", "abs": "https://arxiv.org/abs/2511.21022", "authors": ["Guancheng Lin", "Xiao Yu", "Jacky Keung", "Xing Hu", "Xin Xia", "Alex X. Liu"], "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "comment": null, "summary": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e8610\u79cd\u4e3b\u6d41\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5728\u66f4\u65b0\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u5df2\u5f03\u7528API\u77e5\u8bc6\u65b9\u9762\u7684\u6548\u679c\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5AdaLoRA-L\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u7684\u7279\u5f02\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u8865\u5168\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8bad\u7ec3\u6570\u636e\u65f6\u6548\u6027\u6709\u9650\uff0c\u5e38\u751f\u6210\u5df2\u88ab\u5f03\u7528\u7684API\u3002\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u800c\u73b0\u6709\u8f7b\u91cf\u7ea7\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u662f\u5426\u80fd\u6709\u6548\u66f4\u65b0\u6b64\u7c7b\u77e5\u8bc6\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u6784\u5efa\u5305\u542b70\u591a\u4e2a\u5df2\u5f03\u7528API\u548c3000\u591a\u4e2a\u7f16\u8f91\u5b9e\u4f8b\u7684\u57fa\u51c6EDA PI Bench\uff1b\u5728Qwen2.5-Coder\u3001StarCoder2\u548cDeepSeek-Coder\u4e0a\u8bc4\u4f3010\u79cd\u6a21\u578b\u7f16\u8f91\u6280\u672f\uff1b\u63d0\u51faAdaLoRA-L\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u5206\u201c\u901a\u7528API\u5c42\u201d\u4e0e\u201c\u7279\u5b9aAPI\u5c42\u201d\uff0c\u4ec5\u5bf9\u540e\u8005\u8fdb\u884c\u7f16\u8f91\u4ee5\u63d0\u5347\u7279\u5f02\u6027\u3002", "result": "AdaLoRA\u5728\u751f\u6210\u6b63\u786e\u3001\u6700\u65b0API\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u7279\u5f02\u6027\u4e0d\u8db3\uff1bAdaLoRA-L\u5728\u4fdd\u6301\u5176\u4ed6\u6307\u6807\u76f8\u5f53\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7279\u5f02\u6027\u3002", "conclusion": "\u6a21\u578b\u7f16\u8f91\u53ef\u6709\u6548\u66f4\u65b0LLM\u4e2d\u7684\u8fc7\u65f6API\u77e5\u8bc6\uff0c\u800c\u901a\u8fc7\u5206\u5c42\u7f16\u8f91\u7b56\u7565\uff08\u5982AdaLoRA-L\uff09\u80fd\u517c\u987e\u6027\u80fd\u4e0e\u7f16\u8f91\u7cbe\u5ea6\uff0c\u4e3a\u9ad8\u6548\u7ef4\u62a4\u6a21\u578b\u4ee3\u7801\u77e5\u8bc6\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2511.21151", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21151", "abs": "https://arxiv.org/abs/2511.21151", "authors": ["M. Alecci", "P. Jim\u00e9nez", "J. Samhi", "T. Bissyand\u00e9", "J. Klein"], "title": "Exploring Hidden Geographic Disparities in Android Apps", "comment": null, "summary": "While mobile app evolution has been widely studied, geographical variation in app behavior remains largely unexplored. This paper presents a large-scale study of location-based Android app differentiation, uncovering two important and underexamined phenomena with security and fairness implications. First, we introduce GeoTwins: apps that are functionally similar and share branding but are released under different package names across countries. Despite their similarity, GeoTwins often diverge in requested permissions, third-party libraries, and privacy disclosures. Second, we examine the Android App Bundle ecosystem and reveal unexpected regional differences in supposedly consistent base.apk files. Contrary to common assumptions, even base.apk files vary by region, exposing hidden customizations that may affect app behavior or security.\n  These discrepancies have concrete consequences. Geographically distinct variants can lead the same app to be labeled benign in one malware study but suspicious in another, depending on the region of download. Such hidden variation undermines reproducibility and introduces geographic bias into assessments of security, privacy, and functionality. It also raises ethical concerns about transparency and consent: visually identical Google Play listings may mask subtle but important differences.\n  To study these issues, we built a distributed app collection pipeline spanning multiple regions and analyzed thousands of apps. We also release a dataset of 81,963 GeoTwins to support future work. Our findings reveal systemic regional disparities in mobile software, with implications for researchers, developers, platform architects, and policymakers.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63ed\u793a\u4e86Android\u5e94\u7528\u5728\u4e0d\u540c\u5730\u7406\u533a\u57df\u5b58\u5728\u663e\u8457\u4f46\u88ab\u5ffd\u89c6\u7684\u884c\u4e3a\u5dee\u5f02\uff0c\u5305\u62ec\u529f\u80fd\u76f8\u4f3c\u4f46\u5305\u540d\u4e0d\u540c\u7684\u201cGeoTwins\u201d\u5e94\u7528\u4ee5\u53ca\u540c\u4e00\u5e94\u7528\u57fa\u7840APK\u6587\u4ef6\u7684\u533a\u57df\u6027\u53d8\u5316\uff0c\u8fd9\u4e9b\u5dee\u5f02\u5f71\u54cd\u5b89\u5168\u8bc4\u4f30\u3001\u9690\u79c1\u900f\u660e\u5ea6\u548c\u7814\u7a76\u53ef\u590d\u73b0\u6027\u3002", "motivation": "\u5c3d\u7ba1\u79fb\u52a8\u5e94\u7528\u6f14\u5316\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u5728\u5730\u7406\u7ef4\u5ea6\u4e0a\u7684\u884c\u4e3a\u5dee\u5f02\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u4f5c\u8005\u65e8\u5728\u63ed\u793a\u8fd9\u79cd\u5730\u57df\u6027\u5dee\u5f02\u53ca\u5176\u5bf9\u5b89\u5168\u3001\u516c\u5e73\u6027\u548c\u7814\u7a76\u53ef\u590d\u73b0\u6027\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u8de8\u5730\u533a\u7684\u5206\u5e03\u5f0f\u5e94\u7528\u6536\u96c6\u7ba1\u9053\uff0c\u91c7\u96c6\u5e76\u5206\u6790\u6570\u5343\u6b3e\u5e94\u7528\uff1b\u8bc6\u522b\u51fa81,963\u4e2aGeoTwins\u6837\u672c\uff0c\u5e76\u5bf9\u6bd4\u5176\u6743\u9650\u8bf7\u6c42\u3001\u7b2c\u4e09\u65b9\u5e93\u548c\u9690\u79c1\u58f0\u660e\u7b49\u5c5e\u6027\uff1b\u540c\u65f6\u68c0\u67e5Android App Bundle\u751f\u6001\u7cfb\u7edf\u4e2dbase.apk\u6587\u4ef6\u7684\u533a\u57df\u6027\u5dee\u5f02\u3002", "result": "\u53d1\u73b0GeoTwins\u5728\u4e0d\u540c\u56fd\u5bb6\u867d\u5916\u89c2\u548c\u529f\u80fd\u76f8\u4f3c\uff0c\u4f46\u5728\u6743\u9650\u3001\u5e93\u4f9d\u8d56\u548c\u9690\u79c1\u62ab\u9732\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u5373\u4f7fbase.apk\u6587\u4ef6\u4e5f\u56e0\u5730\u533a\u800c\u5f02\uff0c\u5bfc\u81f4\u540c\u4e00\u5e94\u7528\u5728\u4e0d\u540c\u5730\u533a\u53ef\u80fd\u88ab\u5224\u5b9a\u4e3a\u826f\u6027\u6216\u53ef\u7591\uff0c\u4ece\u800c\u5f15\u5165\u5730\u7406\u504f\u89c1\u3002", "conclusion": "\u79fb\u52a8\u5e94\u7528\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u5730\u57df\u5dee\u5f02\uff0c\u8fd9\u4e0d\u4ec5\u524a\u5f31\u4e86\u5b89\u5168\u4e0e\u9690\u79c1\u7814\u7a76\u7684\u53ef\u590d\u73b0\u6027\uff0c\u8fd8\u5e26\u6765\u4f26\u7406\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u547c\u5401\u7814\u7a76\u4eba\u5458\u3001\u5f00\u53d1\u8005\u3001\u5e73\u53f0\u8bbe\u8ba1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5173\u6ce8\u5e76\u5e94\u5bf9\u8fd9\u4e00\u73b0\u8c61\u3002"}}
{"id": "2511.21197", "categories": ["cs.SE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.21197", "abs": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "comment": null, "summary": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u516d\u573a\u4e0e58\u540d\u5f00\u53d1\u8005\u7684\u8054\u5408\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u63a2\u7d22\u4e86\u5f00\u53d1\u8005\u5bf9AI\u8f85\u52a9\u5de5\u5177\uff08\u5982\u7f3a\u9677\u68c0\u6d4b\u548c\u4ee3\u7801\u53ef\u8bfb\u6027\u8bc4\u4f30\uff09\u7684\u5fc3\u7406\u6a21\u578b\uff0c\u5e76\u63d0\u70bc\u51fa\u4ee5\u4eba\u4e3a\u672c\u7684AI\u96c6\u6210\u5f00\u53d1\u73af\u5883\uff08IDE\uff09\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u5c3d\u7ba1AI\u8f85\u52a9\u5de5\u5177\u5728\u6280\u672f\u4e0a\u4e0d\u65ad\u8fdb\u6b65\uff0c\u4f46\u4eba\u4eec\u5bf9\u5f00\u53d1\u8005\u5982\u4f55\u7406\u89e3\u8fd9\u4e9b\u5de5\u5177\u3001\u5fc3\u7406\u6a21\u578b\u4e0e\u5b9e\u9645\u529f\u80fd\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u5982\u4f55\u5f71\u54cd\u4fe1\u4efb\u3001\u63a7\u5236\u548c\u91c7\u7eb3\u4ecd\u77e5\u4e4b\u751a\u5c11\u3002", "method": "\u5f00\u5c55\u516d\u573a\u8054\u5408\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u5171\u9080\u8bf758\u540d\u5f00\u53d1\u8005\u53c2\u4e0e\uff0c\u4ee5\u5f15\u51fa\u4ed6\u4eec\u5bf9AI\u8f85\u52a9\u7f3a\u9677\u68c0\u6d4b\u548c\u4ee3\u7801\u53ef\u8bfb\u6027\u529f\u80fd\u7684\u5fc3\u7406\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f00\u53d1\u8005\u5c06\u7f3a\u9677\u68c0\u6d4b\u5de5\u5177\u89c6\u4e3a\u201c\u7f3a\u9677\u4fa6\u63a2\u201d\uff0c\u5f3a\u8c03\u5173\u952e\u95ee\u9898\u9884\u8b66\u3001\u900f\u660e\u6027\u3001\u53ef\u64cd\u4f5c\u53cd\u9988\u548c\u4fe1\u5fc3\u63d0\u793a\uff1b\u800c\u5c06\u53ef\u8bfb\u6027\u8bc4\u4f30\u5de5\u5177\u89c6\u4e3a\u201c\u8d28\u91cf\u6559\u7ec3\u201d\uff0c\u5f3a\u8c03\u60c5\u5883\u5316\u3001\u4e2a\u6027\u5316\u548c\u6e10\u8fdb\u5f0f\u6307\u5bfc\u3002\u4fe1\u4efb\u4f9d\u8d56\u4e8e\u89e3\u91ca\u6e05\u6670\u5ea6\u3001\u65f6\u673a\u548c\u7528\u6237\u63a7\u5236\u3002", "conclusion": "\u7814\u7a76\u63d0\u70bc\u51fa\u4e00\u5957\u9762\u5411IDE\u4e2d\u4eba\u672cAI\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u65e8\u5728\u5e73\u8861\u5e72\u6270\u4e0e\u652f\u6301\u3001\u7b80\u6d01\u4e0e\u6df1\u5ea6\u3001\u81ea\u52a8\u5316\u4e0e\u4eba\u7c7b\u80fd\u52a8\u6027\u3002"}}
{"id": "2511.21380", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21380", "abs": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "comment": null, "summary": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5b9e\u8bc1\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u5982GitHub Copilot\uff09\u5728\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u5de5\u4ef6\u8de8\u6570\u636e\u96c6\u9002\u914d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u7cfb\u7edf\u867d\u80fd\u8bc6\u522b\u5173\u952e\u6587\u4ef6\u5e76\u751f\u6210\u90e8\u5206\u9002\u914d\u4ee3\u7801\uff0c\u4f46\u6781\u5c11\u4ea7\u51fa\u529f\u80fd\u6b63\u786e\u7684\u5b9e\u73b0\uff1b\u901a\u8fc7\u63d0\u4f9b\u6267\u884c\u9519\u8bef\u4fe1\u606f\u548c\u53c2\u8003\u4ee3\u7801\u7b49\u63d0\u793a\u5e72\u9884\uff0c\u53ef\u663e\u8457\u63d0\u5347\u751f\u6210\u4ee3\u7801\u4e0e\u771f\u5b9e\u7ed3\u679c\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u3002", "motivation": "\u81ea\u52a8\u5316\u9002\u914d\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u5de5\u4ef6\u4ee5\u9002\u5e94\u4e0d\u540c\u6570\u636e\u96c6\u5bf9\u53ef\u6269\u5c55\u6027\u548c\u53ef\u590d\u73b0\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u5bf9\u6b64\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76\uff1b\u800c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5c55\u73b0\u51fa\u81ea\u52a8\u5316\u590d\u6742\u5f00\u53d1\u6d41\u7a0b\u7684\u6f5c\u529b\uff0c\u503c\u5f97\u6df1\u5165\u8bc4\u4f30\u3002", "method": "\u4f5c\u8005\u6784\u5efa\u4e86\u4e00\u4e2a\u4e94\u9636\u6bb5\u8bc4\u4f30\u6d41\u7a0b\uff08\u6587\u4ef6\u7406\u89e3\u3001\u4ee3\u7801\u7f16\u8f91\u3001\u547d\u4ee4\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u6700\u7ec8\u6267\u884c\uff09\uff0c\u5728ROCODE\u548cLogHub2.0\u7b49\u57fa\u51c6\u4ed3\u5e93\u4e0a\uff0c\u8bc4\u4f30\u7531GPT-4.1\u548cClaude Sonnet 4\u652f\u6301\u7684Copilot\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5e76\u6d4b\u8bd5\u4e86\u591a\u79cd\u63d0\u793a\u5e72\u9884\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u8bc6\u522b\u5173\u952e\u6587\u4ef6\u5e76\u751f\u6210\u90e8\u5206\u9002\u914d\uff0c\u4f46\u529f\u80fd\u6b63\u786e\u7387\u4f4e\uff1b\u5f15\u5165\u6267\u884c\u9519\u8bef\u4fe1\u606f\u548c\u53c2\u8003\u4ee3\u7801\u7b49\u63d0\u793a\u5e72\u9884\u540e\uff0c\u751f\u6210\u4ee3\u7801\u4e0e\u771f\u5b9e\u5b9e\u73b0\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\u4ece7.25%\u663e\u8457\u63d0\u5347\u81f367.14%\u3002", "conclusion": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u6570\u636e\u96c6\u9002\u914d\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\u4f46\u4ecd\u6709\u660e\u663e\u5c40\u9650\uff1b\u672a\u6765\u5e94\u805a\u7126\u4e8e\u6784\u5efa\u66f4\u5177\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u548c\u81ea\u6211\u7ea0\u9519\u80fd\u529b\u7684\u667a\u80fd\u4f53\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u8f6f\u4ef6\u5de5\u7a0b\u7814\u7a76\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2511.21382", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21382", "abs": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "comment": "33 pages, 8 figures", "summary": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.", "AI": {"tldr": "\u672c\u6587\u5bf92021\u5e745\u6708\u81f32025\u5e748\u6708\u95f4\u53d1\u8868\u7684115\u7bc7\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e2d\u5e94\u7528\u7684\u6587\u732e\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u751f\u547d\u5468\u671f\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u63d0\u793a\u5de5\u7a0b\u3001\u8fed\u4ee3\u9a8c\u8bc1\u4fee\u590d\u7b49\u5173\u952e\u6280\u672f\uff0c\u6307\u51fa\u5f53\u524dLLM\u751f\u6210\u6d4b\u8bd5\u5728\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u548c\u8bc4\u4f30\u6807\u51c6\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u672a\u6765\u5e94\u53d1\u5c55\u81ea\u4e3b\u6d4b\u8bd5\u667a\u80fd\u4f53\u4e0e\u6df7\u5408\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u5316\u5355\u5143\u6d4b\u8bd5\u65b9\u6cd5\u867d\u80fd\u6709\u6548\u63a2\u7d22\u7a0b\u5e8f\u7ed3\u6784\uff0c\u4f46\u7f3a\u4e4f\u8bed\u4e49\u4fe1\u606f\u4ee5\u751f\u6210\u771f\u5b9e\u8f93\u5165\u548c\u65ad\u8a00\uff1b\u800c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u51ed\u501f\u5176\u5bf9\u4ee3\u7801\u8bed\u4e49\u548c\u7f16\u7a0b\u6a21\u5f0f\u7684\u6570\u636e\u9a71\u52a8\u7406\u89e3\uff0c\u6709\u671b\u5f25\u8865\u8fd9\u4e00\u4e0d\u8db3\u3002\u4e3a\u5398\u6e05\u8be5\u9886\u57df\u7684\u7814\u7a76\u73b0\u72b6\u5e76\u6307\u5bfc\u672a\u6765\u53d1\u5c55\uff0c\u4f5c\u8005\u5f00\u5c55\u4e86\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u3002", "method": "\u4f5c\u8005\u5bf92021\u5e745\u6708\u81f32025\u5e748\u6708\u95f4\u53d1\u8868\u7684115\u7bc7\u76f8\u5173\u8bba\u6587\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u6587\u732e\u56de\u987e\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u751f\u547d\u5468\u671f\u7684\u7edf\u4e00\u5206\u7c7b\u6846\u67b6\uff0c\u5c06LLM\u89c6\u4e3a\u9700\u7cfb\u7edf\u5de5\u7a0b\u7ea6\u675f\u7684\u968f\u673a\u751f\u6210\u5668\uff0c\u5e76\u636e\u6b64\u5206\u6790\u6838\u5fc3\u751f\u6210\u7b56\u7565\u53ca\u4ece\u4e0a\u4e0b\u6587\u589e\u5f3a\u5230\u8d28\u91cf\u4fdd\u969c\u7684\u5404\u7c7b\u4f18\u5316\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u63d0\u793a\u5de5\u7a0b\u662f\u4e3b\u6d41\u7b56\u7565\uff0c\u5360\u6240\u8c03\u7814\u7814\u7a76\u768489%\uff1b\u8fed\u4ee3\u9a8c\u8bc1\u4e0e\u4fee\u590d\u673a\u5236\u5df2\u6210\u4e3a\u63d0\u5347\u751f\u6210\u6d4b\u8bd5\u901a\u8fc7\u7387\u7684\u6807\u51c6\u505a\u6cd5\uff1b\u4f46\u751f\u6210\u6d4b\u8bd5\u7684\u7f3a\u9677\u68c0\u6d4b\u80fd\u529b\u8f83\u5f31\uff0c\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u4e8e\u53d1\u5c55\u81ea\u4e3b\u6d4b\u8bd5\u667a\u80fd\u4f53\u53ca\u878d\u5408LLM\u4e0e\u4f20\u7edf\u8f6f\u4ef6\u5de5\u7a0b\u5de5\u5177\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u4ee5\u63a8\u52a8LLM\u5728\u5de5\u4e1a\u7ea7\u6d4b\u8bd5\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
