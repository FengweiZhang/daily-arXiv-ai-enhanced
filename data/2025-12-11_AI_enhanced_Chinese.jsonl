{"id": "2512.09506", "categories": ["cs.CE"], "pdf": "https://arxiv.org/pdf/2512.09506", "abs": "https://arxiv.org/abs/2512.09506", "authors": ["Jinru Ding", "Chao Ding", "Wenrao Pang", "Boyi Xiao", "Zhiqiang Liu", "Pengcheng Chen", "Jiayuan Chen", "Tiantian Yuan", "Junming Guan", "Yidong Jiang", "Dawei Cheng", "Jie Xu"], "title": "CNFinBench: A Benchmark for Safety and Compliance of Large Language Models in Finance", "comment": null, "summary": "Large language models are increasingly deployed across the financial sector for tasks such as research, compliance, risk analysis, and customer service, which makes rigorous safety evaluation essential. However, existing financial benchmarks primarily focus on textbook-style question answering and numerical problem solving, but fail to evaluate models' real-world safety behaviors. They weakly assess regulatory compliance and investor-protection norms, rarely stress-test multi-turn adversarial tactics such as jailbreaks or prompt injection, inconsistently ground answers in long filings, ignore tool- or RAG-induced over-reach risks, and rely on opaque or non-auditable evaluation protocols. To close these gaps, we introduce CNFinBench, a benchmark that employs finance-tailored red-team dialogues and is structured around a Capability-Compliance-Safety triad, including evidence-grounded reasoning over long reports and jurisdiction-aware rule/tax compliance tasks. For systematic safety quantification, we introduce the Harmful Instruction Compliance Score (HICS) to measure how consistently models resist harmful prompts across multi-turn adversarial dialogues. To ensure auditability, CNFinBench enforces strict output formats with dynamic option perturbation for objective tasks and employs a hybrid LLM-ensemble plus human-calibrated judge for open-ended evaluations. Experiments on 21 models across 15 subtasks confirm a persistent capability-compliance gap: models achieve an average score of 61.0 on capability tasks but fall to 34.18 on compliance and risk-control evaluations. Under multi-turn adversarial dialogue tests, most systems reach only partial resistance (HICS 60-79), demonstrating that refusal alone is not a reliable proxy for safety without cited and verifiable reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CNFinBench\uff0c\u4e00\u4e2a\u9762\u5411\u91d1\u878d\u9886\u57df\u7684\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u7ea2\u961f\u5bf9\u8bdd\u3001\u80fd\u529b-\u5408\u89c4-\u5b89\u5168\u4e09\u5143\u7ed3\u6784\u548c\u591a\u8f6e\u5bf9\u6297\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u5408\u89c4\u4e0e\u98ce\u9669\u63a7\u5236\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u91d1\u878d\u9886\u57df\u7684\u5927\u6a21\u578b\u8bc4\u6d4b\u4e3b\u8981\u5173\u6ce8\u6559\u79d1\u4e66\u5f0f\u95ee\u7b54\u548c\u6570\u503c\u8ba1\u7b97\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u573a\u666f\u4e0b\u5b89\u5168\u6027\u884c\u4e3a\uff08\u5982\u76d1\u7ba1\u5408\u89c4\u3001\u6295\u8d44\u8005\u4fdd\u62a4\u3001\u591a\u8f6e\u5bf9\u6297\u653b\u51fb\u9632\u5fa1\u3001\u957f\u6587\u6863\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u98ce\u9669\uff09\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e14\u8bc4\u4f30\u534f\u8bae\u4e0d\u900f\u660e\u3001\u4e0d\u53ef\u5ba1\u8ba1\u3002", "method": "\u6784\u5efaCNFinBench\u57fa\u51c6\uff0c\u5305\u542b\u91d1\u878d\u5b9a\u5236\u7684\u7ea2\u961f\u5bf9\u8bdd\u3001\u57fa\u4e8e\u957f\u62a5\u544a\u7684\u8bc1\u636e\u63a8\u7406\u4efb\u52a1\u548c\u8003\u8651\u53f8\u6cd5\u7ba1\u8f96\u533a\u7684\u89c4\u5219/\u7a0e\u52a1\u5408\u89c4\u4efb\u52a1\uff1b\u63d0\u51fa\u6709\u5bb3\u6307\u4ee4\u9075\u4ece\u5206\u6570\uff08HICS\uff09\u91cf\u5316\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u6297\u4e2d\u7684\u5b89\u5168\u6027\uff1b\u91c7\u7528\u52a8\u6001\u9009\u9879\u6270\u52a8\u548c\u6df7\u5408LLM\u96c6\u6210+\u4eba\u5de5\u6821\u51c6\u7684\u8bc4\u5224\u673a\u5236\u786e\u4fdd\u8bc4\u4f30\u53ef\u5ba1\u8ba1\u3002", "result": "\u572821\u4e2a\u6a21\u578b\u300115\u4e2a\u5b50\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u6a21\u578b\u5728\u80fd\u529b\u4efb\u52a1\u4e0a\u5e73\u5747\u5f97\u5206\u4e3a61.0\uff0c\u4f46\u5728\u5408\u89c4\u4e0e\u98ce\u63a7\u4efb\u52a1\u4e0a\u9aa4\u964d\u81f334.18\uff1b\u591a\u8f6e\u5bf9\u6297\u6d4b\u8bd5\u4e2d\u591a\u6570\u7cfb\u7edf\u4ec5\u8fbe\u5230\u90e8\u5206\u62b5\u6297\uff08HICS 60\u201379\uff09\uff0c\u8868\u660e\u5355\u7eaf\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u4e0d\u8db3\u4ee5\u4fdd\u8bc1\u5b89\u5168\uff0c\u9700\u8f85\u4ee5\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u4f9d\u636e\u3002", "conclusion": "\u5f53\u524d\u91d1\u878d\u5927\u6a21\u578b\u5b58\u5728\u660e\u663e\u7684\u80fd\u529b-\u5408\u89c4\u5dee\u8ddd\uff0c\u9700\u901a\u8fc7\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u3001\u53ef\u5ba1\u8ba1\u4e14\u5f3a\u8c03\u8bc1\u636e\u652f\u6491\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff08\u5982CNFinBench\uff09\u6765\u63a8\u52a8\u6a21\u578b\u5728\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2512.09817", "categories": ["cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09817", "abs": "https://arxiv.org/abs/2512.09817", "authors": ["Habiba BEN ABDERRAHMANE", "Slimane Oulad-Naoui", "Benameur ZIANI"], "title": "A roadmap of geospatial soil quality analysis systems", "comment": null, "summary": "Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u591a\u6e90\u571f\u58e4\u6570\u636e\u3001GIS\u3001\u9065\u611f\u548c\u673a\u5668\u5b66\u4e60\u7684\u6a21\u5757\u5316\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u900f\u660e\u4e14\u53ef\u6269\u5c55\u7684\u571f\u58e4\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u571f\u58e4\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\uff0c\u7a7a\u95f4\u548c\u65f6\u95f4\u8986\u76d6\u6709\u9650\uff0c\u4e9f\u9700\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e00\u4e2a\u96c6\u6210\u591a\u6e90\u571f\u58e4\u6570\u636e\u3001\u5730\u7406\u4fe1\u606f\u7cfb\u7edf\uff08GIS\uff09\u3001\u9065\u611f\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u7edf\u4e00\u6a21\u5757\u5316\u6d41\u7a0b\u3002", "result": "\u8be5\u6846\u67b6\u6574\u5408\u4e86\u571f\u58e4\u8d28\u91cf\u8bc4\u4f30\u5168\u6d41\u7a0b\u4e2d\u7684\u6700\u65b0\u6280\u672f\uff0c\u652f\u6301\u900f\u660e\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\uff0c\u5e76\u5305\u542b\u5b9e\u9645\u5e94\u7528\u6848\u4f8b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7efc\u5408\u8def\u7ebf\u56fe\u514b\u670d\u4e86\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u4e00\u53c2\u6570\u6216\u7279\u5b9a\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u66f4\u900f\u660e\u3001\u81ea\u9002\u5e94\u4e14\u5951\u5408\u53ef\u6301\u7eed\u571f\u5730\u7ba1\u7406\u7684\u571f\u58e4\u8d28\u91cf\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2512.09312", "categories": ["cs.NI"], "pdf": "https://arxiv.org/pdf/2512.09312", "abs": "https://arxiv.org/abs/2512.09312", "authors": ["Ziheng Yang", "Kun Qiu", "Zhe Chen", "Wenjun Zhu", "Yue Gao"], "title": "Tyche: A Hybrid Computation Framework of Illumination Pattern for Satellite Beam Hopping", "comment": null, "summary": "High-Throughput Satellites (HTS) use beam hopping to handle non-uniform and time-varying ground traffic demand. A significant technical challenge in beam hopping is the computation of effective illumination patterns. Traditional algorithms, like the genetic algorithm, require over 300 seconds to compute a single illumination pattern for just 37 cells, whereas modern HTS typically covers over 300 cells, rendering current methods impractical for real-world applications. Advanced approaches, such as multi-agent deep reinforcement learning, face convergence issues when the number of cells exceeds 40. In this paper, we introduce Tyche, a hybrid computation framework designed to address this challenge. Tyche incorporates a Monte Carlo Tree Search Beam Hopping (MCTS-BH) algorithm for computing illumination patterns and employs sliding window and pruning techniques to significantly reduce computation time. Specifically, MCTS-BH can compute one illumination pattern for 37 cells in just 12 seconds. To ensure real-time computation, we use a Greedy Beam Hopping (G-BH) algorithm, which provides a provisional solution while MCTS-BH completes its computation in the background. Our evaluation results show that MCTS-BH can increase throughput by up to 98.76%, demonstrating substantial improvements over existing solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTyche\u6846\u67b6\uff0c\u7ed3\u5408MCTS-BH\u4e0eG-BH\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u901a\u91cf\u536b\u661f\u6ce2\u675f\u8df3\u53d8\u4e2d\u7167\u660e\u6a21\u5f0f\u7684\u8ba1\u7b97\u6548\u7387\u4e0e\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u6ce2\u675f\u8df3\u53d8\u7167\u660e\u6a21\u5f0f\u8ba1\u7b97\u65b9\u6cd5\uff08\u5982\u9057\u4f20\u7b97\u6cd5\u3001\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u9762\u5bf9300\u4ee5\u4e0a\u5730\u9762\u5c0f\u533a\u65f6\u5b58\u5728\u8ba1\u7b97\u8017\u65f6\u8fc7\u957f\u6216\u96be\u4ee5\u6536\u655b\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51faTyche\u6df7\u5408\u8ba1\u7b97\u6846\u67b6\uff1a\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6ce2\u675f\u8df3\u53d8\uff08MCTS-BH\uff09\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u4e0e\u526a\u679d\u6280\u672f\u52a0\u901f\u8ba1\u7b97\uff1b\u540c\u65f6\u5f15\u5165\u8d2a\u5fc3\u6ce2\u675f\u8df3\u53d8\uff08G-BH\uff09\u7b97\u6cd5\u63d0\u4f9b\u5b9e\u65f6\u4e34\u65f6\u89e3\u3002", "result": "MCTS-BH\u572837\u4e2a\u5c0f\u533a\u573a\u666f\u4e0b\u4ec5\u970012\u79d2\u5373\u53ef\u751f\u6210\u7167\u660e\u6a21\u5f0f\uff0c\u76f8\u8f83\u4f20\u7edf\u65b9\u6cd5\u5927\u5e45\u63d0\u901f\uff1b\u7cfb\u7edf\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u534798.76%\u3002", "conclusion": "Tyche\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u901a\u91cf\u536b\u661f\u6ce2\u675f\u8df3\u53d8\u4e2d\u5927\u89c4\u6a21\u5c0f\u533a\u7167\u660e\u6a21\u5f0f\u7684\u9ad8\u6548\u8ba1\u7b97\u95ee\u9898\uff0c\u517c\u5177\u5b9e\u65f6\u6027\u4e0e\u9ad8\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u524d\u666f\u3002"}}
{"id": "2512.09304", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09304", "abs": "https://arxiv.org/abs/2512.09304", "authors": ["Siyuan Ma", "Jiajun Hu", "Jeeho Ryoo", "Aman Arora", "Lizy Kurian John"], "title": "RACAM: Enhancing DRAM with Reuse-Aware Computation and Automated Mapping for ML Inference", "comment": null, "summary": "In-DRAM Processing-In-Memory (DRAM-PIM) has emerged as a promising approach to accelerate memory-intensive workloads by mitigating data transfer overhead between DRAM and the host processor. Bit-serial DRAM-PIM architectures, further enhance efficiency by supporting runtime variable data precision, which is critical for emerging workloads, such as large language model (LLM) inference. However, existing works still have major limitations: lack of data reuse, significant amounts of redundant data transfer, and insufficient support for workload mapping. To address these issues, we propose RACAM, the first in-DRAM bit-serial architecture which uses dedicated locality buffers, bit-serial PEs, popcount reduction units and broadcast units to enable data reuse and alleviate redundant data transfers. Furthermore, a workload mapping mechanism is proposed to fully explore the massive parallelism of DRAM architecture and identify the best mapping scheme of a given workload. We evaluate RACAM against GPUs and the state-of-the-art, in-DRAM PIM system, Proteus, across end-to-end LLM inferences. RACAM achieves 9x to 102x speedup over GPUs and 233x higher performance per mm2 compared to Proteus in case of GPT3.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faRACAM\uff0c\u4e00\u79cd\u65b0\u578b\u7684DRAM\u5185\u4f4d\u4e32\u884c\u8ba1\u7b97\u67b6\u6784\uff0c\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u6027\u7f13\u51b2\u3001\u4f4d\u4e32\u884c\u5904\u7406\u5355\u5143\u3001popcount\u5f52\u7ea6\u5355\u5143\u548c\u5e7f\u64ad\u5355\u5143\uff0c\u6709\u6548\u63d0\u5347\u6570\u636e\u590d\u7528\u5e76\u51cf\u5c11\u5197\u4f59\u6570\u636e\u4f20\u8f93\uff0c\u540c\u65f6\u7ed3\u5408\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u673a\u5236\uff0c\u5728LLM\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8eGPU\u548c\u73b0\u6709DRAM-PIM\u7cfb\u7edfProteus\u3002", "motivation": "\u73b0\u6709DRAM-PIM\u67b6\u6784\u5728\u652f\u6301\u53ef\u53d8\u7cbe\u5ea6\u8ba1\u7b97\u65f6\u5b58\u5728\u6570\u636e\u590d\u7528\u4e0d\u8db3\u3001\u5197\u4f59\u6570\u636e\u4f20\u8f93\u4e25\u91cd\u4ee5\u53ca\u7f3a\u4e4f\u9ad8\u6548\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u673a\u5236\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7b49\u65b0\u5174\u5185\u5b58\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u53d1\u6325\u3002", "method": "\u63d0\u51faRACAM\u67b6\u6784\uff0c\u96c6\u6210\u4e13\u7528\u5c40\u90e8\u6027\u7f13\u51b2\u533a\u3001\u4f4d\u4e32\u884c\u5904\u7406\u5355\u5143\u3001popcount\u5f52\u7ea6\u5355\u5143\u548c\u5e7f\u64ad\u5355\u5143\uff0c\u5e76\u8bbe\u8ba1\u914d\u5957\u7684\u5de5\u4f5c\u8d1f\u8f7d\u6620\u5c04\u673a\u5236\uff0c\u4ee5\u5145\u5206\u5229\u7528DRAM\u7684\u9ad8\u5e76\u884c\u6027\u5e76\u4f18\u5316\u4efb\u52a1\u8c03\u5ea6\u3002", "result": "\u5728\u7aef\u5230\u7aef\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cRACAM\u76f8\u6bd4GPU\u63d0\u901f9\u81f3102\u500d\uff0c\u76f8\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684DRAM-PIM\u7cfb\u7edfProteus\uff0c\u5728GPT-3\u4efb\u52a1\u4e2d\u5b9e\u73b0233\u500d\u7684\u6bcf\u5e73\u65b9\u6beb\u7c73\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "RACAM\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709DRAM-PIM\u67b6\u6784\u5728\u6570\u636e\u590d\u7528\u3001\u5197\u4f59\u4f20\u8f93\u548c\u6620\u5c04\u7b56\u7565\u65b9\u9762\u7684\u5173\u952e\u74f6\u9888\uff0c\u4e3a\u9ad8\u6548\u652f\u6301\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7b49\u65b0\u5174\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u3001\u9ad8\u80fd\u6548\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09277", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.09277", "abs": "https://arxiv.org/abs/2512.09277", "authors": ["Yanpeng Yu", "Haiyue Ma", "Krish Agarwal", "Nicolai Oswald", "Qijing Huang", "Hugo Linsenmaier", "Chunhui Mei", "Ritchie Zhao", "Ritika Borkar", "Bita Darvish Rouhani", "David Nellans", "Ronny Krashinsky", "Anurag Khandelwal"], "title": "Efficient MoE Serving in the Memory-Bound Regime: Balance Activated Experts, Not Tokens", "comment": null, "summary": "Expert Parallelism (EP) permits Mixture of Experts (MoE) models to scale beyond a single GPU. To address load imbalance across GPUs in EP, existing approaches aim to balance the number of tokens each GPU processes. Surprisingly, we find that this objective degrades performance rather than improving it when processing is memory-bound - a common occurrence in MoE serving, especially in the decode phase. Our analysis reveals that balancing the number of tokens processed per GPU increases the number of activated experts, exacerbating memory pressure in the memory-bound regime.\n  We propose Minimum Expert Token ROuting, a novel token-routing algorithm for high-performance expert-parallel MoE serving in the memory-bound regime that balances the number of activated experts per GPU rather than token counts. METRO achieves near-optimal routing quality with minimal computational overhead by jointly optimizing algorithmic efficiency and leveraging the GPU's parallel processing power. To guarantee routing quality, METRO also employs a novel allGather scheme to gather global top-k knowledge, which has minimal overhead compared to conventional allToAll. Our evaluation of METRO against EPLB on both real systems (vLLM over 8 A100 GPUs) and a proprietary simulator (8-16 B200 GPUs) shows that METRO reduces decode latency by 11 - 22%, and total token throughput by 3 - 21% for Qwen3 and DeepSeek-V3 serving, where prefill and decode phases are co-deployed. In addition, by trading latency headroom for throughput, METRO improves decode throughput by up to 4.11x over EPLB at a fixed decode SLO.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMETRO\u7684\u65b0\u8def\u7531\u7b97\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u6bcf\u4e2aGPU\u4e0a\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\uff08\u800c\u975e\u5904\u7406\u7684token\u6570\u91cf\uff09\uff0c\u5728\u5185\u5b58\u53d7\u9650\u7684MoE\u63a8\u7406\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e13\u5bb6\u5e76\u884c\uff08EP\uff09\u65b9\u6cd5\u901a\u8fc7\u5e73\u8861\u5404GPU\u5904\u7406\u7684token\u6570\u91cf\u6765\u7f13\u89e3\u8d1f\u8f7d\u4e0d\u5747\uff0c\u4f46\u5728\u5185\u5b58\u53d7\u9650\u7684MoE\u670d\u52a1\uff08\u5c24\u5176\u662f\u89e3\u7801\u9636\u6bb5\uff09\u4e2d\uff0c\u8fd9\u79cd\u7b56\u7565\u53cd\u800c\u56e0\u589e\u52a0\u6fc0\u6d3b\u4e13\u5bb6\u6570\u800c\u52a0\u5267\u5185\u5b58\u538b\u529b\uff0c\u964d\u4f4e\u6027\u80fd\u3002", "method": "\u63d0\u51faMinimum Expert Token ROuting\uff08METRO\uff09\u7b97\u6cd5\uff0c\u5728\u8def\u7531\u65f6\u5e73\u8861\u6bcfGPU\u6fc0\u6d3b\u7684\u4e13\u5bb6\u6570\u91cf\uff1b\u8bbe\u8ba1\u4f4e\u5f00\u9500\u7684allGather\u673a\u5236\u4ee5\u83b7\u53d6\u5168\u5c40top-k\u4fe1\u606f\uff0c\u5e76\u8054\u5408\u4f18\u5316\u7b97\u6cd5\u6548\u7387\u4e0eGPU\u5e76\u884c\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u7cfb\u7edf\uff088\u00d7A100\uff09\u548c\u6a21\u62df\u5668\uff088\u201316\u00d7B200\uff09\u4e0a\u8bc4\u4f30\u8868\u660e\uff0c\u76f8\u6bd4EPLB\uff0cMETRO\u5728Qwen3\u548cDeepSeek-V3\u670d\u52a1\u4e2d\u964d\u4f4e\u89e3\u7801\u5ef6\u8fdf11\u201322%\uff0c\u63d0\u5347\u603b\u541e\u54103\u201321%\uff1b\u5728\u56fa\u5b9aSLO\u4e0b\uff0c\u89e3\u7801\u541e\u5410\u6700\u9ad8\u63d0\u53474.11\u500d\u3002", "conclusion": "\u5728\u5185\u5b58\u53d7\u9650\u7684MoE\u670d\u52a1\u573a\u666f\u4e2d\uff0c\u5e73\u8861\u6fc0\u6d3b\u4e13\u5bb6\u6570\u6bd4\u5e73\u8861token\u6570\u66f4\u6709\u6548\uff1bMETRO\u901a\u8fc7\u65b0\u8def\u7531\u7b56\u7565\u548c\u901a\u4fe1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u4e0e\u541e\u5410\u3002"}}
{"id": "2512.09006", "categories": ["cs.SE", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09006", "abs": "https://arxiv.org/abs/2512.09006", "authors": ["Dyna Soumhane Ouchebara", "St\u00e9phane Dupont"], "title": "Llama-based source code vulnerability detection: Prompt engineering vs Fine tuning", "comment": "20 pages, Accepted at ESORICS 2025", "summary": "The significant increase in software production, driven by the acceleration of development cycles over the past two decades, has led to a steady rise in software vulnerabilities, as shown by statistics published yearly by the CVE program. The automation of the source code vulnerability detection (CVD) process has thus become essential, and several methods have been proposed ranging from the well established program analysis techniques to the more recent AI-based methods. Our research investigates Large Language Models (LLMs), which are considered among the most performant AI models to date, for the CVD task. The objective is to study their performance and apply different state-of-the-art techniques to enhance their effectiveness for this task. We explore various fine-tuning and prompt engineering settings. We particularly suggest one novel approach for fine-tuning LLMs which we call Double Fine-tuning, and also test the understudied Test-Time fine-tuning approach. We leverage the recent open-source Llama-3.1 8B, with source code samples extracted from BigVul and PrimeVul datasets. Our conclusions highlight the importance of fine-tuning to resolve the task, the performance of Double tuning, as well as the potential of Llama models for CVD. Though prompting proved ineffective, Retrieval augmented generation (RAG) performed relatively well as an example selection technique. Overall, some of our research questions have been answered, and many are still on hold, which leaves us many future work perspectives. Code repository is available here: https://github.com/DynaSoumhaneOuchebara/Llama-based-vulnerability-detection.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6e90\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\uff08CVD\uff09\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53cc\u91cd\u5fae\u8c03\u201d\uff08Double Fine-tuning\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u6d4b\u8bd5\u65f6\u5fae\u8c03\uff08Test-Time fine-tuning\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b49\u6280\u672f\u3002\u5b9e\u9a8c\u57fa\u4e8eLlama-3.1 8B\u6a21\u578b\u53caBigVul\u548cPrimeVul\u6570\u636e\u96c6\uff0c\u7ed3\u679c\u8868\u660e\u5fae\u8c03\u5bf9\u63d0\u5347\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u800c\u5355\u7eaf\u63d0\u793a\u6548\u679c\u4e0d\u4f73\u3002", "motivation": "\u968f\u7740\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u52a0\u901f\uff0c\u8f6f\u4ef6\u6f0f\u6d1e\u6570\u91cf\u6301\u7eed\u4e0a\u5347\uff0c\u81ea\u52a8\u5316\u6f0f\u6d1e\u68c0\u6d4b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7a0b\u5e8f\u5206\u6790\u65b9\u6cd5\u548c\u65b0\u5174AI\u65b9\u6cd5\u5df2\u88ab\u63d0\u51fa\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u5c1a\u9700\u6df1\u5165\u63a2\u7d22\u3002", "method": "\u4f5c\u8005\u91c7\u7528\u5f00\u6e90\u7684Llama-3.1 8B\u6a21\u578b\uff0c\u5728BigVul\u548cPrimeVul\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u63a2\u7d22\u591a\u79cd\u63d0\u793a\u5de5\u7a0b\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u5305\u62ec\u63d0\u51fa\u7684\u65b0\u65b9\u6cd5\u201c\u53cc\u91cd\u5fae\u8c03\u201d\u4ee5\u53ca\u6d4b\u8bd5\u65f6\u5fae\u8c03\uff0c\u5e76\u8bc4\u4f30RAG\u4f5c\u4e3a\u793a\u4f8b\u9009\u62e9\u6280\u672f\u7684\u6548\u679c\u3002", "result": "\u5fae\u8c03\u88ab\u8bc1\u660e\u5bf9CVD\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u5176\u4e2d\u201c\u53cc\u91cd\u5fae\u8c03\u201d\u8868\u73b0\u4f18\u5f02\uff1bLlama\u6a21\u578b\u5c55\u73b0\u51fa\u826f\u597d\u6f5c\u529b\uff1b\u5355\u7eaf\u63d0\u793a\u65e0\u6548\uff0c\u4f46RAG\u4f5c\u4e3a\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\u8868\u73b0\u76f8\u5bf9\u8f83\u597d\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6e90\u4ee3\u7801\u6f0f\u6d1e\u68c0\u6d4b\u4e2d\u5177\u6709\u5e94\u7528\u524d\u666f\uff0c\u5c24\u5176\u901a\u8fc7\u6709\u6548\u7684\u5fae\u8c03\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u7814\u7a76\u56de\u7b54\u4e86\u90e8\u5206\u95ee\u9898\uff0c\u4f46\u4ecd\u6709\u8bb8\u591a\u65b9\u5411\u503c\u5f97\u672a\u6765\u63a2\u7d22\u3002"}}
{"id": "2512.09427", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09427", "abs": "https://arxiv.org/abs/2512.09427", "authors": ["Guoqiang Zou", "Wanyu Wang", "Hao Zheng", "Longxiang Yin", "Yinhe Han"], "title": "ODMA: On-Demand Memory Allocation Framework for LLM Serving on LPDDR-Class Accelerators", "comment": "10 pages, 5 figures", "summary": "Serving large language models (LLMs) on accelerators with poor random-access bandwidth (e.g., LPDDR5-based) is limited by current memory managers. Static pre-allocation wastes memory, while fine-grained paging (e.g., PagedAttention) is ill-suited due to high random-access costs. Existing HBM-centric solutions do not exploit the characteristics of random-access-constrained memory (RACM) accelerators like Cambricon MLU370. We present ODMA, an on-demand memory allocation framework for RACM. ODMA addresses distribution drift and heavy-tailed requests by coupling a lightweight length predictor with dynamic bucket partitioning and a large-bucket safeguard. Boundaries are periodically updated from live traces to maximize utilization. On Alpaca and Google-NQ, ODMA improves prediction accuracy of prior work significantly (e.g., from 82.68% to 93.36%). Serving DeepSeek-R1-Distill-Qwen-7B on Cambricon MLU370-X4, ODMA raises memory utilization from 55.05% to 72.45% and improves RPS and TPS by 29% and 27% over static baselines. This demonstrates that hardware-aware allocation unlocks efficient LLM serving on RACM platforms.", "AI": {"tldr": "ODMA \u662f\u4e00\u79cd\u9762\u5411\u968f\u673a\u8bbf\u95ee\u53d7\u9650\u5185\u5b58\uff08RACM\uff09\u52a0\u901f\u5668\u7684\u6309\u9700\u5185\u5b58\u5206\u914d\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u957f\u5ea6\u9884\u6d4b\u3001\u52a8\u6001\u6876\u5212\u5206\u548c\u5927\u6876\u4fdd\u62a4\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347 LLM \u63a8\u7406\u65f6\u7684\u5185\u5b58\u5229\u7528\u7387\u4e0e\u541e\u5410\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5728\u968f\u673a\u8bbf\u95ee\u5e26\u5bbd\u8f83\u5dee\u7684\u52a0\u901f\u5668\uff08\u5982\u57fa\u4e8e LPDDR5 \u7684 Cambricon MLU370\uff09\u4e0a\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\uff0c\u73b0\u6709\u5185\u5b58\u7ba1\u7406\u7b56\u7565\u5b58\u5728\u4e0d\u8db3\uff1a\u9759\u6001\u9884\u5206\u914d\u6d6a\u8d39\u5185\u5b58\uff0c\u7ec6\u7c92\u5ea6\u5206\u9875\u56e0\u9ad8\u968f\u673a\u8bbf\u95ee\u5f00\u9500\u800c\u4e0d\u9002\u7528\uff0c\u800c\u4ee5 HBM \u4e3a\u4e2d\u5fc3\u7684\u65b9\u6848\u672a\u9488\u5bf9 RACM \u7279\u6027\u4f18\u5316\u3002", "method": "ODMA \u6846\u67b6\u7ed3\u5408\u8f7b\u91cf\u7ea7\u8bf7\u6c42\u957f\u5ea6\u9884\u6d4b\u5668\u3001\u52a8\u6001\u6876\u5206\u533a\u53ca\u5927\u6876\u4fdd\u62a4\u673a\u5236\uff0c\u5e76\u5b9a\u671f\u4ece\u8fd0\u884c\u65f6\u8f68\u8ff9\u66f4\u65b0\u6876\u8fb9\u754c\uff0c\u4ee5\u5e94\u5bf9\u8bf7\u6c42\u957f\u5ea6\u5206\u5e03\u6f02\u79fb\u548c\u91cd\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u5185\u5b58\u5229\u7528\u6548\u7387\u3002", "result": "\u5728 Alpaca \u548c Google-NQ \u6570\u636e\u96c6\u4e0a\uff0cODMA \u5c06\u8bf7\u6c42\u957f\u5ea6\u9884\u6d4b\u51c6\u786e\u7387\u4ece 82.68% \u63d0\u5347\u81f3 93.36%\uff1b\u5728 Cambricon MLU370-X4 \u4e0a\u8fd0\u884c DeepSeek-R1-Distill-Qwen-7B \u65f6\uff0c\u5185\u5b58\u5229\u7528\u7387\u4ece 55.05% \u63d0\u9ad8\u5230 72.45%\uff0cRPS \u548c TPS \u5206\u522b\u63d0\u5347 29% \u548c 27%\u3002", "conclusion": "\u786c\u4ef6\u611f\u77e5\u7684\u5185\u5b58\u5206\u914d\u7b56\u7565\uff08\u5982 ODMA\uff09\u80fd\u6709\u6548\u91ca\u653e RACM \u5e73\u53f0\u5728 LLM \u670d\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u663e\u8457\u63d0\u5347\u8d44\u6e90\u5229\u7528\u7387\u548c\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2512.09309", "categories": ["cs.DC", "cs.CR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09309", "abs": "https://arxiv.org/abs/2512.09309", "authors": ["Zihao Ding", "Mufeng Zhu", "Zhongze Tang", "Sheng Wei", "Yao Liu"], "title": "A Distributed Framework for Privacy-Enhanced Vision Transformers on the Edge", "comment": "16 pages, 7 figures. Published in the Proceedings of the Tenth ACM/IEEE Symposium on Edge Computing (SEC '25), Dec 3-6, 2025, Washington, D.C., USA", "summary": "Nowadays, visual intelligence tools have become ubiquitous, offering all kinds of convenience and possibilities. However, these tools have high computational requirements that exceed the capabilities of resource-constrained mobile and wearable devices. While offloading visual data to the cloud is a common solution, it introduces significant privacy vulnerabilities during transmission and server-side computation. To address this, we propose a novel distributed, hierarchical offloading framework for Vision Transformers (ViTs) that addresses these privacy challenges by design. Our approach uses a local trusted edge device, such as a mobile phone or an Nvidia Jetson, as the edge orchestrator. This orchestrator partitions the user's visual data into smaller portions and distributes them across multiple independent cloud servers. By design, no single external server possesses the complete image, preventing comprehensive data reconstruction. The final data merging and aggregation computation occurs exclusively on the user's trusted edge device. We apply our framework to the Segment Anything Model (SAM) as a practical case study, which demonstrates that our method substantially enhances content privacy over traditional cloud-based approaches. Evaluations show our framework maintains near-baseline segmentation performance while substantially reducing the risk of content reconstruction and user data exposure. Our framework provides a scalable, privacy-preserving solution for vision tasks in the edge-cloud continuum.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u89c6\u89c9Transformer\u7684\u5206\u5e03\u5f0f\u5206\u5c42\u5378\u8f7d\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u53ef\u4fe1\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5206\u5272\u56fe\u50cf\u5e76\u5206\u53d1\u81f3\u591a\u4e2a\u4e91\u670d\u52a1\u5668\uff0c\u5728\u4fdd\u969c\u9690\u79c1\u7684\u540c\u65f6\u7ef4\u6301\u63a5\u8fd1\u539f\u59cb\u6027\u80fd\u7684\u89c6\u89c9\u4efb\u52a1\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u667a\u80fd\u5de5\u5177\u8ba1\u7b97\u9700\u6c42\u9ad8\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u548c\u53ef\u7a7f\u6234\u8bbe\u5907\u4e0a\u8fd0\u884c\uff1b\u5c06\u6570\u636e\u4e0a\u4f20\u81f3\u4e91\u7aef\u867d\u53ef\u7f13\u89e3\u8ba1\u7b97\u538b\u529b\uff0c\u4f46\u4f1a\u5e26\u6765\u4f20\u8f93\u4e0e\u670d\u52a1\u7aef\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u5229\u7528\u672c\u5730\u53ef\u4fe1\u8fb9\u7f18\u8bbe\u5907\uff08\u5982\u624b\u673a\u6216Jetson\uff09\u4f5c\u4e3a\u534f\u8c03\u5668\uff0c\u5c06\u7528\u6237\u89c6\u89c9\u6570\u636e\u5206\u5272\u540e\u5206\u53d1\u5230\u591a\u4e2a\u72ec\u7acb\u4e91\u670d\u52a1\u5668\uff0c\u786e\u4fdd\u4efb\u4e00\u670d\u52a1\u5668\u65e0\u6cd5\u83b7\u5f97\u5b8c\u6574\u56fe\u50cf\uff1b\u6700\u7ec8\u805a\u5408\u8ba1\u7b97\u4ec5\u5728\u8fb9\u7f18\u8bbe\u5907\u5b8c\u6210\uff0c\u5e76\u4ee5Segment Anything Model\uff08SAM\uff09\u4e3a\u6848\u4f8b\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u63a5\u8fd1\u57fa\u7ebf\u5206\u5272\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5bb9\u91cd\u5efa\u4e0e\u7528\u6237\u6570\u636e\u66b4\u9732\u7684\u98ce\u9669\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u4e3a\u8fb9\u7f18-\u4e91\u534f\u540c\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6ce8\u91cd\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09331", "categories": ["cs.DC", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09331", "abs": "https://arxiv.org/abs/2512.09331", "authors": ["Nam Anh Dang", "Ben Landrum", "Ken Birman"], "title": "Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN", "comment": "12 pages, 14 figures, submitted to VLDB 2026", "summary": "Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.", "AI": {"tldr": "BatANN \u662f\u4e00\u79cd\u65b0\u578b\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u68c0\u7d22\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u670d\u52a1\u5668\u95f4\u4f20\u9012\u5b8c\u6574\u67e5\u8be2\u72b6\u6001\u4ee5\u7ef4\u6301\u5c40\u90e8\u6027\uff0c\u5728\u5341\u4ebf\u7ea7\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7ebf\u6027\u7684\u541e\u5410\u6269\u5c55\u548c\u4f4e\u4e8e6\u6beb\u79d2\u7684\u5e73\u5747\u5ef6\u8fdf\u3002", "motivation": "\u968f\u7740\u5411\u91cf\u6570\u636e\u96c6\u89c4\u6a21\u6269\u5c55\u81f3\u6570\u5341\u4ebf\u7ea7\u522b\uff0c\u5355\u673a\u78c1\u76d8\u5411\u91cf\u68c0\u7d22\u5df2\u96be\u4ee5\u6ee1\u8db3\u9700\u6c42\uff0c\u4e9f\u9700\u4e00\u79cd\u53ef\u6a2a\u5411\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u4f4e\u5ef6\u8fdf\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa BatANN \u7cfb\u7edf\uff0c\u5176\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u8de8\u673a\u5668\u8bbf\u95ee\u90bb\u57df\u65f6\u5c06\u5b8c\u6574\u67e5\u8be2\u72b6\u6001\u53d1\u9001\u81f3\u76ee\u6807\u673a\u5668\u7ee7\u7eed\u6267\u884c\uff0c\u4ece\u800c\u4fdd\u6301\u67e5\u8be2\u5c40\u90e8\u6027\uff0c\u5e76\u5728\u6807\u51c6 TCP \u4e0a\u6784\u5efa\u5355\u4e00\u5168\u5c40\u56fe\u7ed3\u6784\u5b9e\u73b0\u5206\u5e03\u5f0f\u78c1\u76d8 ANN \u68c0\u7d22\u3002", "result": "\u572810\u53f0\u670d\u52a1\u5668\u30010.95\u53ec\u56de\u7387\u6761\u4ef6\u4e0b\uff0cBatANN \u57281\u4ebf\u548c10\u4ebf\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u5230\u57fa\u7ebf\u65b9\u6cd56.21\u20136.49\u500d\u548c2.5\u20135.10\u500d\u7684\u541e\u5410\u91cf\uff0c\u540c\u65f6\u5e73\u5747\u5ef6\u8fdf\u4f4e\u4e8e6\u6beb\u79d2\u3002", "conclusion": "BatANN \u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5355\u4e00\u5168\u5c40\u56fe\u7684\u5f00\u6e90\u5206\u5e03\u5f0f\u78c1\u76d8\u5411\u91cf\u68c0\u7d22\u7cfb\u7edf\uff0c\u5728\u4fdd\u8bc1\u5bf9\u6570\u7ea7\u641c\u7d22\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u541e\u5410\u80fd\u529b\uff0c\u4e3a\u8d85\u5927\u89c4\u6a21\u5411\u91cf\u68c0\u7d22\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2512.09196", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09196", "abs": "https://arxiv.org/abs/2512.09196", "authors": ["Haonan Li", "Keyu Man", "Partha Kanuparthy", "Hanning Chen", "Wei Sun", "Sreen Tallam", "Chenguang Zhu", "Kevin Zhu", "Zhiyun Qian"], "title": "TritonForge: Profiling-Guided Framework for Automated Triton Kernel Optimization", "comment": null, "summary": "High-performance GPU kernel optimization remains a critical yet labor-intensive task in modern machine learning workloads. Although Triton, a domain-specific language for GPU programming, enables developers to write efficient kernels with concise code, achieving expert-level performance still requires deep understanding of GPU architectures and low-level performance trade-offs. We present TritonForge, a profiling-guided framework for automated Triton kernel optimization. TritonForge integrates kernel analysis, runtime profiling, and iterative code transformation to streamline the optimization process. By incorporating data-driven feedback from profiling results, the system identifies performance bottlenecks, proposes targeted code modifications, and evaluates their impact automatically. While our prototype leverages large language models (LLMs) to assist in code reasoning and transformation, the framework remains modular and model-agnostic. Across diverse kernel types and GPU architectures, TritonForge achieves up to 5x performance improvement over baseline implementations and on average 1.76x of the cases are successful, providing a foundation for future research in automated GPU performance optimization.", "AI": {"tldr": "TritonForge \u662f\u4e00\u4e2a\u57fa\u4e8e\u6027\u80fd\u5206\u6790\u7684\u81ea\u52a8\u5316 Triton \u5185\u6838\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5185\u6838\u5206\u6790\u3001\u8fd0\u884c\u65f6\u5256\u6790\u548c\u8fed\u4ee3\u4ee3\u7801\u8f6c\u6362\uff0c\u5728\u591a\u79cd GPU \u67b6\u6784\u4e0a\u5e73\u5747\u5b9e\u73b0 1.76 \u500d\u3001\u6700\u9ad8\u8fbe 5 \u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u9ad8\u6027\u80fd GPU \u5185\u6838\u4f18\u5316\u5bf9\u73b0\u4ee3\u673a\u5668\u5b66\u4e60\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5373\u4fbf\u4f7f\u7528 Triton \u8fd9\u7c7b\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u4ecd\u9700\u4e13\u5bb6\u7ea7\u786c\u4ef6\u77e5\u8bc6\uff0c\u8fc7\u7a0b\u7e41\u7410\u4e14\u4f9d\u8d56\u7ecf\u9a8c\u3002", "method": "TritonForge \u7ed3\u5408\u5185\u6838\u5206\u6790\u3001\u8fd0\u884c\u65f6\u6027\u80fd\u5256\u6790\u4e0e\u8fed\u4ee3\u5f0f\u4ee3\u7801\u53d8\u6362\uff0c\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u53cd\u9988\u8bc6\u522b\u74f6\u9888\u5e76\u81ea\u52a8\u63d0\u51fa\u548c\u8bc4\u4f30\u4ee3\u7801\u4fee\u6539\uff1b\u5176\u539f\u578b\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u4ee3\u7801\u63a8\u7406\uff0c\u4f46\u6574\u4f53\u67b6\u6784\u6a21\u5757\u5316\u4e14\u6a21\u578b\u65e0\u5173\u3002", "result": "\u5728\u591a\u79cd\u5185\u6838\u7c7b\u578b\u548c GPU \u67b6\u6784\u4e0a\uff0cTritonForge \u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b0\u6700\u9ad8\u83b7\u5f97 5 \u500d\u52a0\u901f\uff0c\u5e73\u5747\u5728 1.76 \u500d\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u6210\u529f\u3002", "conclusion": "TritonForge \u6709\u6548\u964d\u4f4e\u4e86 Triton \u5185\u6838\u4f18\u5316\u95e8\u69db\uff0c\u4e3a\u81ea\u52a8\u5316 GPU \u6027\u80fd\u4f18\u5316\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6a21\u5757\u5316\u7684\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2512.09300", "categories": ["cs.OS", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.09300", "abs": "https://arxiv.org/abs/2512.09300", "authors": ["Guangxian Zou", "Isaac Zhang", "Ryan Zarick", "Kelvin Wong", "Thomas Kim", "Daniel L. -K. Wong", "Saeid Yazdinejad", "Dan Boneh"], "title": "ZeroOS: A Universal Modular Library OS for zkVMs", "comment": null, "summary": "zkVMs promise general-purpose verifiable computation through ISA-level compatibility with modern programs and toolchains. However, compatibility extends further than just the ISA; modern programs often cannot run or even compile without an operating system and libc. zkVMs attempt to address this by maintaining forks of language-specific runtimes and statically linking them into applications to create self-contained unikernels, but this ad-hoc approach leads to version hell and burdens verifiable applications (vApps) with an unnecessarily large trusted computing base. We solve this problem with ZeroOS, a modular library operating system (libOS) for vApp unikernels; vApp developers can use off-the-shelf toolchains to compile and link only the exact subset of the Linux ABI their vApp needs. Any zkVM team can easily leverage the ZeroOS ecosystem by writing a ZeroOS bootloader for their platform, resulting in a reduced maintainence burden and unifying the entire zkVM ecosystem with consolidated development and audit resources. ZeroOS is free and open-sourced at https://github.com/LayerZero-Labs/ZeroOS.", "AI": {"tldr": "ZeroOS \u662f\u4e00\u4e2a\u4e3a\u53ef\u9a8c\u8bc1\u5e94\u7528\uff08vApp\uff09\u8bbe\u8ba1\u7684\u6a21\u5757\u5316\u5e93\u64cd\u4f5c\u7cfb\u7edf\uff08libOS\uff09\uff0c\u901a\u8fc7\u4ec5\u94fe\u63a5 vApp \u6240\u9700\u7684 Linux ABI \u5b50\u96c6\uff0c\u89e3\u51b3\u4e86 zkVM \u4e2d\u56e0\u9759\u6001\u94fe\u63a5\u8fd0\u884c\u65f6\u5bfc\u81f4\u7684\u7248\u672c\u6df7\u4e71\u548c\u53ef\u4fe1\u8ba1\u7b97\u57fa\u8fc7\u5927\u7684\u95ee\u9898\uff0c\u4ece\u800c\u964d\u4f4e\u7ef4\u62a4\u8d1f\u62c5\u5e76\u7edf\u4e00 zkVM \u751f\u6001\u3002", "motivation": "\u73b0\u4ee3\u7a0b\u5e8f\u4f9d\u8d56\u64cd\u4f5c\u7cfb\u7edf\u548c libc \u624d\u80fd\u7f16\u8bd1\u548c\u8fd0\u884c\uff0c\u800c zkVM \u901a\u5e38\u901a\u8fc7\u5206\u53c9\u8bed\u8a00\u8fd0\u884c\u65f6\u5e76\u9759\u6001\u94fe\u63a5\u6210 unikernel \u6765\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u8fd9\u79cd\u65b9\u5f0f\u5bfc\u81f4\u7248\u672c\u7ba1\u7406\u56f0\u96be\u5e76\u6269\u5927\u4e86\u53ef\u4fe1\u8ba1\u7b97\u57fa\u3002", "method": "\u63d0\u51fa ZeroOS\uff0c\u4e00\u79cd\u6a21\u5757\u5316\u7684 libOS\uff0c\u5141\u8bb8\u5f00\u53d1\u8005\u4f7f\u7528\u73b0\u6210\u5de5\u5177\u94fe\u4ec5\u94fe\u63a5\u5176 vApp \u6240\u9700\u7684 Linux ABI \u5b50\u96c6\uff1bzkVM \u56e2\u961f\u53ea\u9700\u4e3a\u5176\u5e73\u53f0\u7f16\u5199 ZeroOS \u5f15\u5bfc\u52a0\u8f7d\u5668\u5373\u53ef\u63a5\u5165\u8be5\u751f\u6001\u3002", "result": "\u663e\u8457\u51cf\u5c11 vApp \u7684\u53ef\u4fe1\u8ba1\u7b97\u57fa\u548c\u7ef4\u62a4\u6210\u672c\uff0c\u5e76\u4fc3\u8fdb zkVM \u751f\u6001\u5728\u5f00\u53d1\u4e0e\u5ba1\u8ba1\u8d44\u6e90\u4e0a\u7684\u6574\u5408\u3002", "conclusion": "ZeroOS \u4e3a zkVM \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u8f7b\u91cf\u4e14\u6613\u4e8e\u7ef4\u62a4\u7684\u8fd0\u884c\u65f6\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u53ef\u9a8c\u8bc1\u8ba1\u7b97\u751f\u6001\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.09622", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.09622", "abs": "https://arxiv.org/abs/2512.09622", "authors": ["Xiao Yan", "Tiezheng Nie", "Boyang Fang", "Derong Shen", "Kou Yue", "Yu Ge"], "title": "CUBE: A Cardinality Estimator Based on Neural CDF", "comment": "13 pages", "summary": "Modern database optimizer relies on cardinality estimator, whose accuracy directly affects the optimizer's ability to choose an optimal execution plan. Recent work on data-driven methods has leveraged probabilistic models to achieve higher estimation accuracy, but these approaches cannot guarantee low inference latency at the same time and neglect scalability. As data dimensionality grows, optimization time can even exceed actual query execution time. Furthermore, inference with probabilistic models by sampling or integration procedures unpredictable estimation result and violate stability, which brings unstable performance with query execution and make database tuning hard for database users. In this paper, we propose a novel approach to cardinality estimation based on cumulative distribution function(CDF), which calculates range query cardinality without sampling or integration, ensuring accurate and predictable estimation results. With inference acceleration by merging calculations, we can achieve fast and nearly constant inference speed while maintaining high accuracy, even as dimensionality increases, which is over 10x faster than current state-of-the-art data-driven cardinality estimator. This demonstrates its excellent dimensional scalability, making it well-suited for real-world database applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff08CDF\uff09\u7684\u57fa\u6570\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u91c7\u6837\u6216\u79ef\u5206\u5373\u53ef\u5b9e\u73b0\u5feb\u901f\u3001\u51c6\u786e\u4e14\u53ef\u9884\u6d4b\u7684\u8303\u56f4\u67e5\u8be2\u57fa\u6570\u4f30\u8ba1\uff0c\u5728\u9ad8\u7ef4\u6570\u636e\u4e0b\u4ecd\u4fdd\u6301\u8fd1\u4f3c\u6052\u5b9a\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u6bd4\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5feb10\u500d\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6982\u7387\u6a21\u578b\u7684\u6570\u636e\u9a71\u52a8\u57fa\u6570\u4f30\u8ba1\u65b9\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u4e0b\u5b58\u5728\u63a8\u7406\u5ef6\u8fdf\u9ad8\u3001\u53ef\u6269\u5c55\u6027\u5dee\u3001\u4f30\u8ba1\u7ed3\u679c\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u67e5\u8be2\u4f18\u5316\u5668\u9009\u62e9\u6700\u4f18\u6267\u884c\u8ba1\u5212\u5e76\u589e\u52a0\u6570\u636e\u5e93\u8c03\u4f18\u96be\u5ea6\u3002", "method": "\u5229\u7528\u7d2f\u79ef\u5206\u5e03\u51fd\u6570\uff08CDF\uff09\u76f4\u63a5\u8ba1\u7b97\u8303\u56f4\u67e5\u8be2\u7684\u57fa\u6570\uff0c\u907f\u514d\u91c7\u6837\u6216\u79ef\u5206\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5408\u5e76\u8ba1\u7b97\u64cd\u4f5c\u52a0\u901f\u63a8\u7406\u3002", "result": "\u6240\u63d0\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u4f30\u8ba1\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4f3c\u6052\u5b9a\u4e14\u6781\u5feb\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u5c24\u5176\u5728\u9ad8\u7ef4\u573a\u666f\u4e0b\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u5feb10\u500d\u4ee5\u4e0a\uff0c\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u7ef4\u5ea6\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u57fa\u4e8eCDF\u7684\u57fa\u6570\u4f30\u8ba1\u65b9\u6cd5\u517c\u987e\u51c6\u786e\u6027\u3001\u7a33\u5b9a\u6027\u548c\u9ad8\u6548\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u9ad8\u7ef4\u6570\u636e\u5e93\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.09472", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09472", "abs": "https://arxiv.org/abs/2512.09472", "authors": ["Chiheng Lou", "Sheng Qi", "Rui Kang", "Yong Zhang", "Chen Sun", "Pengcheng Wang", "Bingyang Liu", "Xuanzhe Liu", "Xin Jin"], "title": "WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving", "comment": null, "summary": "Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.\n  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\\times$ more requests compared to the GPU-sharing system.", "AI": {"tldr": "WarmServe \u662f\u4e00\u79cd\u65b0\u578b\u591a\u5927\u8bed\u8a00\u6a21\u578b\uff08multi-LLM\uff09\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u901a\u7528 GPU \u5de5\u4f5c\u8282\u70b9\u548c\u57fa\u4e8e\u672a\u6765\u8d1f\u8f7d\u9884\u6d4b\u7684\u4e3b\u52a8\u9884\u70ed\u673a\u5236\uff0c\u5728\u63d0\u5347 GPU \u8d44\u6e90\u5229\u7528\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u9996 token \u5ef6\u8fdf\uff08TTFT\uff09\uff0c\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\u6700\u9ad8\u53ef\u5c06 TTFT \u964d\u4f4e 50.8 \u500d\uff0c\u5e76\u652f\u6301\u5904\u7406 2.5 \u500d\u66f4\u591a\u7684\u8bf7\u6c42\u3002", "motivation": "\u73b0\u6709 multi-LLM \u670d\u52a1\u7cfb\u7edf\u5728\u4f18\u5316 GPU \u5229\u7528\u7387\u65f6\u727a\u7272\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u5c24\u5176\u662f\u9996 token \u5ef6\u8fdf\uff08TTFT\uff09\uff0c\u5176\u6839\u672c\u539f\u56e0\u5728\u4e8e\u672a\u8003\u8651\u672a\u6765\u5de5\u4f5c\u8d1f\u8f7d\u7279\u5f81\u3002\u800c\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684 LLM \u670d\u52a1\u8d1f\u8f7d\u5177\u6709\u9ad8\u5ea6\u5468\u671f\u6027\u548c\u957f\u671f\u53ef\u9884\u6d4b\u6027\uff0c\u8fd9\u4e3a\u6539\u8fdb\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u201c\u901a\u7528 GPU \u5de5\u4f5c\u8282\u70b9\u201d\u6982\u5ff5\uff0c\u6784\u5efa WarmServe \u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u9879\u5173\u952e\u6280\u672f\uff1a(1) \u91c7\u7528\u611f\u77e5\u9a71\u9010\u7684\u6a21\u578b\u653e\u7f6e\u7b56\u7565\u4ee5\u51cf\u5c11\u96c6\u7fa4\u7ea7\u9884\u70ed\u5e72\u6270\uff1b(2) \u57fa\u4e8e\u672a\u6765\u8d1f\u8f7d\u9884\u6d4b\u8fdb\u884c\u4e3b\u52a8\u9884\u70ed\uff1b(3) \u8bbe\u8ba1\u96f6\u5f00\u9500\u7684 GPU \u5185\u5b58\u5207\u6362\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cWarmServe \u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u7cfb\u7edf\uff0cTTFT \u6700\u9ad8\u63d0\u5347 50.8 \u500d\uff1b\u76f8\u6bd4 GPU \u5171\u4eab\u7cfb\u7edf\uff0c\u53ef\u5904\u7406\u591a\u8fbe 2.5 \u500d\u7684\u8bf7\u6c42\u91cf\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528 LLM \u670d\u52a1\u8d1f\u8f7d\u7684\u53ef\u9884\u6d4b\u6027\u5e76\u5f15\u5165\u901a\u7528 GPU \u5de5\u4f5c\u8282\u70b9\u4e0e\u9ad8\u6548\u5185\u5b58\u7ba1\u7406\u673a\u5236\uff0cWarmServe \u5728\u4fdd\u8bc1\u9ad8\u541e\u5410\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u4e3a\u591a\u6a21\u578b\u5171\u4eab GPU \u96c6\u7fa4\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09216", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09216", "abs": "https://arxiv.org/abs/2512.09216", "authors": ["Guangzong Cai", "Zengyang Li", "Peng Liang", "Ran Mo", "Hui Liu", "Yutao Ma"], "title": "Bug Priority Change Prediction: An Exploratory Study on Apache Software", "comment": "Preprint accepted for publication in ACM Transactions on Software Engineering and Methodology (TOSEM), 2025", "summary": "Bug fixing is a critical activity in the software development process. In issue tracking systems such as JIRA, each bug report is assigned a priority level to indicate the urgency and importance level of the bug. The priority may change during the bug fixing process, indicating that the urgency and importance level of the bug will change with the bug fixing. However, manually evaluating priority changes for bugs is a tedious process that heavily relies on the subjective judgment of developers and project managers, leading to incorrect priority changes and thus hindering timely bug fixes. Given the lack of research on bug priority change prediction, we propose a novel two-phase bug report priority change prediction method based on bug fixing evolution features and class imbalance handling strategy. Specifically, we divided the bug lifecycle into two phases: bug reporting and bug fixing, and constructed bug priority change prediction models for each phase. To evaluate the performance of our method, we conducted experiments on a bug dataset constructed from 32 non-trivial Apache projects. The experimental results show that our proposed bug fixing evolution features and the adopted class imbalance handling strategy can effectively improve the performance of prediction models. The F1-score of the prediction model constructed for the bug reporting phase reached 0.798, while the F1-weighted and F1-macro of the prediction model constructed for the bug fixing phase were 0.712 and 0.613, respectively. Furthermore, we explored the cross-project applicability of our prediction models and their performance at different priority levels. The findings indicate large variations in model performance across different projects, although the overall scores remain decent. Meanwhile, the predictive performance across various priority levels remained relatively consistently high.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f3a\u9677\u4fee\u590d\u6f14\u5316\u7279\u5f81\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\u7b56\u7565\u7684\u4e24\u9636\u6bb5\u7f3a\u9677\u62a5\u544a\u4f18\u5148\u7ea7\u53d8\u66f4\u9884\u6d4b\u65b9\u6cd5\uff0c\u5728Apache\u9879\u76ee\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u624b\u52a8\u8bc4\u4f30\u7f3a\u9677\u4f18\u5148\u7ea7\u53d8\u66f4\u4f9d\u8d56\u5f00\u53d1\u4eba\u5458\u548c\u9879\u76ee\u7ecf\u7406\u7684\u4e3b\u89c2\u5224\u65ad\uff0c\u6613\u51fa\u9519\u4e14\u6548\u7387\u4f4e\uff0c\u800c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u7f3a\u9677\u4f18\u5148\u7ea7\u53d8\u66f4\u9884\u6d4b\u7684\u6709\u6548\u65b9\u6cd5\u3002", "method": "\u5c06\u7f3a\u9677\u751f\u547d\u5468\u671f\u5212\u5206\u4e3a\u7f3a\u9677\u62a5\u544a\u9636\u6bb5\u548c\u7f3a\u9677\u4fee\u590d\u9636\u6bb5\uff0c\u5206\u522b\u6784\u5efa\u4f18\u5148\u7ea7\u53d8\u66f4\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u7f3a\u9677\u4fee\u590d\u6f14\u5316\u7279\u5f81\u4e0e\u7c7b\u522b\u4e0d\u5e73\u8861\u5904\u7406\u7b56\u7565\u3002", "result": "\u572832\u4e2aApache\u9879\u76ee\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u62a5\u544a\u9636\u6bb5\u6a21\u578bF1-score\u8fbe0.798\uff1b\u4fee\u590d\u9636\u6bb5\u6a21\u578bF1-weighted\u548cF1-macro\u5206\u522b\u4e3a0.712\u548c0.613\u3002\u8de8\u9879\u76ee\u9002\u7528\u6027\u548c\u4e0d\u540c\u4f18\u5148\u7ea7\u4e0b\u7684\u6027\u80fd\u4e5f\u8fdb\u884c\u4e86\u5206\u6790\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u7f3a\u9677\u4f18\u5148\u7ea7\u53d8\u66f4\u9884\u6d4b\u6027\u80fd\uff0c\u5c3d\u7ba1\u8de8\u9879\u76ee\u8868\u73b0\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u6574\u4f53\u6548\u679c\u826f\u597d\uff0c\u4e14\u5728\u4e0d\u540c\u4f18\u5148\u7ea7\u4e0b\u4fdd\u6301\u8f83\u9ad8\u9884\u6d4b\u4e00\u81f4\u6027\u3002"}}
{"id": "2512.09695", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.09695", "abs": "https://arxiv.org/abs/2512.09695", "authors": ["Hyunjoon Kim", "Chaerim Lim", "Hyeonjun An", "Rathijit Sen", "Kwanghyun Park"], "title": "Exqutor: Extended Query Optimizer for Vector-augmented Analytical Queries", "comment": null, "summary": "Vector similarity search is becoming increasingly important for data science pipelines, particularly in Retrieval-Augmented Generation (RAG), where it enhances large language model inference by enabling efficient retrieval of relevant external knowledge. As RAG expands with table-augmented generation to incorporate structured data, workloads integrating table and vector search are becoming more prevalent. However, efficiently executing such queries remains challenging due to inaccurate cardinality estimation for vector search components, leading to suboptimal query plans. In this paper, we propose Exqutor, an extended query optimizer for vector-augmented analytical queries. Exqutor is a pluggable cardinality estimation framework designed to address this issue, leveraging exact cardinality query optimization techniques to enhance estimation accuracy when vector indexes (e.g., HNSW, IVF) are available. In scenarios lacking these indexes, we employ a sampling-based approach with adaptive sampling size adjustment, dynamically tuning the sample size to balance estimation accuracy and sampling overhead. This allows Exqutor to efficiently approximate vector search cardinalities while minimizing computational costs. We integrate our framework into pgvector, VBASE, and DuckDB, demonstrating performance improvements of up to four orders of magnitude on vector-augmented analytical queries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faExqutor\uff0c\u4e00\u79cd\u7528\u4e8e\u5411\u91cf\u589e\u5f3a\u5206\u6790\u67e5\u8be2\u7684\u53ef\u63d2\u62d4\u57fa\u6570\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u7cbe\u786e\u57fa\u6570\u4f18\u5316\u6280\u672f\u548c\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u67e5\u8be2\u6027\u80fd\u3002", "motivation": "\u5728\u7ed3\u5408\u8868\u683c\u4e0e\u5411\u91cf\u641c\u7d22\u7684RAG\u573a\u666f\u4e2d\uff0c\u7531\u4e8e\u5411\u91cf\u641c\u7d22\u90e8\u5206\u7684\u57fa\u6570\u4f30\u8ba1\u4e0d\u51c6\u786e\uff0c\u5bfc\u81f4\u67e5\u8be2\u8ba1\u5212\u6b21\u4f18\uff0c\u5f71\u54cd\u6574\u4f53\u6548\u7387\u3002", "method": "Exqutor\u5728\u5b58\u5728\u5411\u91cf\u7d22\u5f15\uff08\u5982HNSW\u3001IVF\uff09\u65f6\u91c7\u7528\u7cbe\u786e\u57fa\u6570\u4f30\u8ba1\u6280\u672f\uff1b\u5728\u65e0\u7d22\u5f15\u65f6\u4f7f\u7528\u81ea\u9002\u5e94\u91c7\u6837\u65b9\u6cd5\u52a8\u6001\u8c03\u6574\u6837\u672c\u5927\u5c0f\uff0c\u4ee5\u5e73\u8861\u7cbe\u5ea6\u4e0e\u5f00\u9500\uff0c\u5e76\u96c6\u6210\u5230pgvector\u3001VBASE\u548cDuckDB\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u7cfb\u7edf\u4e2d\u96c6\u6210\u540e\uff0cExqutor\u5728\u5411\u91cf\u589e\u5f3a\u5206\u6790\u67e5\u8be2\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad8\u8fbe\u56db\u4e2a\u6570\u91cf\u7ea7\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Exqutor\u6709\u6548\u89e3\u51b3\u4e86\u5411\u91cf-\u8868\u683c\u6df7\u5408\u67e5\u8be2\u4e2d\u7684\u57fa\u6570\u4f30\u8ba1\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u6267\u884c\u6548\u7387\uff0c\u5177\u6709\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.09543", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09543", "abs": "https://arxiv.org/abs/2512.09543", "authors": ["Arihant Tripathy", "Ch Pavan Harshit", "Karthik Vaidhyanathan"], "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs", "comment": "8 pages, 5 figures, 1 table. Accepted to AGENT 2026 (ICSE 2026 workshop)", "summary": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.\n  Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.\n  Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.\n  Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.\n  Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u56db\u4e2a\u4e3b\u6d41\u667a\u80fd\u4f53\u6846\u67b6\u5728\u4f7f\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u8fdb\u884c\u81ea\u52a8\u5316\u95ee\u9898\u4fee\u590d\u65f6\u7684\u6027\u80fd\u3001\u80fd\u6548\u4e0e\u8d44\u6e90\u6d88\u8017\uff0c\u53d1\u73b0\u5f53\u524d\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1\u7684\u6846\u67b6\u5728\u642d\u914dSLMs\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u80fd\u91cf\u4e3b\u8981\u88ab\u6d6a\u8d39\u5728\u65e0\u6548\u63a8\u7406\u5faa\u73af\u4e2d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u8f6f\u4ef6\u5de5\u7a0b\u667a\u80fd\u4f53\u4f9d\u8d56\u4e13\u6709\u5927\u6a21\u578b\uff0c\u96be\u4ee5\u672c\u5730\u90e8\u7f72\uff0c\u4fc3\u4f7f\u7814\u7a76\u8005\u5173\u6ce8\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\uff0c\u4f46SLMs\u5728\u590d\u6742\u667a\u80fd\u4f53\u6846\u67b6\u4e2d\u7684\u5b9e\u9645\u6548\u80fd\u548c\u6548\u7387\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5728\u56fa\u5b9a\u786c\u4ef6\u4e0a\uff0c\u4f7f\u7528\u4e24\u4e2aSLMs\uff08Gemma-3 4B\u3001Qwen-3 1.7B\uff09\u5bf9\u56db\u4e2a\u667a\u80fd\u4f53\u6846\u67b6\uff08SWE-Agent\u3001OpenHands\u3001Mini SWE Agent\u3001AutoCodeRover\uff09\u5728SWE-bench Verified Mini\u57fa\u51c6\u4e0a\u8fdb\u884c\u53d7\u63a7\u8bc4\u4f30\uff0c\u6bcf\u79cd\u914d\u7f6e\u8fd0\u884c150\u6b21\uff0c\u6d4b\u91cf\u80fd\u8017\u3001\u8017\u65f6\u3001token\u4f7f\u7528\u91cf\u548c\u5185\u5b58\u5360\u7528\u3002", "result": "\u6846\u67b6\u67b6\u6784\u662f\u80fd\u8017\u7684\u4e3b\u8981\u51b3\u5b9a\u56e0\u7d20\uff1a\u6700\u8017\u80fd\u7684AutoCodeRover\uff08Gemma\uff09\u5e73\u5747\u6bd4\u6700\u8282\u80fd\u7684OpenHands\uff08Gemma\uff09\u591a\u6d88\u80179.4\u500d\u80fd\u91cf\uff1b\u4f46\u4efb\u52a1\u89e3\u51b3\u7387\u63a5\u8fd1\u96f6\uff0c\u8868\u660eSLMs\u7684\u6709\u9650\u63a8\u7406\u80fd\u529b\u662f\u6210\u529f\u74f6\u9888\uff0c\u800c\u6846\u67b6\u8bbe\u8ba1\u662f\u6548\u7387\u74f6\u9888\u3002", "conclusion": "\u5f53\u524d\u4e3a\u5f3a\u5927LLM\u8bbe\u8ba1\u7684\u667a\u80fd\u4f53\u6846\u67b6\u5728\u642d\u914dSLMs\u65f6\u6548\u7387\u4f4e\u4e0b\u4e14\u80fd\u8017\u6d6a\u8d39\u4e25\u91cd\uff1b\u5b9e\u73b0\u4f4e\u80fd\u8017\u53ef\u884c\u65b9\u6848\u9700\u4ece\u88ab\u52a8\u7f16\u6392\u8f6c\u5411\u4e3b\u52a8\u7ba1\u7406SLM\u5f31\u70b9\u7684\u67b6\u6784\u3002"}}
{"id": "2512.09762", "categories": ["cs.DB"], "pdf": "https://arxiv.org/pdf/2512.09762", "abs": "https://arxiv.org/abs/2512.09762", "authors": ["Jonathan Edwards", "Tomas Petricek"], "title": "Baseline: Operation-Based Evolution and Versioning of Data", "comment": "Submitted to The Art, Science, and Engineering of Programming", "summary": "Baseline is a platform for richly structured data supporting change in multiple dimensions: mutation over time, collaboration across space, and evolution through design changes. It is built upon Operational Differencing, a new technique for managing data in terms of high-level operations that include refactorings and schema changes. We use operational differencing to construct an operation-based form of version control on data structures used in programming languages and relational databases.\n  This approach to data version control does fine-grained diffing and merging despite intervening structural transformations like schema changes. It offers users a simplified conceptual model of version control for ad hoc usage: There is no repo; Branching is just copying. The informaton maintained in a repo can be synthesized more precisely from the append-only histories of branches. Branches can be flexibly shared as is commonly done with document files, except with the added benefit of diffing and merging.\n  We conjecture that queries can be operationalized into a sequence of schema and data operations. We develop that idea on a query language fragment containing selects and joins.\n  Operationalized queries are represented as a future timeline that is speculatively executed as a branch off of the present state, returning a value from its hypothetical future. Operationalized queries get rewritten to accommodate schema change \"for free\" by the machinery of operational differencing.\n  Altogether we develop solutions to four of the eight challenge problems of schema evolution identified in a recent paper.", "AI": {"tldr": "Baseline \u662f\u4e00\u4e2a\u652f\u6301\u591a\u7ef4\u53d8\u66f4\uff08\u65f6\u95f4\u4e0a\u7684\u4fee\u6539\u3001\u7a7a\u95f4\u4e0a\u7684\u534f\u4f5c\u548c\u8bbe\u8ba1\u6f14\u8fdb\uff09\u7684\u7ed3\u6784\u5316\u6570\u636e\u5e73\u53f0\uff0c\u57fa\u4e8e\u4e00\u79cd\u540d\u4e3a\u201c\u64cd\u4f5c\u5dee\u5f02\uff08Operational Differencing\uff09\u201d\u7684\u65b0\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9\u7f16\u7a0b\u8bed\u8a00\u548c\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\u6570\u636e\u7ed3\u6784\u7684\u64cd\u4f5c\u7ea7\u7248\u672c\u63a7\u5236\uff0c\u80fd\u5904\u7406\u5305\u62ec\u6a21\u5f0f\u53d8\u66f4\u5728\u5185\u7684\u7ed3\u6784\u8f6c\u6362\uff0c\u5e76\u7b80\u5316\u4e86\u7528\u6237\u5bf9\u7248\u672c\u63a7\u5236\u7684\u7406\u89e3\u4e0e\u4f7f\u7528\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u7248\u672c\u63a7\u5236\u7cfb\u7edf\u96be\u4ee5\u6709\u6548\u5904\u7406\u6d89\u53ca\u7ed3\u6784\u53d8\u5316\uff08\u5982\u6a21\u5f0f\u6f14\u5316\uff09\u7684\u7ec6\u7c92\u5ea6\u5dee\u5f02\u4e0e\u5408\u5e76\u3002\u4f5c\u8005\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u66f4\u7075\u6d3b\u3001\u76f4\u89c2\u4e14\u80fd\u81ea\u7136\u652f\u6301\u7ed3\u6784\u6f14\u5316\u7684\u6570\u636e\u7248\u672c\u63a7\u5236\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u201c\u64cd\u4f5c\u5dee\u5f02\u201d\u6280\u672f\uff0c\u5c06\u6570\u636e\u53d8\u66f4\u8868\u793a\u4e3a\u9ad8\u5c42\u64cd\u4f5c\uff08\u5305\u62ec\u91cd\u6784\u548c\u6a21\u5f0f\u53d8\u66f4\uff09\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u6784\u5efa\u65e0\u4ed3\u5e93\uff08repo-less\uff09\u3001\u4ee5\u5206\u652f\u5373\u590d\u5236\u4e3a\u6838\u5fc3\u7406\u5ff5\u7684\u64cd\u4f5c\u578b\u7248\u672c\u63a7\u5236\u7cfb\u7edf\uff1b\u540c\u65f6\u63a2\u7d22\u5c06\u67e5\u8be2\uff08\u5982 select \u548c join\uff09\u64cd\u4f5c\u5316\u4e3a\u4e00\u7cfb\u5217\u6a21\u5f0f\u4e0e\u6570\u636e\u64cd\u4f5c\uff0c\u5e76\u5728\u5047\u8bbe\u6027\u672a\u6765\u5206\u652f\u4e0a\u6267\u884c\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u5305\u542b\u6a21\u5f0f\u53d8\u66f4\u5728\u5185\u7684\u7ed3\u6784\u5316\u6570\u636e\u7684\u7ec6\u7c92\u5ea6 diff \u4e0e merge\uff0c\u7b80\u5316\u4e86\u7528\u6237\u6a21\u578b\uff0c\u5e76\u80fd\u81ea\u52a8\u91cd\u5199\u64cd\u4f5c\u5316\u67e5\u8be2\u4ee5\u9002\u5e94\u6a21\u5f0f\u53d8\u5316\uff1b\u89e3\u51b3\u4e86\u8fd1\u671f\u4e00\u7bc7\u8bba\u6587\u63d0\u51fa\u7684\u516b\u4e2a\u6a21\u5f0f\u6f14\u5316\u6311\u6218\u4e2d\u7684\u56db\u4e2a\u3002", "conclusion": "\u64cd\u4f5c\u5dee\u5f02\u4e3a\u6570\u636e\u7248\u672c\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u7edf\u4e00\u5904\u7406\u6570\u636e\u53d8\u66f4\u4e0e\u7ed3\u6784\u6f14\u5316\uff0c\u5728\u63d0\u5347\u7075\u6d3b\u6027\u7684\u540c\u65f6\u964d\u4f4e\u7528\u6237\u8ba4\u77e5\u8d1f\u62c5\uff0c\u4e3a\u89e3\u51b3\u6a21\u5f0f\u6f14\u5316\u96be\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\u3002"}}
{"id": "2512.09568", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09568", "abs": "https://arxiv.org/abs/2512.09568", "authors": ["Zhi Zhao", "Hang Xiao", "Wei Rang"], "title": "PHWSOA: A Pareto-based Hybrid Whale-Seagull Scheduling for Multi-Objective Tasks in Cloud Computing", "comment": "24 pages,5 figures", "summary": "Task scheduling is a critical research challenge in cloud computing, a transformative technology widely adopted across industries. Although numerous scheduling solutions exist, they predominantly optimize singular or limited metrics such as execution time or resource utilization often neglecting the need for comprehensive multi-objective optimization. To bridge this gap, this paper proposes the Pareto-based Hybrid Whale-Seagull Optimization Algorithm (PHWSOA). This algorithm synergistically combines the strengths of the Whale Optimization Algorithm (WOA) and the Seagull Optimization Algorithm (SOA), specifically mitigating WOA's limitations in local exploitation and SOA's constraints in global exploration. Leveraging Pareto dominance principles, PHWSOA simultaneously optimizes three key objectives: makespan, virtual machine (VM) load balancing, and economic cost. Key enhancements include: Halton sequence initialization for superior population diversity, a Pareto-guided mutation mechanism to avert premature convergence, and parallel processing for accelerated convergence. Furthermore, a dynamic VM load redistribution mechanism is integrated to improve load balancing during task execution. Extensive experiments conducted on the CloudSim simulator, utilizing real-world workload traces from NASA-iPSC and HPC2N, demonstrate that PHWSOA delivers substantial performance gains. Specifically, it achieves up to a 72.1% reduction in makespan, a 36.8% improvement in VM load balancing, and 23.5% cost savings. These results substantially outperform baseline methods including WOA, GA, PEWOA, and GCWOA underscoring PHWSOA's strong potential for enabling efficient resource management in practical cloud environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e15\u7d2f\u6258\u7684\u6df7\u5408\u9cb8\u9c7c-\u6d77\u9e25\u4f18\u5316\u7b97\u6cd5\uff08PHWSOA\uff09\uff0c\u7528\u4e8e\u4e91\u73af\u5883\u4e2d\u591a\u76ee\u6807\u4efb\u52a1\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b8c\u5de5\u65f6\u95f4\u3001\u865a\u62df\u673a\u8d1f\u8f7d\u5747\u8861\u548c\u7ecf\u6d4e\u6210\u672c\u4e09\u9879\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u4e91\u4efb\u52a1\u8c03\u5ea6\u65b9\u6cd5\u591a\u805a\u7126\u5355\u4e00\u6216\u6709\u9650\u76ee\u6807\uff08\u5982\u6267\u884c\u65f6\u95f4\u6216\u8d44\u6e90\u5229\u7528\u7387\uff09\uff0c\u7f3a\u4e4f\u5bf9\u591a\u76ee\u6807\u534f\u540c\u4f18\u5316\u7684\u7efc\u5408\u8003\u8651\u3002", "method": "\u7ed3\u5408\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\uff08WOA\uff09\u4e0e\u6d77\u9e25\u4f18\u5316\u7b97\u6cd5\uff08SOA\uff09\u7684\u4f18\u52bf\uff0c\u5f15\u5165Halton\u5e8f\u5217\u521d\u59cb\u5316\u3001\u5e15\u7d2f\u6258\u5f15\u5bfc\u53d8\u5f02\u673a\u5236\u3001\u5e76\u884c\u5904\u7406\u53ca\u52a8\u6001\u865a\u62df\u673a\u8d1f\u8f7d\u91cd\u5206\u914d\u7b56\u7565\uff0c\u6784\u5efa\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u3002", "result": "\u5728CloudSim\u4e0a\u57fa\u4e8eNASA-iPSC\u548cHPC2N\u771f\u5b9e\u8d1f\u8f7d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPHWSOA\u76f8\u8f83WOA\u3001GA\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6700\u591a\u53ef\u51cf\u5c1172.1%\u7684\u5b8c\u5de5\u65f6\u95f4\u3001\u63d0\u534736.8%\u7684\u8d1f\u8f7d\u5747\u8861\u6027\uff0c\u5e76\u8282\u770123.5%\u7684\u6210\u672c\u3002", "conclusion": "PHWSOA\u5728\u591a\u76ee\u6807\u4efb\u52a1\u8c03\u5ea6\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5177\u5907\u5728\u5b9e\u9645\u4e91\u73af\u5883\u4e2d\u9ad8\u6548\u7ba1\u7406\u8d44\u6e90\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2512.09562", "categories": ["cs.SE", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.09562", "abs": "https://arxiv.org/abs/2512.09562", "authors": ["Radoslaw Klimek", "Jakub Blazowski"], "title": "Explainable Verification of Hierarchical Workflows Mined from Event Logs with Shapley Values", "comment": "This manuscript has been submitted to Rank A/A* conference", "summary": "Workflow mining discovers hierarchical process trees from event logs, but it remains unclear why such models satisfy or violate logical properties, or how individual elements contribute to overall behavior. We propose to translate mined workflows into logical specifications and analyze properties such as satisfiability, liveness, and safety with automated theorem provers. On this basis, we adapt Shapley values from cooperative game theory to attribute outcomes to workflow elements and quantify their contributions. Experiments on benchmark datasets show that this combination identifies critical nodes, reveals redundancies, and exposes harmful structures. This outlines a novel direction for explainable workflow analysis with direct relevance to software engineering practice, supporting compliance checks, process optimization, redundancy reduction, and the design of next-generation process mining tools.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5de5\u4f5c\u6d41\u6316\u6398\u5f97\u5230\u7684\u6d41\u7a0b\u6a21\u578b\u8f6c\u5316\u4e3a\u903b\u8f91\u89c4\u8303\uff0c\u5229\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u5206\u6790\u5176\u5c5e\u6027\uff0c\u5e76\u7ed3\u5408Shapley\u503c\u91cf\u5316\u5404\u5143\u7d20\u5bf9\u6574\u4f53\u884c\u4e3a\u7684\u8d21\u732e\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u6d41\u6316\u6398\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6240\u53d1\u73b0\u6a21\u578b\u4e3a\u4f55\u6ee1\u8db3\u6216\u8fdd\u53cd\u7279\u5b9a\u903b\u8f91\u5c5e\u6027\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u4e5f\u96be\u4ee5\u8bc4\u4f30\u5355\u4e2a\u5143\u7d20\u5bf9\u6574\u4f53\u884c\u4e3a\u7684\u5177\u4f53\u8d21\u732e\u3002", "method": "\u5c06\u6316\u6398\u51fa\u7684\u5de5\u4f5c\u6d41\u6a21\u578b\u7ffb\u8bd1\u4e3a\u903b\u8f91\u89c4\u8303\uff0c\u4f7f\u7528\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u5668\u5206\u6790\u5176\u53ef\u6ee1\u8db3\u6027\u3001\u6d3b\u6027\u548c\u5b89\u5168\u6027\u7b49\u5c5e\u6027\uff0c\u5e76\u5f15\u5165\u5408\u4f5c\u535a\u5f08\u8bba\u4e2d\u7684Shapley\u503c\u6765\u91cf\u5316\u5404\u5de5\u4f5c\u6d41\u5143\u7d20\u5bf9\u5206\u6790\u7ed3\u679c\u7684\u8d21\u732e\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u5173\u952e\u8282\u70b9\u3001\u63ed\u793a\u5197\u4f59\u7ed3\u6784\u5e76\u66b4\u9732\u6709\u5bb3\u7ec4\u4ef6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u89e3\u91ca\u7684\u5de5\u4f5c\u6d41\u5206\u6790\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5728\u5408\u89c4\u68c0\u67e5\u3001\u6d41\u7a0b\u4f18\u5316\u3001\u5197\u4f59\u6d88\u9664\u53ca\u65b0\u4e00\u4ee3\u6d41\u7a0b\u6316\u6398\u5de5\u5177\u8bbe\u8ba1\u7b49\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.09836", "categories": ["cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09836", "abs": "https://arxiv.org/abs/2512.09836", "authors": ["Bernhard St\u00f6ckl", "Maximilian E. Sch\u00fcle"], "title": "Fast Factorized Learning: Powered by In-Memory Database Systems", "comment": null, "summary": "Learning models over factorized joins avoids redundant computations by identifying and pre-computing shared cofactors. Previous work has investigated the performance gain when computing cofactors on traditional disk-based database systems. Due to the absence of published code, the experiments could not be reproduced on in-memory database systems. This work describes the implementation when using cofactors for in-database factorized learning. We benchmark our open-source implementation for learning linear regression on factorized joins with PostgreSQL -- as a disk-based database system -- and HyPer -- as an in-memory engine. The evaluation shows a performance gain of factorized learning on in-memory database systems by 70\\% to non-factorized learning and by a factor of 100 compared to disk-based database systems. Thus, modern database engines can contribute to the machine learning pipeline by pre-computing aggregates prior to data extraction to accelerate training.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9e\u73b0\u4e86\u57fa\u4e8e\u56e0\u5b50\u5316\u8fde\u63a5\u7684\u6570\u636e\u5e93\u5185\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\uff08\u5982HyPer\uff09\u548c\u78c1\u76d8\u6570\u636e\u5e93\u7cfb\u7edf\uff08\u5982PostgreSQL\uff09\u4e0a\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u56e0\u5b50\u5316\u5b66\u4e60\u5728\u5185\u5b58\u7cfb\u7edf\u4e2d\u6bd4\u975e\u56e0\u5b50\u5316\u65b9\u6cd5\u5feb70%\uff0c\u6bd4\u78c1\u76d8\u7cfb\u7edf\u5feb100\u500d\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u56e0\u5b50\u5316\u8fde\u63a5\u5b66\u4e60\u7684\u7814\u7a76\u7f3a\u4e4f\u5f00\u6e90\u4ee3\u7801\uff0c\u65e0\u6cd5\u5728\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\u590d\u73b0\u5b9e\u9a8c\uff1b\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u56e0\u5b50\u5316\u5b66\u4e60\u5728\u73b0\u4ee3\u5185\u5b58\u6570\u636e\u5e93\u4e2d\u7684\u6027\u80fd\u4f18\u52bf\u3002", "method": "\u4f5c\u8005\u5b9e\u73b0\u4e86\u652f\u6301\u56e0\u5b50\u5316\u8fde\u63a5\u7684\u7ebf\u6027\u56de\u5f52\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728PostgreSQL\uff08\u78c1\u76d8\u578b\uff09\u548cHyPer\uff08\u5185\u5b58\u578b\uff09\u4e24\u79cd\u6570\u636e\u5e93\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5f00\u6e90\u5b9e\u73b0\u4e0e\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5185\u5b58\u6570\u636e\u5e93\u7cfb\u7edf\u4e2d\uff0c\u56e0\u5b50\u5316\u5b66\u4e60\u76f8\u6bd4\u975e\u56e0\u5b50\u5316\u65b9\u6cd5\u6027\u80fd\u63d0\u534770%\uff0c\u76f8\u6bd4\u78c1\u76d8\u6570\u636e\u5e93\u7cfb\u7edf\u63d0\u5347\u8fbe100\u500d\u3002", "conclusion": "\u73b0\u4ee3\u6570\u636e\u5e93\u5f15\u64ce\u53ef\u901a\u8fc7\u5728\u6570\u636e\u63d0\u53d6\u524d\u9884\u8ba1\u7b97\u805a\u5408\u4fe1\u606f\uff0c\u6709\u6548\u52a0\u901f\u673a\u5668\u5b66\u4e60\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5c24\u5176\u5728\u5185\u5b58\u6570\u636e\u5e93\u4e2d\u6548\u679c\u663e\u8457\u3002"}}
{"id": "2512.09664", "categories": ["cs.DC", "cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.09664", "abs": "https://arxiv.org/abs/2512.09664", "authors": ["Antonio Terpin", "Alan Bonomi", "Francesco Banelli", "Raffaello D'Andrea"], "title": "SynthPix: A lightspeed PIV images generator", "comment": "Code: https://github.com/antonioterpin/synthpix", "summary": "We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.", "AI": {"tldr": "SynthPix \u662f\u4e00\u4e2a\u57fa\u4e8e JAX \u5b9e\u73b0\u7684\u9ad8\u6027\u80fd\u5e76\u884c\u5408\u6210\u56fe\u50cf\u751f\u6210\u5668\uff0c\u4e13\u4e3a\u7c92\u5b50\u56fe\u50cf\u6d4b\u901f\uff08PIV\uff09\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u5bf9\u751f\u6210\u901f\u5ea6\uff0c\u652f\u6301\u73b0\u6709\u5de5\u5177\u7684\u914d\u7f6e\u53c2\u6570\uff0c\u65e8\u5728\u52a0\u901f\u6570\u636e\u5bc6\u96c6\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u8bad\u7ec3\u548c\u5b9e\u65f6 PIV \u53cd\u9988\u4e2d\u7684\u6d41\u573a\u4f30\u8ba1\u7814\u7a76\u3002", "motivation": "\u4e3a\u652f\u6301\u6570\u636e\u5bc6\u96c6\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6d41\u573a\u4f30\u8ba1\u4e2d\u7684\u8bad\u7ec3\uff0c\u5e76\u7f29\u77ed\u5728\u5b9e\u65f6 PIV \u53cd\u9988\u63a7\u5236\u7814\u7a76\u4e2d\u5f00\u53d1\u5feb\u901f\u6d41\u573a\u4f30\u8ba1\u65b9\u6cd5\u7684\u8fed\u4ee3\u65f6\u95f4\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u5e76\u884c\u5316\u7684\u5408\u6210 PIV \u56fe\u50cf\u751f\u6210\u5de5\u5177\u3002", "method": "\u4f7f\u7528 JAX \u5b9e\u73b0 SynthPix\uff0c\u5229\u7528\u5176\u5728\u52a0\u901f\u5668\u4e0a\u7684\u9ad8\u6027\u80fd\u548c\u5e76\u884c\u80fd\u529b\uff0c\u751f\u6210\u7b26\u5408\u73b0\u6709 PIV \u5de5\u5177\u914d\u7f6e\u53c2\u6570\u7684\u5408\u6210\u56fe\u50cf\u5bf9\u3002", "result": "SynthPix \u5728\u6bcf\u79d2\u751f\u6210\u56fe\u50cf\u5bf9\u7684\u6570\u91cf\u4e0a\u6bd4\u73b0\u6709\u5de5\u5177\u9ad8\u51fa\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u6548\u7387\u3002", "conclusion": "SynthPix \u4e3a\u6d41\u4f53\u529b\u5b66\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5408\u6210 PIV \u56fe\u50cf\u751f\u6210\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6d41\u573a\u4f30\u8ba1\u548c\u5b9e\u65f6\u63a7\u5236\u7814\u7a76\u3002"}}
{"id": "2512.09685", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09685", "abs": "https://arxiv.org/abs/2512.09685", "authors": ["Zeyu Zhang", "Haiying Shen"], "title": "Straggler Tolerant and Resilient DL Training on Homogeneous GPUs", "comment": null, "summary": "Despite the popularity of homogeneous GPU-based deep learning (DL) training, the prevalence, causes and impact of stragglers and the effectiveness of existing straggler mitigation approaches are still not well understood in this scenario due to limited research on these questions. To fill this gap, we conducted comprehensive experiments and found that stragglers remain widespread due to CPU and bandwidth usage imbalances. Additionally, existing mitigation methods that switch from synchronous stochastic gradient descent (SSGD) to asynchronous SGD (ASGD) may not improve Time-To-Accuracy (TTA) and can even generate more stragglers due to its higher resource consumption. To address these newly found problems, we propose the Straggler Tolerant And Resilient DL training system (STAR). STAR includes new synchronization modes that group workers for each parameter updating. It has a heuristic and an ML method to choose the optimal synchronization mode for minimizing TTA, and reallocates resources to support the selected mode while minimizing the impact on co-located jobs. Moreover, it proactively prevents stragglers by avoiding overloading the CPU and bandwidth resources in allocating PSs (which consume high CPU and bandwidth) and in gradient transmission. Our trace-driven evaluation on AWS shows that STAR generates 48-84% and 51-70% lower TTA than state-of-the-art systems in the PS and all-reduce architectures, respectively, while maintaining the converged accuracy of SSGD. The code for STAR is open-sourced.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u540c\u6784GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u62d6\u6162\u8282\u70b9\uff08straggler\uff09\u7684\u666e\u904d\u6027\u3001\u6210\u56e0\u53ca\u5176\u5f71\u54cd\uff0c\u53d1\u73b0CPU\u4e0e\u5e26\u5bbd\u4f7f\u7528\u4e0d\u5747\u8861\u662f\u4e3b\u8981\u539f\u56e0\uff0c\u5e76\u6307\u51fa\u73b0\u6709\u4ece\u540c\u6b65SGD\u5207\u6362\u5230\u5f02\u6b65SGD\u7684\u7f13\u89e3\u65b9\u6cd5\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u3002\u4e3a\u6b64\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86STAR\u7cfb\u7edf\uff0c\u901a\u8fc7\u65b0\u578b\u540c\u6b65\u6a21\u5f0f\u3001\u542f\u53d1\u5f0f\u4e0e\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9009\u62e9\u6700\u4f18\u540c\u6b65\u7b56\u7565\uff0c\u5e76\u4e3b\u52a8\u907f\u514d\u8d44\u6e90\u8fc7\u8f7d\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u65f6\u95f4\uff08TTA\uff09\u4e14\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u5bf9\u540c\u6784GPU\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e2d\u62d6\u6162\u8282\u70b9\u7684\u6210\u56e0\u3001\u5f71\u54cd\u53ca\u73b0\u6709\u7f13\u89e3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51faSTAR\u7cfb\u7edf\uff0c\u5305\u542b\u65b0\u7684\u53c2\u6570\u66f4\u65b0\u540c\u6b65\u6a21\u5f0f\u3001\u57fa\u4e8e\u542f\u53d1\u5f0f\u548c\u673a\u5668\u5b66\u4e60\u7684\u540c\u6b65\u6a21\u5f0f\u9009\u62e9\u673a\u5236\u3001\u8d44\u6e90\u91cd\u5206\u914d\u7b56\u7565\uff0c\u4ee5\u53ca\u5728\u53c2\u6570\u670d\u52a1\u5668\u5206\u914d\u548c\u68af\u5ea6\u4f20\u8f93\u4e2d\u4e3b\u52a8\u907f\u514dCPU\u4e0e\u5e26\u5bbd\u8fc7\u8f7d\u7684\u673a\u5236\u3002", "result": "\u5728AWS\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTAR\u5728\u53c2\u6570\u670d\u52a1\u5668\u67b6\u6784\u548cAllReduce\u67b6\u6784\u4e0b\u5206\u522b\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\u964d\u4f4e48-84%\u548c51-70%\u7684TTA\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u540c\u6b65SGD\u76f8\u540c\u7684\u6536\u655b\u7cbe\u5ea6\u3002", "conclusion": "\u62d6\u6162\u8282\u70b9\u5728\u540c\u6784GPU\u8bad\u7ec3\u4e2d\u4ecd\u666e\u904d\u5b58\u5728\uff0c\u4e3b\u8981\u7531CPU\u548c\u5e26\u5bbd\u4e0d\u5e73\u8861\u5f15\u8d77\uff1b\u7b80\u5355\u5207\u6362\u81f3\u5f02\u6b65SGD\u672a\u5fc5\u6709\u6548\uff0c\u800cSTAR\u901a\u8fc7\u667a\u80fd\u540c\u6b65\u4e0e\u8d44\u6e90\u7ba1\u7406\u53ef\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2512.09627", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09627", "abs": "https://arxiv.org/abs/2512.09627", "authors": ["Jingwei Ye", "Zhi Wang", "Chenbin Su", "Jieshuai Yang", "Jiayi Ding", "Chunbo Liu", "Ge Chu"], "title": "LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection", "comment": null, "summary": "Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.", "AI": {"tldr": "LogICL \u662f\u4e00\u79cd\u7528\u4e8e\u8de8\u57df\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u7f16\u7801\u5668\u4e2d\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u591a\u76ee\u6807\u635f\u5931\u4f18\u5316\u8868\u793a\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u589e\u91cf\u5f97\u5206\u68c0\u7d22\u793a\u8303\u6837\u672c\uff0c\u4ece\u800c\u5728\u5c11\u91cf\u6216\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e Transformer \u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u76ee\u6807\u57df\u65e5\u5fd7\u7a00\u7f3a\u65f6\u9762\u4e34\u51b7\u542f\u52a8\u95ee\u9898\uff1b\u800c\u5f53\u524d\u8de8\u57df\u65b9\u6cd5\u591a\u4f9d\u8d56\u8868\u9762\u8bcd\u6c47\u76f8\u4f3c\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u7ed3\u6784\u5dee\u5f02\u4e0b\u7684\u6f5c\u5728\u8bed\u4e49\u7b49\u4ef7\u5173\u7cfb\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa LogICL \u6846\u67b6\uff1a\u8bad\u7ec3\u9636\u6bb5\u6784\u5efa delta \u77e9\u9635\u8861\u91cf\u793a\u8303\u6837\u672c\u76f8\u5bf9\u4e8e\u96f6\u6837\u672c\u63a8\u7406\u7684\u6548\u7528\uff0c\u91c7\u7528\u591a\u76ee\u6807\u635f\u5931\uff08\u5305\u62ec ICL \u5f15\u5bfc\u9879\u3001\u6700\u5927\u5747\u503c\u5dee\u5f02\u57df\u5bf9\u9f50\u9879\u548c\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\uff09\u4f18\u5316\u8f7b\u91cf\u7f16\u7801\u5668\uff1b\u63a8\u7406\u9636\u6bb5\u5229\u7528\u8bed\u4e49\u76f8\u4f3c\u6027\u548c delta \u5206\u6570\u68c0\u7d22\u793a\u8303\u6837\u672c\uff0c\u9a71\u52a8\u51bb\u7ed3 LLM \u8fdb\u884c\u5e26\u601d\u7ef4\u94fe\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002", "result": "\u5728\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLogICL \u5728\u5f02\u6784\u7cfb\u7edf\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u53ef\u89c6\u5316\u4e0e\u6848\u4f8b\u5206\u6790\u8868\u660e\u5176\u80fd\u6709\u6548\u5f25\u5408\u8bed\u4e49\u9e3f\u6c9f\uff0c\u8d85\u8d8a\u8868\u9762\u8bcd\u6c47\u5339\u914d\uff0c\u6355\u6349\u6f5c\u5728\u8bed\u4e49\u7b49\u4ef7\u5173\u7cfb\u3002", "conclusion": "LogICL \u901a\u8fc7\u5c06 LLM \u63a8\u7406\u80fd\u529b\u84b8\u998f\u4e3a\u8f7b\u91cf\u7f16\u7801\u5668\uff0c\u5e76\u7ed3\u5408\u591a\u76ee\u6807\u4f18\u5316\u4e0e\u63a8\u7406\u611f\u77e5\u7684\u793a\u8303\u68c0\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u4e0e\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2512.09710", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.09710", "abs": "https://arxiv.org/abs/2512.09710", "authors": ["Hagit Attiya", "Panagiota Fatourou", "Eleftherios Kosmas", "Yuanhao Wei"], "title": "Recoverable Lock-Free Locks", "comment": null, "summary": "This paper presents the first transformation that introduces both lock-freedom and recoverability. Our transformation starts with a lock-based implementation, and provides a recoverable, lock-free substitution to lock acquire and lock release operations. The transformation supports nested locks for generality and ensures recoverability without jeopardising the correctness of the lock-based implementation it is applied on.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u540c\u65f6\u5b9e\u73b0\u65e0\u9501\u6027\u548c\u53ef\u6062\u590d\u6027\u7684\u8f6c\u6362\u65b9\u6cd5\uff0c\u5c06\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u8f6c\u5316\u4e3a\u53ef\u6062\u590d\u4e14\u65e0\u9501\u7684\u7248\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u65e0\u9501\u6027\u548c\u7cfb\u7edf\u5d29\u6e83\u540e\u7684\u53ef\u6062\u590d\u6027\uff0c\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4ece\u57fa\u4e8e\u9501\u7684\u5b9e\u73b0\u51fa\u53d1\uff0c\u5bf9\u52a0\u9501\u548c\u91ca\u653e\u9501\u64cd\u4f5c\u8fdb\u884c\u66ff\u6362\uff0c\u6784\u9020\u51fa\u53ef\u6062\u590d\u4e14\u65e0\u9501\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u652f\u6301\u5d4c\u5957\u9501\u4ee5\u589e\u5f3a\u901a\u7528\u6027\u3002", "result": "\u8be5\u8f6c\u6362\u5728\u4e0d\u635f\u5bb3\u539f\u6709\u9501\u5b9e\u73b0\u6b63\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u53ef\u6062\u590d\u6027\u548c\u65e0\u9501\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f6c\u6362\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u65e0\u9501\u4e0e\u53ef\u6062\u590d\u7279\u6027\uff0c\u4e3a\u5e76\u53d1\u6570\u636e\u7ed3\u6784\u7684\u5bb9\u9519\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.09679", "categories": ["cs.SE"], "pdf": "https://arxiv.org/pdf/2512.09679", "abs": "https://arxiv.org/abs/2512.09679", "authors": ["Naizhu Jin", "Zhong Li", "Guang Yang", "Tian Zhang", "Qingkai Zeng"], "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis", "comment": null, "summary": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4fe1\u606f\u8bba\u65b9\u6cd5\u7cfb\u7edf\u7814\u7a76\u4e86\u601d\u7ef4\u94fe\uff08CoT\uff09\u5728\u795e\u7ecf\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u7ed3\u6784\u5316CoT\u5728\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\uff0c\u5e76\u63ed\u793a\u4e86CoT\u6548\u679c\u4f9d\u8d56\u4e8e\u8bed\u8a00\u7c7b\u578b\u7cfb\u7edf\u3001\u6a21\u578b\u5bb9\u91cf\u53ca\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u601d\u7ef4\u94fe\uff08CoT\uff09\u63d0\u793a\u4e3a\u4f55\u6709\u6548\u53ca\u5176\u4f5c\u7528\u673a\u5236\u5c1a\u4e0d\u660e\u786e\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u5b9e\u8bc1\u4e0e\u7406\u8bba\u5206\u6790\u3002", "method": "\u4f5c\u8005\u5728\u516d\u79cdPython\u57fa\u51c6\u3001\u6db5\u76d612\u79cd\u8bed\u8a00\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u4ee5\u53ca7B\u81f3480B\u53c2\u6570\u7684\u516d\u79cd\u6a21\u578b\u4e0a\uff0c\u8bc4\u4f30\u4e94\u79cdCoT\u8303\u5f0f\uff08\u96f6\u6837\u672c\u3001\u96f6\u6837\u672cCoT\u3001\u81ea\u89c4\u5212\u3001\u7ed3\u6784\u5316CoT\u3001\u63a8\u7406\u578bCoT\uff09\uff0c\u5e76\u4ee5\u6761\u4ef6\u4e92\u4fe1\u606f $I(Y;C|X)$ \u4f5c\u4e3a\u5206\u6790\u6846\u67b6\u3002", "result": "\u5916\u90e8\u5f15\u5bfc\u7684CoT\u59cb\u7ec8\u4f18\u4e8e\u76f4\u63a5\u751f\u6210\uff1b\u7ed3\u6784\u5316CoT\u5e73\u5747\u63d0\u5347Pass@1\u8fbe5\u201312%\uff0c\u4e14\u6bd4\u53cd\u601d\u5f0f\u63a8\u7406\u4f7f\u7528\u66f4\u5c11token\uff1bCoT\u6548\u679c\u53d7\u8bed\u8a00\u7c7b\u578b\u7cfb\u7edf\u548c\u6a21\u578b\u5bb9\u91cf\u5f71\u54cd\uff1b\u9ad8\u8d28\u91cf\u7ed3\u6784\u5316CoT\u663e\u8457\u4f18\u4e8e\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\uff0c\u800c\u6734\u7d20\u96f6\u6837\u672cCoT\u751a\u81f3\u53ef\u80fd\u635f\u5bb3\u6027\u80fd\u3002", "conclusion": "\u5e94\u6839\u636e\u6a21\u578b\u5bb9\u91cf\u3001\u8bed\u8a00\u7279\u6027\u548c\u4efb\u52a1\u590d\u6742\u5ea6\u9009\u62e9\u5408\u9002\u7684CoT\u7b56\u7565\uff0c\u7ed3\u6784\u5316\u4e14\u9ad8\u8d28\u91cf\u7684\u63a8\u7406\u8def\u5f84\u5bf9\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2512.09775", "categories": ["cs.SE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09775", "abs": "https://arxiv.org/abs/2512.09775", "authors": ["Vladimir Balditsyn", "Philippe Lalanda", "German Vega", "St\u00e9phanie Chollet"], "title": "Quantifying Uncertainty in Machine Learning-Based Pervasive Systems: Application to Human Activity Recognition", "comment": null, "summary": "The recent convergence of pervasive computing and machine learning has given rise to numerous services, impacting almost all areas of economic and social activity. However, the use of AI techniques precludes certain standard software development practices, which emphasize rigorous testing to ensure the elimination of all bugs and adherence to well-defined specifications. ML models are trained on numerous high-dimensional examples rather than being manually coded. Consequently, the boundaries of their operating range are uncertain, and they cannot guarantee absolute error-free performance. In this paper, we propose to quantify uncertainty in ML-based systems. To achieve this, we propose to adapt and jointly utilize a set of selected techniques to evaluate the relevance of model predictions at runtime. We apply and evaluate these proposals in the highly heterogeneous and evolving domain of Human Activity Recognition (HAR). The results presented demonstrate the relevance of the approach, and we discuss in detail the assistance provided to domain experts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5728\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u7cfb\u7edf\u4e2d\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u5728\u8fd0\u884c\u65f6\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u7684\u76f8\u5173\u6027\uff0c\u5e76\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u9886\u57df\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u9ad8\u7ef4\u6570\u636e\u8bad\u7ec3\u800c\u975e\u624b\u52a8\u7f16\u7801\uff0c\u5176\u8fd0\u884c\u8fb9\u754c\u4e0d\u786e\u5b9a\u4e14\u65e0\u6cd5\u4fdd\u8bc1\u5b8c\u5168\u65e0\u9519\uff0c\u56e0\u6b64\u4f20\u7edf\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u4e25\u683c\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u9002\u7528\uff0c\u4e9f\u9700\u65b0\u7684\u624b\u6bb5\u6765\u8bc4\u4f30\u6a21\u578b\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "method": "\u7ed3\u5408\u5e76\u8c03\u6574\u4e00\u7ec4\u9009\u5b9a\u7684\u6280\u672f\uff0c\u5728\u8fd0\u884c\u65f6\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u7684\u76f8\u5173\u6027\uff0c\u4ee5\u91cf\u5316\u5176\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u8fd9\u4e00\u9ad8\u5ea6\u5f02\u6784\u4e14\u52a8\u6001\u53d8\u5316\u7684\u9886\u57df\u4e2d\uff0c\u6240\u63d0\u65b9\u6cd5\u6709\u6548\u5c55\u793a\u4e86\u5176\u5728\u91cf\u5316\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u76f8\u5173\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8f85\u52a9\u9886\u57df\u4e13\u5bb6\u7406\u89e3\u548c\u5e94\u5bf9\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u7cfb\u7edf\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
