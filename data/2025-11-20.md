<div id=toc></div>

# Table of Contents

- [cs.SE](#cs.SE) [Total: 15]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [1] [Hybrid Quantum-Classical Machine Learning with PennyLane: A Comprehensive Guide for Computational Research](https://arxiv.org/abs/2511.14786)
*Sidney Shapiro*

Main category: cs.SE

TL;DR: 本文介绍了PennyLane这一Python框架，用于实现混合量子-经典机器学习，支持量子核方法、变分量子本征求解器、投资组合优化等应用，并与PyTorch、TensorFlow、JAX等经典机器学习框架无缝集成。


<details>
  <summary>Details</summary>
Motivation: 推动混合量子-经典机器学习的发展，为研究人员提供一个统一、高效的工具，以结合量子计算潜力与经典优化技术，促进量子增强数据科学的实践应用。

Method: 通过PennyLane框架构建和优化变分量子算法，利用其自动微分、量子电路构建及与经典ML库（如scikit-learn、pandas、matplotlib）的集成功能，实现混合优化工作流。

Result: 展示了PennyLane在多个领域的具体应用案例，证明其在量子机器学习、优化和量子化学中的实用性与灵活性。

Conclusion: PennyLane作为连接量子计算与经典机器学习的桥梁，是实现混合量子-经典工作流的重要方法论工具，适合成为相关Python研究的标准引用。

Abstract: Hybrid quantum-classical machine learning represents a frontier in computational research, combining the potential advantages of quantum computing with established classical optimization techniques. PennyLane provides a Python framework that seamlessly bridges quantum circuits and classical machine learning, enabling researchers to build, optimize, and deploy variational quantum algorithms. This paper introduces PennyLane as a versatile tool for quantum machine learning, optimization, and quantum chemistry applications. We demonstrate use cases including quantum kernel methods, variational quantum eigensolvers, portfolio optimization, and integration with classical ML frameworks such as PyTorch, TensorFlow, and JAX. Through concrete Python examples with widely used libraries such as scikit-learn, pandas, and matplotlib, we show how PennyLane facilitates efficient quantum circuit construction, automatic differentiation, and hybrid optimization workflows. By situating PennyLane within the broader context of quantum computing and machine learning, we highlight its role as a methodological building block for quantum-enhanced data science. Our goal is to provide researchers and practitioners with a concise reference that bridges foundational quantum computing concepts and applied machine learning practice, making PennyLane a default citation for hybrid quantum-classical workflows in Python-based research.

</details>


### [2] [Enabling Predictive Maintenance in District Heating Substations: A Labelled Dataset and Fault Detection Evaluation Framework based on Service Data](https://arxiv.org/abs/2511.14791)
*Cyriana M. A. Roelofs,Edison Guevara Bastidas,Thomas Hugo,Stefan Faulstich,Anna Cadenbach*

Main category: cs.SE

TL;DR: 本文提出了一个开源框架，用于早期检测区域供热换热站的故障，包含公开数据集、评估指标（准确性、可靠性、早期性）、基于EnergyFaultDetector的基线结果，并支持使用ARCANA进行根因分析。


<details>
  <summary>Details</summary>
Motivation: 区域供热换热站故障的早期检测对降低回水温度和提升系统效率至关重要，但该领域研究受限于缺乏公开标注的数据集。

Method: 构建了一个包含93个换热站运行时序数据的公开数据集，结合Accuracy、事件级F-score（β=0.5）和Earliness三项指标进行评估，并利用开源Python框架EnergyFaultDetector实现基线模型，同时集成ARCANA工具支持根因分析。

Result: 模型在正常行为识别上准确率达0.98，事件级F-score达0.83，能在用户报修前检测到60%的故障，平均提前3.9天。

Conclusion: 该框架通过整合开源数据集、评估指标、代码与基线，建立了一个可复现、以故障为中心且具有实际运行意义的基准，有助于推动区域供热换热站早期故障检测与诊断方法的发展。

Abstract: Early detection of faults in district heating substations is imperative to reduce return temperatures and enhance efficiency. However, progress in this domain has been hindered by the limited availability of public, labelled datasets. We present an open source framework combining a service report validated public dataset, an evaluation method based on Accuracy, Reliability, and Earliness, and baseline results implemented with EnergyFaultDetector, an open source Python framework.
  The dataset contains time series of operational data from 93 substations across two manufacturers, annotated with a list of disturbances due to faults and maintenance actions, a set of normal-event examples and detailed fault metadata. We evaluate the EnergyFaultDetector using three metrics: Accuracy for recognising normal behaviour, an eventwise F Score for reliable fault detection with few false alarms, and Earliness for early detection. The framework also supports root cause analysis using ARCANA. We demonstrate three use cases to assist operators in interpreting anomalies and identifying underlying faults. The models achieve high normal-behaviour accuracy (0.98) and eventwise F-score (beta=0.5) of 0.83, detecting 60% of the faults in the dataset before the customer reports a problem, with an average lead time of 3.9 days.
  Integrating an open dataset, metrics, open source code, and baselines establishes a reproducible, fault centric benchmark with operationally meaningful evaluation, enabling consistent comparison and development of early fault detection and diagnosis methods for district heating substations.

</details>


### [3] [irace-evo: Automatic Algorithm Configuration Extended With LLM-Based Code Evolution](https://arxiv.org/abs/2511.14794)
*Camilo Chacón Sartori,Christian Blum*

Main category: cs.SE

TL;DR: 本文提出irace-evo，一种结合自动参数调优与大语言模型驱动代码演化的框架，在低计算和经济成本下显著提升元启发式算法性能。


<details>
  <summary>Details</summary>
Motivation: 传统自动算法配置工具（如irace）仅优化参数而无法修改算法代码，限制了性能提升空间；作者旨在通过引入代码演化能力，联合探索参数与代码空间以改进启发式算法。

Method: 在irace基础上扩展，集成大语言模型进行代码演化，支持多语言（如C++、Python），采用渐进式上下文管理和“Always-From-Original”原则控制演化过程，并在VSBPP问题的CMSA元启发式算法上进行实验验证。

Result: irace-evo发现了优于当前最优CMSA实现的新算法变体，且使用轻量级模型（如Claude Haiku 3.5）总成本低于2欧元，展现出高效性和经济性。

Conclusion: 将自动配置与大语言模型驱动的代码演化相结合，为启发式设计和元启发式优化提供了一种强大且低成本的有效途径。

Abstract: Automatic algorithm configuration tools such as irace efficiently tune parameter values but leave algorithmic code unchanged. This paper introduces a first version of irace-evo, an extension of irace that integrates code evolution through large language models (LLMs) to jointly explore parameter and code spaces. The proposed framework enables multi-language support (e.g., C++, Python), reduces token consumption via progressive context management, and employs the Always-From-Original principle to ensure robust and controlled code evolution. We evaluate irace-evo on the Construct, Merge, Solve & Adapt (CMSA) metaheuristic for the Variable-Sized Bin Packing Problem (VSBPP). Experimental results show that irace-evo can discover new algorithm variants that outperform the state-of-the-art CMSA implementation while maintaining low computational and monetary costs. Notably, irace-evo generates competitive algorithmic improvements using lightweight models (e.g., Claude Haiku 3.5) with a total usage cost under 2 euros. These results demonstrate that coupling automatic configuration with LLM-driven code evolution provides a powerful, cost-efficient avenue for advancing heuristic design and metaheuristic optimization.

</details>


### [4] [Evaluating Generative AI for CS1 Code Grading: Direct vs Reverse Methods](https://arxiv.org/abs/2511.14798)
*Ahmad Memon,Abdallah Mohamed*

Main category: cs.SE

TL;DR: 本文比较了两种基于大语言模型的编程作业自动评分方法：直接应用评分标准的“Direct”方法和通过先修复代码再反推分数的新型“Reverse”方法，发现后者在细粒度评估方面更具优势，但两者均需精心设计提示词，并探讨了人机混合评分系统的未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统编程作业人工评分耗时且易不一致，而现有基于单元测试的自动评分多为二元通过/失败模式，缺乏部分得分机制；大语言模型的发展为实现更客观、可扩展的自动评分提供了新可能。

Method: 提出并对比两种AI评分方法——Direct（AI直接依据评分标准评分）与Reverse（AI先修复学生代码，再根据修复内容反推分数）；在原始评分尺度及十倍扩展尺度下，将AI评分结果与人类助教评分进行对比，并利用Gemini Flash 2.0生成的合成代码测试AI评分器在不同错误类型和难度下的表现。

Result: 初步结果表明，Direct方法更快捷直接，而Reverse方法通过关注修复工作量能提供更细致的评分；两种方法均对提示工程敏感，尤其在逻辑错误处理和部分得分分配方面；合成数据有助于提升评估覆盖范围。

Conclusion: Reverse方法在细粒度评分上表现更优，但两种AI评分方法都需要精细的提示设计；未来应探索结合人类判断与AI效率的混合评分系统，以提升计算机课程评分的一致性、效率与公平性。

Abstract: Manual grading of programming assignments in introductory computer science courses can be time-consuming and prone to inconsistencies. While unit testing is commonly used for automatic evaluation, it typically follows a binary pass/fail model and does not give partial marks. Recent advances in large language models (LLMs) offer the potential for automated, scalable, and more objective grading.
  This paper compares two AI-based grading techniques: \textit{Direct}, where the AI model applies a rubric directly to student code, and \textit{Reverse} (a newly proposed approach), where the AI first fixes errors, then deduces a grade based on the nature and number of fixes. Each method was evaluated on both the instructor's original grading scale and a tenfold expanded scale to assess the impact of range on AI grading accuracy. To assess their effectiveness, AI-assigned scores were evaluated against human tutor evaluations on a range of coding problems and error types.
  Initial findings suggest that while the Direct approach is faster and straightforward, the Reverse technique often provides a more fine-grained assessment by focusing on correction effort. Both methods require careful prompt engineering, particularly for allocating partial credit and handling logic errors. To further test consistency, we also used synthetic student code generated using Gemini Flash 2.0, which allowed us to evaluate AI graders on a wider range of controlled error types and difficulty levels. We discuss the strengths and limitations of each approach, practical considerations for prompt design, and future directions for hybrid human-AI grading systems that aim to improve consistency, efficiency, and fairness in CS courses.

</details>


### [5] [Scalable and Efficient Large-Scale Log Analysis with LLMs: An IT Software Support Case Study](https://arxiv.org/abs/2511.14803)
*Pranjal Gupta,Karan Bhukar,Harshit Kumar,Seema Nagar,Prateeti Mohapatra,Debanjana Kar*

Main category: cs.SE

TL;DR: 本文提出了一种基于大语言模型（LLM）的日志分析工具，用于自动化处理和诊断IT系统日志，并通过在CPU上高效运行LLM实现大规模日志的快速分析。该工具自2024年3月上线以来，已覆盖70个软件产品，处理超2000个工单，每月节省300多人工时及约15,444美元人力成本。


<details>
  <summary>Details</summary>
Motivation: IT环境中日志数量庞大，人工检查不现实，亟需自动化的日志分析手段以提升IT软件支持效率。

Method: 利用大语言模型（LLM）进行日志数据处理与问题诊断，并提出一种在CPU上高效运行LLM的新方法，以在保证输出质量的同时快速处理海量日志。

Result: 工具已在生产环境中部署，覆盖70个软件产品，处理2000多个工单，每月节省300+人工时和约15,444美元的人力成本。

Conclusion: 基于LLM的日志分析工具能显著提升日志处理效率和问题诊断速度，在实际生产中展现出可观的成本节约效果，验证了其在IT支持中的实用价值。

Abstract: IT environments typically have logging mechanisms to monitor system health and detect issues. However, the huge volume of generated logs makes manual inspection impractical, highlighting the importance of automated log analysis in IT Software Support. In this paper, we propose a log analytics tool that leverages Large Language Models (LLMs) for log data processing and issue diagnosis, enabling the generation of automated insights and summaries. We further present a novel approach for efficiently running LLMs on CPUs to process massive log volumes in minimal time without compromising output quality. We share the insights and lessons learned from deployment of the tool - in production since March 2024 - scaled across 70 software products, processing over 2000 tickets for issue diagnosis, achieving a time savings of 300+ man hours and an estimated $15,444 per month in manpower costs compared to the traditional log analysis practices.

</details>


### [6] [Automatic Pipeline Provisioning](https://arxiv.org/abs/2511.14825)
*Alexandre-Xavier Labonté-Lamoureux,Simon Boyer*

Main category: cs.SE

TL;DR: 本文探讨了自动流水线配置的优势及其在CI（持续集成）中的应用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索自动流水线配置的好处，并明确其适用方式，尤其关注CI流水线。

Method: 通过定义自动流水线配置为快速部署软件工程项目流水线的过程，对CI流水线进行分析。

Result: 尚未提供具体结果，但预期该方法对CD（持续交付）流水线同样适用。

Conclusion: 自动流水线配置有望提升CI/CD流程的效率和部署速度。

Abstract: The goal of this paper is to explore the benefits of automatic pipeline provisioning and identify how it can be applied. Automatic pipeline provisioning can be defined as a process of quickly deploying a pipeline for a software engineering project. This research will focus on CI pipelines, although the outcomes of this approach on CD pipelines will likely be similar.

</details>


### [7] [MermaidSeqBench: An Evaluation Benchmark for LLM-to-Mermaid Sequence Diagram Generation](https://arxiv.org/abs/2511.14967)
*Basel Shbita,Farhan Ahmed,Chad DeLuca*

Main category: cs.SE

TL;DR: 本文提出了MermaidSeqBench，一个用于评估大语言模型（LLM）从自然语言生成Mermaid序列图能力的新基准，包含132个人工验证与LLM扩展的样本，并采用细粒度指标和LLM-as-a-judge方法进行评估。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统性评估LLM在从自然语言生成结构化图表（如Mermaid序列图）任务中正确性的基准，限制了该领域的研究进展。

Method: 构建了一个包含132个样本的基准数据集MermaidSeqBench，通过人工标注、上下文提示LLM生成和基于规则的变体扩增相结合的方式构建；并使用LLM-as-a-judge模型对语法正确性、激活处理、错误处理和实用性等细粒度指标进行评估。

Result: 对多个前沿LLM的初步评估揭示了不同模型和评估模式之间存在显著的能力差距，验证了该基准的有效性和灵活性。

Conclusion: MermaidSeqBench为结构化图表生成研究提供了坚实基础，并推动更严格、细粒度的评估方法的发展。

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in generating structured diagrams from natural language descriptions. In particular, they have shown great promise in generating sequence diagrams for software engineering, typically represented in a text-based syntax such as Mermaid. However, systematic evaluations in this space remain underdeveloped as there is a lack of existing benchmarks to assess the LLM's correctness in this task. To address this shortcoming, we introduce MermaidSeqBench, a human-verified and LLM-synthetically-extended benchmark for assessing an LLM's capabilities in generating Mermaid sequence diagrams from textual prompts. The benchmark consists of a core set of 132 samples, starting from a small set of manually crafted and verified flows. These were expanded via a hybrid methodology combining human annotation, in-context LLM prompting, and rule-based variation generation. Our benchmark uses an LLM-as-a-judge model to assess Mermaid sequence diagram generation across fine-grained metrics, including syntax correctness, activation handling, error handling, and practical usability. We perform initial evaluations on numerous state-of-the-art LLMs and utilize multiple LLM judge models to demonstrate the effectiveness and flexibility of our benchmark. Our results reveal significant capability gaps across models and evaluation modes. Our proposed benchmark provides a foundation for advancing research in structured diagram generation and for developing more rigorous, fine-grained evaluation methodologies.

</details>


### [8] [FRIENDS GUI: A graphical user interface for data collection and visualization of vaping behavior from a passive vaping monitor](https://arxiv.org/abs/2511.15007)
*Shehan I Pranto,Brett Fassler,Md Rafi Islam,Ashley Schenkel,Larry W Hawk,Edward Sazonov*

Main category: cs.SE

TL;DR: 本文介绍了FRIENDS GUI，一个基于Python的开源工具，用于提取、解码和可视化电子尼古丁传送系统（ENDS）的24小时抽吸数据，提升了FRIENDS设备所收集数据的可访问性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 为提升对电子尼古丁传送系统（ENDS）使用行为（如抽吸持续时间、抽吸间隔和每次使用抽吸次数）的理解，以评估其使用模式、有害物暴露水平并支持监管决策。

Method: 开发并验证了一个名为FRIENDS GUI的开源Python工具，用于处理FRIENDS设备记录的抽吸与触摸事件数据，实现数据的提取、解码和可视化。

Result: 通过24小时实验数据验证，该GUI能准确转换时间戳、可靠解码事件，并有效可视化用户行为；软件已在GitHub上公开发布。

Conclusion: FRIENDS GUI显著提高了ENDS抽吸拓扑数据的可用性和解读效率，为研究人员和监管机构提供了有力工具。

Abstract: Understanding puffing topography (PT), which includes puff duration, intra puff interval, and puff count per session, is critical for evaluating Electronic Nicotine Delivery Systems (ENDS) use, toxicant exposure, and informing regulatory decisions. We developed FRIENDS (Flexible Robust Instrumentation of ENDS), an open-source device that records puffing and touch events of ENDS by attaching to it. This paper introduces the FRIENDS GUI that improves accessibility and interpretability of data collected by FRIENDS. The GUI is a Python-based open-source tool that extracts, decodes, and visualizes 24-hour puffing data from the FRIENDS device. Validation using 24-hour experimental data confirmed accurate timestamp conversion, reliable event decoding, and effective behavioral visualization. The software is freely available on GitHub for public use.

</details>


### [9] [Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework](https://arxiv.org/abs/2511.15168)
*Nguyen-Khang Le,Nguyen Hiep,Minh Nguyen,Son Luu,Trung Vo,Quan Bui,Nomura Shoshin,Le-Minh Nguyen*

Main category: cs.SE

TL;DR: 本文提出了一种训练大语言模型（LLM）生成高质量 Selenium 表单交互测试用例的新方法，构建了合成与人工标注数据集，并在语法正确性、脚本可执行性和字段覆盖率等指标上显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏用于评估大语言模型在 Web 表单交互测试任务中表现的公开基准和数据集，且该任务在 LLM 研究中尚未得到充分探索。

Method: 构建包含合成与人工标注的多样化表单数据集，用于训练和评估 LLM；定义语法正确性、脚本可执行性及输入字段覆盖率等评估指标；训练 LLM 生成符合要求的 Selenium 测试脚本。

Result: 所提方法在所有评估指标上均显著优于包括 GPT-4o 在内的强基线模型。

Conclusion: 该研究为基于 LLM 的 Web 自动化测试奠定了基础，并提供了可用于后续研究的数据资源和评估框架。

Abstract: Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.

</details>


### [10] [From Code Smells to Best Practices: Tackling Resource Leaks in PyTorch, TensorFlow, and Keras](https://arxiv.org/abs/2511.15229)
*Bashar Abdallah,Martyna E. Wojciechowska,Gustavo Santos,Edmand Yu,Maxime Lamothe,Alain Abran,Mohammad Hamdaqa*

Main category: cs.SE

TL;DR: 本研究首次系统识别了PyTorch、TensorFlow和Keras中导致资源泄漏的代码坏味道，提出了50种推荐编码实践，以提升机器学习应用的资源效率与可持续性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习研究多关注模型性能指标，忽视了长期可持续性和资源效率；高效资源管理对稳健部署同样关键，因此需系统识别并解决ML应用中的资源泄漏问题。

Method: 通过实证分析开发者讨论和真实代码片段，识别PyTorch、TensorFlow和Keras中的资源泄漏相关代码坏味道，并基于根本原因和框架特性进行分类；为每种坏味道提炼最佳实践，并通过三阶段验证流程确保结果有效性。

Result: 识别出30种PyTorch相关和16种TensorFlow/Keras相关的资源泄漏代码坏味道，归纳出50项推荐编码模式，并首次提供跨主流ML框架的资源泄漏成因结构化视图。

Conclusion: 该研究填补了ML应用资源效率研究的空白，为开发者构建更高效、可持续的机器学习系统提供了可操作的最佳实践和理论支持。

Abstract: Much of the existing ML research focuses on model performance metrics, leaving limited attention to the long-term sustainability and resource efficiency of ML applications. While high performance is essential, ensuring efficient resource management is equally critical for robust deployment. This study addresses this gap by systematically identifying code smells that lead to resource leaks in ML applications. We conducted an empirical investigation of developer discussions and real-world code snippets from PyTorch, TensorFlow, and Keras. The analysis identified 30 PyTorch-related smells and 16 TensorFlow/Keras smells linked to resource leaks. These smells were categorized in two ways: (1) based on their root causes, and (2) as general ML smells with framework-specific characteristics. For each smell, we derived at least one best practice, resulting in 50 recommended coding patterns aimed at reducing resource leakage and improving efficiency. To ensure the validity of our findings, we employed a three-phase validation process involving independent analysis by three authors followed by consensus discussions. This is the first comprehensive study to examine resource-leak-inducing code smells across major ML frameworks and to present actionable best practices for mitigating them. The contributions support developers in building more efficient and sustainable ML applications and offer a structured view of the underlying causes of resource leaks.

</details>


### [11] [M, Toolchain and Language for Reusable Model Compilation](https://arxiv.org/abs/2511.15257)
*Hiep Hong Trinh,Federico Ciccozzi,Abu Naser Masud,Marjan Sirjani,Mikael Sjödin*

Main category: cs.SE

TL;DR: 本文提出了一种名为 M 的建模语言和工具链，支持对复杂、并发、时间敏感系统进行建模，并实现面向多种目标（如仿真、部署、形式化验证）的语义一致的模型编译。


<details>
  <summary>Details</summary>
Motivation: 现有建模语言通常只针对单一用途（如仿真或实现），缺乏对多目标编译的支持；若未在语言设计初期考虑多目标编译，后期实现将极为困难。因此，亟需一种从源头支持多目标生成的建模方法。

Method: 设计并实现了一种基于演员模型（actor model）并扩展离散事件调度语义的文本化、语法驱动建模语言 M；该语言支持系统实体建模、基于消息的交互以及时间和状态触发的反应，并以此为基础构建多目标编译工具链。

Result: M 能够从统一的高层模型系统地生成多种目标产物（如用于仿真、部署或验证的代码），并在生成过程中保持与原始模型的语义一致性；此外，M 还可作为中间语言，供其他建模语言接入其编译框架。

Conclusion: M 语言及其工具链为复杂并发系统的模型驱动工程提供了一个灵活、语义保真的多目标编译解决方案，有助于提升开发效率与安全性。

Abstract: Complex software-driven systems often interleave distributed, concurrent computation processes with physical interactions with the environment. Developing these systems more efficiently and safely can be achieved by employing actionable, software-based models. From a high-level system model, engineers often need to derive multiple specialized models for different purposes, including simulation, deployment, and formal verification. Each of these target models usually rely on its own formalism, specification language, and execution platform. Traditionally, a compiler analyzes a program written in a programming language and generates executable code. In contrast, a model compiler processes a source model written in a modeling language and should ideally support the generation of multiple heterogeneous targets. However, most existing modeling languages are designed with a narrow focus, typically targeting only simulation or implementation. Multi-target compilation, when not considered during the language's early design, becomes significantly harder to achieve. In this paper, we introduce our initiative: a toolchain and modeling language called M, designed to support system modeling and multi-target compilation for model-driven engineering of complex, concurrent, and time-aware systems. M is a textual, grammar-driven language based on the actor model and extended with discrete-event scheduling semantics. It provides constructs for modeling system entities, message-based interactions, and time- or state-triggered reactions. From such models, M enables the systematic generation of diverse target artifacts while preserving semantic conformance to the original model. Moreover, M can serve as a middle language to which other modeling languages may anchor, thereby allowing them to benefit from its compilation framework.

</details>


### [12] [A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development](https://arxiv.org/abs/2511.15293)
*Jia Li,Zhi Jin,Kechi Zhang,Huangzhao Zhang,Jiaru Qian,Tiankuo Zhao*

Main category: cs.SE

TL;DR: 本文提出了一种端到端自动软件开发范式 AutoSW，通过分析-规划-实现-交付的闭环流程，使 AI 系统作为主要参与者将自然语言描述的人类意图转化为可执行软件，并通过原型验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 传统软件开发自动化仍需大量人工参与，而新兴的“vibe coding”虽强调 AI 自动生成与迭代代码，但尚未形成系统化范式。作者旨在探索一种融合两者优势、实现全周期自动化的软件开发新路径。

Method: 提出 AutoSW 范式，构建一个分析-规划-实现-交付的迭代闭环，让 AI 作为核心角色将自然语言需求转化为可运行软件，并开发轻量级原型进行验证。

Result: 原型在多个代表性案例中成功生成可执行软件，验证了该范式在端到端自动化软件开发中的可行性。

Conclusion: AutoSW 为实现真正意义上的端到端自动化软件开发提供了可行方向，预示 AI 将在软件开发生命周期中扮演第一类参与者角色。

Abstract: Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.

</details>


### [13] [From Machine Learning Documentation to Requirements: Bridging Processes with Requirements Languages](https://arxiv.org/abs/2511.15340)
*Yi Peng,Hans-Martin Heyn,Jennifer Horkoff*

Main category: cs.SE

TL;DR: 该研究探讨了如何从ModelCards和DataSheets等机器学习文档中提取与需求工程（RE）相关的信息，并评估了三种RE表示方法（EARS、Rupp模板和Volere）在将这些信息结构化为需求方面的有效性，结果表明这些文档包含大量RE相关信息，且可有效转化为结构化需求。


<details>
  <summary>Details</summary>
Motivation: 在面向机器学习系统的软件工程过程中，传统需求工程方法难以应对模型与数据需求的规范问题，而ModelCards和DataSheets等ML文档作为潜在的需求信息来源尚未被充分探索。

Method: 首先分析20份公开的ModelCards和DataSheets中RE相关信息的数量与性质；随后使用EARS、Rupp模板和Volere三种需求表示方法对这些信息进行结构化，并评估其效果。

Result: ML文档中包含大量潜在的RE相关信息，且通过现有RE表示方法可有效将其转化为结构化需求。

Conclusion: ML文档（如ModelCards和DataSheets）可作为需求工程的重要信息源，有助于将ML特定知识整合进面向ML系统的软件工程流程中。

Abstract: In software engineering processes for machine learning (ML)-enabled systems, integrating and verifying ML components is a major challenge. A prerequisite is the specification of ML component requirements, including models and data, an area where traditional requirements engineering (RE) processes face new obstacles. An underexplored source of RE-relevant information in this context is ML documentation such as ModelCards and DataSheets. However, it is uncertain to what extent RE-relevant information can be extracted from these documents. This study first investigates the amount and nature of RE-relevant information in 20 publicly available ModelCards and DataSheets. We show that these documents contain a significant amount of potentially RE-relevant information. Next, we evaluate how effectively three established RE representations (EARS, Rupp's template, and Volere) can structure this knowledge into requirements. Our results demonstrate that there is a pathway to transform ML-specific knowledge into structured requirements, incorporating ML documentation in software engineering processes for ML systems.

</details>


### [14] [MutDafny: A Mutation-Based Approach to Assess Dafny Specifications](https://arxiv.org/abs/2511.15403)
*Isabel Amaral,Alexandra Mendes,José Campos*

Main category: cs.SE

TL;DR: 本文提出MutDafny工具，利用变异测试检测Dafny形式化规约中的弱点，通过引入32种变异算子，在794个真实程序中发现多个弱规约。


<details>
  <summary>Details</summary>
Motivation: Dafny等验证感知语言中的形式化规约易出错，可能导致“已验证”程序行为偏离预期，因此需提高规约可靠性。

Method: 开发MutDafny工具，采用变异测试方法：对代码引入变异，若带变异的程序仍能通过形式化验证，则表明规约可能存在弱点；结合现有变异算子与从GitHub Dafny项目bug修复提交中提炼的新算子，共实现32种适用于Dafny的变异算子。

Result: 在794个真实Dafny程序上评估MutDafny，手动分析未被检测到的变异体，发现5个弱规约（平均每241行代码出现1个），表明该工具能有效识别需加强的规约。

Conclusion: MutDafny能有效提升Dafny形式化规约的质量，通过自动检测潜在弱点，帮助开发者强化规约，从而增强验证结果的可信度。

Abstract: This paper explores the use of mutation testing to reveal weaknesses in formal specifications written in Dafny. In verification-aware programming languages, such as Dafny, despite their critical role, specifications are as prone to errors as implementations. Flaws in specs can result in formally verified programs that deviate from the intended behavior.
  We present MutDafny, a tool that increases the reliability of Dafny specifications by automatically signaling potential weaknesses. Using a mutation testing approach, we introduce faults (mutations) into the code and rely on formal specifications for detecting them. If a program with a mutant verifies, this may indicate a weakness in the specification. We extensively analyze mutation operators from popular tools, identifying the ones applicable to Dafny. In addition, we synthesize new operators tailored for Dafny from bugfix commits in publicly available Dafny projects on GitHub. Drawing from both, we equipped our tool with a total of 32 mutation operators. We evaluate MutDafny's effectiveness and efficiency in a dataset of 794 real-world Dafny programs and we manually analyze a subset of the resulting undetected mutants, identifying five weak real-world specifications (on average, one at every 241 lines of code) that would benefit from strengthening.

</details>


### [15] [EPSO: A Caching-Based Efficient Superoptimizer for BPF Bytecode](https://arxiv.org/abs/2511.15589)
*Qian Zhu,Yuxuan Liu,Ziyuan Zhu,Shangqing Liu,Lei Bu*

Main category: cs.SE

TL;DR: 本文提出EPSO，一种基于缓存的eBPF超级优化器，通过离线发现重写规则并在运行时复用，显著减小程序规模并提升性能。


<details>
  <summary>Details</summary>
Motivation: eBPF程序受内核验证器严格限制，现有编译器（如Clang）优化能力有限，且许多语义保持的变换被验证器拒绝，手工优化规则设计困难且效果有限；而传统超级优化方法计算开销大、难以扩展。

Method: EPSO采用缓存机制，先通过离线超级优化发现有效的重写规则，再在运行时复用这些规则对eBPF程序进行高效优化，兼顾优化质量和运行时开销。

Result: 在Linux内核及多个eBPF项目（如Cilium、Katran等）的基准测试中，EPSO发现了795条重写规则，相比Clang平均减少24.37%的程序大小（最高达68.87%），优于现有优化器K2和Merlin，并平均降低6.60%的运行时间。

Conclusion: EPSO有效解决了eBPF程序优化中的验证器兼容性与性能问题，在程序规模和运行效率上均取得显著提升，具有实际应用价值。

Abstract: Extended Berkeley Packet Filter (eBPF) allows developers to extend Linux kernel functionality without modifying its source code. To ensure system safety, an in-kernel safety checker, the verifier, enforces strict safety constraints (for example, a limited program size) on eBPF programs loaded into the kernel. These constraints, combined with eBPF's performance-critical use cases, make effective optimization essential. However, existing compilers (such as Clang) offer limited optimization support, and many semantics-preserving transformations are rejected by the verifier, which makes handcrafted optimization rule design both challenging and limited in effectiveness. Superoptimization overcomes the limitations of rule-based methods by automatically discovering optimal transformations, but its high computational cost limits scalability. To address this, we propose EPSO, a caching-based superoptimizer that discovers rewrite rules via offline superoptimization and reuses them to achieve high-quality optimizations with minimal runtime overhead. We evaluate EPSO on benchmarks from the Linux kernel and several eBPF-based projects, including Cilium, Katran, hXDP, Sysdig, Tetragon, and Tracee. EPSO discovers 795 rewrite rules and achieves up to 68.87 percent (average 24.37 percent) reduction in program size compared to Clang's output, outperforming the state-of-the-art BPF optimizer K2 on all benchmarks and Merlin on 92.68 percent of them. Additionally, EPSO reduces program runtime by an average of 6.60 percent, improving throughput and lowering latency in network applications.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [16] [A High-Fidelity Neurosurgical Training Platform for Bimanual Procedures: A Feasibility Study](https://arxiv.org/abs/2511.14879)
*Houssem-Eddine Gueziri,Abicumaran Uthamacumaran,Widad Safih,Abdulrahman Almansouri,Nour Abou Hamdan,Jose A. Correa,Étienne Léger,D. Louis Collins,Rolando F. Del Maestro*

Main category: cs.CE

TL;DR: 该研究开发了一个基于离体牛脑模型和多摄像头追踪系统的神经外科模拟平台，用于训练和客观评估双器械皮层切除术中的双手协调能力，并成功提取了可区分不同经验水平学员的运动指标。


<details>
  <summary>Details</summary>
Motivation: 神经外科手术中双手精细操作能力至关重要，但难以训练且缺乏客观评估手段，因此需要一种能支持训练并量化评估双手操作表现的模拟平台。

Method: 构建了一个结合离体牛脑模型与多摄像头仪器追踪系统的神经外科模拟平台，实时记录双手手术器械轨迹及同步视频，从中提取运动、时间及双手协调相关指标；对47名涵盖四个培训阶段（医学生、低年资住院医师、高年资住院医师和神经外科医生）的参与者进行案例研究。

Result: 追踪系统在81%的器械使用时段内成功捕获运动数据；多个指标（如器械使用时长、器械尖端间距、同时使用时间等）能显著区分不同专业水平的参与者。

Conclusion: 该平台在复杂双手任务中实现器械追踪具有可行性，所开发的指标为客观评估手术表现奠定了基础，展示了运动分析在提升神经外科培训与评估中的潜力。

Abstract: Background. Bimanual psychomotor proficiency is fundamental to neurosurgical procedures, yet it remains difficult for trainees to acquire and for educators to objectively evaluate performance. In this study, we investigate the feasibility of a neurosurgical simulation platform that integrates an anatomically realistic brain model with surgical instrument tracking to support training and objective assessment of bimanual tasks in the context of subpial corticectomy. Methods. We developed and evaluated a neurosurgical simulation platform based on an ex-vivo calf brain model and a multi-camera tracking system capable of simultaneously capturing the motion of surgical instruments in both hands, including collection of real-time instrument trajectories and synchronized video recordings. These enabled extraction of motion-based, time-based, and bimanual coordination metrics. We conducted a case series involving 47 participants across four training levels: medical students, junior residents, senior residents, and neurosurgeons. Results. The tracking system successfully captured instrument motion during 81% of the periods when instruments were actively used throughout the simulation procedure. Several extracted metrics were able to significantly differentiate between levels of surgical expertise. In particular, instrument usage duration and custom-defined bimanual coordination metrics such as instrument tip separation distance and simultaneous usage time, show potential as features to identify participant expertise levels with different instruments. Conclusions. We demonstrated the feasibility of tracking surgical instruments during complex bimanual tasks in an ex-vivo brain simulation platform. The metrics developed provide a foundation for objective performance assessment and highlight the potential of motion analysis to improve neurosurgical training and evaluation.

</details>


### [17] [The Walls Have Ears: Unveiling Cross-Chain Sandwich Attacks in DeFi](https://arxiv.org/abs/2511.15245)
*Chuanlei Li,Zhicheng Sun,Jing Xin Yuu,Xuechao Wang*

Main category: cs.CE

TL;DR: 本文揭示了一种针对基于流动性池的跨链桥协议的新型跨链三明治攻击，攻击者利用源链事件提前获知目标链交易细节，从而在MEV机器人之前执行前置和后置交易，现有防御机制对此无效；通过对Symbiosis协议两个月数据的实证分析，发现此类攻击已获利超527万美元，占总跨链交易量的1.28%。


<details>
  <summary>Details</summary>
Motivation: 现有跨链桥协议在实现资产转移和应用互操作性的同时，其消息透明性可能泄露敏感交易信息，为攻击者提供可利用的信息优势，而当前针对三明治攻击的防御措施无法应对这种新型跨链攻击模式。

Method: 作者通过分析跨链桥协议中源链事件与目标链交易之间的时序关系，构建启发式检测模型，并利用Symbiosis协议在2025年8月10日至10月10日期间的两个月跨链交易数据进行实证研究，识别并量化此类跨链三明治攻击的规模与收益。

Result: 研究识别出大量跨链三明治攻击，累计获利超过527万美元，占所分析时间段内总跨链交易量的1.28%，证实了该攻击方式的有效性和现实危害性。

Conclusion: 跨链桥协议中的信息泄露漏洞可被用于实施高效的三明治攻击，现有MEV防御机制难以应对；需重新设计跨链消息传递机制或引入隐私保护措施以缓解此类风险。

Abstract: Cross-chain interoperability is a core component of modern blockchain infrastructure, enabling seamless asset transfers and composable applications across multiple blockchain ecosystems. However, the transparency of cross-chain messages can inadvertently expose sensitive transaction information, creating opportunities for adversaries to exploit value through manipulation or front-running strategies.
  In this work, we investigate cross-chain sandwich attacks targeting liquidity pool-based cross-chain bridge protocols. We uncover a critical vulnerability where attackers can exploit events emitted on the source chain to learn transaction details on the destination chain before they appear in the destination chain mempool. This information advantage allows attackers to strategically place front-running and back-running transactions, ensuring that their front-running transactions always precede those of existing MEV bots monitoring the mempool of the destination chain. Moreover, current sandwich-attack defenses are ineffective against this new cross-chain variant. To quantify this threat, we conduct an empirical study using two months (August 10 to October 10, 2025) of cross-chain transaction data from the Symbiosis protocol and a tailored heuristic detection model. Our analysis identifies attacks that collectively garnered over \(5.27\) million USD in profit, equivalent to 1.28\% of the total bridged volume.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [18] [A Graph-Based, Distributed Memory, Modeling Abstraction for Optimization](https://arxiv.org/abs/2511.14966)
*David L. Cole,Jordan Jalving,Jonah Langlieb,Jesse D. Jenkins*

Main category: cs.DC

TL;DR: 本文提出了 RemoteOptiGraph 抽象模型，用于在分布式内存环境中构建和求解优化问题，通过 InterWorkerEdges 管理跨工作节点的耦合约束，并在 Plasmo.jl 中实现，结合 Benders 分解在大规模容量扩展问题上实现了 7.5 倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有分布式优化建模方法多为定制化，缺乏统一、灵活的抽象；作者旨在提供一种通用建模框架，以支持在分布式内存系统中高效表达和求解结构化优化问题。

Method: 提出 RemoteOptiGraph 抽象，扩展 Plasmo.jl 中的 OptiGraph 模型，引入 InterWorkerEdges 来处理跨工作节点的链接约束，从而支持分布式内存环境下的建模与算法开发（如 Benders 或拉格朗日分解）。

Result: 在包含超过 1200 万个变量和约束的美国西部混合整数容量扩展模型上，使用 RemoteOptiGraph 与 Benders 分解相比无分解方法提速 7.5 倍。

Conclusion: RemoteOptiGraph 提供了一种统一且高效的分布式优化建模抽象，能够显著提升大规模结构化问题的求解效率，并为通用分布式元算法的开发奠定基础。

Abstract: We present a general, flexible modeling abstraction for building and working with distributed optimization problems called a RemoteOptiGraph. This abstraction extends the OptiGraph model in Plasmo.jl, where optimization problems are represented as hypergraphs with nodes that define modular subproblems (variables, constraints, and objectives) and edges that encode algebraic linking constraints between nodes. The RemoteOptiGraph allows OptiGraphs to be utilized in distributed memory environments through InterWorkerEdges, which manage linking constraints that span workers. This abstraction offers a unified approach for modeling optimization problems on distributed memory systems (avoiding bespoke modeling approaches), and provides a basis for developing general-purpose meta-algorithms that can exploit distributed memory structure such as Benders or Lagrangian decompositions. We implement this abstraction in the open-source package, Plasmo.jl and we illustrate how it can be used by solving a mixed integer capacity expansion model for the western United States containing over 12 million variables and constraints. The RemoteOptiGraph abstraction together with Benders decomposition performs 7.5 times faster than solving the same problem without decomposition.

</details>


### [19] [BlueBottle: Fast and Robust Blockchains through Subsystem Specialization](https://arxiv.org/abs/2511.15361)
*Preston Vander Vos,Alberto Sonnino,Giorgos Tsimos,Philipp Jovanovic,Lefteris Kokoris-Kogias*

Main category: cs.DC

TL;DR: BlueBottle 是一种双层区块链共识架构，在保持强安全性和活跃性的同时，实现了高吞吐量和亚秒级确认延迟。


<details>
  <summary>Details</summary>
Motivation: 解决区块链共识中的三难困境（安全性、延迟、去中心化），在不显著牺牲去中心化或安全性的情况下提升性能。

Method: 提出 BlueBottle 架构：核心层 BB-Core 采用 n=5f+1 协议，以适度降低容错能力换取更低的确认延迟；守护层 BB-Guard 提供去中心化时间戳、主动检测核心层异常行为，并在发生安全或活跃性故障时进行恢复。

Result: 实验表明，BB-Core 相较 Mysticeti 降低延迟 20–25%，整体系统在温和同步假设下实现高吞吐与亚秒级最终性。

Conclusion: BlueBottle 通过双层设计有效平衡了区块链共识的三难问题，在保障强安全与活跃性的同时显著提升了性能。

Abstract: Blockchain consensus faces a trilemma of security, latency, and decentralization. High-throughput systems often require a reduction in decentralization or robustness against strong adversaries, while highly decentralized and secure systems tend to have lower performance. We present BlueBottle, a two-layer consensus architecture. The core layer, BB-Core, is an n=5f+1 protocol that trades some fault tolerance for a much lower finality latency with a medium-sized core validator set. Our experiments show that BB-Core reduces latency by 20-25% in comparison to Mysticeti. The guard layer, BB-Guard, provides decentralized timestamping, proactive misbehavior detection in BB-Core, and a synchronous recovery path. When it observes equivocations or liveness failures in the core -- while tolerating up to f<3n/5 faulty nodes in the primary layer -- guard validators disseminate evidence, agree on misbehaving parties for exclusion or slashing, and either restart the core protocol (for liveness violations) or select a canonical fork (for safety violations). Together, these layers enable optimistic sub-second finality at high throughput while maintaining strong safety and liveness under a mild synchrony assumption.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [20] [CoroAMU: Unleashing Memory-Driven Coroutines through Latency-Aware Decoupled Operations](https://arxiv.org/abs/2511.14990)
*Zhuolun Jiang,Songyue Wang,Xiaokun Pei,Tianyue Lu,Mingyu Chen*

Main category: cs.AR

TL;DR: 本文提出了CoroAMU，一种软硬件协同设计的内存中心化协程系统，通过编译器优化与硬件支持（如解耦内存操作和新型内存引导分支预测），在FPGA平台上显著提升了协程在高延迟分离式内存系统中的性能。


<details>
  <summary>Details</summary>
Motivation: 现代数据密集型应用在分离式内存系统中面临严重的内存延迟问题；尽管协程可有效隐藏延迟，但现有方法难以兼顾延迟隐藏效率与运行时开销。

Method: CoroAMU结合编译器优化（减少上下文、合并请求）与硬件增强（异步内存单元、协程专用内存操作、内存引导分支预测），并在LLVM和开源XiangShan RISC-V处理器上实现。

Result: 在Intel服务器上，CoroAMU编译器比现有协程方法快1.51倍；在FPGA模拟的分离式内存系统中，结合优化硬件后，在200ns和800ns延迟下分别获得3.39倍和4.87倍的平均性能提升。

Conclusion: CoroAMU通过软硬件协同设计，有效解决了协程在高延迟内存环境下的效率与开销平衡问题，显著提升了系统性能。

Abstract: Modern data-intensive applications face memory latency challenges exacerbated by disaggregated memory systems. Recent work shows that coroutines are promising in effectively interleaving tasks and hiding memory latency, but they struggle to balance latency-hiding efficiency with runtime overhead. We present CoroAMU, a hardware-software co-designed system for memory-centric coroutines. It introduces compiler procedures that optimize coroutine code generation, minimize context, and coalesce requests, paired with a simple interface. With hardware support of decoupled memory operations, we enhance the Asynchronous Memory Unit to further exploit dynamic coroutine schedulers by coroutine-specific memory operations and a novel memory-guided branch prediction mechanism. It is implemented with LLVM and open-source XiangShan RISC-V processor over the FPGA platform. Experiments demonstrate that the CoroAMU compiler achieves a 1.51x speedup over state-of-the-art coroutine methods on Intel server processors. When combined with optimized hardware of decoupled memory access, it delivers 3.39x and 4.87x average performance improvements over the baseline processor on FPGA-emulated disaggregated systems under 200ns and 800ns latency respectively.

</details>


### [21] [A Tensor Compiler for Processing-In-Memory Architectures](https://arxiv.org/abs/2511.15503)
*Peiming Yang,Sankeerth Durvasula,Ivan Fernandez,Mohammad Sadrosadati,Onur Mutlu,Gennady Pekhimenko,Christina Giannoula*

Main category: cs.AR

TL;DR: 本文提出DCC，首个面向PIM系统的以数据为中心的机器学习编译器，通过联合优化数据重排与计算代码，在多种PIM后端上显著加速ML内核和LLM推理。


<details>
  <summary>Details</summary>
Motivation: 主机处理器（如GPU）与PIM核心对数据布局需求不同，导致ML内核执行中需频繁进行数据重排，带来性能与编程挑战；现有编译方法缺乏对多样化PIM后端下数据重排与计算代码的联合优化。

Method: 设计DCC编译器，引入多层PIM抽象，将数据划分策略映射到计算循环划分，应用PIM特定代码优化，并结合快速准确的性能预测模型，在统一调优过程中协同优化数据重排与计算。

Result: 在HBM-PIM和AttAcc PIM后端上，DCC相比纯GPU执行在单个ML内核上分别最高提速7.68倍和13.17倍；在端到端LLM推理中，对GPT-3和LLaMA-2最高提速7.71倍。

Conclusion: DCC通过数据与计算的联合优化，有效解决了PIM系统中因数据布局差异带来的性能瓶颈，显著提升了ML模型尤其是LLM在异构PIM架构上的执行效率。

Abstract: Processing-In-Memory (PIM) devices integrated with high-performance Host processors (e.g., GPUs) can accelerate memory-intensive kernels in Machine Learning (ML) models, including Large Language Models (LLMs), by leveraging high memory bandwidth at PIM cores. However, Host processors and PIM cores require different data layouts: Hosts need consecutive elements distributed across DRAM banks, while PIM cores need them within local banks. This necessitates data rearrangements in ML kernel execution that pose significant performance and programmability challenges, further exacerbated by the need to support diverse PIM backends. Current compilation approaches lack systematic optimization for diverse ML kernels across multiple PIM backends and may largely ignore data rearrangements during compute code optimization. We demonstrate that data rearrangements and compute code optimization are interdependent, and need to be jointly optimized during the tuning process. To address this, we design DCC, the first data-centric ML compiler for PIM systems that jointly co-optimizes data rearrangements and compute code in a unified tuning process. DCC integrates a multi-layer PIM abstraction that enables various data distribution and processing strategies on different PIM backends. DCC enables effective co-optimization by mapping data partitioning strategies to compute loop partitions, applying PIM-specific code optimizations and leveraging a fast and accurate performance prediction model to select optimal configurations. Our evaluations in various individual ML kernels demonstrate that DCC achieves up to 7.68x speedup (2.7x average) on HBM-PIM and up to 13.17x speedup (5.75x average) on AttAcc PIM backend over GPU-only execution. In end-to-end LLM inference, DCC on AttAcc accelerates GPT-3 and LLaMA-2 by up to 7.71x (4.88x average) over GPU.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [22] [RAID: In-Network RA Signaling Storm Detection for 5G Open RAN](https://arxiv.org/abs/2511.14921)
*Mohamed Rouili,Yang Xiao,Sihang Liu,Raouf Boutaba*

Main category: cs.NI

TL;DR: 本文提出RAID系统，利用P4可编程交换机ASIC在数据平面嵌入轻量级随机森林分类器，实现对5G O-RAN中随机接入信令风暴的微秒级实时检测与过滤，准确率超94%，有效保障控制面服务质量。


<details>
  <summary>Details</summary>
Motivation: 5G开放无线接入网（O-RAN）的解耦与虚拟化引入了新的控制面漏洞，尤其是由恶意或异常用户设备引发的随机接入（RA）信令风暴会迅速耗尽中央单元（CU）处理能力，导致大规模连接失败；而现有基于近实时RAN智能控制器（n-RT RIC）的检测方法因通用处理器架构的非确定性延迟（数十至数百毫秒），无法及时应对。

Method: 提出RAID系统，将轻量级随机森林（RF）分类器嵌入可编程Tofino交换机中，利用P4可编程交换机ASIC在数据平面直接进行基于机器学习的RA请求检测与过滤，实现线速流分类和确定性的微秒级推理延迟。

Result: RAID在多种流量负载下均保持超过94%的检测准确率，每流推理延迟固定约为3.4微秒，满足O-RAN控制面严格的时延要求，有效防止恶意RA请求冲击RRC。

Conclusion: RAID是一种快速、可扩展的解决方案，能够在5G O-RAN中实现实时、高效的信令风暴检测与缓解，显著优于现有基于RIC的方法。

Abstract: The disaggregation and virtualization of 5G Open RAN (O-RAN) introduces new vulnerabilities in the control plane that can greatly impact the quality of service (QoS) of latency-sensitive 5G applications and services. One critical issue is Random Access (RA) signaling storms where, a burst of illegitimate or misbehaving user equipments (UEs) send Radio Resource Control (RRC) connection requests that rapidly saturate a Central Unit's (CU) processing pipeline. Such storms trigger widespread connection failures within the short contention resolution window defined by 3GPP. Existing detection and mitigation approaches based on near-real-time RAN Intelligent Controller (n-RT RIC) applications cannot guarantee a timely reaction to such attacks as RIC control loops incur tens to hundreds of milliseconds of latency due to the non-deterministic nature of their general purpose processor (GPP) based architectures. This paper presents RAID, an in-network RA signaling storm detection and mitigation system that leverages P4-programmable switch ASICs to enable real-time protection from malicious attacks. RAID embeds a lightweight Random Forest (RF) classifier into a programmable Tofino switch, enabling line-rate flow classification with deterministic microsecond-scale inference delay. By performing ML-based detection directly in the data plane, RAID catches and filters malicious RA requests before they reach and overwhelm the RRC. RAID achieves above 94% detection accuracy with a fixed per-flow inference delay on the order of 3.4 microseconds, effectively meeting strict O-RAN control-plane deadlines. These improvements are sustained across multiple traffic loads, making RAID a fast and scalable solution for the detection and mitigation of signaling storms in 5G O-RAN.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [23] [Castle: Causal Cascade Updates in Relational Databases with Large Language Models](https://arxiv.org/abs/2511.14762)
*Yongye Su,Yucheng Zhang,Zeru Shi,Bruno Ribeiro,Elisa Bertino*

Main category: cs.DB

TL;DR: Castle 是首个仅基于数据库模式、利用大语言模型（LLMs）生成级联更新 SQL 的框架，支持通过自然语言指令生成多列、因果一致的 UPDATE 语句，且无需暴露表内容。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 在 Text2SQL 中主要关注 SELECT 查询，忽视了 SQL 更新操作及其在现代去规范化数据库中的级联影响；传统 CASCADE UPDATE 约束静态且不适用，需动态、上下文感知的更新机制。

Method: 将 UPDATE SQL 生成建模为分而治之任务，利用 LLM 的推理能力判断需直接更新的列及其在模式中的传播路径，通过嵌套查询和子结构生成级联更新语句，同时保障数据保密性。

Result: 在真实因果更新场景中评估表明，Castle 能准确生成符合语义的 SQL 更新语句，验证了 LLM 在自动化数据库管理中的推理能力。

Conclusion: Castle 成功展示了 LLM 在复杂 SQL 更新任务中的潜力，为无内容泄露的动态级联更新提供了可行方案，拓展了 LLM 在数据库系统中的应用边界。

Abstract: This work introduces Castle, the first framework for schema-only cascade update generation using large language models (LLMs). Despite recent advances in LLMs for Text2SQL code generation, existing approaches focus primarily on SELECT queries, neglecting the challenges of SQL update operations and their ripple effects. Traditional CASCADE UPDATE constraints are static and unsuitable for modern, denormalized databases, which demand dynamic, context-aware updates. Castle enables natural language instructions to trigger multi-column, causally consistent SQL UPDATE statements, without revealing table content to the model. By framing UPDATE SQL generation as a divide-and-conquer task with LLMs' reasoning capacity, Castle can determine not only which columns must be directly updated, but also how those updates propagate through the schema, causing cascading updates -- all via nested queries and substructures that ensure data confidentiality. We evaluate it on real-world causal update scenarios, demonstrating its ability to produce accurate SQL updates, and thereby highlighting the reasoning ability of LLMs in automated DBMS.

</details>


### [24] [BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer](https://arxiv.org/abs/2511.15090)
*Wenhan Yu,Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Lei Sha,Deguo Xia,Jizhou Huang*

Main category: cs.DB

TL;DR: 本文提出了BBox DocVQA，一个大规模、基于边界框的文档视觉问答数据集，旨在提升视觉语言模型在空间推理和证据定位方面的能力，并通过自动化构建流程与人工验证确保数据质量。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA数据集多局限于页面级别，缺乏细粒度的空间定位信息，限制了视觉语言模型的可解释性与推理能力。

Method: 提出名为“Segment-Judge-and-Generate”的自动化构建流程：结合区域分割模型、用于语义判断的VLM、用于问答生成的高级VLM，并辅以人工验证；构建包含3.6K文档和32K问答对的数据集，每个问答均标注明确边界框。

Result: 在BBox DocVQA上对多个先进VLM（如GPT-5、Qwen2.5-VL、InternVL）进行评测，发现其在空间定位与推理准确性方面仍存在挑战；在该数据集上微调显著提升了模型的边界框定位与答案生成性能。

Conclusion: BBox DocVQA有效增强了视觉语言模型的空间语义对齐与推理能力，数据集和代码将公开以推动可解释、空间定位精准的多模态研究。

Abstract: Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.

</details>


### [25] [B+ANN: A Fast Billion-Scale Disk-based Nearest-Neighbor Index](https://arxiv.org/abs/2511.15557)
*Selim Furkan Tekin,Rajesh Bordawekar*

Main category: cs.DB

TL;DR: 本文提出了一种新型基于磁盘的近似最近邻（ANN）索引结构 B+ANN，通过将语义相似的数据分块并结合改进的 B+ 树组织方式，在内存与磁盘中高效存储和检索向量，显著提升了查询准确率（Recall）、吞吐量（QPS），降低了缓存缺失率和内存占用，并支持 HNSW 等现有方法无法处理的不相似性查询。


<details>
  <summary>Details</summary>
Motivation: 当前主流向量数据库广泛采用 HNSW 算法，但其存在内存依赖强、缓存效率低、计算粒度细导致加速受限、仅支持相似性查询等问题。为克服这些限制，作者设计了一种兼顾性能、可扩展性和功能扩展性的新型磁盘友好型 ANN 索引。

Method: B+ANN 首先将输入向量划分为语义相近的数据块，然后构建一种变体 B+ 树结构，实现数据块在内存与磁盘上的统一管理，并支持混合边遍历与块遍历的内存内检索策略。

Result: 实验表明，相比 HNSW，B+ANN 在 Recall 和 QPS 上均有提升；相比 DiskANN，内存消耗和磁盘构建时间减少 24 倍，缓存未命中率降低 19.23%，同时首次支持不相似性查询。

Conclusion: B+ANN 是一种高效、可扩展且功能更全面的磁盘型 ANN 索引结构，有效解决了 HNSW 的多项局限，在性能、资源利用和查询能力方面均取得显著进步。

Abstract: Storing and processing of embedding vectors by specialized Vector databases (VDBs) has become the linchpin in building modern AI pipelines. Most current VDBs employ variants of a graph-based ap- proximate nearest-neighbor (ANN) index algorithm, HNSW, to an- swer semantic queries over stored vectors. Inspite of its wide-spread use, the HNSW algorithm suffers from several issues: in-memory design and implementation, random memory accesses leading to degradation in cache behavior, limited acceleration scope due to fine-grained pairwise computations, and support of only semantic similarity queries. In this paper, we present a novel disk-based ANN index, B+ANN, to address these issues: it first partitions input data into blocks containing semantically similar items, then builds an B+ tree variant to store blocks both in-memory and on disks, and finally, enables hybrid edge- and block-based in-memory traversals. As demonstrated by our experimantal evaluation, the proposed B+ANN disk-based index improves both quality (Recall value), and execution performance (Queries per second/QPS) over HNSW, by improving spatial and temporal locality for semantic operations, reducing cache misses (19.23% relative gain), and decreasing the memory consumption and disk-based build time by 24x over the DiskANN algorithm. Finally, it enables dissimilarity queries, which are not supported by similarity-oriented ANN indices.

</details>


### [26] [A Decade of Systems for Human Data Interaction](https://arxiv.org/abs/2511.15585)
*Eugene Wu,Yiru Chen,Haneen Mohammed,Zezhou Huang*

Main category: cs.DB

TL;DR: 本文探讨了人与数据交互（HDI）系统在延迟、正确性和一致性方面不同于传统数据管理的独特挑战，强调界面与系统的紧密耦合需协同设计，并指出这种耦合为数据库技术创新和新型交互可视化设计提供了研究机遇。


<details>
  <summary>Details</summary>
Motivation: 传统数据管理系统无法满足人与数据交互场景中由用户体验驱动的延迟、正确性和一致性需求，且界面与系统高度耦合，需整体优化。

Method: 综述作者实验室十年来的研究成果，强调通过协同设计界面与系统来应对HDI挑战，并探索数据库理论对交互与可视化设计的启发。

Result: 展示了HDI系统在支持可靠、交互式、AI驱动应用方面的关键作用，并提出系统创新可反向推动交互设计的发展。

Conclusion: HDI系统是构建可靠、交互式、AI驱动应用的基础，其研究应重视系统与界面的协同设计，并利用数据库理论激发新的交互范式。

Abstract: Human-data interaction (HDI) presents fundamentally different challenges from traditional data management. HDI systems must meet latency, correctness, and consistency needs that stem from usability rather than query semantics; failing to meet these expectations breaks the user experience. Moreover, interfaces and systems are tightly coupled; neither can easily be optimized in isolation, and effective solutions demand their co-design. This dependence also presents a research opportunity: rather than adapt systems to interface demands, systems innovations and database theory can also inspire new interaction and visualization designs. We survey a decade of our lab's work that embraces this coupling and argue that HDI systems are the foundation for reliable, interactive, AI-driven applications.

</details>
