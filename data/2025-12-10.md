<div id=toc></div>

# Table of Contents

- [cs.NI](#cs.NI) [Total: 3]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.SE](#cs.SE) [Total: 18]
- [cs.DB](#cs.DB) [Total: 3]


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [1] [Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks](https://arxiv.org/abs/2512.08341)
*Thai Duong Nguyen,Ngoc-Tan Nguyen,Thanh-Dao Nguyen,Nguyen Van Huynh,Dinh-Hieu Tran,Symeon Chatzinotas*

Main category: cs.NI

TL;DR: 本文提出一种基于多智能体强化学习的无人机群协同通信中继方法，在对抗干扰和避障的同时显著提升系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在对抗性环境中部署无人机群作为动态通信中继需兼顾吞吐量、避障与抗干扰，而现有启发式方法难以有效处理这一多目标动态问题。

Method: 将问题建模为合作型多智能体强化学习（MARL）任务，采用集中训练分散执行（CTDE）框架，利用集中式评论家指导仅依赖局部观测的分散式执行器。

Result: 仿真表明，所提方法相较启发式基线提升系统吞吐量约50%，同时实现近乎零碰撞率，并自发形成抗干扰策略。

Conclusion: 该MARL框架能有效协调无人机群在复杂对抗环境中的通信中继任务，展现出良好的性能与自适应能力。

Abstract: The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users.

</details>


### [2] [Improvement and Stabilization of Output Voltages in a Vertical Tidal Turbine Using Intelligent Control Strategies](https://arxiv.org/abs/2512.08416)
*Fanambinantsoa Philibert Andriniriniaimalaza,Nour Murad,Randriamaitso Telesphore,Bilal Habachi,Randriatefison Nirilalaina,Manasina Ruffin,Andrianirina Charles Bernard,Ravelo Blaise*

Main category: cs.NI

TL;DR: 本文研究了基于人工智能的控制策略，用于提升和稳定垂直轴潮流能永磁同步发电机（PMSG）系统的交直流输出电压，通过TSR、ANN-Fuzzy、PSO及混合ANN-PSO方法优化最大功率点跟踪，其中混合ANN-PSO方法在1.5 m/s流速下显著提高了电压稳定性与调节性能。


<details>
  <summary>Details</summary>
Motivation: 提高由垂直轴潮流涡轮驱动的PMSG系统输出电压的稳定性与效率，解决传统MPPT方法在动态海洋环境中适应性不足的问题。

Method: 结合TSR、ANN-Fuzzy控制器、粒子群优化（PSO）以及混合ANN-PSO方法，动态优化涡轮参考转速以实现更优的最大功率点跟踪和电压调节。

Result: 在1.5 m/s水流速度下的仿真表明，PSO控制优于传统MPPT-TSR和ANN-Fuzzy方法；混合ANN-PSO技术能实时调整参考转速，显著改善电压稳定性。

Conclusion: AI驱动的混合优化策略有效提升了潮流能发电系统的电压稳定性与运行效率，为可再生能源应用提供了可靠的技术路径。

Abstract: This article investigates on the improvement and stabilization of alternating current (AC) and direct current (DC) output voltages in a Permanent Magnet Synchronous Generator (PMSG) driven by a vertical-axis tidal turbine using advanced control strategies. The research integrates artificial intelligence (AI)-based techniques to enhance voltage stability and efficiency. Initially, the Maximum Power Point Tracking (MPPT) approach based on Tip Speed Ratio (TSR) and Artificial Neural Network (ANN) Fuzzy logic controllers is explored. To further optimize the performance, Particle Swarm Optimization (PSO) and a hybrid ANN-PSO methodology are implemented. These strategies aim to refine the reference rotational speed of the turbine while minimizing deviations from optimal power extraction conditions. The simulation results of a tidal turbine operating at a water flow velocity of 1.5 m/s demonstrate that the PSO-based control approach significantly enhances the voltage stability compared to conventional MPPT-TSR and ANN-Fuzzy controllers. The hybrid ANN-PSO technique improves the voltage regulation by dynamically adapting to system variations and providing real-time reference speed adjustments. This research highlights the AI-based hybrid optimization benefit to stabilize the output voltage of tidal energy systems, thereby increasing reliability and efficiency in renewable energy applications.

</details>


### [3] [Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR](https://arxiv.org/abs/2512.08626)
*Agrim Bari,Gustavo de Veciana,Yuqi Zhou*

Main category: cs.NI

TL;DR: 本文提出了一种新的缓存策略LFRU，通过捕捉客户端请求间的因果关联，在VR等具有强请求相关性的场景中显著优于传统LRU和LFU策略。


<details>
  <summary>Details</summary>
Motivation: 传统缓存策略（如LRU、LFU）在处理具有上下文共享或协同行为导致的请求相关性时表现不佳，尤其在虚拟现实（VR）等场景中，客户端请求高度相关，亟需能适应此类相关性的新型缓存机制。

Method: 作者提出了“分组客户端请求模型”以刻画请求相关性，并在此基础上设计了新型在线缓存策略LFRU，该策略通过动态推断请求间的因果关系来优化缓存替换决策。

Result: 在基于VR构建的真实相关请求数据集上，LFRU始终不劣于LRU和LFU，在某些设置下性能比LRU提升高达2.9倍，比LFU提升达1.9倍。

Conclusion: LFRU能够有效适应具有结构化相关性的请求模式，在边缘缓存中实现接近离线最优策略（Belady）的性能，显著优于现有经典缓存算法。

Abstract: Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.
  In this paper, we introduce the \textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.
  We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [4] [Mechanical behaviour of brain-skull interface (meninges) under shear loading through experiment and finite element modelling: Preliminary results](https://arxiv.org/abs/2512.08425)
*Sajjad Arzemanzadeh,Karol Miller,Tim Rosenow,Sjoerd B. Vos,Adam Wittek*

Main category: cs.CE

TL;DR: 本研究通过结合实验与有限元建模，量化了脑-颅骨界面（脑膜）在剪切载荷下的力学特性，提出使用内聚层模型更真实地模拟该界面行为，从而提升头部计算模型的生物保真度。


<details>
  <summary>Details</summary>
Motivation: 现有计算模型常因缺乏实验数据而对脑-颅骨界面采用简化的理想接触条件，难以准确反映其在头部冲击中的力学行为，因此需要基于实验数据建立更真实的界面模型。

Method: 从羊尸头中提取脑组织和脑-颅骨复合样本，进行剪切加载实验；利用MRI获取样本三维几何结构，构建有限元模型；脑组织采用二阶Ogden超弹性模型，界面采用内聚层模型进行模拟，并通过实验校准界面参数。

Result: 内聚层模型能有效捕捉脑-颅骨界面的力-位移响应和损伤起始行为；校准后的界面参数在样本间具有一致性，法向最大牵引力为2.8–3.4 kPa，切向最大牵引力为1.8–2.1 kPa。

Conclusion: 该研究提供了一种基于实验数据的脑-颅骨界面建模框架，可替代传统任意边界条件，显著提升计算头部模型在脑损伤预测和神经外科规划中的生物真实性。

Abstract: The brain-skull interface (meninges) plays a critical role in governing brain motion during head impacts, yet computational models often simplify this interface using idealized contact conditions due to limited experimental data. This study presents an improved protocol combining experimental testing and computational modelling to determine the mechanical properties of the brain-skull interface under shear loading. Brain tissue and brain-skull complex samples were extracted from sheep cadaver heads and subjected to shear loading. Magnetic resonance imaging (MRI) was used to obtain accurate 3D geometries of the samples, which were then used to create computational grids (meshes) for simulation of the experiments using finite element (FE) models to determine subject-specific properties of the brain tissue and brain-skull interface. A second-order Ogden hyperelastic model was used for the brain tissue, and a cohesive layer was employed to model the brain-skull interface. Our results indicate that a cohesive layer captures the force-displacement and damage initiation of the brain-skull interface. The calibrated cohesive properties showed consistent patterns across samples, with maximum normal tractions ranging from 2.8-3.4 kPa and maximum tangential tractions from 1.8-2.1 kPa. This framework provides a foundation for improving the biofidelity of computational head models used in injury prediction and neurosurgical planning by replacing arbitrary boundary conditions with formulations derived from experimental data on brain-skull interface (meninges) biomechanical behaviour.

</details>


### [5] [Financial News Summarization: Can extractive methods still offer a true alternative to LLMs?](https://arxiv.org/abs/2512.08764)
*Nicolas Reche,Elvys Linhares-Pontes,Juan-Manuel Torres-Moreno*

Main category: cs.CE

TL;DR: 该研究评估了多种金融新闻摘要方法，发现大型语言模型（LLMs）生成的摘要更连贯、信息更丰富，但存在资源消耗大和幻觉问题；而抽取式方法在短文本上表现良好且更高效；微调后的FT-Mistral-7B模型取得最佳ROUGE分数，但数据可靠性有限。


<details>
  <summary>Details</summary>
Motivation: 金融新闻数量庞大且市场变化迅速，投资者需要快速获取准确简洁的摘要以支持决策，因此有必要评估并比较不同自动化摘要方法的效果。

Method: 使用FinLLMs Challenge数据集，对比从简单抽取式方法到先进大型语言模型（LLMs）等多种摘要技术的性能。

Result: LLMs生成的摘要更具连贯性和信息量，但计算成本高且易产生幻觉；抽取式方法在短文本上表现良好且效率更高；微调后的FT-Mistral-7B模型获得最高ROUGE分数。

Conclusion: 尽管微调LLM在指标上表现最佳，但由于数据可靠性有限及LLM潜在风险，在金融摘要任务中仍需谨慎选择方法，抽取式方法可作为高效可靠的替代方案。

Abstract: Financial markets change rapidly due to news, economic shifts, and geopolitical events. Quick reactions are vital for investors to avoid losses or capture short-term gains. As a result, concise financial news summaries are critical for decision-making. With over 50,000 financial articles published daily, automation in summarization is necessary. This study evaluates a range of summarization methods, from simple extractive techniques to advanced large language models (LLMs), using the FinLLMs Challenge dataset. LLMs generated more coherent and informative summaries, but they are resource-intensive and prone to hallucinations, which can introduce significant errors into financial summaries. In contrast, extractive methods perform well on short, well-structured texts and offer a more efficient alternative for this type of article. The best ROUGE results come from fine-tuned LLM model like FT-Mistral-7B, although our data corpus has limited reliability, which calls for cautious interpretation.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [6] [NysX: An Accurate and Energy-Efficient FPGA Accelerator for Hyperdimensional Graph Classification at the Edge](https://arxiv.org/abs/2512.08089)
*Jebacyril Arockiaraj,Dhruv Parikh,Viktor Prasanna*

Main category: cs.AR

TL;DR: 本文提出NysX，一种面向边缘设备的端到端FPGA加速器，用于基于Nyström近似的超维计算（HDC）图分类。通过四项关键优化，显著提升速度、能效与准确率。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上实现图分类需要兼顾实时性与能效，而现有基于Nyström核近似的HDC方法在硬件加速方面面临样本冗余、内存限制、查表开销大和负载不均衡等挑战。

Method: NysX采用四大优化策略：(i) 结合均匀采样与行列式点过程（DPP）的混合地标选择；(ii) 流式Nyström投影矩阵架构以高效利用外部带宽；(iii) 最小完美哈希查表引擎实现O(1)映射；(iv) 静态负载均衡的稀疏矩阵向量乘（SpMV）引擎。

Result: 在AMD Zynq UltraScale+ FPGA上实现的NysX相比优化后的CPU（GPU）基线，分别获得6.85倍（4.32倍）加速和169倍（314倍）能效提升，并在TUDataset基准上平均提升3.4%的分类准确率。

Conclusion: NysX有效解决了Nyström-HDC图分类在边缘设备部署中的关键瓶颈，实现了高准确率、高能效与实时推理能力，为资源受限平台上的图学习提供了可行方案。

Abstract: Real-time, energy-efficient inference on edge devices is essential for graph classification across a range of applications. Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that encodes input features into low-precision, high-dimensional vectors with simple element-wise operations, making it well-suited for resource-constrained edge platforms. Recent work enhances HDC accuracy for graph classification via Nyström kernel approximations. Edge acceleration of such methods faces several challenges: (i) redundancy among (landmark) samples selected via uniform sampling, (ii) storing the Nyström projection matrix under limited on-chip memory, (iii) expensive, contention-prone codebook lookups, and (iv) load imbalance due to irregular sparsity in SpMV. To address these challenges, we propose NysX, the first end-to-end FPGA accelerator for Nyström-based HDC graph classification at the edge. NysX integrates four key optimizations: (i) a hybrid landmark selection strategy combining uniform sampling with determinantal point processes (DPPs) to reduce redundancy while improving accuracy; (ii) a streaming architecture for Nyström projection matrix maximizing external memory bandwidth utilization; (iii) a minimal-perfect-hash lookup engine enabling $O(1)$ key-to-index mapping with low on-chip memory overhead; and (iv) sparsity-aware SpMV engines with static load balancing. Together, these innovations enable real-time, energy-efficient inference on resource-constrained platforms. Implemented on an AMD Zynq UltraScale+ (ZCU104) FPGA, NysX achieves $6.85\times$ ($4.32\times$) speedup and $169\times$ ($314\times$) energy efficiency gains over optimized CPU (GPU) baselines, while improving classification accuracy by $3.4\%$ on average across TUDataset benchmarks, a widely used standard for graph classification.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [7] [NecoFuzz: Effective Fuzzing of Nested Virtualization via Fuzz-Harness Virtual Machines](https://arxiv.org/abs/2512.08858)
*Reima Ishii,Takaaki Fukai,Takahiro Shinagawa*

Main category: cs.OS

TL;DR: NecoFuzz 是首个专门针对嵌套虚拟化逻辑的模糊测试框架，通过生成接近有效/无效边界状态的可执行虚拟机实例，在 Intel VT-x 和 AMD-V 上实现了高代码覆盖率，并发现了多个新漏洞。


<details>
  <summary>Details</summary>
Motivation: 嵌套虚拟化增加了宿主机管理程序的复杂性并引入了新的攻击面，而现有模糊测试方法未能有效覆盖嵌套虚拟化场景，主要因为难以在庞大的虚拟机状态空间中生成有效的模糊测试输入。

Method: NecoFuzz 利用对硬件辅助虚拟化规范的近似建模，生成内部状态处于有效与无效边界附近的可执行模糊测试虚拟机，并基于 AFL++ 扩展实现对 Intel VT-x 和 AMD-V 的支持。

Result: 在 Intel VT-x 和 AMD-V 上分别达到 84.7% 和 74.2% 的嵌套虚拟化相关代码覆盖率，并在三个管理程序中发现了六个此前未知的漏洞，其中两个已分配 CVE 编号。

Conclusion: NecoFuzz 证明了通过规范引导、边界导向的虚拟机生成策略能有效提升嵌套虚拟化安全关键代码的模糊测试覆盖率，并成功揭示了真实系统中的安全漏洞。

Abstract: Nested virtualization is now widely supported by major cloud vendors, allowing users to leverage virtualization-based technologies in the cloud. However, supporting nested virtualization significantly increases host hypervisor complexity and introduces a new attack surface in cloud platforms. While many prior studies have explored hypervisor fuzzing, none has explicitly addressed nested virtualization due to the challenge of generating effective virtual machine (VM) instances with a vast state space as fuzzing inputs.
  We present NecoFuzz, the first fuzzing framework that systematically targets nested virtualization-specific logic in hypervisors. NecoFuzz synthesizes executable fuzz-harness VMs with internal states near the boundary between valid and invalid, guided by an approximate model of hardware-assisted virtualization specifications. Since vulnerabilities in nested virtualization often stem from incorrect handling of unexpected VM states, this specification-guided, boundary-oriented generation significantly improves coverage of security-critical code across different hypervisors.
  We implemented NecoFuzz on Intel VT-x and AMD-V by extending AFL++ to support fuzz-harness VMs. NecoFuzz achieved 84.7% and 74.2% code coverage for nested virtualization-specific code on Intel VT-x and AMD-V, respectively, and uncovered six previously unknown vulnerabilities across three hypervisors, including two assigned CVEs.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [8] [Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency](https://arxiv.org/abs/2512.08242)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: 本文提出了 Chopper，一个用于多粒度分析大语言模型（LLM）训练中 GPU 性能的剖析框架，并基于 AMD MI300X 平台对 Llama 3 8B 的 FSDP 训练进行了全面分析，揭示了频率调控（DVFS）是性能差距的主要来源。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于单 GPU 或内核级性能，缺乏对多 GPU 分布式 LLM 训练中通信、计算、内存与功耗管理之间复杂交互的系统性刻画。

Method: 开发 Chopper 框架，收集、对齐并可视化多粒度（从内核到 GPU）的 GPU 内核轨迹与硬件性能计数器，在八卡 AMD MI300X 节点上对 Llama 3 8B 的 FSDP 训练进行端到端剖析。

Result: 发现内存确定性可提升 GPU 和显存频率稳定性；识别出 DVFS 引起的频率开销是理论与实际性能差距的最大因素，超过 MFMA 利用率损失、通信/计算重叠不足及内核启动开销。

Conclusion: Chopper 首次实现了对 AMD MI300X 上 LLM 训练的多粒度整体刻画，为训练框架优化、电源管理策略改进及未来 GPU 架构设计提供了可操作的洞见。

Abstract: Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.

</details>


### [9] [Modeling the Potential of Message-Free Communication via CXL.mem](https://arxiv.org/abs/2512.08005)
*Stepan Vanecek,Matthew Turner,Manisha Gajbe,Matthew Wolf,Martin Schulz*

Main category: cs.DC

TL;DR: 本文提出了一种用于评估CXL.mem在MPI通信中性能潜力的新工具链和扩展性能模型，通过分析MPI应用的数据访问模式，预测哪些数据传输可通过CXL.mem获得比传统MPI消息更高的性能，并在两个应用上验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着异构内存技术的发展，CXL.mem支持多节点共享内存池，为高效跨节点通信提供了新机会。然而，尚缺乏有效工具来评估其在实际MPI应用中的性能收益。

Method: 作者扩展了内存追踪采样工具Mitos，构建了一个性能评估工具链，可分析MPI应用的片上缓冲区访问和跨节点通信流量，并基于每个MPI调用粒度建立扩展性能模型，以预测CXL.mem带来的潜在加速效果。

Result: 在2D热传导miniapp和HPCG基准测试上的实验验证了所提模型的准确性，并展示了如何利用该模型指导针对CXL.mem的优化。

Conclusion: 该工具链和性能模型能有效识别MPI程序中适合采用CXL.mem进行优化的通信操作，为未来高性能计算系统中CXL.mem的实际部署提供了实用的分析与优化手段。

Abstract: Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.
  In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.
  For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.

</details>


### [10] [CapsuleFS A Multi-credential DataCapsule Filesystem](https://arxiv.org/abs/2512.08067)
*Qingyang Hu,Yucheng Huang,Manshi Yang*

Main category: cs.DC

TL;DR: CapsuleFS（CFS）是首个在 POSIX 兼容框架中集成多凭证功能的文件系统，基于边缘计算中的全局数据平面，利用 DataCapsule 作为存储提供者。其架构包括 DataCapsule 服务器、可信执行环境中的中间件和 POSIX 兼容客户端。实验表明 CFS 功能正确性高，虽读写性能一般，但适用于实际软件开发场景。


<details>
  <summary>Details</summary>
Motivation: 现有文件系统缺乏在 POSIX 框架下支持多凭证访问的能力，而边缘计算环境中对安全、灵活的数据访问机制需求日益增长，因此需要一种新型文件系统来填补这一空白。

Method: 设计并实现 CapsuleFS（CFS），其架构包含三部分：1）DataCapsule 服务器，负责在边缘存储、分发和复制 DataCapsule；2）运行于可信执行环境（TEE）的中间件，用于管理写权限和请求；3）POSIX 兼容的客户端，支持多种硬件架构。

Result: 实验评估显示 CFS 在保持高功能正确性的同时，读写性能相对较低，但仍足以支持真实软件开发场景的应用。

Conclusion: CapsuleFS 成功实现了多凭证 POSIX 文件系统的原型，验证了其在边缘计算环境中的可行性，并为未来提升其实用性提供了明确的优化方向。

Abstract: CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.

</details>


### [11] [Synergizing Monetization, Orchestration, and Semantics in Computing Continuum](https://arxiv.org/abs/2512.08288)
*Chinmaya Kumar Dehury,Lauri Lovén,Praveen Kumar Donta,Ilir Murturi,Schahram Dustdar*

Main category: cs.DC

TL;DR: HERMES is a novel framework that enables intelligent orchestration, monetization, and semantic interoperability across cloud-to-edge resources to support next-generation hyper-distributed applications.


<details>
  <summary>Details</summary>
Motivation: Current solutions fail to meet the growing industry demands for hyper-distributed applications due to limitations in scalability, interoperability, and trust across cloud-to-edge environments.

Method: The paper proposes HERMES—a framework integrating resource orchestration, distributed data/service monetization, and semantic interoperability to create an open, seamless, and secure computing continuum.

Result: HERMES enables efficient, trustworthy, and autonomous distributed applications by unifying heterogeneous resources and enabling intelligent coordination and knowledge sharing.

Conclusion: HERMES provides a foundational architecture for future hyper-distributed systems that overcome current limitations in scalability, interoperability, and trust.

Abstract: Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.

</details>


### [12] [Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem](https://arxiv.org/abs/2512.08321)
*Yuki Uchino,Qianxiang Ma,Toshiyuki Imamura,Katsuhisa Ozaki,Patrick Lars Gutsche*

Main category: cs.DC

TL;DR: 本文基于Ozaki-II框架，提出了在INT8矩阵计算单元上高效模拟单精度和双精度复数矩阵乘法的方法，在NVIDIA B200 GPU上相比cuBLAS原生实现获得4.0–6.5倍加速，并具备灵活的精度-性能权衡能力。


<details>
  <summary>Details</summary>
Motivation: 现代计算架构中低精度矩阵乘法单元具有更高吞吐量，因此利用低精度硬件高效模拟高精度（尤其是复数）矩阵乘法成为高性能计算的重要研究方向。

Method: 基于Ozaki-II方案，设计适用于INT8矩阵引擎的单精度与双精度复数矩阵乘法的高性能量化模拟方法。

Result: 在NVIDIA B200 GPU上，对足够大的问题规模，所提方法相比cuBLAS原生复数矩阵乘法分别实现4.0–5.6倍（单精度）和4.4–6.5倍（双精度）加速；同时支持在略低或更高精度下进一步优化性能或准确性。

Conclusion: 所提出的方法在性能和精度之间具有良好的可调性，有望成为多种应用场景下的默认复数矩阵乘法算法。

Abstract: Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura introduced the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routine is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can also deliver higher accuracy than the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.

</details>


### [13] [Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging](https://arxiv.org/abs/2512.08365)
*Yi Pan,Wenbo Qian,Dedong Xie,Ruiyan Hu,Yigong Hu,Baris Kasikci*

Main category: cs.DC

TL;DR: 提出了一种名为“差分能耗调试”的新方法，并实现了名为Magneton的能耗分析工具，通过在算子级别比较不同机器学习系统间的能耗差异，自动定位导致能耗过高的代码区域和配置选择，在9个主流ML系统中成功识别出16个已知和8个未知的软件能耗问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型的训练与部署能耗极高，现有优化主要聚焦于硬件能效，而忽视了由软件设计不良（如冗余或低效操作）引起的能耗浪费。开发者缺乏有效工具来检测和诊断此类问题。

Method: 提出差分能耗调试方法，设计并实现Magneton能耗分析器，通过在算子级别对比功能相似但能耗差异显著的ML系统，自动识别高能耗的代码区域和配置。

Result: 在9个流行的ML系统（涵盖大语言模型推理、通用ML框架和图像生成）中，Magneton成功检测并诊断了16个已知的软件能耗低效案例，并新发现了8个此前未知的问题，其中7个已获开发者确认。

Conclusion: 软件层面的能耗浪费是ML系统中一个被忽视但重要的问题；通过差分能耗调试和Magneton工具，可有效识别并修复此类问题，为提升ML系统整体能效提供了新思路和实用工具。

Abstract: The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.
  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.

</details>


### [14] [Basic Lock Algorithms in Lightweight Thread Environments](https://arxiv.org/abs/2512.08563)
*Taras Skazhenik,Nikolai Korobenikov,Andrei Churbanov,Anton Malakhov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 本文研究了适用于轻量级线程（如协程）的互斥锁设计，指出传统面向操作系统线程的锁在轻量级线程环境下可能引发死锁，并提出针对TTAS和MCS锁的修改方案；同时建议在跨库兼容性要求高时使用结合两者优点的Cohort锁。


<details>
  <summary>Details</summary>
Motivation: 传统多线程数据结构主要面向操作系统线程设计，而轻量级线程（如协程）虽具有更低的启动与上下文切换开销，但需手动插入上下文切换点以实现并行。现有面向OS线程的互斥锁在轻量级线程环境中可能产生死锁，且不同语言/库对轻量级线程的实现差异导致锁实现难以通用。

Method: 作者对经典的TTAS和MCS互斥锁进行修改，使其适配轻量级线程的两种上下文切换机制（yielding和sleeping），并分析其在不同轻量级线程库下的行为与性能表现；同时引入Cohort锁作为兼顾兼容性与性能的折中方案。

Result: 修改后的TTAS和MCS锁可在轻量级线程环境下避免死锁并正确运行，但其性能高度依赖具体运行环境和线程库；相比之下，Cohort锁通过结合多个MCS队列与共享TTAS，在不同库中表现出更稳定的性能。

Conclusion: 面向轻量级线程的互斥锁设计需考虑其独特的协作式调度特性；为实现跨库通用性，推荐采用Cohort锁结构，它在性能与兼容性之间取得了良好平衡。

Abstract: Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.
  In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.
  In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.

</details>


### [15] [Model-based Testing of Practical Distributed Systems in Actor Model](https://arxiv.org/abs/2512.08698)
*Ilya Kokorin,Evgeny Chernatskiy,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 本文提出一种无需修改代码或干扰运行环境的模型驱动测试方法，用于为基于Actor模型的分布式系统自动生成覆盖全部状态与转移的完备测试套件，并以Viewstamped Replication复制算法实现为例进行了验证。


<details>
  <summary>Details</summary>
Motivation: 尽管分布式系统常配有经模型检测验证的形式化规约，但其实现仍可能包含未被发现的缺陷，缺乏与规约的一致性保证。因此，需要一种能弥合形式化模型与实际实现之间差距的有效验证手段。

Method: 将系统模型视为有限状态自动机，采用模型驱动测试方法，为基于Actor模型的分布式系统自动生成穷尽式测试套件，且无需修改源代码或干预其执行环境。

Result: 成功为一个基于Viewstamped Replication的真实复制算法实现生成了覆盖所有状态和转移的测试套件，验证了该方法的可行性与有效性。

Conclusion: 所提出的方法能够在不侵入系统实现的前提下，有效弥合分布式系统形式化规约与实际代码之间的验证鸿沟，提升实现的可靠性。

Abstract: Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.
  To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.
  In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.

</details>


### [16] [Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads](https://arxiv.org/abs/2512.08725)
*Giulio Attenni,Youssef Moawad,Novella Bartolini,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 本文通过模拟研究发现，空间和时间上的云工作负载迁移可显著降低碳、水和土地使用足迹，其中空间迁移效果尤为突出，二者结合可实现最大减排效益。


<details>
  <summary>Details</summary>
Motivation: 为减少云计算对环境的影响（包括碳排放、水资源消耗和土地占用），探索利用空间和时间维度上的工作负载迁移策略来优化资源调度。

Method: 基于来自AWS和Azure的真实数据以及不同类型应用（大数据分析和FaaS）的工作负载轨迹，开展模拟实验，评估空间与时间迁移策略在不同场景和优化目标下的环境效益，并进行敏感性分析。

Result: 空间迁移可将碳、水和土地使用足迹降低20%至85%；时间迁移也有一定效果但较弱；两者结合效果最佳。策略对电网结构预测误差和季节变化具有鲁棒性。

Conclusion: 空间与时间工作负载迁移是有效且稳健的绿色云计算策略，尤其以空间迁移为主导，可显著减少云服务的环境足迹。

Abstract: In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [17] [CFD-copilot: leveraging domain-adapted large language model and model context protocol to enhance simulation automation](https://arxiv.org/abs/2512.07917)
*Zhehao Dong,Shanghai Du,Zhen Lu,Yue Yang*

Main category: cs.SE

TL;DR: 本文提出了CFD-copilot，一个专用于计算流体力学（CFD）的领域定制大语言模型框架，通过自然语言驱动从建模到后处理的完整CFD工作流，并结合多智能体系统与模型上下文协议（MCP）提升自动化水平。


<details>
  <summary>Details</summary>
Motivation: CFD仿真配置需要深厚的物理建模和数值方法专业知识，对非专家构成门槛；尽管大语言模型（LLM）在科学任务自动化方面受到关注，但将其应用于端到端CFD流程仍面临挑战。

Method: 该框架采用微调后的LLM将用户自然语言描述直接转化为可执行CFD设置，并通过多智能体系统集成仿真执行、自动纠错和结果分析；后处理阶段利用模型上下文协议（MCP）实现LLM与外部工具的解耦，支持统一接口调用多种专用后处理功能。

Result: 在NACA 0012和三段式30P-30N翼型基准测试中，CFD-copilot表现出更高的可靠性和效率，验证了领域定制与MCP结合的有效性。

Conclusion: 将领域专用微调与MCP相结合，能显著提升LLM在复杂工程任务如CFD中的自动化能力，为非专家用户提供高效、可靠的仿真解决方案。

Abstract: Configuring computational fluid dynamics (CFD) simulations requires significant expertise in physics modeling and numerical methods, posing a barrier to non-specialists. Although automating scientific tasks with large language models (LLMs) has attracted attention, applying them to the complete, end-to-end CFD workflow remains a challenge due to its stringent domain-specific requirements. We introduce CFD-copilot, a domain-specialized LLM framework designed to facilitate natural language-driven CFD simulation from setup to post-processing. The framework employs a fine-tuned LLM to directly translate user descriptions into executable CFD setups. A multi-agent system integrates the LLM with simulation execution, automatic error correction, and result analysis. For post-processing, the framework utilizes the model context protocol (MCP), an open standard that decouples LLM reasoning from external tool execution. This modular design allows the LLM to interact with numerous specialized post-processing functions through a unified and scalable interface, improving the automation of data extraction and analysis. The framework was evaluated on benchmarks including the NACA~0012 airfoil and the three-element 30P-30N airfoil. The results indicate that domain-specific adaptation and the incorporation of the MCP jointly enhance the reliability and efficiency of LLM-driven engineering workflows.

</details>


### [18] [DeepCode: Open Agentic Coding](https://arxiv.org/abs/2512.07921)
*Zongwei Li,Zhonghang Li,Zirui Guo,Xubin Ren,Chao Huang*

Main category: cs.SE

TL;DR: DeepCode 是一个全自动框架，通过优化信息流管理，在有限上下文限制下高效地将科研论文转化为高质量代码库，显著优于现有商业编码智能体和人类专家。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在将科研论文等复杂文档高保真地转化为完整代码库时面临信息过载与上下文瓶颈之间的根本矛盾。

Method: DeepCode 将代码库合成视为信道优化问题，通过四种信息操作协同工作：蓝图蒸馏实现源压缩、基于状态化代码记忆的结构化索引、检索增强生成实现条件知识注入，以及闭环错误修正。

Result: 在 PaperBench 基准测试中，DeepCode 在关键复现指标上显著超越 Cursor 和 Claude Code 等主流商业智能体，并优于顶尖机构的博士级人类专家。

Conclusion: DeepCode 为自主科学复现奠定了新基础，能系统性地将论文规范转化为媲美人类专家的生产级代码，从而加速科研评估与发现。

Abstract: Recent advances in large language models (LLMs) have given rise to powerful coding agents, making it possible for code assistants to evolve into code engineers. However, existing methods still face significant challenges in achieving high-fidelity document-to-codebase synthesis--such as scientific papers to code--primarily due to a fundamental conflict between information overload and the context bottlenecks of LLMs. In this work, we introduce DeepCode, a fully autonomous framework that fundamentally addresses this challenge through principled information-flow management. By treating repository synthesis as a channel optimization problem, DeepCode seamlessly orchestrates four information operations to maximize task-relevant signals under finite context budgets: source compression via blueprint distillation, structured indexing using stateful code memory, conditional knowledge injection via retrieval-augmented generation, and closed-loop error correction. Extensive evaluations on the PaperBench benchmark demonstrate that DeepCode achieves state-of-the-art performance, decisively outperforming leading commercial agents such as Cursor and Claude Code, and crucially, surpassing PhD-level human experts from top institutes on key reproduction metrics. By systematically transforming paper specifications into production-grade implementations comparable to human expert quality, this work establishes new foundations for autonomous scientific reproduction that can accelerate research evaluation and discovery.

</details>


### [19] [An Empirical Framework for Evaluating Semantic Preservation Using Hugging Face](https://arxiv.org/abs/2512.07983)
*Nan Jia,Anita Raja,Raffi Khatchadourian*

Main category: cs.SE

TL;DR: 本文提出一个基于HuggingFace数据的实证框架，用于评估学习型软件系统（LESS）中智能组件优化是否保持语义一致性，并通过大规模模型演化数据和案例研究揭示语义漂移现象及常见重构模式。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在高自主系统中的广泛应用，确保学习型软件系统的可信性变得至关重要；然而，机器学习的非确定性和运行时定义语义使得传统软件重构方法难以适用，因此需要新的方法来保障语义一致性。

Method: 从HuggingFace平台采集170万条模型记录，构建可复现的数据管道，提取提交历史、模型卡片和性能指标；针对536个模型和4000多个指标建立评估流程，并通过三个领域的案例研究分析版本间的性能变化和提交信息，以识别语义漂移和重构模式。

Result: 研究成功检测到跨提交的语义漂移现象，识别出常见的重构模式，并构建了一个大规模ML模型演化数据集和评估语义一致性的实用流程。

Conclusion: 该工作为定义社区认可的语义保持边界奠定了基础，有助于提升机器学习系统的可维护性与可信度。

Abstract: As machine learning (ML) becomes an integral part of high-autonomy systems, it is critical to ensure the trustworthiness of learning-enabled software systems (LESS). Yet, the nondeterministic and run-time-defined semantics of ML complicate traditional software refactoring. We define semantic preservation in LESS as the property that optimizations of intelligent components do not alter the system's overall functional behavior. This paper introduces an empirical framework to evaluate semantic preservation in LESS by mining model evolution data from HuggingFace. We extract commit histories, $\textit{Model Cards}$, and performance metrics from a large number of models. To establish baselines, we conducted case studies in three domains, tracing performance changes across versions. Our analysis demonstrates how $\textit{semantic drift}$ can be detected via evaluation metrics across commits and reveals common refactoring patterns based on commit message analysis. Although API constraints limited the possibility of estimating a full-scale threshold, our pipeline offers a foundation for defining community-accepted boundaries for semantic preservation. Our contributions include: (1) a large-scale dataset of ML model evolution, curated from 1.7 million Hugging Face entries via a reproducible pipeline using the native HF hub API, (2) a practical pipeline for the evaluation of semantic preservation for a subset of 536 models and 4000+ metrics and (3) empirical case studies illustrating semantic drift in practice. Together, these contributions advance the foundations for more maintainable and trustworthy ML systems.

</details>


### [20] [Formally and Empirically Verified Methodologies for Scalable Hierarchical Full-Stack Systems](https://arxiv.org/abs/2510.00002)
*Dong Liu*

Main category: cs.SE

TL;DR: 本文提出了两种可扩展的工业级全栈软件开发方法——主广度优先开发（PBFD）和主深度优先开发（PDFD），结合图论建模与形式化验证，确保结构与行为正确性；同时引入三层封装（TLE）位掩码编码方案，实现高效存储与更新。PBFD在八年企业部署中表现卓越，相较现有技术显著提升开发速度、查询性能并大幅减少存储开销。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程方法在工业级全栈开发中难以兼顾可扩展性、正确性与性能，缺乏将形式化方法有效融入实践的系统性框架。

Method: 将软件开发建模为带统一状态机的分层有向图，利用通信顺序进程（CSP）和线性时序逻辑（LTL）进行形式化验证；提出三层封装（TLE）位掩码编码方案，并通过CSP失效-发散精化验证其操作。

Result: PBFD在八年企业应用中实现零关键故障，开发速度比Salesforce OmniScript快约20倍，查询性能提升7-8倍，存储占用减少11.7倍；开源MVP验证了有界精化和常数时间位掩码操作等关键性质。

Conclusion: PBFD和PDFD通过图论建模与形式化方法的融合，为工业级软件开发提供了兼具正确性、可扩展性与高性能的可行路径，TLE编码进一步支撑了其高效实现，实证结果与开源资源支持其广泛适用性。

Abstract: This paper introduces Primary Breadth-First Development (PBFD) and Primary Depth-First Development (PDFD)-formally and empirically verified methodologies for scalable, industrial-grade full-stack software engineering. Both approaches enforce structural and behavioral correctness through graph-theoretic modeling, bridging formal methods and real-world practice. PBFD and PDFD model software development as layered directed graphs with unified state machines, verified using Communicating Sequential Processes (CSP) and Linear Temporal Logic (LTL). This guarantees bounded-refinement termination, deadlock freedom, and structural completeness. To manage hierarchical data at scale, we present the Three-Level Encapsulation (TLE)-a novel bitmask-based encoding scheme. TLE operations are verified via CSP failures-divergences refinement, ensuring constant-time updates and compact storage that underpin PBFD's robust performance. PBFD demonstrates exceptional industrial viability through eight years of enterprise deployment with zero critical failures, achieving approximately 20x faster develop-ment than Salesforce OmniScript, 7-8x faster query performance, and 11.7x storage reduction compared to conventional relational models. These results are established through longitudinal observational studies, quasi-experimental runtime comparisons, and controlled schema-level experiments. Open-source Minimum Viable Product implementations validate key behavioral properties, including bounded refinement and constant-time bitmask operations, un-der reproducible conditions. All implementations, formal specifications, and non-proprietary datasets are publicly available.

</details>


### [21] [A Gray Literature Study on Fairness Requirements in AI-enabled Software Engineering](https://arxiv.org/abs/2512.07990)
*Thanh Nguyen,Chaima Boufaied,Ronnie de Souza Santos*

Main category: cs.SE

TL;DR: 本文综述了灰色文献中关于人工智能系统公平性需求的定义、在软件开发生命周期中的管理方式、违反原因及其后果，强调需将公平性与有效性同等重视。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能和机器学习在各类软件应用中日益普及，但研究多聚焦于模型有效性（如F1分数），而对公平性关注不足。为弥补这一差距，本文系统梳理公平性需求的相关实践与挑战。

Method: 通过回顾现有灰色文献，分析AI系统中公平性需求的定义、在软件开发生命周期（SDLC）各阶段的管理实践、违反公平性要求的原因及相应后果。

Result: 研究发现，公平性需求通常强调在人口统计和社会属性上的非歧视与平等对待；其管理实践在SDLC中差异较大，尤其体现在模型训练、偏见缓解、公平性监控与评估及数据处理方面；违反原因主要包括数据代表性偏差、算法与模型设计偏差、人为判断以及评估与透明度缺失；后果涵盖广泛伤害，如职业与社会影响、刻板印象强化、数据隐私风险及对AI决策信任的丧失。

Conclusion: 应建立一致的框架和实践，将公平性系统性地融入AI软件开发全过程，并给予其与有效性同等的关注。

Abstract: Today, with the growing obsession with applying Artificial Intelligence (AI), particularly Machine Learning (ML), to software across various contexts, much of the focus has been on the effectiveness of AI models, often measured through common metrics such as F1- score, while fairness receives relatively little attention. This paper presents a review of existing gray literature, examining fairness requirements in AI context, with a focus on how they are defined across various application domains, managed throughout the Software Development Life Cycle (SDLC), and the causes, as well as the corresponding consequences of their violation by AI models. Our gray literature investigation shows various definitions of fairness requirements in AI systems, commonly emphasizing non-discrimination and equal treatment across different demographic and social attributes. Fairness requirement management practices vary across the SDLC, particularly in model training and bias mitigation, fairness monitoring and evaluation, and data handling practices. Fairness requirement violations are frequently linked, but not limited, to data representation bias, algorithmic and model design bias, human judgment, and evaluation and transparency gaps. The corresponding consequences include harm in a broad sense, encompassing specific professional and societal impacts as key examples, stereotype reinforcement, data and privacy risks, and loss of trust and legitimacy in AI-supported decisions. These findings emphasize the need for consistent frameworks and practices to integrate fairness into AI software, paying as much attention to fairness as to effectiveness.

</details>


### [22] [What Pulls the Strings? Understanding the Characteristics and Role of Argumentation in Open-Source Software Usability Discussions](https://arxiv.org/abs/2512.08032)
*Arghavan Sanei,Chaima Amiri,Atefeh Shokrizadeh,Jinghui Cheng*

Main category: cs.SE

TL;DR: 该研究分析了五个开源软件项目中关于可用性的讨论，发现这些讨论主要由论证驱动，但论证质量参差不齐，且对参与者后续行为有不同影响，从而为提升开源软件可用性及支持协作社区提供洞见。


<details>
  <summary>Details</summary>
Motivation: 开源软件（OSS）的可用性常被忽视，而论证在利益相关者讨论可用性时起关键作用，但目前缺乏对这类论证话语特征的理解，难以有效支持讨论参与者。

Method: 对五个开源软件项目中的可用性讨论进行论证话语及其质量的全面分析。

Result: 可用性讨论主要由论证驱动，但质量不一；问题评论中的论证质量低于问题帖，表明社区在可用性方面缺乏集体智慧；论证话语和质量对参与者后续行为有不同影响。

Conclusion: 本研究为开源软件利益相关者提供了构建更有效论证的见解，有助于提升可用性，并可推广至其他分布式协作社区的研究。

Abstract: The usability of open-source software (OSS) is important but frequently overlooked in favor of technical and functional complexity. Argumentation can be a pivotal device for diverse stakeholders in OSS usability discussions to express opinions and persuade others. However, the characteristics of argument discourse in those discussions remain unknown, resulting in difficulties in providing effective support for discussion participants. We address this through a comprehensive analysis of argument discourse and quality in five OSS projects. Our results indicated that usability discussions are predominantly argument-driven, although their qualities vary. Issue comments exhibit lower-quality arguments than the issue posts, suggesting a shortage of collective intelligence about usability in OSS communities. Moreover, argument discourse and quality have various impacts on the subsequent behavior of participants. Overall, this research offers insights to help OSS stakeholders build more effective arguments and eventually improve OSS usability. These insights can also inform studies about other distributed collaborative communities.

</details>


### [23] [Secure or Suspect? Investigating Package Hallucinations of Shell Command in Original and Quantized LLMs](https://arxiv.org/abs/2512.08213)
*Md Nazmul Haque,Elizabeth Lin,Lawrence Arkoh,Biruk Tadesse,Bowen Xu*

Main category: cs.SE

TL;DR: 本文首次系统研究了量化对大语言模型生成Go依赖项时的包幻觉和安全漏洞风险的影响，发现低精度量化（尤其是4-bit）显著增加了幻觉率和漏洞存在率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成代码依赖推荐时存在幻觉和引入已知安全漏洞的风险，而量化虽被广泛用于降低推理成本，但其对依赖项正确性与安全性的影响尚不明确。

Method: 在全精度、8-bit和4-bit三种量化设置下，评估五种规模的Qwen模型在三个数据集（SO、MBPP和paraphrase）上生成Go包的表现，分析包幻觉率（PHR）和漏洞存在率（VPR）的变化。

Result: 量化显著提高了包幻觉率，4-bit模型退化最严重；即使在正确生成的包中，随着精度降低，漏洞存在率也上升；幻觉输出多为类似真实URL的伪造Go模块路径（如无效GitHub或golang.org仓库）。

Conclusion: 部署用于代码生成和依赖推荐的量化大语言模型时，需警惕其在可靠性和安全性方面的潜在风险，尤其在低精度场景下。

Abstract: Large Language Models for code (LLMs4Code) are increasingly used to generate software artifacts, including library and package recommendations in languages such as Go. However, recent evidence shows that LLMs frequently hallucinate package names or generate dependencies containing known security vulnerabilities, posing significant risks to developers and downstream software supply chains. At the same time, quantization has become a widely adopted technique to reduce inference cost and enable deployment of LLMs on resource-constrained environments. Despite its popularity, little is known about how quantization affects the correctness and security of LLM-generated software dependencies while generating shell commands for package installation.
  In this work, we conduct the first systematic empirical study of the impact of quantization on package hallucination and vulnerability risks in LLM-generated Go packages. We evaluate five Qwen model sizes under full-precision, 8-bit, and 4-bit quantization across three datasets (SO, MBPP, and paraphrase). Our results show that quantization substantially increases the package hallucination rate (PHR), with 4-bit models exhibiting the most severe degradation. We further find that even among the correctly generated packages, the vulnerability presence rate (VPR) rises as precision decreases, indicating elevated security risk in lower-precision models. Finally, our analysis of hallucinated outputs reveals that most fabricated packages resemble realistic URL-based Go module paths, such as most commonly malformed or non-existent GitHub and golang.org repositories, highlighting a systematic pattern in how LLMs hallucinate dependencies. Overall, our findings provide actionable insights into the reliability and security implications of deploying quantized LLMs for code generation and dependency recommendation.

</details>


### [24] [Migrating QAOA from Qiskit 1.x to 2.x: An experience report](https://arxiv.org/abs/2512.08245)
*Julien Cardinal,Imen Benzarti,Ghizlane El boussaidi,Christophe Pere*

Main category: cs.SE

TL;DR: 在将QAOA从Qiskit 1.x迁移到2.x时，尽管电路、优化器和哈密顿量相同，结果却显著不同；根本原因在于采样预算（shots）的默认值变化，增加shots至250,000可恢复准确性。


<details>
  <summary>Details</summary>
Motivation: 量子算法在不同框架版本间迁移时可能出现行为差异，影响结果的准确性和可复现性，亟需识别并解决此类隐藏问题。

Method: 系统分析QAOA在Qiskit 1.x与2.x之间迁移过程中产生的结果差异，重点排查采样次数（shots）等隐式参数的影响，并通过调整shots数量进行验证。

Result: 发现Qiskit 2.x默认的10,000次采样仅覆盖23%的状态空间，而将采样数提升至250,000后可恢复与旧版库相当的精度。

Conclusion: 量子-经典交互层中的隐藏参数（如采样预算）对混合量子算法性能具有决定性影响，开发者和框架设计者应显式控制此类参数以确保迁移过程中的结果可复现。

Abstract: Migrating quantum algorithms across evolving frameworks introduces subtle behavioral changes that affect accuracy and reproducibility. This paper reports our experience converting the Quantum Approximate Optimization Algorithm (QAOA) from Qiskit Algorithms with Qiskit 1.x (v1 primitives) to a custom implementation using Qiskit 2.x (v2 primitives). Despite identical circuits, optimizers, and Hamiltonians, the new version produced drastically different results. A systematic analysis revealed the root cause: the sampling budget -- the number of circuit executions (shots) per iteration. The library's implicit use of unlimited shots yielded dense probability distributions, whereas the v2 default of 10 000 shots captured only 23% of the state space. Increasing shots to 250 000 restored library-level accuracy. This study highlights how hidden parameters at the quantum--classical interaction level can dominate hybrid algorithm performance and provides actionable recommendations for developers and framework designers to ensure reproducible results in quantum software migration.

</details>


### [25] [Token Sugar: Making Source Code Sweeter for LLMs through Token-Efficient Shorthand](https://arxiv.org/abs/2512.08266)
*Zhensu Sun,Chengran Yang,Xiaoning Du,Zhou Yang,Li Li,David Lo*

Main category: cs.SE

TL;DR: 本文提出Token Sugar方法，通过将源代码中高频且冗长的代码模式替换为可逆的简洁简写形式，在保持模型性能的同时显著减少大语言模型在代码生成和理解任务中的token使用量。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码任务中表现优异，但编程语言的冗长性（如格式元素和样板代码）导致输入和输出token数量膨胀，增加计算开销。现有方法仅关注语法层面的简化，未能充分利用语义层面的token压缩机会。

Method: 提出Token Sugar概念：从代码语料库中挖掘高频、高token消耗的代码模式，为每个模式分配唯一简写，并通过代码转换将其整合进大语言模型的预训练数据中。

Result: 获得了799个（代码模式，简写）对，最多可减少源代码15.1%的token数量；在三个主流大语言模型上的实验表明，使用Token Sugar增强数据训练的模型在生成时最多减少11.2%的token，同时Pass@1分数与基线几乎一致。

Conclusion: Token Sugar是一种有效且互补的token压缩方法，能在不损害模型性能的前提下显著降低大语言模型处理代码时的token开销。

Abstract: Large language models (LLMs) have shown exceptional performance in code generation and understanding tasks, yet their high computational costs hinder broader adoption. One important factor is the inherent verbosity of programming languages, such as unnecessary formatting elements and lengthy boilerplate code. This leads to inflated token counts in both input and generated outputs, which increases inference costs and slows down the generation process. Prior work improves this through simplifying programming language grammar, reducing token usage across both code understanding and generation tasks. However, it is confined to syntactic transformations, leaving significant opportunities for token reduction unrealized at the semantic level.
  In this work, we propose Token Sugar, a concept that replaces frequent and verbose code patterns with reversible, token-efficient shorthand in the source code. To realize this concept in practice, we designed a systematic solution that mines high-frequency, token-heavy patterns from a code corpus, maps each to a unique shorthand, and integrates them into LLM pretraining via code transformation. With this solution, we obtain 799 (code pattern, shorthand) pairs, which can reduce up to 15.1% token count in the source code and is complementary to existing syntax-focused methods. We further trained three widely used LLMs on Token Sugar-augmented data. Experimental results show that these models not only achieve significant token savings (up to 11.2% reduction) during generation but also maintain near-identical Pass@1 scores compared to baselines trained on unprocessed code.

</details>


### [26] [FedLAD: A Modular and Adaptive Testbed for Federated Log Anomaly Detection](https://arxiv.org/abs/2512.08277)
*Yihan Liao,Jacky Keung,Zhenyu Mao,Jingyu Zhang,Jialong Li*

Main category: cs.SE

TL;DR: 提出了FedLAD，一个面向日志异常检测（LAD）的联邦学习统一平台，支持模型、数据集和聚合策略的即插即用，并提供自监控、自配置与自适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有LAD方法多依赖集中式训练，在隐私限制和日志分散的实际场景中难以应用；同时缺乏专为联邦LAD设计的测试平台。

Method: 构建FedLAD平台，整合多种LAD模型、基准数据集和聚合策略，支持运行时的验证日志记录、参数调优和自适应策略控制。

Result: FedLAD实现了可复现、可扩展的联邦LAD实验环境，弥合了联邦学习框架与LAD需求之间的差距。

Conclusion: FedLAD为联邦日志异常检测研究提供了坚实基础，推动该领域进一步发展。

Abstract: Log-based anomaly detection (LAD) is critical for ensuring the reliability of large-scale distributed systems. However, most existing LAD approaches assume centralized training, which is often impractical due to privacy constraints and the decentralized nature of system logs. While federated learning (FL) offers a promising alternative, there is a lack of dedicated testbeds tailored to the needs of LAD in federated settings. To address this, we present FedLAD, a unified platform for training and evaluating LAD models under FL constraints. FedLAD supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, while offering runtime support for validation logging (self-monitoring), parameter tuning (self-configuration), and adaptive strategy control (self-adaptation). By enabling reproducible and scalable experimentation, FedLAD bridges the gap between FL frameworks and LAD requirements, providing a solid foundation for future research. Project code is publicly available at: https://github.com/AA-cityu/FedLAD.

</details>


### [27] [Measuring Agile Agreement: Development and Validation of the Manifesto and Principle Scales](https://arxiv.org/abs/2512.08461)
*Nicolas Matton,Anthony Simonofski,Marie-Ange Remiche,Benoît Vanderose*

Main category: cs.SE

TL;DR: 本文开发并验证了两个用于测量个体对敏捷宣言价值观（MAS）和12项原则实践（PAS）认同度的量表，证明二者虽中度相关但不可互换，为更细致地评估“人-敏捷”匹配提供了工具。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能区分个体对敏捷宣言抽象价值观与具体日常实践的认同，导致“敏捷认同”难以准确测量。作者旨在填补这一方法论空白。

Method: 通过系统化的条目创建与筛选、问卷设计及验证流程，构建了Manifesto Agreement Scale (MAS) 和 Principle Agreement Scale (PAS) 两个量表，并采用内部一致性检验、建构效度分析以及包括比例优势逻辑回归、Bland-Altman图和组内相关系数（ICC）在内的收敛与区分效度分析进行验证。

Result: 两个量表均展现出良好的内部一致性和建构效度；收敛与区分分析表明它们中度相关但测量的是敏捷认同的不同维度，不可互换。

Conclusion: 本研究提供了两个公开可用的、在比利时IT从业者群体中验证过的量表，为未来更精细地区分不同层面的敏捷认同和理解个体与敏捷方法的适配性奠定了基础。

Abstract: While the importance of human factors in agile software development is widely acknowledged, the measurement of an individual's "agile agreement" remains an ill-defined and challenging area. A key limitation in existing research is the failure to distinguish between agreement with the abstract, high-level values of the Agile Manifesto and agreement with the concrete, day-to-day practices derived from the 12 Principles. This paper addresses this methodological gap by presenting the design and validation of two distinct instruments: the novel Manifesto Agreement Scale (MAS), and the Principle Agreement Scale (PAS), which is a systematic adaptation and refinement of a prior instrument.
  We detail the systematic process of item creation and selection, survey design, and validation. The results demonstrate that both scales possess important internal consistency and construct validity. A convergence and divergence analysis, including Proportional Odds Logistic Regression, a Bland-Altman plot, and an Intraclass Correlation Coefficient (ICC), reveals that while the two scales are moderately correlated, they are not interchangeable and capture distinct dimensions of agile agreement. The primary contribution of this work is a pair of publicly available instruments, validated within a specific demographic of Belgian IT professionals. These scales represent a critical initial step toward facilitating a more nuanced measurement of agile agreement, distinguishing agile agreement across various levels of perception and aiding in a more refined interpretation of person-agile fit.

</details>


### [28] [Measuring Computer Science Enthusiasm: A Questionnaire-Based Analysis of Age and Gender Effects on Students' Interest](https://arxiv.org/abs/2512.08472)
*Kai Marquardt,Robert Hanak,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 本研究基于兴趣的人-物理论，通过开发问卷评估计算机科学（CS）干预活动激发热情的潜力，发现年龄比性别对青少年CS兴趣发展影响更大，早期干预并非唯一有效路径，针对不同年龄段设计动态教学策略更为关键。


<details>
  <summary>Details</summary>
Motivation: 现有研究多强调早期接触对培养CS兴趣的重要性，但忽视了年龄与性别在兴趣发展中的独立作用。作者旨在厘清二者对青少年CS兴趣（特别是短期热情）的影响，以优化CS教育干预策略。

Method: 基于人-物兴趣理论构建“热情”概念，并开发前后测问卷；收集400余名参与在线CS课程的青少年数据，分析年龄与性别对热情变化的影响。

Result: 研究发现：1）早期青春期（尤其女生）热情显著下降；2）年龄比性别更能预测兴趣变化；3）存在关键的发展转折点；4）年长学生虽初始态度较低，但干预后热情提升最明显。

Conclusion: CS教育应采用动态、年龄敏感的框架，依据发展阶段调整教学策略，而非仅聚焦早期干预；短期高质量活动可在较晚阶段有效重燃学生兴趣。

Abstract: This study offers new insights into students' interest in computer science (CS) education by disentangling the distinct effects of age and gender across a diverse adolescent sample. Grounded in the person-object theory of interest (POI), we conceptualize enthusiasm as a short-term, activating expression of interest that combines positive affect, perceived relevance, and intention to re-engage. Experiencing such enthusiasm can temporarily shift CS attitudes and strengthen future engagement intentions, making it a valuable lens for evaluating brief outreach activities. To capture these dynamics, we developed a theoretically grounded questionnaire for pre-post assessment of the enthusiasm potential of CS interventions. Using data from more than 400 students participating in online CS courses, we examined age- and gender-related patterns in enthusiasm. The findings challenge the prevailing belief that early exposure is the primary pathway to sustained interest in CS. Instead, we identify a marked decline in enthusiasm during early adolescence, particularly among girls, alongside substantial variability in interest trajectories across age groups. Crucially, our analyses reveal that age is a more decisive factor than gender in shaping interest development and uncover key developmental breakpoints. Despite starting with lower baseline attitudes, older students showed the largest positive changes following the intervention, suggesting that well-designed short activities can effectively re-activate interest even at later ages. Overall, the study highlights the need for a dynamic, age-sensitive framework for CS education in which instructional strategies are aligned with developmental trajectories.

</details>


### [29] [Gamification with Purpose: What Learners Prefer to Motivate Their Learning](https://arxiv.org/abs/2512.08551)
*Kai Marquardt,Mona Schulz,Anne Koziolek,Lucia Happe*

Main category: cs.SE

TL;DR: 本研究通过系统文献综述和最佳-最差量表调查，识别学习者在教育环境中对游戏化设计元素的偏好，发现学习者更青睐能直接支持学习过程（如进度条、概念图、即时反馈和成就）且与教学内容深度融合的元素。


<details>
  <summary>Details</summary>
Motivation: 为制定以目标为导向的游戏化策略，需了解学习者对游戏设计元素的真实偏好，避免因不当使用外在激励而削弱内在动机，从而实现以学习者为中心、与教学目标一致的游戏化设计。

Method: 采用系统性文献综述识别出十个常用游戏设计元素，为其制作视觉原型，并通过包含125名参与者的最佳-最差量表（BWS）调查获取偏好排序，同时收集质性反馈以揭示动机驱动因素。

Result: 学习者最偏好直接支持学习过程的游戏元素，包括进度条、概念图、即时反馈和成就；质性分析提炼出六个动机主题：可见的进展、内容相关性、建设性反馈等。

Conclusion: 有效的教育游戏化应优先选择与学习内容深度融合、能可视化学习进程并提供可操作反馈的元素，以支持内在动机，而非依赖单纯的外在奖励。

Abstract: This study investigates learners' preferences for game design elements (GDEs) in educational contexts to inform the development of purpose-driven gamification strategies. It emphasizes a learner-centered approach that aligns gamification design with pedagogical goals, while mitigating risks such as the erosion of intrinsic motivation. A systematic literature review was conducted to identify ten widely discussed GDEs. Visual prototypes representing each element were developed, and a best-worst scaling (BWS) survey with 125 participants was administered to elicit preference rankings. Qualitative feedback was also collected to uncover motivational drivers. Learners consistently preferred GDEs that support learning processes directly-most notably progress bars, concept maps, immediate feedback, and achievements. Qualitative analysis revealed six recurring motivational themes, including visible progress, content relevance, and constructive feedback. The findings suggest that learners value gamification elements that are meaningfully integrated with educational content and support intrinsic motivation. Purpose-aligned gamification should prioritize tools that visualize learning progress and provide actionable feedback, rather than relying solely on extrinsic incentives.

</details>


### [30] [Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain](https://arxiv.org/abs/2512.08657)
*Renato Cordeiro Ferreira,Aditya Dhinavahi,Rowanne Trapmann,Willem-Jan van den Heuvel*

Main category: cs.SE

TL;DR: 本文通过构建名为Ocean Guard的海事异常检测ML系统，探讨了在单一代码库中复用Ports and Adapters（六边形架构）模式来支持多个微服务的经验、挑战与启示。


<details>
  <summary>Details</summary>
Motivation: ML系统结构复杂，需多组件协同；作者旨在通过复用软件架构提升开发效率与系统可维护性，并推广六边形架构在ML系统中的应用。

Method: 采用Ports and Adapters（即Hexagonal Architecture）模式，在单一代码库中构建多个微服务，实现软件架构的复用。

Result: 成功在Ocean Guard项目中复用架构，识别出实施过程中的关键挑战并总结了实用经验。

Conclusion: 六边形架构能有效支持ML系统的模块化与复用，值得ML工程师和软件开发者借鉴。

Abstract: ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.

</details>


### [31] [RESTifAI: LLM-Based Workflow for Reusable REST API Testing](https://arxiv.org/abs/2512.08706)
*Leon Kogler,Maximilian Ehrhart,Benedikt Dornauer,Eduard Paul Enoiu*

Main category: cs.SE

TL;DR: 本文提出了RESTifAI，一种基于大语言模型（LLM）的自动生成可重用、CI/CD就绪的REST API“正向路径”测试用例的方法，并能进一步推导出负向测试用例以验证系统健壮性。


<details>
  <summary>Details</summary>
Motivation: 现有工具多关注内部服务器错误，缺乏对有效业务场景（happy path）的系统性测试支持，且在可重用性、断言复杂性和集成能力方面存在不足。

Method: 利用大语言模型自动构建符合业务逻辑的正向测试用例，并从中衍生出用于验证4xx响应的负向测试，生成的测试可直接用于CI/CD流程。

Result: RESTifAI在性能上与当前先进工具（如AutoRestTest和LogiAgent）相当，同时在可重用性、断言生成和工业集成方面表现更优，并已在实际工业服务中验证其有效性。

Conclusion: RESTifAI提供了一种高效、实用且可集成的API测试生成方案，显著提升了REST API测试的自动化水平和质量保障能力。

Abstract: With this paper, we introduce RESTifAI, an LLM-driven approach for generating reusable, CI/CD ready REST API tests, following the happy-path approach. Unlike existing tools that often focus primarily on internal server errors, RESTifAI systematically constructs valid test scenarios (happy paths) and derives negative cases to verify both intended functionality (2xx responses) and robustness against invalid inputs or business-rule violations (4xx responses). The results indicate that RESTifAI performs on par with the latest LLM tools, i.e., AutoRestTest and LogiAgent, while addressing limitations related to reusability, oracle complexity, and integration. To support this, we provide common comparative results and demonstrate the tool's applicability in industrial services. For tool demonstration, please refer to https://www.youtube.com/watch?v=2vtQo0T0Lo4. RESTifAI is publicly available at https://github.com/casablancahotelsoftware/RESTifAI.

</details>


### [32] [Multicalibration for LLM-based Code Generation](https://arxiv.org/abs/2512.08810)
*Viola Campos,Robin Kuschnereit,Adrian Ulges*

Main category: cs.SE

TL;DR: 该论文研究了代码大语言模型（LLM）的多校准方法，以提升其置信度分数与代码正确性之间的一致性，在多个基准上验证了多校准优于未校准和基线校准方法。


<details>
  <summary>Details</summary>
Motivation: 随着基于AI的代码生成广泛应用，确保代码大模型输出的置信度能真实反映代码正确性的概率变得至关重要。现有校准方法未能充分考虑编程问题的多种因素（如复杂度、代码长度、编程语言等），因此作者引入多校准方法来解决这一问题。

Method: 作者在三个函数合成基准上，使用最新的代码大模型（Qwen3 Coder、GPT-OSS、DeepSeek-R1-Distill），评估了四种多校准方法，并通过消融实验分析不同因素对校准效果的影响。

Result: 多校准方法在技能得分上相比未校准的token似然提升了+1.03，相比基线校准方法提升了+0.37；同时作者公开了包含代码生成、似然值和正确性标签的数据集。

Conclusion: 多校准能有效提升代码大模型置信度的可靠性，且考虑问题相关因素有助于更精细的校准，为未来代码LLM校准研究提供了数据基础和方法参考。

Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.

</details>


### [33] [SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA](https://arxiv.org/abs/2512.08867)
*Jing Zhang,Lianghong Guo,Yanlin Wang,Mingwei Liu,Jiachi Chen,Yuchi Ma,Ensheng Shi,Terry Yue Zhuo,Hongyu Zhang,Zibin Zheng*

Main category: cs.SE

TL;DR: 该论文提出并研究了开发知识问答（Dev Knowledge QA）任务，发现其在真实用户与大语言模型（LLM）交互中占比最高（39.6%），但现有基准数据集覆盖范围有限且缺乏真实用户查询。为此，作者构建了一个基于真实对话的多语言基准SimpleDevQA（含2740个问答对，涵盖英、中、俄三语），并通过实验揭示了代码专用LLM优于通用LLM、RAG策略可提升准确率11.3%、LLM存在系统性过度自信等关键发现。


<details>
  <summary>Details</summary>
Motivation: 现有开发知识问答基准存在两大局限：一是知识范围狭窄，主要聚焦代码理解而忽略更广泛的开发知识；二是未基于真实用户查询构建，难以真实评估LLM在实际开发场景中的知识问答能力。

Method: 作者设计了一个三阶段流程，将WildChat中的真实用户-LLM对话转化为简洁的开发知识问答对，并据此构建多语言基准SimpleDevQA。该基准包含2740个具有唯一、简短、可验证答案的问题，覆盖英语、中文和俄语。

Result: 实验表明：(1) 代码专用LLM在Dev Knowledge QA任务上普遍优于同规模通用LLM；(2) 采用检索增强生成（RAG）策略平均提升LLM准确率11.3%；(3) LLM在此任务中表现出系统性过度自信，且其回答准确率与其自述置信度呈正相关；(4) 代码生成能力强的LLM通常在Dev Knowledge QA任务中也表现更强。

Conclusion: Dev Knowledge QA是软件开发中高频且重要的需求，但长期被忽视。通过构建基于真实对话的SimpleDevQA基准，本研究揭示了当前LLM在该任务上的能力与局限，为未来模型优化和评估提供了新方向。

Abstract: The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.

</details>


### [34] [Exploring the Garden of Forking Paths in Empirical Software Engineering Research: A Multiverse Analysis](https://arxiv.org/abs/2512.08910)
*Nathan Cassee,Robert Feldt*

Main category: cs.SE

TL;DR: 该论文通过多宇宙分析（multiverse analysis）对一篇已发表的软件工程实证研究进行复现，发现仅有不到0.2%的分析路径能复现原始结果，表明分析决策对结论影响巨大，呼吁研究者加强稳健性检验并提供方法选择的明确理由。


<details>
  <summary>Details</summary>
Motivation: 在实证软件工程研究中，研究者在数据处理、操作化定义和统计模型选择上具有高度自由，这种“分叉路径花园”虽具灵活性，却可能损害研究的稳健性和可复现性。作者旨在量化这种分析自由带来的风险。

Method: 作者选取一篇典型的软件仓库挖掘（MSR）研究，识别出9个关键分析决策点（每个至少有一个同样合理的替代方案），系统构建并运行全部3,072种可能的分析流程（即“多宇宙”），比较其结果与原始论文的一致性。

Result: 在3,072种分析路径中，仅有6种（<0.2%）复现了原始论文的结果；绝大多数路径得出了定性不同甚至相反的结论。

Conclusion: 分析决策对实证软件工程研究结果的影响远超通常认知。作者建议研究者应通过多宇宙分析等稳健性检验来评估结果可靠性，并提出一个结构化分类模型以帮助阐明和论证方法选择，从而提升研究的可复现性与科学性。

Abstract: In empirical software engineering (SE) research, researchers have considerable freedom to decide how to process data, what operationalizations to use, and which statistical model to fit. Gelman and Loken refer to this freedom as leading to a "garden of forking paths". Although this freedom is often seen as an advantage, it also poses a threat to robustness and replicability: variations in analytical decisions, even when justifiable, can lead to divergent conclusions.
  To better understand this risk, we conducted a so-called multiverse analysis on a published empirical SE paper. The paper we picked is a Mining Software Repositories study, as MSR studies commonly use non-trivial statistical models to analyze post-hoc, observational data. In the study, we identified nine pivotal analytical decisions-each with at least one equally defensible alternative and systematically reran all the 3,072 resulting analysis pipelines on the original dataset. Interestingly, only 6 of these universes (<0.2%) reproduced the published results; the overwhelming majority produced qualitatively different, and sometimes even opposite, findings.
  This case study of a data analytical method commonly applied to empirical software engineering data reveals how methodological choices can exert a more profound influence on outcomes than is often acknowledged. We therefore advocate that SE researchers complement standard reporting with robustness checks across plausible analysis variants or, at least, explicitly justify each analytical decision. We propose a structured classification model to help classify and improve justification for methodological choices. Secondly, we show how the multiverse analysis is a practical tool in the methodological arsenal of SE researchers, one that can help produce more reliable, reproducible science.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [35] [NeurIDA: Dynamic Modeling for Effective In-Database Analytics](https://arxiv.org/abs/2512.08483)
*Lingze Zeng,Naili Xing,Shaofeng Cai,Peng Lu,Gang Chen,Jian Pei,Beng Chin Ooi*

Main category: cs.DB

TL;DR: NeurIDA 是一个端到端的数据库内分析系统，通过动态调整预训练的基础模型以适应不同分析任务，支持自然语言查询并显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型在关系型数据库环境中存在静态性和任务特定性问题，难以应对多样化的动态分析需求，导致开发成本高、ML 在数据库分析中难以广泛采用。

Method: 提出 NeurIDA 系统，采用动态数据库内建模范式，预训练可组合的基础模型架构；根据任务和数据特征动态选择与配置模型组件，并结合大语言模型支持自然语言查询与分析报告生成。

Result: 在五个真实数据集上的十项任务中，NeurIDA 相比基线方法最高提升 12% 的 AUC-ROC，平均绝对误差（MAE）降低 25%。

Conclusion: NeurIDA 实现了高效、易用且高性能的数据库内 AI 分析，有效解决了传统 ML 模型在 RDBMS 中部署与适配的难题。

Abstract: Relational Database Management Systems (RDBMS) manage complex, interrelated data and support a broad spectrum of analytical tasks. With the growing demand for predictive analytics, the deep integration of machine learning (ML) into RDBMS has become critical. However, a fundamental challenge hinders this evolution: conventional ML models are static and task-specific, whereas RDBMS environments are dynamic and must support diverse analytical queries. Each analytical task entails constructing a bespoke pipeline from scratch, which incurs significant development overhead and hence limits wide adoption of ML in analytics.
  We present NeurIDA, an autonomous end-to-end system for in-database analytics that dynamically "tweaks" the best available base model to better serve a given analytical task. In particular, we propose a novel paradigm of dynamic in-database modeling to pre-train a composable base model architecture over the relational data. Upon receiving a task, NeurIDA formulates the task and data profile to dynamically select and configure relevant components from the pool of base models and shared model components for prediction. For friendly user experience, NeurIDA supports natural language queries; it interprets user intent to construct structured task profiles, and generates analytical reports with dedicated LLM agents. By design, NeurIDA enables ease-of-use and yet effective and efficient in-database AI analytics. Extensive experiment study shows that NeurIDA consistently delivers up to 12% improve- ment in AUC-ROC and 25% relative reduction in MAE across ten tasks on five real-world datasets. The source code is available at https://github.com/Zrealshadow/NeurIDA

</details>


### [36] [Analyzing Deviations from Monotonic Trends through Database Repair](https://arxiv.org/abs/2512.08526)
*Shunit Agmon,Jonathan Gal,Amir Gilad,Ester Livshits,Or Mutay,Brit Youngmann,Benny Kimelfeld*

Main category: cs.DB

TL;DR: 本文提出聚合序依赖（AOD）来量化数据集中违反预期单调趋势的程度，并研究了通过删除最少元组使数据满足AOD的修复问题，设计了通用算法框架、优化策略与高效启发式方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现实数据集中常出现违背预期单调趋势的现象（如教育水平越高平均薪资反而越低），现有方法难以有效度量和修复此类偏差，因此需要一种能刻画聚合值与分组属性之间单调关系的形式化模型。

Method: 引入聚合序依赖（AOD）概念，将AOD修复问题建模为最小元组删除问题；提出通用算法模板，并针对常用聚合函数进行实例化；结合优化技术和启发式策略提升效率。

Result: 在真实与合成数据集上的实验表明所提算法具有良好的实际效率，启发式方法表现接近最优解；案例研究展示了该框架在发现和解释异常AOD违反方面的实用性。

Conclusion: 聚合序依赖（AOD）为量化和修复数据中违反单调趋势的问题提供了有效工具，所提出的算法和优化策略在实践中高效可行，有助于提升数据质量和可解释性。

Abstract: Datasets often exhibit violations of expected monotonic trends - for example, higher education level correlating with higher average salary, newer homes being more expensive, or diabetes prevalence increasing with age. We address the problem of quantifying how far a dataset deviates from such trends. To this end, we introduce Aggregate Order Dependencies (AODs), an aggregation-centric extension of the previously studied order dependencies. An AOD specifies that the aggregated value of a target attribute (e.g., mean salary) should monotonically increase or decrease with the grouping attribute (e.g., education level).
  We formulate the AOD repair problem as finding the smallest set of tuples to delete from a table so that the given AOD is satisfied. We analyze the computational complexity of this problem and propose a general algorithmic template for solving it. We instantiate the template for common aggregation functions, introduce optimization techniques that substantially improve the runtime of the template instances, and develop efficient heuristic alternatives. Our experimental study, carried out on both real-world and synthetic datasets, demonstrates the practical efficiency of the algorithms and provides insight into the performance of the heuristics. We also present case studies that uncover and explain unexpected AOD violations using our framework.

</details>


### [37] [Causal Explanations for Disparate Trends: Where and Why?](https://arxiv.org/abs/2512.08679)
*Tal Blau,Brit Youngmann,Anna Fariha,Yuval Moskovitch*

Main category: cs.DB

TL;DR: ExDis 是一个用于发现两组间差异的因果解释的新框架，能识别差异最显著的子群体及其因果因素，并在真实数据集上验证了其有效性与可扩展性。


<details>
  <summary>Details</summary>
Motivation: 在数据分析中，理解两组之间的差异需要可解释且可操作的因果解释，而现有方法难以在高维大数据集中自动发现此类解释。

Method: 提出 ExDis 框架，形式化定义差异因果解释的优化问题，分析其复杂性，并设计高效算法求解，以识别差异显著的子群体及其中的因果因素。

Result: 在三个真实数据集上的实验表明，ExDis 能生成有意义的因果解释，在性能上优于已有方法，并能有效扩展到大规模高维数据。

Conclusion: ExDis 提供了一种可解释、可操作且高效的因果解释方法，有助于用户基于数据做出明智决策，适用于复杂高维场景下的差异分析。

Abstract: During data analysis, we are often perplexed by certain disparities observed between two groups of interest within a dataset. To better understand an observed disparity, we need explanations that can pinpoint the data regions where the disparity is most pronounced, along with its causes, i.e., factors that alleviate or exacerbate the disparity. This task is complex and tedious, particularly for large and high-dimensional datasets, demanding an automatic system for discovering explanations (data regions and causes) of an observed disparity. It is critical that explanations for disparities are not only interpretable but also actionable-enabling users to make informed, data-driven decisions. This requires explanations to go beyond surface-level correlations and instead capture causal relationships. We introduce ExDis, a framework for discovering causal Explanations for Disparities between two groups of interest. ExDis identifies data regions (subpopulations) where disparities are most pronounced (or reversed), and associates specific factors that causally contribute to the disparity within each identified data region. We formally define the ExDis framework and the associated optimization problem, analyze its complexity, and develop an efficient algorithm to solve the problem. Through extensive experiments over three real-world datasets, we demonstrate that ExDis generates meaningful causal explanations, outperforms prior methods, and scales effectively to handle large, high-dimensional datasets.

</details>
