<div id=toc></div>

# Table of Contents

- [cs.AR](#cs.AR) [Total: 1]
- [cs.CE](#cs.CE) [Total: 4]
- [cs.SE](#cs.SE) [Total: 3]
- [cs.DB](#cs.DB) [Total: 6]
- [cs.DC](#cs.DC) [Total: 9]


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [1] [PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration](https://arxiv.org/abs/2512.11550)
*Yifan Zhang,Zhiheng Chen,Ye Qiao,Sitao Huang*

Main category: cs.AR

TL;DR: 本文提出PD-Swap，一种基于动态局部重配置（DPR）的预填充-解码分离式LLM加速器，通过在边缘FPGA上时分复用注意力模块，在不增加面积成本的前提下显著提升长上下文推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 边缘FPGA部署超低比特大语言模型（如1.58-bit BitNet）时，随着上下文长度增长，预填充阶段的二次复杂度和KV缓存带宽需求导致性能急剧下降；而传统静态加速器难以兼顾计算密集的预填充与内存带宽受限的解码阶段，造成资源浪费和模型规模受限。

Method: 设计PD-Swap加速器，将注意力模块划分为可重配置区域，包含两个专用架构：面向预填充的高计算并行引擎和面向解码的KV缓存优化引擎；核心三值矩阵乘法与权重缓冲保持静态；利用屋顶线模型与设计空间探索联合优化重配置区域大小、并行度及布线约束，并通过计算掩盖重配置延迟。

Result: PD-Swap在边缘FPGA上实现最高27 tokens/s的解码吞吐量，相比现有最先进方案提升1.3–2.1倍（上下文越长增益越大），且未增加额外硬件面积。

Conclusion: 通过预填充与解码阶段的架构解耦和动态重配置，PD-Swap有效解决了长上下文下边缘LLM加速器的性能瓶颈，在有限资源下实现了高效推理。

Abstract: Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [2] [Integrated Prediction and Multi-period Portfolio Optimization](https://arxiv.org/abs/2512.11273)
*Qi Deng,Yuxuan Linghu,Zhiyuan Liu*

Main category: cs.CE

TL;DR: 本文提出IPMO模型，将多期收益预测与带换手惩罚的均值-方差投资组合优化端到端集成，通过可微凸优化层和高效的MDFP梯度计算方法，在真实市场数据上显著优于传统两阶段方法。


<details>
  <summary>Details</summary>
Motivation: 传统多期投资组合优化采用两阶段框架，先预测收益再进行优化，导致预测与决策目标不一致且忽略交易成本，难以获得最优投资表现。

Method: 提出IPMO模型，将时间序列预测模型与含换手惩罚的多期均值-方差优化问题整合为端到端可微架构；引入镜像下降不动点（MDFP）微分方案，避免KKT系统分解，实现稳定梯度与良好扩展性。

Result: 在真实市场数据和两种代表性预测模型上的实验表明，IPMO在扣除交易成本后的风险调整收益和资产配置路径一致性方面均优于两阶段基准方法。

Conclusion: 将机器学习预测与多期投资组合优化端到端集成能有效提升金融绩效，同时保持计算可行性。

Abstract: Multi-period portfolio optimization is important for real portfolio management, as it accounts for transaction costs, path-dependent risks, and the intertemporal structure of trading decisions that single-period models cannot capture. Classical methods usually follow a two-stage framework: machine learning algorithms are employed to produce forecasts that closely fit the realized returns, and the predicted values are then used in a downstream portfolio optimization problem to determine the asset weights. This separation leads to a fundamental misalignment between predictions and decision outcomes, while also ignoring the impact of transaction costs. To bridge this gap, recent studies have proposed the idea of end-to-end learning, integrating the two stages into a single pipeline. This paper introduces IPMO (Integrated Prediction and Multi-period Portfolio Optimization), a model for multi-period mean-variance portfolio optimization with turnover penalties. The predictor generates multi-period return forecasts that parameterize a differentiable convex optimization layer, which in turn drives learning via portfolio performance. For scalability, we introduce a mirror-descent fixed-point (MDFP) differentiation scheme that avoids factorizing the Karush-Kuhn-Tucker (KKT) systems, which thus yields stable implicit gradients and nearly scale-insensitive runtime as the decision horizon grows. In experiments with real market data and two representative time-series prediction models, the IPMO method consistently outperforms the two-stage benchmarks in risk-adjusted performance net of transaction costs and achieves more coherent allocation paths. Our results show that integrating machine learning prediction with optimization in the multi-period setting improves financial outcomes and remains computationally tractable.

</details>


### [3] [Task-Specific Sparse Feature Masks for Molecular Toxicity Prediction with Chemical Language Models](https://arxiv.org/abs/2512.11412)
*Kwun Sy Lee,Jiawei Chen,Fuk Sheng Ford Chung,Tianyu Zhao,Zhenyuan Chen,Debby D. Wang*

Main category: cs.CE

TL;DR: 本文提出了一种新的多任务学习框架，通过在共享化学语言模型上引入带L1稀疏惩罚的任务特定注意力模块，在提升毒性预测准确性的同时，增强了模型的可解释性，并在多个基准数据集上取得优于基线方法的表现。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的分子毒性预测模型多为黑箱模型，缺乏可解释性，难以满足药物研发中对高风险安全决策所需的结构化依据，因此亟需兼具高准确率与可解释性的新方法。

Method: 构建一个端到端训练的多任务学习框架，整合共享的化学语言模型与任务特定的注意力模块，并在注意力权重上施加L1稀疏性约束，使模型聚焦于每个毒性终点最关键的分子片段。

Result: 在ClinTox、SIDER和Tox21数据集上的实验表明，该方法在预测性能上优于单任务和标准多任务学习基线，同时生成的稀疏注意力权重能提供直观的化学片段可视化。

Conclusion: 所提出的稀疏注意力多任务学习框架有效平衡了预测准确性和模型可解释性，为药物发现中的毒性预测提供了可靠且透明的工具。

Abstract: Reliable in silico molecular toxicity prediction is a cornerstone of modern drug discovery, offering a scalable alternative to experimental screening. However, the black-box nature of state-of-the-art models remains a significant barrier to adoption, as high-stakes safety decisions demand verifiable structural insights alongside predictive performance. To address this, we propose a novel multi-task learning (MTL) framework designed to jointly enhance accuracy and interpretability. Our architecture integrates a shared chemical language model with task-specific attention modules. By imposing an L1 sparsity penalty on these modules, the framework is constrained to focus on a minimal set of salient molecular fragments for each distinct toxicity endpoint. The resulting framework is trained end-to-end and is readily adaptable to various transformer-based backbones. Evaluated on the ClinTox, SIDER, and Tox21 benchmark datasets, our approach consistently outperforms both single-task and standard MTL baselines. Crucially, the sparse attention weights provide chemically intuitive visualizations that reveal the specific fragments influencing predictions, thereby enhancing insight into the model's decision-making process.

</details>


### [4] [Contiguous Storage of Grid Data for Heterogeneous Computing](https://arxiv.org/abs/2512.11473)
*Fan Gu,Xiangyu Hu*

Main category: cs.CE

TL;DR: 本文提出了一种面向GPU优化的结构化笛卡尔网格存储架构，通过SYCL统一编程模型提升稀疏网格及网格-粒子耦合模拟在异构平台上的性能与可移植性。


<details>
  <summary>Details</summary>
Motivation: 现有结构化笛卡尔网格框架主要针对CPU优化，在GPU上因并行性受限和内存访问延迟高而表现不佳，难以高效支持稀疏域中的数值模拟。

Method: 设计了一种新的GPU兼容存储架构，抽象底层GPU细节，采用基于SYCL的统一编程模型，实现主机与设备环境的无缝集成。

Result: 所提架构简化了用户GPU编程，同时在稀疏网格和网格-粒子耦合数值模拟中展现出更好的可扩展性和跨平台可移植性。

Conclusion: 该工作有效解决了传统结构化网格在GPU上效率低下的问题，为异构计算环境下的高性能数值模拟提供了实用且可移植的解决方案。

Abstract: Structured Cartesian grids are a fundamental component in numerical simulations. Although these grids facilitate straightforward discretization schemes, their naïve use in sparse domains leads to excessive memory overhead and inefficient computation. Existing frameworks address are primarily optimized for CPU execution and exhibit performance bottlenecks on GPU architectures due to limited parallelism and high memory access latency. This work presents a redesigned storage architecture optimized for GPU compatibility and efficient execution across heterogeneous platforms. By abstracting low-level GPU-specific details and adopting a unified programming model based on SYCL, the proposed data structure enables seamless integration across host and device environments. This architecture simplifies GPU programming for end-users while improving scalability and portability in sparse-grid and gird-particle coupling numerical simulations.

</details>


### [5] [Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation](https://arxiv.org/abs/2512.11748)
*Mohammed El Fallaki Idrissi,Jad Mounayer,Sebastian Rodriguez,Fodil Meraghni,Francisco Chinesta*

Main category: cs.CE

TL;DR: 本文提出了一种名为生成式参数化设计（GPD）的新框架，结合两个秩降维自编码器（RRAE）分别处理几何设计与稀疏PGD模态解，并通过潜在空间中的回归技术实现二者关联，从而支持高效的设计探索、优化及数字/混合孪生应用。


<details>
  <summary>Details</summary>
Motivation: 传统仿真驱动的工程设计方法在处理多参数设计与实时决策时效率较低，缺乏将几何设计与其对应的参数化解高效耦合的能力，限制了设计探索和数字孪生的发展。

Method: 提出GPD框架，利用两个RRAE分别编码生成几何设计和sPGD模态解，并在潜在空间中通过回归技术建立两者映射关系，实现从设计到参数化解的快速生成。

Result: 该框架成功应用于双相微结构问题，能有效处理两个关键材料参数变化下的多参数解，支持高效设计生成与实时预测。

Conclusion: GPD框架为仿真驱动的工程科学提供了一种新范式，显著提升了设计探索、优化能力以及数字/混合孪生系统的预测与决策性能。

Abstract: This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [6] [Coverage Isn't Enough: SBFL-Driven Insights into Manually Created vs. Automatically Generated Tests](https://arxiv.org/abs/2512.11223)
*Sasara Shimizu,Yoshiki Higo*

Main category: cs.SE

TL;DR: 本文比较了自动生成测试用例与手动编写测试用例在分支覆盖率和基于频谱的故障定位（SBFL）得分上的表现，发现自动生成测试覆盖更高但SBFL得分较低，尤其在深层嵌套代码中。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注测试用例的覆盖率，较少评估其在故障定位（特别是通过变异测试引入的人工故障）中的有效性；本文旨在通过引入SBFL得分这一较少使用的指标，深入比较两类测试方法的优劣。

Method: 通过实验对比自动生成测试与手动测试在分支覆盖率和SBFL得分上的差异，使用SBFL得分作为衡量故障定位准确性的主要指标。

Result: 自动生成的测试用例实现了更高的分支覆盖率，但在SBFL得分上低于手动测试，尤其在处理深层嵌套结构的代码时表现更差。

Conclusion: 自动生成和手动创建的测试各有优势，应结合使用以提升整体测试效果，特别是在需要高故障定位精度的场景中。

Abstract: The testing phase is an essential part of software development, but manually creating test cases can be time-consuming. Consequently, there is a growing need for more efficient testing methods. To reduce the burden on developers, various automated test generation tools have been developed, and several studies have been conducted to evaluate the effectiveness of the tests they produce. However, most of these studies focus primarily on coverage metrics, and only a few examine how well the tests support fault localization-particularly using artificial faults introduced through mutation testing. In this study, we compare the SBFL (Spectrum-Based Fault Localization) score and code coverage of automatically generated tests with those of manually created tests. The SBFL score indicates how accurately faults can be localized using SBFL techniques. By employing SBFL score as an evaluation metric-an approach rarely used in prior studies on test generation-we aim to provide new insights into the respective strengths and weaknesses of manually created and automatically generated tests. Our experimental results show that automatically generated tests achieve higher branch coverage than manually created tests, but their SBFL score is lower, especially for code with deeply nested structures. These findings offer guidance on how to effectively combine automatically generated and manually created testing approaches.

</details>


### [7] [REMODEL-LLM: Transforming C code to Java using LLMs](https://arxiv.org/abs/2512.11402)
*Aryan Gupta,Y. Raghu Reddy*

Main category: cs.SE

TL;DR: 本文评估了19个参数小于200亿的量化大语言模型（LLM）在C到Java代码自动翻译任务中的表现，提出了一种结合抽象语法树（AST）与规则约束提示的混合流程。结果表明，绝大多数模型无法生成可运行的Java代码，仅三个模型（phi4、deepseek-coder-v2、codeqwen）表现尚可，但在处理函数指针、sizeof和枚举等复杂C语言特性时仍存在明显局限。


<details>
  <summary>Details</summary>
Motivation: C到Java的自动翻译面临范式差异（过程式 vs 面向对象）、内存模型不同（手动指针 vs 垃圾回收）及数据类型不兼容等根本性挑战，现有方法效果有限，亟需探索轻量级量化LLM在此任务中的潜力与瓶颈。

Method: 采用一种新颖的混合流水线方法：利用抽象语法树（AST）进行语义分解，并结合高度约束的基于规则的提示策略，对19个小型量化LLM进行系统评估。

Result: 模型表现呈现明显三级分化：大多数模型（Tier 3）完全失败；少数中等模型（Tier 2）可生成可运行代码但存在严重语义错误；仅三个模型（Tier 1）通过超50%测试用例，但仍无法正确处理函数指针、sizeof和enum等复杂C特性。

Conclusion: 当前的小型量化大语言模型在C到Java翻译任务中能力有限，即使表现最佳的模型也难以处理C语言中的高级特性，揭示了其在复杂代码推理任务上的能力上限。

Abstract: The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.

</details>


### [8] [Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models](https://arxiv.org/abs/2512.11482)
*Melih Catal,Pooja Rani,Harald C. Gall*

Main category: cs.SE

TL;DR: 本文首次系统评估了差分隐私（DP）在代码大语言模型（CodeLLMs）中的应用效果，发现DP能显著降低模型对训练数据的记忆风险，同时基本保持甚至略微提升其代码生成能力，且对训练效率和能耗影响甚微。


<details>
  <summary>Details</summary>
Motivation: CodeLLMs可能无意中记忆并复现训练数据中的代码片段，带来隐私泄露和知识产权侵犯的风险，限制了其在敏感领域的部署；因此需要在不损害模型性能的前提下缓解记忆问题。

Method: 应用差分隐私（DP）技术，在CodeLLMs微调过程中注入校准噪声，并系统评估DP对减少记忆行为和保留代码生成能力的影响。

Result: DP显著减少了各类代码片段的记忆现象，尤其对最易被记忆的片段效果最好；虽然略微增加困惑度，但代码生成能力得以保留甚至增强；同时DP对训练时间和能耗影响不大。

Conclusion: 差分隐私是一种实用且有效的方法，可在保护隐私、防止知识产权侵权的同时，维持CodeLLMs的实用性能，适合用于隐私保护场景下的模型训练。

Abstract: Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [9] [Query Optimization Beyond Data Systems: The Case for Multi-Agent Systems](https://arxiv.org/abs/2512.11001)
*Zoi Kaoudi,Ioana Giurgiu*

Main category: cs.DB

TL;DR: 本文提出了一种面向多智能体工作流的新型查询优化框架构想，旨在解决当前智能体架构在通用性、可扩展性和系统性优化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型（LLM）的多智能体工作流构建方法较为随意，缺乏通用性、可扩展性及系统性优化能力，难以高效处理异构数据源和查询引擎中的多智能体协同问题。

Method: 通过分析多智能体工作流的特点，结合现实案例，重新设计查询优化原则，提出一个支持自动化模型选择、工作流组合与跨异构引擎执行的多智能体查询优化框架构想。

Result: 论文勾勒出该框架的核心架构，并识别了实现过程中面临的主要研究挑战，包括多样化智能体的编排、昂贵LLM调用下的成本效率以及任务冗余等问题。

Conclusion: 该构想为新兴多智能体架构中的查询优化奠定了基础，并指明了若干未来研究方向。

Abstract: The proliferation of large language models (LLMs) has accelerated the adoption of agent-based workflows, where multiple autonomous agents reason, invoke functions, and collaborate to compose complex data pipelines. However, current approaches to building such agentic architectures remain largely ad hoc, lacking generality, scalability, and systematic optimization. Existing systems often rely on fixed models and single execution engines and are unable to efficiently optimize multiple agents operating over heterogeneous data sources and query engines. This paper presents a vision for a next-generation query optimization framework tailored to multi-agent workflows. We argue that optimizing these workflows can benefit from redesigning query optimization principles to account for new challenges: orchestration of diverse agents, cost efficiency under expensive LLM calls and across heterogeneous engines, and redundancy across tasks. Led by a real-world example and building on an analysis of multi-agent workflows, we outline our envisioned architecture and the main research challenges of building a multi-agent query optimization framework, which aims at enabling automated model selection, workflow composition, and execution across heterogeneous engines. This vision establishes the groundwork for query optimization in emerging multi-agent architectures and opens up a set of future research directions.

</details>


### [10] [KathDB: Explainable Multimodal Database Management System with Human-AI Collaboration](https://arxiv.org/abs/2512.11067)
*Guorui Xiao,Enhao Zhang,Nicole Sullivan,Will Hansen,Magdalena Balazinska*

Main category: cs.DB

TL;DR: KathDB 是一个融合关系型数据库语义与基础模型推理能力的新系统，支持多模态数据查询，并在查询解析、执行和结果解释过程中引入人机交互机制，以提供可解释且跨模态的答案。


<details>
  <summary>Details</summary>
Motivation: 传统 DBMS 虽然对结构化数据提供强语义保证和高级查询优化，但难以编写复杂 SQL；而现有支持多模态数据的系统要么依赖用户手动编写机器学习 UDF，要么完全依赖黑盒大语言模型，牺牲了可用性或可解释性。

Method: 提出 KathDB 系统，将关系型语义与基础模型在多模态数据上的推理能力相结合，并在查询解析、执行和结果解释阶段嵌入人机交互通道。

Result: 用户能够通过迭代方式获得跨模态、可解释的查询结果，兼顾可用性与可解释性。

Conclusion: KathDB 有效弥合了传统关系型数据库与现代多模态系统之间的鸿沟，在保持语义严谨性的同时提升了多模态查询的交互性与透明度。

Abstract: Traditional DBMSs execute user- or application-provided SQL queries over relational data with strong semantic guarantees and advanced query optimization, but writing complex SQL is hard and focuses only on structured tables. Contemporary multimodal systems (which operate over relations but also text, images, and even videos) either expose low-level controls that force users to use (and possibly create) machine learning UDFs manually within SQL or offload execution entirely to black-box LLMs, sacrificing usability or explainability. We propose KathDB, a new system that combines relational semantics with the reasoning power of foundation models over multimodal data. Furthermore, KathDB includes human-AI interaction channels during query parsing, execution, and result explanation, such that users can iteratively obtain explainable answers across data modalities.

</details>


### [11] [Acyclic Conjunctive Regular Path Queries are no Harder than Corresponding Conjunctive Queries](https://arxiv.org/abs/2512.11129)
*Mahmoud Abo Khamis,Alexandru-Mihai Hurjui,Ahmet Kara,Dan Olteanu,Dan Suciu*

Main category: cs.DB

TL;DR: 本文提出了一种用于评估无环连接正则路径查询（CRPQ）的输出敏感算法，其复杂度与对应连接查询（CQ）的最佳已知输出敏感复杂度一致，表明CRPQ中的递归特性在输出敏感分析下未引入额外复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有针对无环CRPQ的输出敏感算法在复杂度上仍有改进空间；同时，由于CRPQ等价于递归Datalog程序，其复杂性尚不清晰，因此有必要探究其是否比对应的非递归CQ更复杂。

Method: 设计一种新的输出敏感算法，其复杂度依赖于输入大小、输出大小以及查询的“free-connex fractional hypertree width”参数，并将其与对应CQ的复杂度进行对比分析。

Result: 所提算法在复杂度上优于近期提出的同类算法，且对于任意无环CRPQ Q，其复杂度与结构对应的CQ的最佳输出敏感复杂度一致。

Conclusion: 无环CRPQ在输出敏感评估中并不比其对应的非递归CQ更复杂，表明递归特性在此类查询中未带来额外复杂度；任何对CRPQ复杂度的进一步改进都将依赖于CQ领域的新突破。

Abstract: We present an output-sensitive algorithm for evaluating an acyclic Conjunctive Regular Path Query (CRPQ). Its complexity is written in terms of the input size, the output size, and a well-known parameter of the query that is called the "free-connex fractional hypertree width". Our algorithm improves upon the complexity of the recently introduced output-sensitive algorithm for acyclic CRPQs. More notably, the complexity of our algorithm for a given acyclic CRPQ Q matches the best known output-sensitive complexity for the "corresponding" conjunctive query (CQ), that is the CQ that has the same structure as the CRPQ Q except that each RPQ is replaced with a binary atom (or a join of two binary atoms). This implies that it is not possible to improve upon our complexity for acyclic CRPQs without improving the state-of-the-art on output-sensitive evaluation for acyclic CQs. Our result is surprising because RPQs, and by extension CRPQs, are equivalent to recursive Datalog programs, which are generally poorly understood from a complexity standpoint. Yet, our result implies that the recursion aspect of acyclic CRPQs does not add any extra complexity on top of the corresponding (non-recursive) CQs, at least as far as output-sensitive analysis is concerned.

</details>


### [12] [Benchmarking RL-Enhanced Spatial Indices Against Traditional, Advanced, and Learned Counterparts](https://arxiv.org/abs/2512.11161)
*Guanli Liu,Renata Borovica-Gajic,Hai Lan,Zhifeng Bao*

Main category: cs.DB

TL;DR: 本文提出了首个面向强化学习增强空间索引（RLESIs）的模块化基准框架，并通过全面实验发现，尽管RLESIs在调优后可降低查询延迟，但在查询效率和构建开销方面普遍不如先进传统索引和学习型索引。


<details>
  <summary>Details</summary>
Motivation: 现有RLESIs缺乏统一实现与系统评估，尤其在基于磁盘的环境中其实际优势尚不明确，亟需一个可复现、可扩展的评估平台以厘清其性能边界与适用场景。

Method: 构建了一个模块化、可扩展的基准框架，基于现有空间索引库，将索引训练与构建解耦，支持参数调优，并实现与传统、先进及学习型空间索引的一致性对比；在六个数据集和多种查询负载下，使用延迟、I/O和索引统计指标对12种代表性索引进行评估。

Result: 实验表明，RLESIs虽可通过调优减少查询延迟，但在查询效率和索引构建成本上始终逊于学习型和先进传统索引；其高调优成本与泛化能力有限制约了实际应用。

Conclusion: RLESIs虽具备良好的架构兼容性，但当前在性能和实用性方面尚未超越现有方法，未来需降低调优开销并提升泛化能力以推动实际部署。

Abstract: Reinforcement learning has recently been used to enhance index structures, giving rise to reinforcement learning-enhanced spatial indices (RLESIs) that aim to improve query efficiency during index construction. However, their practical benefits remain unclear due to the lack of unified implementations and comprehensive evaluations, especially in disk-based settings.
  We present the first modular and extensible benchmark for RLESIs. Built on top of an existing spatial index library, our framework decouples index training from building, supports parameter tuning, and enables consistent comparison with traditional, advanced, and learned spatial indices.
  We evaluate 12 representative spatial indices across six datasets and diverse workloads, including point, range, kNN, spatial join, and mixed read/write queries. Using latency, I/O, and index statistics as metrics, we find that while RLESIs can reduce query latency with tuning, they consistently underperform learned spatial indices and advanced variants in both query efficiency and index build cost. These findings highlight that although RLESIs offer promising architectural compatibility, their high tuning costs and limited generalization hinder practical adoption.

</details>


### [13] [A Cross-Chain Event-Driven Data Infrastructure for Aave Protocol Analytics and Applications](https://arxiv.org/abs/2512.11363)
*Junyi Fan,Li Sun*

Main category: cs.DB

TL;DR: 本文构建了首个覆盖六条主流EVM链的Aave V3事件级数据集，包含超过5000万条结构化记录，支持对去中心化借贷市场的细粒度研究。


<details>
  <summary>Details</summary>
Motivation: 现有针对去中心化借贷协议（如Aave V3）的实证研究受限于缺乏标准化、跨链的事件级数据集，阻碍了对资本流动、利率机制和系统性风险等关键问题的深入分析。

Method: 通过开源Python管道，从各链部署区块至2025年10月采集并完整解码Aave V3的八类核心事件，结合动态批处理与自动分片策略，生成带区块元数据和美元估值的结构化数据集。

Result: 成功构建涵盖以太坊、Arbitrum、Optimism、Polygon、Avalanche和Base六条链的高质量、可复现、公开可用的Aave V3事件数据集，包含超5000万条记录。

Conclusion: 该数据基础设施为研究去中心化借贷市场中的资金流、利率动态、清算级联及跨链用户行为提供了基础资源，有望推动相关领域的实证研究发展。

Abstract: Decentralized lending protocols, exemplified by Aave V3, have transformed financial intermediation by enabling permissionless, multi-chain borrowing and lending without intermediaries. Despite managing over $10 billion in total value locked, empirical research remains severely constrained by the lack of standardized, cross-chain event-level datasets.
  This paper introduces the first comprehensive, event-driven data infrastructure for Aave V3 spanning six major EVM-compatible chains (Ethereum, Arbitrum, Optimism, Polygon, Avalanche, and Base) from respective deployment blocks through October 2025. We collect and fully decode eight core event types -- Supply, Borrow, Withdraw, Repay, LiquidationCall, FlashLoan, ReserveDataUpdated, and MintedToTreasury -- producing over 50 million structured records enriched with block metadata and USD valuations.
  Using an open-source Python pipeline with dynamic batch sizing and automatic sharding (each file less than or equal to 1 million rows), we ensure strict chronological ordering and full reproducibility. The resulting publicly available dataset enables granular analysis of capital flows, interest rate dynamics, liquidation cascades, and cross-chain user behavior, providing a foundational resource for future studies on decentralized lending markets and systemic risk.

</details>


### [14] [Bridging Textual Data and Conceptual Models: A Model-Agnostic Structuring Approach](https://arxiv.org/abs/2512.11403)
*Jacques Chabin,Mirian Halfeld Ferrari,Nicolas Hiot*

Main category: cs.DB

TL;DR: 本文提出了一种自动化方法，将文本数据结构化为与模型无关的模式，并同时生成该模式及其实例，以临床医学案例作为验证。


<details>
  <summary>Details</summary>
Motivation: 现有文本数据缺乏通用且自动化的结构化方法，难以适配不同数据库模型；作者旨在构建一种模型无关的方案，实现文本到结构化数据的通用转换。

Method: 将文本数据表示为语义增强的语法树，通过基于属性文法元模型 \metaG 的迭代树重写和文法提取，逐步精炼生成通用模式及其实例。

Result: 该方法成功应用于临床医学案例，证明其能有效生成与任意数据库模型对齐的结构化模式及其实例。

Conclusion: 所提出的方法提供了一种通用、自动化的文本结构化途径，具备良好的模型无关性和实际应用潜力，尤其适用于如医疗等复杂领域。

Abstract: We introduce an automated method for structuring textual data into a model-agnostic schema, enabling alignment with any database model. It generates both a schema and its instance. Initially, textual data is represented as semantically enriched syntax trees, which are then refined through iterative tree rewriting and grammar extraction, guided by the attribute grammar meta-model \metaG. The applicability of this approach is demonstrated using clinical medical cases as a proof of concept.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [15] [Agentic Operator Generation for ML ASICs](https://arxiv.org/abs/2512.10977)
*Alec M. Hammond,Aram Markosyan,Aman Dontula,Simon Mahns,Zacharias Fisches,Dmitrii Pedchenko,Keyur Muzumdar,Natacha Supper,Mark Saroufim,Joe Isaacson,Laura Wang,Warren Hunt,Kaustubh Gondkar,Roman Levenstein,Gabriel Synnaeve,Richard Li,Jacob Kahn,Ajit Mathews*

Main category: cs.DC

TL;DR: TritorX 是一个基于大语言模型的智能系统，可大规模生成功能正确的 Triton PyTorch ATen 内核，覆盖 481 个算子并通过超过 2 万个 OpInfo 测试，强调正确性与通用性，适用于新兴加速器平台。


<details>
  <summary>Details</summary>
Motivation: 现有内核生成方法通常只关注少数高频算子的性能，缺乏对整个算子集的覆盖和正确性保障。为支持新加速器平台快速部署完整的 PyTorch 后端，亟需一种能兼顾广泛算子、数据类型、张量形状和参数模式的自动化内核生成方案。

Method: TritorX 结合开源大语言模型、自定义代码检查器（linter）、JIT 编译器以及基于 PyTorch OpInfo 的测试框架，构建了一套端到端的内核生成与验证流水线，兼容真实 MTIA 硬件及下一代设备的仿真环境。

Result: 在实验中，TritorX 成功为 481 个唯一的 ATen 算子生成了通过全部对应 OpInfo 测试（总计超 20,000 项）的内核及封装代码。

Conclusion: TritorX 展示了在一夜之间为新型加速器平台自动生成完整 PyTorch ATen 后端的可行性，为未来硬件生态的快速软件适配提供了有效路径。

Abstract: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.

</details>


### [16] [An Efficient Approach for Energy Conservation in Cloud Computing Environment](https://arxiv.org/abs/2512.10974)
*Sohan Kumar Pande,Sanjaya Kumar Panda,Preeti Ranjan Sahu*

Main category: cs.DC

TL;DR: 本文提出了一种新的任务调度算法，通过综合考虑CPU、磁盘和I/O资源利用率及任务处理时间来提高资源利用效率，从而降低云服务的能耗；仿真实验表明，该算法比现有MaxUtil算法更节能。


<details>
  <summary>Details</summary>
Motivation: 当前云计算服务能耗高，而能源有限且对环境有温室效应，亟需开发能效更高的算法；现有研究多关注平均资源利用率或任务完成时间，但忽略了物理机中多种资源类型的协同利用。

Method: 提出一种任务调度算法，引入一个综合CPU、磁盘、I/O利用率及任务处理时间的适应度函数，以显式提升各类资源的利用率，进而提高活跃资源的整体使用效率。

Result: 通过在合成数据集上对所提算法与MaxUtil算法进行仿真实验，结果表明所提算法在能耗方面表现更优，具有更好的能效。

Conclusion: 所提出的任务调度算法能有效提升多维资源利用率，显著降低云数据中心的能耗，优于现有MaxUtil算法。

Abstract: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.

</details>


### [17] [Seamless Transitions: A Comprehensive Review of Live Migration Technologies](https://arxiv.org/abs/2512.10979)
*Sima Attar-Khorasani,Lincoln Sherpa,Matthias Lieber,Siavash Ghiasvand*

Main category: cs.DC

TL;DR: 本文综述了实时迁移技术，整合现有研究并深入分析容器与虚拟机迁移方法、迁移单元及基础设施特性，探讨其在实际应用中的挑战与适用性差异，并为未来研究提供方向。


<details>
  <summary>Details</summary>
Motivation: 现有综述常忽略实时迁移在真实场景中面临的关键技术细节和实践挑战，本文旨在填补这一空白。

Method: 综合现有文献，从迁移技术、迁移单元和基础设施特征等多个维度对基于容器和虚拟机的实时迁移技术进行系统性分析，并评估其在不同目标和约束下的适用性。

Result: 揭示了容器与虚拟机迁移技术在采纳程度上的差距，阐明了系统因素对迁移效果的影响，并指出在某些情况下迁移的复杂性和资源开销可能超过其收益。

Conclusion: 本文不仅为研究者和实践者提供了关于实时迁移的全面参考，还通过识别当前技术瓶颈，为未来研究与实际部署指明了方向。

Abstract: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.

</details>


### [18] [Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling](https://arxiv.org/abs/2512.10980)
*Akhmadillo Mamirov*

Main category: cs.DC

TL;DR: 本文针对GPU集群利用率低的问题，提出三种动态调度器（HPS、PBS、SBS），在真实混合工作负载下显著提升利用率、吞吐量与公平性，优于传统静态调度策略。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统依赖GPU集群，但实际部署中平均利用率仅约50%，主要受限于资源碎片、异构工作负载及静态调度策略的不足。

Method: 设计并评估三种专用动态调度器：混合优先级调度器（HPS）、预测回填调度器（PBS）和智能批处理调度器（SBS），在包含训练、推理和研究任务的64-GPU、8节点集群上进行1000个AI作业的仿真测试。

Result: 动态调度器显著优于静态基线（FIFO等）：HPS实现最高利用率（78.2%）、最高吞吐量（25.8作业/小时）和最低公平性方差（457），饥饿作业从156降至12；PBS和SBS分别在碎片处理和结构相似作业效率方面表现优异。

Conclusion: 面向异构AI集群的多目标动态调度策略能有效提升GPU利用效率，在吞吐量、等待时间、公平性和饥饿控制方面全面优于单目标启发式方法，为未来生产调度框架提供实用基础。

Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.

</details>


### [19] [Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems](https://arxiv.org/abs/2512.10987)
*Sumit Chongder*

Main category: cs.DC

TL;DR: 该论文比较了集中式分层联邦学习（HFL）与两种去中心化方法——聚合联邦学习（AFL）和持续联邦学习（CFL），发现AFL和CFL在多个指标上优于HFL。


<details>
  <summary>Details</summary>
Motivation: 集中式HFL存在通信瓶颈和隐私问题，因此需要探索去中心化的替代方案以提升联邦学习的性能与安全性。

Method: 在Fashion MNIST和MNIST数据集上对HFL、AFL和CFL三种架构进行实验评估，比较其在精度、召回率、F1分数和平衡准确率等方面的表现。

Result: AFL和CFL在所有评估指标上均优于HFL，表明去中心化聚合机制能更有效地支持分布式设备间的协同模型训练。

Conclusion: 去中心化联邦学习方法（如AFL和CFL）在性能和隐私保护方面具有显著优势，为未来联邦学习研究和实践提供了重要方向。

Abstract: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.

</details>


### [20] [Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.DC

TL;DR: 该论文提出三种GPU原生编译方法以解决AI代码生成中的CPU-GPU数据传输延迟问题，包括并行传统编译、神经编译和混合架构，理论分析表明可实现10-100倍的迭代加速。


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成系统在编译、执行和测试阶段因CPU与GPU间的数据传输而面临显著延迟瓶颈，亟需消除此类开销以提升效率。

Method: 提出三种互补的GPU原生编译策略：(1) 适配GPU执行的并行传统编译；(2) 基于序列到序列学习与概率验证的神经编译；(3) 融合前两者的混合架构，并建立相应的理论模型与验证框架。

Result: 理论推导显示，传统GPU编译通过消除数据传输获得2-5倍加速，神经编译借助大规模并行实现10-100倍加速，混合方法则兼顾实用性与正确性保障。

Conclusion: GPU原生编译能显著降低AI代码生成的延迟与能耗，所提出的概率验证框架支持在编译精度与并行探索之间权衡，为自改进AI系统及未来类比计算平台提供新路径。

Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.

</details>


### [21] [RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306)
*Tianyuan Wu,Lunxi Cao,Yining Wei,Wei Gao,Yuheng Zhao,Dakai An,Shaopan Xiong,Zhiqiang Lv,Ju Huang,Siran Yang,Yinghao Yu,Jiamang Wang,Lin Qu,Wei Wang*

Main category: cs.DC

TL;DR: 本文提出RollMux，一种用于强化学习后训练阶段的跨集群调度框架，通过协同调度rollout与训练任务，有效利用因同步等待造成的空闲资源，显著提升硬件利用率和成本效率。


<details>
  <summary>Details</summary>
Motivation: 在强化学习后训练中，on-policy算法要求严格的同步，导致rollout集群与训练集群之间出现严重的依赖空泡（dependency bubbles），造成一方长时间闲置，降低整体硬件效率。

Method: RollMux引入“协同执行组”（co-execution group）抽象，将集群划分为隔离的局部性域，并采用两层调度架构：组间调度器使用保守随机规划优化作业放置，组内调度器执行可证明最优的轮询调度；同时通过驻留约束保持模型状态缓存，实现“热启动”上下文切换。

Result: 在包含328个H20和328个H800 GPU的生产级测试平台上，RollMux相比标准解耦架构提升1.84倍成本效率，比当前最先进的同置基线提升1.38倍，且100%满足服务等级目标（SLO）。

Conclusion: RollMux通过跨集群协同调度有效回收因同步等待造成的资源空闲，在保证SLO的同时显著提升强化学习后训练阶段的成本效率和资源利用率。

Abstract: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

</details>


### [22] [Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532)
*Chong Tang,Hao Dai,Jagmohan Chauhan*

Main category: cs.DC

TL;DR: Parallax 是一个无需模型重构或自定义算子即可加速移动端动态 DNN 推理的框架，通过计算图划分、分支感知内存管理和自适应调度，在多种设备上显著降低延迟、控制内存开销并节省能耗。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上实时 DNN 应用对低延迟推理的需求日益增长，但现有框架在处理动态控制流和不支持的算子回退到 CPU 时效率低下，导致高延迟和内存峰值。

Method: Parallax 通过划分计算 DAG 暴露并行性，采用分支感知的内存管理（专用内存区与缓冲复用），并结合自适应调度器根据设备内存约束执行分支，同时利用细粒度子图控制实现异构推理。

Result: 在三种移动设备上对五个典型 DNN 的评估表明，Parallax 相比最先进框架最多降低 46% 延迟，平均内存开销仅增加 26.5%，并最多节省 30% 能耗。

Conclusion: Parallax 有效提升了移动端动态 DNN 推理的效率，在延迟、内存和能耗方面均优于现有方案，满足了实时推理的响应性需求。

Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.

</details>


### [23] [ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727)
*Yuze He,Ferdi Kossmann,Srinivasan Seshan,Peter Steenkiste*

Main category: cs.DC

TL;DR: ECCO is a resource-efficient video analytics framework that reduces compute and communication costs by grouping cameras with similar data drift and retraining shared models, achieving higher accuracy or supporting more cameras than existing methods.


<details>
  <summary>Details</summary>
Motivation: Current video analytics systems retrain separate lightweight DNN models per camera to handle data drift, but this approach incurs high compute and communication overhead, limiting scalability.

Method: ECCO leverages temporal and spatial correlations in data drift across nearby cameras. It includes: (i) a dynamic camera grouping algorithm, (ii) a GPU allocator for fair and accurate retraining across groups, and (iii) a per-camera transmission controller that adjusts frame sampling and bandwidth based on allocated GPU resources.

Result: Evaluations on three datasets and two vision tasks show ECCO improves retraining accuracy by 6.7%–18.1% under the same resource budget, or supports 3.3× more concurrent cameras at the same accuracy compared to state-of-the-art baselines.

Conclusion: By exploiting correlated data drift among cameras and enabling shared model retraining, ECCO offers a scalable and efficient solution for continuous learning in video analytics systems.

Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.

</details>
