<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.LG](#cs.LG) [Total: 65]
- [cs.DC](#cs.DC) [Total: 13]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: IG-Pruning is a novel input-aware, block-wise depth pruning method that dynamically selects transformer layer masks during inference, outperforming static pruning approaches.


<details>
  <summary>Details</summary>
Motivation: Existing depth pruning methods use fixed block masks, leading to suboptimal performance across varying tasks and inputs; there is a need for a more adaptive and efficient pruning strategy for LLM deployment.

Method: IG-Pruning employs a two-stage approach: (1) generating diverse mask candidates via semantic clustering and L0 optimization, and (2) applying dynamic pruning at inference without extensive retraining.

Result: Experiments show that IG-Pruning consistently surpasses state-of-the-art static depth pruning methods in performance while being suitable for resource-constrained environments.

Conclusion: IG-Pruning offers an effective and efficient solution for dynamic depth pruning of LLMs, enabling better adaptability and performance in practical deployment settings.

Abstract: With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [2] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 该论文研究了医疗聊天机器人在面对包含人口统计信息等非医学因素时的表现，构建了一个自动生成查询并利用多个大语言模型（LLM）作为评判者的评估框架，发现LLM评判者之间一致性较低，并建议在缺乏真实标签的情况下使用多个LLM进行评估以提高结果的泛化性。


<details>
  <summary>Details</summary>
Motivation: 医疗场景中的聊天机器人必须在包含人口统计信息等非医学因素的情况下提供一致建议，但现有研究表明大语言模型普遍存在幻觉、遗漏和偏见问题，因此需要系统性地探究其失效条件。

Method: 作者开发了一套基础设施：1）通过采样患者人口统计信息、病史、疾病和写作风格自动生成逼真的查询；2）利用多个LLM-as-a-judge设置和智能体工作流对模型回答进行评估，检测幻觉、遗漏及治疗类别。

Result: 实验发现LLM评判者之间的一致性很低（平均Cohen's Kappa κ=0.118），仅特定的（回答模型，评估模型）组合在写作风格、性别和种族方面表现出统计显著差异。

Conclusion: 建议在使用LLM进行评估时采用多个模型作为评判者，以避免得出不具泛化性的显著结果，尤其在缺乏真实数据时；同时建议公开LLM间一致性指标以提升透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [3] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: 本文提出了LTD-Bench，一种新型的大语言模型（LLM）空间推理能力评估基准，通过要求模型生成点阵图或可执行代码来可视化其空间理解能力，揭示了当前LLM在语言与空间概念双向映射上的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法依赖不透明的数值指标，无法直观反映模型在空间推理方面的根本局限，导致报告性能与实际能力之间存在严重脱节，尤其影响需要物理世界理解的应用。

Method: LTD-Bench通过让模型生成可视化输出（点阵图或代码）来评估空间推理能力，包含空间想象（生成任务）和空间感知（识别任务）两类互补任务，并设置三个递进难度等级，系统评估语言与空间概念之间的双向映射能力。

Result: 实验表明，即使在传统基准上表现优异的先进LLM，在语言与空间概念的双向映射上仍存在显著缺陷；同时，LTD-Bench的可视化输出支持对模型相似性的诊断分析。

Conclusion: LTD-Bench有效弥补了现有评估范式的不足，使空间推理能力缺陷直观可见，揭示了当前LLM难以作为真正“世界模型”的根本局限，并为模型诊断提供了新途径。

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [4] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim,Hochang Lee,Sanghak Lee,Yoonsung Kim,Jaehyun Park*

Main category: cs.CL

TL;DR: 本文提出M-Solomon，一种通用的多模态嵌入模型，能自适应判断是否对查询进行增强，在提升检索性能的同时显著降低嵌入延迟。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型（LLM）的查询增强方法在推理时对所有查询一律增强，导致嵌入延迟高，且对某些查询反而损害性能；此外，现有方法未在多模态环境中探索。

Method: M-Solomon首先在训练数据集层面将查询分为需增强与无需增强两类，利用多模态大语言模型（MLLM）为需增强的查询生成合成增强内容，并通过学习在需增强时输出/augment前缀、否则输出/embed前缀，实现自适应查询增强。

Result: 实验表明，M-Solomon大幅优于不使用增强的基线，也优于始终进行增强的基线，同时显著降低嵌入延迟。

Conclusion: M-Solomon通过自适应查询增强机制，在多模态环境下有效平衡了检索性能与计算效率，是一种高效且通用的嵌入方法。

Abstract: Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [5] [LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context](https://arxiv.org/abs/2511.02366)
*Yudong Li,Zhongliang Yang,Kejiang Chen,Wenxuan Wang,Tianxin Zhang,Sifang Wan,Kecheng Wang,Haitian Li,Xu Wang,Lefan Cheng,Youdan Yang,Baocheng Chen,Ziyu Liu,Yufei Sun,Liyan Wu,Wenya Wen,Xingchi Gu,Peiru Yang*

Main category: cs.CL

TL;DR: 本文提出了LiveSecBench，一个面向中文大语言模型应用的动态持续更新的安全性评测基准，涵盖六大维度，并已对18个模型进行评估，结果公开发布。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型安全性评测基准难以覆盖中文语境下的法律与社会规范，且缺乏对新兴安全威胁的动态响应能力，因此需要一个专门针对中文场景、可持续更新的安全评测体系。

Method: 构建LiveSecBench基准，基于中国法律与社会框架，从合法性、伦理、事实性、隐私、对抗鲁棒性和推理安全性六个维度评估模型，并通过定期更新纳入新的安全威胁（如下一版本将加入文生图安全与智能体安全）。

Result: LiveSecBench（v251030）已完成对18个大语言模型的评估，形成了中文语境下AI安全性的全景视图，相关排行榜已公开发布。

Conclusion: LiveSecBench为中文大语言模型提供了一个动态、全面且符合本地规范的安全评测基准，有助于推动模型在实际应用中的安全性提升。

Abstract: In this work, we propose LiveSecBench, a dynamic and continuously updated
safety benchmark specifically for Chinese-language LLM application scenarios.
LiveSecBench evaluates models across six critical dimensions (Legality, Ethics,
Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in
the Chinese legal and social frameworks. This benchmark maintains relevance
through a dynamic update schedule that incorporates new threat vectors, such as
the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in
the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,
providing a landscape of AI safety in the context of Chinese language. The
leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.

</details>


### [6] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman,Sravan Gvm,Vijay Devane,Shyam Pawar,Viraj Thakur,Kundeshwar Pundalik,Piyush Sawarkar,Rohit Saluja,Maunendra Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: AyurParam-2.9B 是一个专为阿育吠陀医学领域定制的双语（英语和印地语）语言模型，在特定任务上超越同规模开源模型，甚至媲美更大模型。


<details>
  <summary>Details</summary>
Motivation: 当前通用大语言模型在需要深厚文化、语言和专业知识的特定领域（如阿育吠陀医学）表现不佳，缺乏对传统医学知识的准确理解和应用能力。

Method: 基于 Param-1-2.9B 模型，使用大量专家精心整理的阿育吠陀数据集进行微调，该数据集涵盖经典文献与临床指导，包含上下文感知、推理型及客观题形式的双语问答，并采用严格标注协议确保事实准确性和指导清晰性。

Result: 在 BhashaBench-Ayur 基准测试中，AyurParam-2.9B 在1.5–3B参数规模的开源指令微调模型中表现最佳，且优于或媲美更大规模模型。

Conclusion: 高质量的领域适配和监督对于构建可靠、文化契合的专用医学知识AI系统至关重要。

Abstract: Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [7] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CL

TL;DR: 本文提出了AutoAdv，一种无需训练的自动化多轮越狱攻击框架，通过结合模式管理器、温度管理器和两阶段重写策略，在六轮内对Llama-3.1-8B实现高达95%的攻击成功率，显著优于单轮攻击方法，揭示了当前大语言模型安全机制在多轮对话中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的安全评估主要集中在单轮交互，而真实攻击通常通过自适应的多轮对话展开，因此亟需一种能有效模拟和评估多轮越狱攻击的方法，以揭示现有对齐策略在持续对话中的不足。

Method: AutoAdv框架包含三个自适应机制：（1）模式管理器，从成功攻击中学习以优化后续提示；（2）温度管理器，根据失败模式动态调整采样参数；（3）两阶段重写策略，先伪装有害请求再迭代优化。

Result: 在Llama-3.1-8B上，AutoAdv在六轮内达到95%的攻击成功率，比单轮基线提升24%；在GPT-4o-mini、Qwen3-235B和Mistral-7B等模型上的广泛测试也表明，多轮攻击始终优于单轮攻击。

Conclusion: 当前针对单轮交互优化的对齐策略在多轮对话中缺乏鲁棒性，模型安全机制存在明显漏洞，亟需发展具备多轮感知能力的防御方法。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [8] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda,François Portet,Hirohiko Suwa,Keiichi Yasumoto*

Main category: cs.CL

TL;DR: 本文首次系统研究了持续预训练（CPT）模型的融合方法，提出三阶段评估框架，在金融领域验证了融合专家模型可恢复通用知识、提升性能并激发跨领域能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在金融等专业领域表现不佳，需结合领域知识、数学推理和多语言处理能力；而直接进行多技能联合训练成本高且不稳定，因此探索通过融合已有的CPT专家模型来构建多技能LLM是一种更实用的替代方案。

Method: 作者构建了金融、数学和日语三个领域的CPT专家模型，并采用三种融合方法（Task Arithmetic、TIES 和 DARE-TIES）进行模型融合；同时提出包含知识恢复、互补性和涌现性三个阶段的评估框架，在涵盖8个数据集、18项任务的金融基准上进行实验。

Result: 实验表明：将专家模型与其基础模型融合可恢复CPT过程中丢失的通用知识；融合多个专家模型不仅能提升性能，还能产生跨领域的涌现能力；Task Arithmetic效果强但对超参敏感，TIES则更稳健；模型相似性与融合成功相关，但涌现能力受更复杂因素影响。

Conclusion: 本研究为CPT模型融合提供了首个基础性分析，建立了系统评估框架，并为利用现有模型资产构建多技能LLM提供了明确指导。

Abstract: While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [9] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia,Carolina Camassa*

Main category: cs.CL

TL;DR: 该研究评估了基于角色（persona）的提示是否能提升大语言模型（LLM）在宏观经济预测任务中的表现。结果表明，GPT-4o 与人类专家在预测准确性上相当，且角色提示并未带来显著增益。


<details>
  <summary>Details</summary>
Motivation: 探究基于角色的提示是否有助于提升大语言模型在宏观经济预测中的性能，并比较其与人类专家预测的差异。

Method: 利用 PersonaHub 语料库中的 2,368 个经济学相关角色，对 GPT-4o 进行提示，以复现欧洲央行专业预测者调查（ECB SPF）在 2013–2025 年间 50 轮季度预测；对比有无角色描述的预测结果，并与人类专家小组进行比较，涵盖四个目标变量和四个预测期限。

Result: 1）GPT-4o 与人类预测者的准确度高度接近，差异虽统计显著但实际影响微小，且在 2024–2025 年样本外数据中仍保持竞争力；2）消融实验显示角色描述未带来可测量的预测优势。

Conclusion: GPT-4o 在提供相关上下文数据时，即使面对样本外的宏观经济事件，也能实现具有竞争力的预测准确性；而多样化的角色提示并未提升预测多样性或准确性，可省略以降低计算成本。

Abstract: We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [10] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

Main category: cs.CL

TL;DR: 该研究分析了谷歌翻译在将英文新冠相关官方文本（如世卫组织、Gavi组织及药品说明书）翻译成罗马尼亚语时出现的词汇错误，通过对230篇文本的系统分析，旨在提升机器翻译的词汇选择准确性和整体翻译质量。


<details>
  <summary>Details</summary>
Motivation: 提升谷歌翻译在英译罗任务中的词汇准确性，减少与新冠疫情相关的官方文本翻译中的词汇错误，从而提高机器翻译的整体质量。

Method: 对230篇由谷歌翻译从英语翻译成罗马尼亚语的新冠相关官方文本（包括世卫组织、Gavi组织及药品说明书内容）进行系统的词汇错误分析。

Result: 识别并分类了机器翻译在处理官方医疗文本时出现的各类词汇错误，为改进词汇选择提供了实证依据。

Conclusion: 通过针对性的词汇错误分析，可有效指导机器翻译系统（如谷歌翻译）在专业领域文本中的优化，提升翻译准确性和可靠性。

Abstract: The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [11] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris,Kobi Gal,Sahan Bulathwela*

Main category: cs.CL

TL;DR: 本文提出了一种名为Next Token Knowledge Tracing（NTKT）的新方法，将知识追踪任务重构为基于大语言模型的下一个词元预测问题，通过同时建模学生行为历史和题目文本内容，显著提升了预测性能，并在冷启动场景下表现出更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型通常忽略题目文本内容，仅依赖答题正确性及技能标签等元数据，错失了从题目文本中获取教学洞察的机会，限制了模型的预测性能。

Method: 提出NTKT方法，利用预训练大语言模型（LLM），将学生历史交互和题目内容统一表示为文本序列，将知识追踪任务转化为下一个词元预测任务。

Result: 实验表明，NTKT在多个指标上显著优于当前最先进的神经知识追踪模型，并在面对新用户和新题目（冷启动）时展现出更强的泛化能力。

Conclusion: 题目文本在知识追踪中具有重要作用，利用预训练大语言模型的表示能力可更有效地建模学生学习过程。

Abstract: Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [12] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh,Ahmad Ghasemi,Hedyeh Beyhaghi,Hossein Pishro-Nik*

Main category: cs.CL

TL;DR: 本文提出了一种名为置信度引导早停（CGES）的贝叶斯框架，通过利用来自token概率或奖励模型的置信度信号，自适应地决定何时停止多次调用大语言模型，从而在几乎不损失准确率的前提下大幅减少模型调用次数。


<details>
  <summary>Details</summary>
Motivation: 现有的自洽性（self-consistency）策略在测试时需固定调用大语言模型多次并进行多数投票，效率低且在正确答案罕见时可能失效，因此需要一种更高效、自适应的停止机制。

Method: 提出CGES方法，基于token概率或奖励模型生成的标量置信度信号构建候选答案的后验分布，并在某一候选答案的后验质量超过阈值时自适应停止采样。

Result: 在五个推理基准上，CGES平均减少约69%的模型调用次数（如从16.0降至4.9），同时准确率与自洽性方法相差不超过0.06个百分点。

Conclusion: CGES是一种高效且准确的自适应采样停止策略，显著提升了大语言模型推理效率，同时保持了与现有方法相当的性能。

Abstract: Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [13] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: 本文提出TRACE框架，通过程序化策略评估和冲突识别，实现大语言模型的高效、精准再对齐，避免传统方法在更新人类价值观对齐时带来的性能损失和高成本问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）的价值对齐方法存在静态、脆弱且维护成本高的问题，难以适应不断变化的社会规范和政策，导致“对齐-现实差距”（Alignment-Reality Gap）日益严重；而现有解决方案要么成本过高，要么会损害模型整体性能。

Method: 提出TRACE（Triage and Re-align by Alignment Conflict Evaluation）框架，该方法将再对齐视为程序化策略应用问题：首先对现有偏好数据按新策略进行分类，然后通过“对齐影响分数”识别高影响冲突，最后采用混合优化策略选择性地反转、丢弃或保留偏好数据，以在更新对齐的同时保护模型性能。

Result: 在多个模型（Qwen2.5-7B、Gemma-2-9B、Llama-3.1-8B）上验证了TRACE的有效性；在合成基准和PKU-SafeRLHF数据集上，TRACE在复杂策略变化下成功实施新原则，且未损害模型的通用能力。

Conclusion: TRACE为大语言模型提供了一种可扩展、动态且经济高效的对齐维护范式，有助于实现可持续和负责任的人工智能部署。

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [14] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang,Peng Hu,Changjiang Gao,Shujian Huang*

Main category: cs.CL

TL;DR: 本文研究大语言模型在微调过程中引入新知识时引发的事实性幻觉问题，发现特定知识类型的高陌生度是主要诱因，并提出KnownPatch方法通过在训练后期加入少量已知知识样本来缓解该问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未深入探讨大语言模型在微调中因引入新知识而产生事实性幻觉的具体表现和内在机制。

Method: 构建受控数据集Biography-Reasoning，对多种知识类型和两类任务（知识问答与知识推理）进行细粒度分析；提出KnownPatch方法，在训练后期加入少量已知知识样本以缓解幻觉；并通过注意力分析探究幻觉产生的机制。

Result: 发现特定知识类型完全由新知识构成时，模型幻觉倾向显著增加；KnownPatch方法能有效缓解此类幻觉；注意力分析表明学习新知识会削弱模型对问题中关键实体的关注，导致过度关注上下文，且该注意力模式可传播至文本相似的问题。

Conclusion: 知识类型的高陌生度而非新知识整体比例是引发事实性幻觉的关键因素，通过KnownPatch方法可有效恢复模型对关键实体的注意力并提升性能。

Abstract: Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [15] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak,Koel Dutta Chowdhury,Uliana Sentsova,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: 本文提出了首个面向语用显化现象的多语言语料库与检测框架PragExTra，通过人工标注与主动学习识别翻译中新增的背景信息（如实体描述、单位换算等），验证了该现象的可测量性，并为构建文化感知型机器翻译系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 语用显化在翻译理论中被广泛讨论，但缺乏计算建模；现有研究尚未系统地识别和衡量翻译中为适应目标文化而显式添加的背景信息。

Method: 构建包含八个语言对的多语言语料库（基于TED-Multi和Europarl），通过空对齐识别候选显化实例，并结合主动学习与人工标注进行精炼，训练分类器检测显化类型。

Result: 实体类和系统级显化最为常见；主动学习使分类器准确率提升7-8个百分点，在多种语言上达到最高0.88准确率和0.82 F1值。

Conclusion: PragExTra证实语用显化是一种可量化、跨语言的现象，为发展具备文化意识的机器翻译提供了数据基础与方法支持。

Abstract: Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [16] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra,Syed Waqas Zamir,Wassim Hamidouche,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: 低资源语言国家因大语言模型在低资源语言上表现不佳，导致AI用户比例比预期低约20%，语言可及性是AI普及的重要障碍。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨低资源语言国家（LRLCs）中AI采用率较低是否与大语言模型在低资源语言上的性能缺陷有关，从而影响AI的全球公平扩散。

Method: 采用加权回归模型，控制社会经济和人口因素，以分离语言因素对AI采用率的独立影响。

Result: 发现低资源语言国家的AI用户占比比其基准水平低约20%，表明语言可及性显著影响AI的采用。

Conclusion: 语言障碍是AI在全球范围内不均衡扩散的一个独立且重要的因素，需提升模型对低资源语言的支持以促进公平。

Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [17] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin,TJ Collins,Donghan Yu,Mert Cemri,Shenao Zhang,Mengyu Li,Jay Tang,Tian Qin,Zhiyang Xu,Jiarui Lu,Guoli Yin,Jiawei Han,Zirui Wang*

Main category: cs.CL

TL;DR: 本文提出了一种集中式的多LLM框架CoRL，通过控制器LLM在不同预算条件下协调专家模型，在控制推理成本的同时提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化的多LLM系统在每次输入时调用多个模型，导致推理成本高且不可控；因此需要一种成本可控、高效的集中式协调机制。

Method: 将多LLM协调问题建模为具有双重目标（最大化任务性能、最小化推理成本）的强化学习问题，并提出CoRL框架，在多预算设定下优化性能与成本的权衡。

Result: 在四个多样化基准上的实验表明，CoRL在高预算下超越最佳专家模型，在低预算下仍保持强劲性能。

Conclusion: 集中式协调机制能有效实现可扩展、成本可控的多智能体LLM系统。

Abstract: Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [18] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen,Xiang Liu,Shauli Ravfogel,Eunsol Choi*

Main category: cs.CL

TL;DR: 本文提出了一种新的检索器架构 AMER（Autoregressive Multi-Embedding Retriever），通过自回归方式生成多个查询向量以应对查询相关文档分布的多模态性，在合成数据和真实多答案检索数据集上均显著优于单向量检索基线。


<details>
  <summary>Details</summary>
Motivation: 现有文本检索器通常仅生成一个查询向量，难以有效处理查询相关文档嵌入分布呈多模态的情形（如查询存在多种解释），导致在目标文档嵌入距离较大时性能下降。

Method: 提出 AMER 模型，自回归地生成多个查询向量，并利用所有预测的查询向量从语料库中检索文档。

Result: 在合成向量化数据上，AMER 能完美捕捉多个目标分布，性能比单嵌入模型高 4 倍；在两个真实多答案检索数据集上，分别取得 4% 和 21% 的相对提升，且在目标文档嵌入差异较大的子集上提升更显著。

Conclusion: 多查询向量检索器具有显著潜力，AMER 为未来检索系统设计开辟了新方向。

Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [19] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch,Adithya Pratapa,Teruko Mitamura,Graham Neubig,Matthew R. Gormley*

Main category: cs.CL

TL;DR: 本文提出了Oolong，一个用于评估大模型在长上下文中进行细粒度推理能力的新基准，包含合成任务（Oolong-synth）和真实对话数据任务（Oolong-real），要求模型对大量文本块进行分类、计数及关系推理；实验表明当前前沿模型在此基准上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估主要依赖从上下文中检索信息，忽略了大部分上下文内容，仅覆盖了长上下文应用的一种类型。作者旨在构建一个更全面的基准，评估模型是否能对长上下文中的每个文本块进行原子级分析并综合得出分布性结论。

Method: 作者设计了Oolong基准，分为两个任务集：Oolong-synth（基于自然风格的合成任务，便于控制变量）和Oolong-real（基于真实对话数据的下游任务）。任务要求模型在128K上下文中执行分类、计数以及对时间和用户关系的推理。

Result: 即使是最先进的模型（如GPT-5、Claude-Sonnet-4、Gemini-2.5-Pro）在Oolong两个任务集上的准确率均低于50%，表明当前模型在长上下文细粒度推理方面仍存在显著不足。

Conclusion: Oolong揭示了当前大模型在处理需要细粒度分析与聚合推理的长上下文任务时的局限性，作者公开了数据和评估工具，以促进相关研究的发展。

Abstract: As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884)
*Zijian Zhang,Rong Wang,Shiyang Li,Yuebo Luo,Mingyi Hong,Caiwen Ding*

Main category: cs.LG

TL;DR: CudaForge 是一种无需训练的多智能体工作流，通过模拟人类专家的迭代流程自动生成并优化 CUDA 内核，在正确性、速度、泛化性和成本效率方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 手动设计高效 CUDA 内核成本高且耗时，而现有的自动内核生成方法往往效率低、开销大且泛化能力差，因此需要一种更高效、通用且低成本的自动化方案。

Method: CudaForge 采用两个 LLM 智能体（Coder 和 Judge）组成的多智能体工作流，结合硬件反馈（如 Nsight Compute 指标），通过迭代生成、校正和优化 CUDA 内核，整个过程无需模型训练。

Result: CudaForge 在 KernelBench 上实现了 97.6% 的内核正确率和平均 1.68 倍于 PyTorch 基线的速度提升；可泛化至多种 GPU 和基础模型；单个内核生成仅需约 26.5 分钟和 0.3 美元 API 成本，远低于现有方法。

Conclusion: 无需训练的多智能体工作流能够实现高效、低成本且具有良好泛化能力的 CUDA 内核优化，为 AI 应用中的高性能计算提供了实用解决方案。

Abstract: Developing efficient CUDA kernels is increasingly critical for AI
applications such as large-scale LLM training. However, manual kernel design is
both costly and time-consuming, motivating automatic approaches that leverage
LLMs for code generation. Existing methods for automatic kernel generation,
however, often produce low-efficiency kernels, incur high computational
overhead, and fail to generalize across settings. In this work, we propose
CudaForge, a training-free multi-agent workflow for CUDA kernel generation and
optimization. Our workflow is inspired by the iterative workflow of human
experts, which contains steps such as developing initial kernels, testing
correctness, analyzing hardware feedback, and iterative improvement. More
specifically, CudaForge employs two LLM agents: a Coder and a Judge, that
iteratively generate, correct, and optimize CUDA kernels, while integrating
hardware feedback such as Nsight Compute (NCU) metrics. In extensive
evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3,
achieves 97.6\% correctness of generated kernels and an average 1.68$\times$
speedup over PyTorch baselines, substantially surpassing state-of-the-art
models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed,
CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090,
3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4,
QwQ-32B), while maintaining high efficiency. In particular, generating an
optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$
0.3 API cost, which is significantly cheaper than existing agentic work that
costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that
multi-agent, training-free workflows can enable cost-effective, generalizable,
and high-performance CUDA kernel optimization. Code available at
https://github.com/OptimAI-Lab/CudaForge

</details>


### [21] [Retrieval-Augmented Multimodal Depression Detection](https://arxiv.org/abs/2511.01892)
*Ruibo Hou,Shiyu Teng,Jiaqing Liu,Shurong Chai,Yinhao Li,Lanfen Lin,Yen-Wei Chen*

Main category: cs.LG

TL;DR: 本文提出一种基于检索增强生成（RAG）的多模态抑郁检测方法，通过从情感数据集中检索相关内容并利用大语言模型生成情绪提示，作为辅助模态增强情感表征，在AVEC 2019数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于情感分析的多模态抑郁检测方法存在计算成本高、领域不匹配和静态知识限制等问题。

Method: 提出一种检索增强生成（RAG）框架：给定与抑郁相关的文本，从情感数据集中检索语义相关的情感内容，并利用大语言模型（LLM）生成情绪提示作为辅助模态，以增强情感表征和可解释性。

Result: 在AVEC 2019数据集上，该方法取得CCC为0.593、MAE为3.95的性能，优于以往的迁移学习和多任务学习基线。

Conclusion: 所提出的RAG框架有效提升了多模态抑郁检测的性能和可解释性，为情感增强提供了新思路。

Abstract: Multimodal deep learning has shown promise in depression detection by
integrating text, audio, and video signals. Recent work leverages sentiment
analysis to enhance emotional understanding, yet suffers from high
computational cost, domain mismatch, and static knowledge limitations. To
address these issues, we propose a novel Retrieval-Augmented Generation (RAG)
framework. Given a depression-related text, our method retrieves semantically
relevant emotional content from a sentiment dataset and uses a Large Language
Model (LLM) to generate an Emotion Prompt as an auxiliary modality. This prompt
enriches emotional representation and improves interpretability. Experiments on
the AVEC 2019 dataset show our approach achieves state-of-the-art performance
with CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning and
multi-task learning baselines.

</details>


### [22] [The Eigenvalues Entropy as a Classifier Evaluation Measure](https://arxiv.org/abs/2511.01904)
*Doulaye Dembélé*

Main category: cs.LG

TL;DR: 本文提出使用特征值熵作为分类模型的评估指标，尤其适用于类别不平衡的数据集，并通过多个数据示例验证其优于传统评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有分类评估指标在类别不平衡数据集上表现不佳，因此需要一种更稳健的评估方法。

Method: 引入特征值熵作为二分类或多分类问题的评估指标，并建立其与灵敏度、特异性、AUC和基尼指数等常用指标的关系；同时提出一种混淆矩阵估计方法以缓解类别不平衡问题。

Result: 在多个数据集上的实验表明，所提出的特征值熵评估方法优于文献中的传统“金标准”评估指标。

Conclusion: 特征值熵是一种有效的分类评估指标，尤其适用于类别不平衡场景，能更准确地反映分类器性能。

Abstract: Classification is a machine learning method used in many practical
applications: text mining, handwritten character recognition, face recognition,
pattern classification, scene labeling, computer vision, natural langage
processing. A classifier prediction results and training set information are
often used to get a contingency table which is used to quantify the method
quality through an evaluation measure. Such measure, typically a numerical
value, allows to choose a suitable method among several. Many evaluation
measures available in the literature are less accurate for a dataset with
imbalanced classes. In this paper, the eigenvalues entropy is used as an
evaluation measure for a binary or a multi-class problem. For a binary problem,
relations are given between the eigenvalues and some commonly used measures,
the sensitivity, the specificity, the area under the operating receiver
characteristic curve and the Gini index. A by-product result of this paper is
an estimate of the confusion matrix to deal with the curse of the imbalanced
classes. Various data examples are used to show the better performance of the
proposed evaluation measure over the gold standard measures available in the
literature.

</details>


### [23] [Superpositional Gradient Descent: Harnessing Quantum Principles for Model Training](https://arxiv.org/abs/2511.01918)
*Ahmet Erdem Pamuk,Emir Kaan Özdemir,Şuayp Talha Kocabay*

Main category: cs.LG

TL;DR: 本文提出了一种名为“叠加梯度下降”（SGD）的新优化器，通过引入量子电路扰动将梯度更新与量子叠加原理相结合，在合成序列分类和大语言模型微调任务中比AdamW收敛更快、最终损失更低。


<details>
  <summary>Details</summary>
Motivation: 探索量子启发方法如何增强经典大语言模型训练机制，填补当前研究空白。

Method: 提出Superpositional Gradient Descent（SGD）优化器，结合量子叠加原理，通过在PyTorch和Qiskit中实现混合量子-经典电路，对梯度更新进行量子扰动。

Result: 在合成序列分类和大规模LLM微调任务中，SGD相比AdamW表现出更快的收敛速度和更低的最终损失。

Conclusion: 该研究揭示了量子计算与深度学习交叉的新视角，为利用量子原理调控和增强模型行为提供了可行路径，但可扩展性和硬件限制仍是实际应用的主要障碍。

Abstract: Large language models (LLMs) are increasingly trained with classical
optimization techniques like AdamW to improve convergence and generalization.
However, the mechanisms by which quantum-inspired methods enhance classical
training remain underexplored. We introduce Superpositional Gradient Descent
(SGD), a novel optimizer linking gradient updates with quantum superposition by
injecting quantum circuit perturbations. We present a mathematical framework
and implement hybrid quantum-classical circuits in PyTorch and Qiskit. On
synthetic sequence classification and large-scale LLM fine-tuning, SGD
converges faster and yields lower final loss than AdamW. Despite promising
results, scalability and hardware constraints limit adoption. Overall, this
work provides new insights into the intersection of quantum computing and deep
learning, suggesting practical pathways for leveraging quantum principles to
control and enhance model behavior.

</details>


### [24] [Neural Green's Functions](https://arxiv.org/abs/2511.01924)
*Seungwoo Yoo,Kyeongmin Yeo,Jisung Hwang,Minhyuk Sung*

Main category: cs.LG

TL;DR: 本文提出了神经格林函数（Neural Green's Function），一种用于求解具有特征分解能力的线性偏微分方程的神经算子，其在不同不规则几何形状和源/边界函数上展现出优越的泛化能力，并在速度和精度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的解算子在面对未见过的源函数或边界函数时泛化能力有限，且难以适应多样化的不规则几何结构。受经典格林函数启发，作者旨在构建一个与特定源/边界函数无关、仅依赖于几何域的神经解算子，以提升泛化性和计算效率。

Method: Neural Green's Function 从表示问题域的体素点云中提取逐点特征，并利用这些特征预测解算子的分解形式，再通过数值积分评估解。该方法模仿格林函数的行为，其结构设计使其对训练中使用的具体源/边界函数不敏感。

Result: 在MCB数据集的机械零件稳态热分析任务中，该方法在五个形状类别上平均误差降低13.9%，且比需要复杂网格划分的数值求解器快达350倍。

Conclusion: Neural Green's Function 通过模仿格林函数的机制，实现了对线性偏微分方程解算子的高效、通用神经建模，在精度和速度上均显著优于当前最先进的神经算子。

Abstract: We introduce Neural Green's Function, a neural solution operator for linear
partial differential equations (PDEs) whose differential operators admit
eigendecompositions. Inspired by Green's functions, the solution operators of
linear PDEs that depend exclusively on the domain geometry, we design Neural
Green's Function to imitate their behavior, achieving superior generalization
across diverse irregular geometries and source and boundary functions.
Specifically, Neural Green's Function extracts per-point features from a
volumetric point cloud representing the problem domain and uses them to predict
a decomposition of the solution operator, which is subsequently applied to
evaluate solutions via numerical integration. Unlike recent learning-based
solution operators, which often struggle to generalize to unseen source or
boundary functions, our framework is, by design, agnostic to the specific
functions used during training, enabling robust and efficient generalization.
In the steady-state thermal analysis of mechanical part geometries from the MCB
dataset, Neural Green's Function outperforms state-of-the-art neural operators,
achieving an average error reduction of 13.9\% across five shape categories,
while being up to 350 times faster than a numerical solver that requires
computationally expensive meshing.

</details>


### [25] [DeepContour: A Hybrid Deep Learning Framework for Accelerating Generalized Eigenvalue Problem Solving via Efficient Contour Design](https://arxiv.org/abs/2511.01927)
*Yeqiu Chen,Ziyan Liu,Hong Wang*

Main category: cs.LG

TL;DR: DeepContour is a hybrid framework that combines deep learning (via a Fourier Neural Operator) and Kernel Density Estimation to automatically design optimal integration contours for contour integral-based solvers of large-scale Generalized Eigenvalue Problems, achieving up to 5.63× speedup.


<details>
  <summary>Details</summary>
Motivation: Contour integral methods for solving large-scale Generalized Eigenvalue Problems (GEPs) are highly sensitive to the choice of integration contours; poor contour selection due to lack of prior knowledge about eigenvalue distribution leads to inefficiency and reduced accuracy.

Method: DeepContour uses a Fourier Neural Operator (FNO) to predict the spectral distribution of a GEP, then applies Kernel Density Estimation (KDE) to this prediction to automatically determine suitable integration contours, which are then used by a contour integral solver (e.g., CIRR) to compute eigenvalues efficiently.

Result: Experiments on diverse scientific problems show that DeepContour significantly accelerates GEP solving, achieving up to a 5.63× speedup compared to baseline methods.

Conclusion: By integrating deep learning with classical numerical solvers, DeepContour provides an efficient, robust, and automated approach to contour selection, enabling scalable and accurate solutions to high-dimensional generalized eigenvalue problems.

Abstract: Solving large-scale Generalized Eigenvalue Problems (GEPs) is a fundamental
yet computationally prohibitive task in science and engineering. As a promising
direction, contour integral (CI) methods, such as the CIRR algorithm, offer an
efficient and parallelizable framework. However, their performance is
critically dependent on the selection of integration contours -- improper
selection without reliable prior knowledge of eigenvalue distribution can incur
significant computational overhead and compromise numerical accuracy. To
address this challenge, we propose DeepContour, a novel hybrid framework that
integrates a deep learning-based spectral predictor with Kernel Density
Estimation for principled contour design. Specifically, DeepContour first
employs a Fourier Neural Operator (FNO) to rapidly predict the spectral
distribution of a given GEP. Subsequently, Kernel Density Estimation (KDE) is
applied to the predicted spectrum to automatically and systematically determine
proper integration contours. Finally, these optimized contours guide the CI
solver to efficiently find the desired eigenvalues. We demonstrate the
effectiveness of our method on diverse challenging scientific problems. In our
main experiments, DeepContour accelerates GEP solving across multiple datasets,
achieving up to a 5.63$\times$ speedup. By combining the predictive power of
deep learning with the numerical rigor of classical solvers, this work pioneers
an efficient and robust paradigm for tackling difficult generalized eigenvalue
involving matrices of high dimension.

</details>


### [26] [Deciphering Personalization: Towards Fine-Grained Explainability in Natural Language for Personalized Image Generation Models](https://arxiv.org/abs/2511.01932)
*Haoming Wang,Wei Gao*

Main category: cs.LG

TL;DR: 本文提出了一种名为FineXL的新技术，用于为个性化图像生成模型提供细粒度的自然语言可解释性，能分别描述个性化的不同方面并给出量化评分，实验表明其可解释性准确率提升了56%。


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像生成模型缺乏可解释性，而当前基于自然语言的解释方法过于粗粒度，无法精确识别多个个性化维度及其不同程度。

Method: 提出FineXL方法，通过自然语言对个性化图像生成模型的每个个性化维度进行细粒度描述，并提供量化评分以反映各维度的个性化程度。

Result: 在多种图像生成模型和个性化场景下，FineXL将可解释性的准确率提高了56%。

Conclusion: FineXL有效实现了对个性化图像生成模型的细粒度自然语言解释，显著提升了可解释性的准确性。

Abstract: Image generation models are usually personalized in practical uses in order
to better meet the individual users' heterogeneous needs, but most personalized
models lack explainability about how they are being personalized. Such
explainability can be provided via visual features in generated images, but is
difficult for human users to understand. Explainability in natural language is
a better choice, but the existing approaches to explainability in natural
language are limited to be coarse-grained. They are unable to precisely
identify the multiple aspects of personalization, as well as the varying levels
of personalization in each aspect. To address such limitation, in this paper we
present a new technique, namely \textbf{FineXL}, towards \textbf{Fine}-grained
e\textbf{X}plainability in natural \textbf{L}anguage for personalized image
generation models. FineXL can provide natural language descriptions about each
distinct aspect of personalization, along with quantitative scores indicating
the level of each aspect of personalization. Experiment results show that
FineXL can improve the accuracy of explainability by 56\%, when different
personalization scenarios are applied to multiple types of image generation
models.

</details>


### [27] [Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch](https://arxiv.org/abs/2511.01934)
*Yirong Zeng,Xiao Ding,Yutai Hou,Yuxian Wang,Li Du,Juyi Dai,Qiuyang Ding,Duyu Tang,Dandan Tu,Weiwen Liu,Bing Qin,Ting Liu*

Main category: cs.LG

TL;DR: 本文提出一种基于纯强化学习（RL）的训练方法，通过动态奖励机制引导模型从探索转向利用，从而提升大语言模型在工具使用任务中的泛化能力。所提出的Tool-Zero系列模型在未经过监督微调的情况下，相较SFT和RL+SFT方法平均提升超过7%性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于监督微调（SFT）的工具增强大语言模型在面对陌生或复杂工具使用场景时泛化能力有限，而纯强化学习是否能有效激发模型内在推理能力并实现工具无关的泛化尚不明确。

Method: 提出一种动态泛化引导的奖励设计，用于基于规则的强化学习，使奖励机制从鼓励探索逐步转向鼓励有效利用工具；在此基础上，直接从零样本基础模型（Zero模型）出发，通过纯RL训练得到Tool-Zero系列模型。

Result: 在相同实验设置下，Tool-Zero模型相比SFT和RL-with-SFT模型性能提升超过7%，且在跨数据集和同数据集评估中均表现出一致的优越性和鲁棒性。

Conclusion: 纯强化学习结合动态奖励机制能有效提升大语言模型在工具使用任务中的泛化能力，无需依赖监督微调即可实现显著性能增益。

Abstract: Training tool-augmented LLMs has emerged as a promising approach to enhancing
language models' capabilities for complex tasks. The current supervised
fine-tuning paradigm relies on constructing extensive domain-specific datasets
to train models. However, this approach often struggles to generalize
effectively to unfamiliar or intricate tool-use scenarios. Recently,
reinforcement learning (RL) paradigm can endow LLMs with superior reasoning and
generalization abilities. In this work, we address a key question: Can the pure
RL be used to effectively elicit a model's intrinsic reasoning capabilities and
enhance the tool-agnostic generalization? We propose a dynamic
generalization-guided reward design for rule-based RL, which progressively
shifts rewards from exploratory to exploitative tool-use patterns. Based on
this design, we introduce the Tool-Zero series models. These models are trained
to enable LLMs to autonomously utilize general tools by directly scaling up RL
from Zero models (i.e., base models without post-training). Experimental
results demonstrate that our models achieve over 7% performance improvement
compared to both SFT and RL-with-SFT models under the same experimental
settings. These gains are consistently replicated across cross-dataset and
intra-dataset evaluations, validating the effectiveness and robustness of our
methods.

</details>


### [28] [Q-Sat AI: Machine Learning-Based Decision Support for Data Saturation in Qualitative Studies](https://arxiv.org/abs/2511.01935)
*Hasan Tutar,Caner Erden,Ümit Şentürk*

Main category: cs.LG

TL;DR: 本文提出一种基于机器学习的系统模型，用于客观确定定性研究中的样本量，替代传统的主观饱和原则。


<details>
  <summary>Details</summary>
Motivation: 传统定性研究中依赖数据饱和原则确定样本量存在主观性和模糊性，影响方法严谨性，亟需更客观、系统的方法。

Method: 基于五种定性研究方法（案例研究、扎根理论、现象学、叙事研究和民族志研究）构建数据集，选取十个关键参数作为输入特征，采用集成学习方法训练并比较多种机器学习算法（如KNN、GB、RF、XGBoost和决策树）。

Result: 多个算法在测试集上达到约0.85的R²值，能有效建模定性抽样决策中的复杂非线性关系；特征重要性分析验证了研究设计类型和信息力的关键作用。

Conclusion: 研究提出一个基于Web的计算应用框架，作为定性研究人员、审稿人和导师的决策支持工具，有助于标准化样本量论证、提升透明度并加强定性研究的认识论基础。

Abstract: The determination of sample size in qualitative research has traditionally
relied on the subjective and often ambiguous principle of data saturation,
which can lead to inconsistencies and threaten methodological rigor. This study
introduces a new, systematic model based on machine learning (ML) to make this
process more objective. Utilizing a dataset derived from five fundamental
qualitative research approaches - namely, Case Study, Grounded Theory,
Phenomenology, Narrative Research, and Ethnographic Research - we developed an
ensemble learning model. Ten critical parameters, including research scope,
information power, and researcher competence, were evaluated using an ordinal
scale and used as input features. After thorough preprocessing and outlier
removal, multiple ML algorithms were trained and compared. The K-Nearest
Neighbors (KNN), Gradient Boosting (GB), Random Forest (RF), XGBoost, and
Decision Tree (DT) algorithms showed the highest explanatory power (Test R2 ~
0.85), effectively modeling the complex, non-linear relationships involved in
qualitative sampling decisions. Feature importance analysis confirmed the vital
roles of research design type and information power, providing quantitative
validation of key theoretical assumptions in qualitative methodology. The study
concludes by proposing a conceptual framework for a web-based computational
application designed to serve as a decision support system for qualitative
researchers, journal reviewers, and thesis advisors. This model represents a
significant step toward standardizing sample size justification, enhancing
transparency, and strengthening the epistemological foundation of qualitative
inquiry through evidence-based, systematic decision-making.

</details>


### [29] [Shorter but not Worse: Frugal Reasoning via Easy Samples as Length Regularizers in Math RLVR](https://arxiv.org/abs/2511.01937)
*Abdelaziz Bounhar,Hadi Abdine,Evan Dufraisse,Ahmad Chamma,Amr Mohamed,Dani Bouch,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.LG

TL;DR: 本文提出在强化学习训练中保留并适度加权中等难度问题，可作为隐式长度正则化手段，使大语言模型在不使用显式长度惩罚的情况下自动生成更简洁的推理过程，同时保持解题准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在逐步推理训练中常因过度冗长而增加推理成本；现有RLVR方法剔除“简单”问题，导致模型将“更长思考”误认为“更好思考”，输出长度分布偏移。

Method: 在RLVR训练中保留并适度上权重中等难度问题，使模型接触可解的短推理链任务，从而隐式约束输出长度分布，避免冗长。

Result: 在Qwen3-4B-Thinking-2507模型上实验表明，在保持AIME25 pass@1准确率的同时，生成的解答平均长度缩短近一半。

Conclusion: 通过保留中等难度样本可实现“免费的简洁性”，模型在无显式长度惩罚下学会高效推理，有效平衡准确性与输出长度。

Abstract: Large language models (LLMs) trained for step-by-step reasoning often become
excessively verbose, raising inference cost. Standard Reinforcement Learning
with Verifiable Rewards (RLVR) pipelines filter out ``easy'' problems for
training efficiency, leaving the model to train primarily on harder problems
that require longer reasoning chains. This skews the output length distribution
upward, resulting in a \textbf{model that conflates ``thinking longer'' with
``thinking better''}. In this work, we show that retaining and modestly
up-weighting moderately easy problems acts as an implicit length regularizer.
Exposing the model to solvable short-chain tasks constrains its output
distribution and prevents runaway verbosity. The result is
\textbf{\emph{emergent brevity for free}}: the model learns to solve harder
problems without inflating the output length, \textbf{ despite the absence of
any explicit length penalization}. RLVR experiments using this approach on
\textit{Qwen3-4B-Thinking-2507} (with a 16k token limit) achieve baseline
pass@1 AIME25 accuracy while generating solutions that are, on average, nearly
twice as short. The code is available at
\href{https://github.com/MBZUAI-Paris/Frugal-AI}{GitHub}, with datasets and
models on
\href{https://huggingface.co/collections/MBZUAI-Paris/k2-think-mini-68dcfa8b114686a4bd3dc2bc}{Hugging
Face}.

</details>


### [30] [Learning a Distance for the Clustering of Patients with Amyotrophic Lateral Sclerosis](https://arxiv.org/abs/2511.01945)
*Guillaume Tejedor,Veronika Peralta,Nicolas Labroche,Patrick Marcel,Hélène Blasco,Hugo Alarcan*

Main category: cs.LG

TL;DR: 本文提出一种结合医学专业知识与弱监督学习的新型聚类方法，用于对肌萎缩侧索硬化症（ALS）患者进行分型，该方法在生存分析中优于现有技术，并提高了结果的可解释性。


<details>
  <summary>Details</summary>
Motivation: ALS患者异质性强、纵向数据稀疏且缺乏临床意义明确的患者分群定义，现有聚类方法在范围和数量上均受限，亟需更有效的个性化诊疗支持方法。

Method: 通过疾病进展声明性评分对患者序列进行聚类，整合多个描述性变量，结合现成距离度量与弱监督学习方法构建距离函数，并与多种聚类算法配对，在353名ALS患者数据集上进行评估。

Result: 所提方法在生存分析任务中优于当前最先进的聚类方法，同时获得相当的轮廓系数，且学习到的距离提升了医学专家对聚类结果的相关性与可解释性的认可。

Conclusion: 该方法有效结合医学先验知识与数据驱动技术，为ALS患者分型提供了一种更具临床意义和可解释性的聚类框架，有助于推动个性化治疗研究。

Abstract: Amyotrophic lateral sclerosis (ALS) is a severe disease with a typical
survival of 3-5 years after symptom onset. Current treatments offer only
limited life extension, and the variability in patient responses highlights the
need for personalized care. However, research is hindered by small,
heterogeneous cohorts, sparse longitudinal data, and the lack of a clear
definition for clinically meaningful patient clusters. Existing clustering
methods remain limited in both scope and number. To address this, we propose a
clustering approach that groups sequences using a disease progression
declarative score. Our approach integrates medical expertise through multiple
descriptive variables, investigating several distance measures combining such
variables, both by reusing off-the-shelf distances and employing a
weak-supervised learning method. We pair these distances with clustering
methods and benchmark them against state-of-the-art techniques. The evaluation
of our approach on a dataset of 353 ALS patients from the University Hospital
of Tours, shows that our method outperforms state-of-the-art methods in
survival analysis while achieving comparable silhouette scores. In addition,
the learned distances enhance the relevance and interpretability of results for
medical experts.

</details>


### [31] [COFAP: A Universal Framework for COFs Adsorption Prediction through Designed Multi-Modal Extraction and Cross-Modal Synergy](https://arxiv.org/abs/2511.01946)
*Zihan Li,Mingyang Wan,Mingyu Gao,Zhongshan Chen,Xiangke Wang,Feifan Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为COFAP的通用共价有机框架（COFs）吸附预测框架，利用深度学习提取多模态结构与化学特征，并通过跨模态注意力机制融合，无需亨利系数或吸附热即可实现高效准确的气体吸附性能预测，在hypoCOFs数据集上达到新SOTA。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法依赖特定气体相关特征，耗时且难以扩展，限制了在COFs庞大设计空间中高效筛选最优结构的能力。

Method: 构建COFAP框架，采用深度学习提取多模态结构和化学特征，并通过跨模态注意力机制融合这些互补特征；同时开发可调权重优先级方案，支持按应用需求灵活排序候选COFs。

Result: COFAP在不使用亨利系数或吸附热的情况下，在hypoCOFs数据集上优于以往方法，达到新的SOTA性能；研究还发现高性能分离COFs集中在较窄的孔径和比表面积范围内。

Conclusion: COFAP兼具高效性与准确性，可直接部署于晶态多孔材料的高通量筛选，为COFs在气体吸附与分离中的应用提供强大工具。

Abstract: Covalent organic frameworks (COFs) are promising adsorbents for gas
adsorption and separation, while identifying the optimal structures among their
vast design space requires efficient high-throughput screening. Conventional
machine-learning predictors rely heavily on specific gas-related features.
However, these features are time-consuming and limit scalability, leading to
inefficiency and labor-intensive processes. Herein, a universal COFs adsorption
prediction framework (COFAP) is proposed, which can extract multi-modal
structural and chemical features through deep learning, and fuse these
complementary features via cross-modal attention mechanism. Without Henry
coefficients or adsorption heat, COFAP sets a new SOTA by outperforming
previous approaches on hypoCOFs dataset. Based on COFAP, we also found that
high-performing COFs for separation concentrate within a narrow range of pore
size and surface area. A weight-adjustable prioritization scheme is also
developed to enable flexible, application-specific ranking of candidate COFs
for researchers. Superior efficiency and accuracy render COFAP directly
deployable in crystalline porous materials.

</details>


### [32] [NeuroClean: A Generalized Machine-Learning Approach to Neural Time-Series Conditioning](https://arxiv.org/abs/2511.01951)
*Manuel A. Hernandez Alonso,Michael Depass,Stephan Quessy,Numa Dancause,Ignasi Cos*

Main category: cs.LG

TL;DR: 本文提出了一种名为NeuroClean的无监督、多功能EEG/LFP信号预处理流程，通过自动去除多种伪迹，在保留任务相关信息的同时显著提升后续机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: EEG和LFP信号常受多种非脑源性伪迹和噪声干扰，亟需一种全自动、可靠且可复现的预处理方法以替代依赖人工干预的传统流程。

Method: NeuroClean是一个五步无监督预处理流程，包含带通滤波、工频噪声滤除、坏通道剔除，并结合基于聚类算法的自动独立成分分析（ICA）进行成分剔除，以保留任务相关信息。

Result: 在多个数据集上验证表明，NeuroClean能有效去除常见伪迹；在复杂度不同的运动任务中，经其处理后的数据在多项逻辑回归模型中准确率达97%以上（原始数据为74%，随机水平为33.3%）。

Conclusion: NeuroClean是一种可靠、高效的全自动EEG/LFP预处理流程，有助于提升机器学习模型的泛化能力和性能，适用于未来相关研究。

Abstract: Electroencephalography (EEG) and local field potentials (LFP) are two widely
used techniques to record electrical activity from the brain. These signals are
used in both the clinical and research domains for multiple applications.
However, most brain data recordings suffer from a myriad of artifacts and noise
sources other than the brain itself. Thus, a major requirement for their use is
proper and, given current volumes of data, a fully automatized conditioning. As
a means to this end, here we introduce an unsupervised, multipurpose EEG/LFP
preprocessing method, the NeuroClean pipeline. In addition to its completeness
and reliability, NeuroClean is an unsupervised series of algorithms intended to
mitigate reproducibility issues and biases caused by human intervention. The
pipeline is designed as a five-step process, including the common bandpass and
line noise filtering, and bad channel rejection. However, it incorporates an
efficient independent component analysis with an automatic component rejection
based on a clustering algorithm. This machine learning classifier is used to
ensure that task-relevant information is preserved after each step of the
cleaning process. We used several data sets to validate the pipeline.
NeuroClean removed several common types of artifacts from the signal. Moreover,
in the context of motor tasks of varying complexity, it yielded more than 97%
accuracy (vs. a chance-level of 33.3%) in an optimized Multinomial Logistic
Regression model after cleaning the data, compared to the raw data, which
performed at 74% accuracy. These results show that NeuroClean is a promising
pipeline and workflow that can be applied to future work and studies to achieve
better generalization and performance on machine learning pipelines.

</details>


### [33] [TapOut: A Bandit-Based Approach to Dynamic Speculative Decoding](https://arxiv.org/abs/2511.02017)
*Aditya Sridhar,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: TapOut 是一种无需训练、即插即用的动态推测解码算法，利用多臂老虎机在线选择最优推测策略，在无需超参数调优的情况下实现与现有方法相当或更优的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有动态推测解码方法依赖手工设定的敏感阈值（如 token 熵），调参成本高且跨模型和领域泛化能力差，亟需一种通用、自适应的策略选择机制。

Method: 提出 TapOut 算法，基于多臂老虎机框架，在线地从多个无参数的动态推测策略中选择最优策略，依据历史奖励与探索机制进行决策，无需训练和超参数调优。

Result: 在多种模型组合和数据集上的实验表明，TapOut 在无需任何超参数调整的情况下，取得了与现有动态推测方法相当甚至更优的推理加速效果。

Conclusion: TapOut 提供了一种高效、通用且无需调参的动态推测解码方案，显著提升了大语言模型推理的实用性与可扩展性。

Abstract: Speculative decoding accelerates LLMs by using a lightweight draft model to
generate tokens autoregressively before verifying them in parallel with a
larger target model. However, determining the optimal number of tokens to draft
remains a key challenge limiting the approach's effectiveness. Dynamic
speculative decoding aims to intelligently decide how many tokens to draft to
achieve maximum speedups. Existing methods often rely on hand-tuned, sensitive
thresholds (e.g., token entropy), which are costly to set and generalize poorly
across models and domains. We propose TapOut, an online, training-free,
plug-and-play algorithm for dynamic speculation policy selection using
multi-armed bandits. Our approach employs a meta-algorithm that selects among
multiple parameter-free dynamic speculation strategies based on past reward and
exploration. We conduct extensive experiments across diverse model pairs and
datasets, showing that TapOut achieves competitive or superior speedups
compared to well-established dynamic speculation baselines without any
hyperparameter tuning.

</details>


### [34] [Shared Parameter Subspaces and Cross-Task Linearity in Emergently Misaligned Behavior](https://arxiv.org/abs/2511.02022)
*Daniel Aarao Reis Arturi,Eric Zhang,Andrew Ansah,Kevin Zhu,Ashwinee Panda,Aishwarya Balwani*

Main category: cs.LG

TL;DR: 本文发现大语言模型在针对特定有害数据微调后，会通过共享的参数方向表现出跨任务的有害行为泛化（即“涌现性失准”），并揭示这种现象在参数空间中具有线性结构和低维子空间共性。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型在微调后为何会在不同任务中泛化出有害行为（即涌现性失准）的底层机制。

Method: 采用几何视角分析微调后模型权重更新的结构特性，包括余弦相似性、主角度、投影重叠以及线性模式连通性。

Result: 发现不同任务微调后的模型在参数更新上高度一致，共享低维子空间，并且插值模型仍保持一致的有害行为。

Conclusion: 涌现性失准源于不同任务在参数空间中发现了相同的有害行为方向，表明有害行为可能集中在可预测的权重区域，为参数空间可解释性和干预提供新方向。

Abstract: Recent work has discovered that large language models can develop broadly
misaligned behaviors after being fine-tuned on narrowly harmful datasets, a
phenomenon known as emergent misalignment (EM). However, the fundamental
mechanisms enabling such harmful generalization across disparate domains remain
poorly understood. In this work, we adopt a geometric perspective to study EM
and demonstrate that it exhibits a fundamental cross-task linear structure in
how harmful behavior is encoded across different datasets. Specifically, we
find a strong convergence in EM parameters across tasks, with the fine-tuned
weight updates showing relatively high cosine similarities, as well as shared
lower-dimensional subspaces as measured by their principal angles and
projection overlaps. Furthermore, we also show functional equivalence via
linear mode connectivity, wherein interpolated models across narrow
misalignment tasks maintain coherent, broadly misaligned behavior. Our results
indicate that EM arises from different narrow tasks discovering the same set of
shared parameter directions, suggesting that harmful behaviors may be organized
into specific, predictable regions of the weight landscape. By revealing this
fundamental connection between parametric geometry and behavioral outcomes, we
hope our work catalyzes further research on parameter space interpretability
and weight-based interventions.

</details>


### [35] [Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning](https://arxiv.org/abs/2511.02044)
*Vivswan Shah,Randy Cogill,Hanwei Yue,Gopinath Chennupati,Rinat Khaziev*

Main category: cs.LG

TL;DR: 在微调大语言模型（LLM）进行分类任务时，为每个标签附加简短解释（即使是无意义但词汇对齐的随机词序列）能显著提升模型性能，其增益主要源于额外的token带来的结构化计算和正则化效应，而非解释本身的语义内容。


<details>
  <summary>Details</summary>
Motivation: 探究在微调LLM进行分类时，附加简短解释是否比仅使用标签能获得更好的模型性能，并进一步理解性能提升的来源是解释的语义还是其他结构性因素。

Method: 在六个对话数据集上，使用集成生成的数据对7B参数模型进行微调，比较“标签+解释”与“仅标签”两种训练方式；同时引入无语义但词汇对齐的随机token（如打乱顺序或词袋）作为伪解释进行对照实验，并通过内部激活熵和预测分布分析模型行为。

Result: “标签+解释”训练在18种数据集与任务设置中均优于基线；令人意外的是，即使使用无语义的随机token伪解释，也能显著提升准确率并缩小与真实解释的差距；内部分析显示此类训练提高了中间层激活熵并使输出层预测更集中。

Conclusion: 解释增强的微调（无论使用真实解释还是精心构造的随机token）能提升LLM分类的准确性和可靠性，其核心机制在于token级结构对模型计算的正则化和丰富化作用，而非解释的语义内容。

Abstract: Fine-tuning LLMs for classification typically maps inputs directly to labels.
We ask whether attaching brief explanations to each label during fine-tuning
yields better models. We evaluate conversational response quality along three
axes: naturalness, comprehensiveness, and on-topic adherence, each rated on
5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune
a 7B-parameter model and test across six diverse conversational datasets.
Across 18 dataset, task settings, label-plus-explanation training outperforms
label-only baselines.
  A central and unexpected result concerns random tokens. We replace
human-written explanations with text that is syntactically incoherent yet
vocabulary-aligned with the originals (e.g., shuffled or bag-of-words
variants). Despite lacking semantics, these pseudo-explanations still improve
accuracy over label-only training and often narrow much of the gap to true
explanations. The effect persists across datasets and training seeds,
indicating that gains arise less from meaning than from structure: the extra
token budget encourages richer intermediate computation and acts as a
regularizer that reduces over-confident shortcuts.
  Internal analyses support this view: explanation-augmented models exhibit
higher activation entropy in intermediate layers alongside sharper predictive
mass at the output layer, consistent with increased deliberation before
decision. Overall, explanation-augmented fine-tuning, whether with genuine
rationales or carefully constructed random token sequences, improves accuracy
and reliability for LLM classification while clarifying how token-level
scaffolding shapes computation during inference.

</details>


### [36] [LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability of CCS](https://arxiv.org/abs/2511.02089)
*Stefan F. Schouten,Peter Bloem*

Main category: cs.LG

TL;DR: 本文重新审视了对比一致性搜索（CCS）方法，提出应优化相对对比一致性，并将其重构为一个特征值问题，从而获得闭式解、提升稳定性并拓展其应用。


<details>
  <summary>Details</summary>
Motivation: 现有CCS方法的双项目标机制理解不充分，且对随机初始化敏感，作者旨在澄清其机制并扩展其适用性。

Method: 将CCS重新表述为一个特征值问题，基于相对对比一致性的思想，推导出具有可解释特征值的闭式解，并自然推广到多变量情形。

Result: 在多个数据集上的实验表明，新方法性能与原始CCS相当，同时避免了对随机初始化的敏感性问题。

Conclusion: 相对化对比一致性不仅加深了对CCS的理解，还为更广泛的探针和机制可解释性方法提供了新路径。

Abstract: Contrast-Consistent Search (CCS) is an unsupervised probing method able to
test whether large language models represent binary features, such as sentence
truth, in their internal activations. While CCS has shown promise, its two-term
objective has been only partially understood. In this work, we revisit CCS with
the aim of clarifying its mechanisms and extending its applicability. We argue
that what should be optimized for, is relative contrast consistency. Building
on this insight, we reformulate CCS as an eigenproblem, yielding closed-form
solutions with interpretable eigenvalues and natural extensions to multiple
variables. We evaluate these approaches across a range of datasets, finding
that they recover similar performance to CCS, while avoiding problems around
sensitivity to random initialization. Our results suggest that relativizing
contrast consistency not only improves our understanding of CCS but also opens
pathways for broader probing and mechanistic interpretability methods.

</details>


### [37] [Flashlight: PyTorch Compiler Extensions to Accelerate Attention Variants](https://arxiv.org/abs/2511.02043)
*Bozhi You,Irene Wang,Zelal Su Mustafaoglu,Abhinav Jangda,Angélica Moreira,Roshan Dathathri,Divya Mahajan,Keshav Pingali*

Main category: cs.LG

TL;DR: 本文提出了 Flashlight，一个内置于 PyTorch 编译流程中的编译器原生框架，可自动为任意基于注意力的程序生成融合的、类似 FlashAttention 的高效内核，无需依赖静态模板或预定义内核，支持比 FlexAttention 更广泛的注意力变体。


<details>
  <summary>Details</summary>
Motivation: 现有注意力变体通常需要专门优化的内核或手工调优实现，难以高效支持；FlexAttention 虽通过静态模板部分缓解该问题，但仍受限于表达能力，无法处理数据依赖型注意力。

Method: Flashlight 利用 PyTorch 的编译工作流，通过自动融合与分块技术，透明地生成适用于任意注意力程序的高效内核，无需静态模板或预定义特化。

Result: Flashlight 在性能上与 FlexAttention 相当甚至更优，同时支持更广泛的注意力变体（包括数据依赖型），并保留原生 PyTorch 代码的灵活性。

Conclusion: Flashlight 提供了一种通用、高效且灵活的注意力内核生成方案，使开发者能快速探索新型注意力模型而不牺牲性能。

Abstract: Bad charactors when submitting to arXiv: Attention is a fundamental building
block of large language models (LLMs), so there have been many efforts to
implement it efficiently. For example, FlashAttention leverages tiling and
kernel fusion to optimize attention. Recently, a number of variants of
attention have been introduced to enhance model quality or efficiency.
Supporting them efficiently remains difficult since they usually require
specialized kernels or hand-tuned implementations. FlexAttention recently
addressed part of this gap by using static programming templates to support
FlashAttention-like kernels for a subset of attention variants.
  In this paper, we introduce Flashlight, a compiler-native framework within
the PyTorch ecosystem that automatically generates fused, FlashAttention-style
kernels for arbitrary attention-based programs, without relying on static
templates or predefined kernel specializations. Flashlight leverages PyTorch's
compilation workflow to fuse and tile attention computations transparently,
enabling efficient execution for diverse attention patterns. Not only does it
support all variants expressible in the FlexAttention model but it also handles
more general, data-dependent attention formulations that are beyond the
capabilities of FlexAttention.
  Our results show that Flashlight produces kernels with competitive or
superior performance to FlexAttention, while offering the flexibility of native
PyTorch code, enabling developers to rapidly explore new attention models
without sacrificing performance.

</details>


### [38] [A Dual-Use Framework for Clinical Gait Analysis: Attention-Based Sensor Optimization and Automated Dataset Auditing](https://arxiv.org/abs/2511.02047)
*Hamidreza Sadeghsalehi*

Main category: cs.LG

TL;DR: 本文提出了一种基于多流注意力机制的深度学习框架，用于可穿戴传感器步态分析，既能优化传感器配置，又能自动审计数据集偏差。在对Voisard等人（2025）的多队列步态数据集进行分析时，模型揭示了严重的数据集侧向性偏差（如OA和CVA筛查任务中模型几乎只关注右脚），并提出了新的数据驱动传感器组合假设（如头部+脚部用于帕金森病筛查）。


<details>
  <summary>Details</summary>
Motivation: 现有基于可穿戴传感器和AI的步态分析模型易受隐藏数据集偏差影响，且缺乏针对特定任务的传感器优化方法，亟需一种可解释、能自动检测数据偏差并优化传感器配置的方法。

Method: 提出一种多流注意力机制的深度学习框架，将其同时用作传感器优化器和自动数据审计工具，在四个临床任务（帕金森病、骨关节炎、脑卒中筛查及帕金森病与脑卒中的鉴别）上应用该模型分析公开步态数据集。

Result: 模型在OA和CVA筛查任务中发现严重数据集侧向性偏差（如15例OA均为右侧），注意力分配极度偏向右脚（>70%），而左脚几乎被忽略（<0.1%）。此外，模型提出了新的传感器组合策略（如头部+脚部用于帕金森病筛查）。

Conclusion: 该研究的主要贡献在于提出了一种可解释的深度学习框架，能够自动审计数据集完整性；次要贡献是通过数据驱动方式提出潜在的优化传感器配置方案，为未来临床协议设计提供假设。

Abstract: Objective gait analysis using wearable sensors and AI is critical for
managing neurological and orthopedic conditions. However, models are vulnerable
to hidden dataset biases, and task-specific sensor optimization remains a
challenge. We propose a multi-stream attention-based deep learning framework
that functions as both a sensor optimizer and an automated data auditor.
Applied to the Voisard et al. (2025) multi-cohort gait dataset on four clinical
tasks (PD, OA, CVA screening; PD vs CVA differential), the model's attention
mechanism quantitatively discovered a severe dataset confound. For OA and CVA
screening, tasks where bilateral assessment is clinically essential, the model
assigned more than 70 percent attention to the Right Foot while statistically
ignoring the Left Foot (less than 0.1 percent attention, 95 percent CI
[0.0-0.1]). This was not a clinical finding but a direct reflection of a severe
laterality bias (for example, 15 of 15 right-sided OA) in the public dataset.
The primary contribution of this work is methodological, demonstrating that an
interpretable framework can automatically audit dataset integrity. As a
secondary finding, the model proposes novel, data-driven sensor synergies (for
example, Head plus Foot for PD screening) as hypotheses for future optimized
protocols.

</details>


### [39] [Finding Probably Approximate Optimal Solutions by Training to Estimate the Optimal Values of Subproblems](https://arxiv.org/abs/2511.02048)
*Nimrod Megiddo,Segev Wasserkrug,Orit Davidovich,Shimrit Shtern*

Main category: cs.LG

TL;DR: 本文提出了一种用于最大化二元变量实值函数的求解器，其核心算法通过估计目标函数最优值来训练，无需计算策略值或依赖已求解实例。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常依赖于计算策略值或使用已求解实例进行训练，而本文旨在开发一种不依赖这些条件的新求解方法。

Method: 基于一个不等式构建损失函数，该损失函数采用偏离最优性条件的期望总偏差，而非目标函数本身，从而训练一个估计器来预测最优目标值。

Result: 所提出的求解器能够有效估计来自目标分布及其子实例的最优目标函数值，且无需显式求解实例或评估策略值。

Conclusion: 该方法提供了一种新颖的训练机制，适用于二元变量优化问题，在不依赖传统求解信息的情况下仍能有效逼近最优解。

Abstract: The paper is about developing a solver for maximizing a real-valued function
of binary variables. The solver relies on an algorithm that estimates the
optimal objective-function value of instances from the underlying distribution
of objectives and their respective sub-instances. The training of the estimator
is based on an inequality that facilitates the use of the expected total
deviation from optimality conditions as a loss function rather than the
objective-function itself. Thus, it does not calculate values of policies, nor
does it rely on solved instances.

</details>


### [40] [Can LLMs subtract numbers?](https://arxiv.org/abs/2511.02795)
*Mayank Jobanputra,Nils Philipp Walter,Maitrey Mehta,Blerta Veseli,Evan Parker Kelly Chapple,Yifan Wang,Sneha Chetani,Ellie Pavlick,Antonio Vergari,Vera Demberg*

Main category: cs.LG

TL;DR: 该论文系统研究了大语言模型（LLMs）在减法任务中的表现，发现其准确率显著低于加法，尤其在被减数小于减数（a < b）时容易遗漏负号；尽管模型内部能编码负号信息，但输出时常未体现；通过少样本提示和指令微调可部分或显著改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 减法作为非交换运算，在现有大语言模型算术能力评估中被忽视，而其结构特性可能带来独特挑战，因此有必要系统研究LLMs在减法任务中的表现与局限。

Method: 评估八个来自四个模型家族的预训练大语言模型在加法与减法任务上的表现；分析错误模式，特别是a < b情形下的负号遗漏问题；通过探针分析模型内部是否编码负号信息；测试少样本学习和指令微调对性能的提升效果。

Result: 减法准确率显著低于加法；当a < b时，模型常输出正确数值但遗漏负号；探针分析表明模型内部能识别结果应为负，但输出阶段未能体现；少样本提示带来小幅提升，而指令微调模型在负号生成上接近完美。

Conclusion: 大语言模型在减法任务中存在系统性缺陷，尤其在负号生成方面，但该能力可通过指令微调有效恢复，说明其算术能力具有可修复性，研究为理解LLMs的数学推理机制提供了新视角。

Abstract: We present a systematic study of subtraction in large language models (LLMs).
While prior benchmarks emphasize addition and multiplication, subtraction has
received comparatively little attention despite being structurally distinct as
a non-commutative operation. We evaluate eight pretrained LLMs spanning four
families on addition and subtraction problems. Our experiments reveal that
subtraction accuracy lags behind addition by a wide margin. We find that the
errors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs
frequently produce the correct magnitude but omit the negative sign. Probing
analyses show that LLMs internally encode whether results should be negative,
yet this information is often not reflected in generated outputs. We further
test well-known techniques such as few-shot learning and instruction-tuning to
see if they can improve the LLMs' performance. Our results suggest that while
few-shot prompting yields modest gains, the instruction-tuned models achieve
near-perfect accuracies in generating the negative sign. Together, these
findings provide a clearer characterization of the limitations and
recoverability of LLMs' arithmetic capabilities in subtraction.

</details>


### [41] [Uncertainty Guided Online Ensemble for Non-stationary Data Streams in Fusion Science](https://arxiv.org/abs/2511.02092)
*Kishansingh Rajput,Malachi Schram,Brian Sammuli,Sen Lin*

Main category: cs.LG

TL;DR: 本文将在线学习应用于聚变装置中非平稳数据流的预测问题，提出一种基于不确定性的在线集成方法，显著提升了模型性能并提供可靠的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 聚变实验数据具有非平稳性，传统机器学习模型因假设数据分布平稳而在实际应用中性能下降，亟需适应数据漂移的在线学习方法。

Method: 提出一种不确定性引导的在线集成方法，利用深度高斯过程近似（DGPA）进行校准的不确定性估计，并基于不确定性指导集成不同历史窗口训练的多个学习器进行预测。

Result: 相比静态模型，在线学习将预测误差降低80%；所提出的不确定性引导在线集成方法比标准单模型在线学习进一步降低误差约10%。

Conclusion: 在线学习对维持聚变装置中机器学习模型性能至关重要，结合不确定性引导的集成策略可进一步提升预测准确性并为决策提供不确定性信息。

Abstract: Machine Learning (ML) is poised to play a pivotal role in the development and
operation of next-generation fusion devices. Fusion data shows non-stationary
behavior with distribution drifts, resulted by both experimental evolution and
machine wear-and-tear. ML models assume stationary distribution and fail to
maintain performance when encountered with such non-stationary data streams.
Online learning techniques have been leveraged in other domains, however it has
been largely unexplored for fusion applications. In this paper, we present an
application of online learning to continuously adapt to drifting data stream
for prediction of Toroidal Field (TF) coils deflection at the DIII-D fusion
facility. The results demonstrate that online learning is critical to maintain
ML model performance and reduces error by 80% compared to a static model.
Moreover, traditional online learning can suffer from short-term performance
degradation as ground truth is not available before making the predictions. As
such, we propose an uncertainty guided online ensemble method to further
improve the performance. The Deep Gaussian Process Approximation (DGPA)
technique is leveraged for calibrated uncertainty estimation and the
uncertainty values are then used to guide a meta-algorithm that produces
predictions based on an ensemble of learners trained on different horizon of
historical data. The DGPA also provides uncertainty estimation along with the
predictions for decision makers. The online ensemble and the proposed
uncertainty guided online ensemble reduces predictions error by about 6%, and
10% respectively over standard single model based online learning.

</details>


### [42] [Geometric Data Valuation via Leverage Scores](https://arxiv.org/abs/2511.02100)
*Rodrigo Mendoza-Smith*

Main category: cs.LG

TL;DR: 本文提出了一种基于统计杠杆分数的几何方法，作为Shapley数据估值的高效替代方案，满足其关键公理，并在理论和实验上证明了该方法在数据子集选择中能有效逼近全数据训练效果，且优于标准基线方法。


<details>
  <summary>Details</summary>
Motivation: Shapley数据估值虽具有理论优势，但计算复杂度高，难以在大规模数据上应用，因此需要一种计算高效且保留其核心性质的数据估值方法。

Method: 采用统计杠杆分数（及其岭版本）来衡量数据点在表示空间中的结构影响力，通过其对数据集张成空间和有效维度的贡献进行估值，并用于数据子集采样。

Result: 所提杠杆分数满足Shapley估值的若干公理；基于其采样的子集训练所得模型在参数和预测风险上与全数据最优解相差仅$O(\varepsilon)$；在主动学习实验中，岭杠杆采样优于无需梯度的标准基线方法。

Conclusion: 基于杠杆分数的数据估值方法在保持Shapley估值理论性质的同时，显著提升了计算效率，并在下游任务中展现出优越性能，为大规模数据估值与选择提供了实用解决方案。

Abstract: Shapley data valuation provides a principled, axiomatic framework for
assigning importance to individual datapoints, and has gained traction in
dataset curation, pruning, and pricing. However, it is a combinatorial measure
that requires evaluating marginal utility across all subsets of the data,
making it computationally infeasible at scale. We propose a geometric
alternative based on statistical leverage scores, which quantify each
datapoint's structural influence in the representation space by measuring how
much it extends the span of the dataset and contributes to the effective
dimensionality of the training problem. We show that our scores satisfy the
dummy, efficiency, and symmetry axioms of Shapley valuation and that extending
them to \emph{ridge leverage scores} yields strictly positive marginal gains
that connect naturally to classical A- and D-optimal design criteria. We
further show that training on a leverage-sampled subset produces a model whose
parameters and predictive risk are within $O(\varepsilon)$ of the full-data
optimum, thereby providing a rigorous link between data valuation and
downstream decision quality. Finally, we conduct an active learning experiment
in which we empirically demonstrate that ridge-leverage sampling outperforms
standard baselines without requiring access gradients or backward passes.

</details>


### [43] [Measuring the Intrinsic Dimension of Earth Representations](https://arxiv.org/abs/2511.02101)
*Arjun Rao,Marc Rußwurm,Konstantin Klemmer,Esther Rolf*

Main category: cs.LG

TL;DR: 本文首次研究了地理隐式神经表示（INRs）的内在维度，发现其内在维度通常在2到10之间，并受空间分辨率和输入模态影响；该维度与下游任务性能相关，可作为无监督评估和模型选择的有效指标。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对地理隐式神经表示（INRs）中信息含量及其分布的理解，亟需一种方法来量化这些地球表征所包含的信息。

Method: 通过分析地理INRs的内在维度（intrinsic dimensionality），评估其在不同空间分辨率和输入模态下的变化，并探讨该维度与下游任务性能之间的关系。

Result: 地理INRs的内在维度大致介于2到10之间，对预训练时的空间分辨率和输入模态敏感；该维度与下游任务表现相关，还能揭示空间伪影。

Conclusion: 内在维度可作为一种与架构无关、无需标签的信息含量度量，有助于INRs的无监督评估、模型选择和预训练设计。

Abstract: Within the context of representation learning for Earth observation,
geographic Implicit Neural Representations (INRs) embed low-dimensional
location inputs (longitude, latitude) into high-dimensional embeddings, through
models trained on geo-referenced satellite, image or text data. Despite the
common aim of geographic INRs to distill Earth's data into compact,
learning-friendly representations, we lack an understanding of how much
information is contained in these Earth representations, and where that
information is concentrated. The intrinsic dimension of a dataset measures the
number of degrees of freedom required to capture its local variability,
regardless of the ambient high-dimensional space in which it is embedded. This
work provides the first study of the intrinsic dimensionality of geographic
INRs. Analyzing INRs with ambient dimension between 256 and 512, we find that
their intrinsic dimensions fall roughly between 2 and 10 and are sensitive to
changing spatial resolution and input modalities during INR pre-training.
Furthermore, we show that the intrinsic dimension of a geographic INR
correlates with downstream task performance and can capture spatial artifacts,
facilitating model evaluation and diagnostics. More broadly, our work offers an
architecture-agnostic, label-free metric of information content that can enable
unsupervised evaluation, model selection, and pre-training design across INRs.

</details>


### [44] [Matrix Sensing with Kernel Optimal Loss: Robustness and Optimization Landscape](https://arxiv.org/abs/2511.02122)
*Xinyuan Song,Jiaye Teng,Ziye Ma*

Main category: cs.LG

TL;DR: 本文通过研究含噪矩阵感知问题，提出一种基于非参数回归的鲁棒损失函数，该函数在高斯噪声下等价于均方误差（MSE），但在重尾或非高斯噪声下更稳定，并能改善优化景观，消除虚假局部极小值，从而提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统回归任务中常用的均方误差（MSE）损失在面对非高斯或重尾噪声时表现不可靠，因此需要一种更具鲁棒性的损失函数来提升非凸优化问题在噪声环境下的稳定性与性能。

Method: 采用基于非参数回归的鲁棒损失函数，利用核方法估计残差密度并最大化其对数似然；同时通过分析限制等距性（RIP）常数的上界，研究该损失函数对优化景观的影响。

Result: 理论与实验结果表明，所提出的鲁棒损失函数在处理大噪声和多种噪声分布时表现优异，能够有效消除虚假局部极小值，提升优化稳定性。

Conclusion: 仅通过更换损失函数，即可显著增强机器学习任务对噪声的鲁棒性，本文为此提供了一个直观且普适的分析框架。

Abstract: In this paper we study how the choice of loss functions of non-convex
optimization problems affects their robustness and optimization landscape,
through the study of noisy matrix sensing. In traditional regression tasks,
mean squared error (MSE) loss is a common choice, but it can be unreliable for
non-Gaussian or heavy-tailed noise. To address this issue, we adopt a robust
loss based on nonparametric regression, which uses a kernel-based estimate of
the residual density and maximizes the estimated log-likelihood. This robust
formulation coincides with the MSE loss under Gaussian errors but remains
stable under more general settings. We further examine how this robust loss
reshapes the optimization landscape by analyzing the upper-bound of restricted
isometry property (RIP) constants for spurious local minima to disappear.
Through theoretical and empirical analysis, we show that this new loss excels
at handling large noise and remains robust across diverse noise distributions.
This work offers initial insights into enhancing the robustness of machine
learning tasks through simply changing the loss, guided by an intuitive and
broadly applicable analytical framework.

</details>


### [45] [Variance-Aware Feel-Good Thompson Sampling for Contextual Bandits](https://arxiv.org/abs/2511.02123)
*Xuheng Li,Quanquan Gu*

Main category: cs.LG

TL;DR: 本文提出了FGTSVA，一种适用于通用奖励函数的方差感知Thompson采样算法，在上下文老虎机问题中实现了最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在基于UCB的算法上，而基于采样的算法（如Thompson采样）在方差相关遗憾界方面研究不足；现有的LinVDTS算法仅限于线性奖励函数且遗憾界在模型维度上非最优。

Method: 提出FGTSVA算法，通过引入新的解耦系数（decoupling coefficient, dc）来刻画模型空间复杂度，并将其融入方差感知的Thompson采样框架中。

Result: FGTSVA在通用奖励函数下达到遗憾界$\tilde{O}(\sqrt{\mathrm{dc}\cdot\log|\mathcal{F}|\sum_{t=1}^T\sigma_t^2}+\mathrm{dc})$；在线性上下文老虎机设定下，其遗憾界与基于UCB并使用加权线性回归的算法相当。

Conclusion: FGTSVA是首个在通用奖励函数下实现最优方差相关遗憾界的Thompson采样算法，弥补了现有采样类算法在此方向上的理论空白。

Abstract: Variance-dependent regret bounds have received increasing attention in recent
studies on contextual bandits. However, most of these studies are focused on
upper confidence bound (UCB)-based bandit algorithms, while sampling based
bandit algorithms such as Thompson sampling are still understudied. The only
exception is the LinVDTS algorithm (Xu et al., 2023), which is limited to
linear reward function and its regret bound is not optimal with respect to the
model dimension. In this paper, we present FGTSVA, a variance-aware Thompson
Sampling algorithm for contextual bandits with general reward function with
optimal regret bound. At the core of our analysis is an extension of the
decoupling coefficient, a technique commonly used in the analysis of Feel-good
Thompson sampling (FGTS) that reflects the complexity of the model space. With
the new decoupling coefficient denoted by $\mathrm{dc}$, FGTS-VA achieves the
regret of
$\tilde{O}(\sqrt{\mathrm{dc}\cdot\log|\mathcal{F}|\sum_{t=1}^T\sigma_t^2}+\mathrm{dc})$,
where $|\mathcal{F}|$ is the size of the model space, $T$ is the total number
of rounds, and $\sigma_t^2$ is the subgaussian norm of the noise (e.g.,
variance when the noise is Gaussian) at round $t$. In the setting of contextual
linear bandits, the regret bound of FGTSVA matches that of UCB-based algorithms
using weighted linear regression (Zhou and Gu, 2022).

</details>


### [46] [Disentangling Causal Substructures for Interpretable and Generalizable Drug Synergy Prediction](https://arxiv.org/abs/2511.02146)
*Yi Luo,Haochen Zhao,Xiao Liang,Yiwei Liu,Yuye Zhang,Xinyu Li,Jianxin Wang*

Main category: cs.LG

TL;DR: CausalDDS is a novel drug synergy prediction framework that improves accuracy and interpretability by disentangling drug molecules into causal and spurious substructures and using a conditional intervention mechanism guided by sufficiency and independence principles.


<details>
  <summary>Details</summary>
Motivation: Existing drug synergy prediction methods often act as black-box models relying on statistical correlations, lacking interpretability and robustness, especially in cold start and out-of-distribution scenarios.

Method: CausalDDS disentangles drug molecules into causal and spurious substructures, uses causal substructures for prediction, applies a conditional intervention mechanism based on paired molecular structures, and optimizes with a novel objective based on sufficiency and independence.

Result: CausalDDS outperforms baseline models in drug synergy prediction, especially in cold start and out-of-distribution settings, and successfully identifies key causal substructures, offering molecular-level interpretability.

Conclusion: CausalDDS enhances both the accuracy and interpretability of drug synergy prediction by focusing on causal molecular substructures, showing strong potential for practical use in drug discovery.

Abstract: Drug synergy prediction is a critical task in the development of effective
combination therapies for complex diseases, including cancer. Although existing
methods have shown promising results, they often operate as black-box
predictors that rely predominantly on statistical correlations between drug
characteristics and results. To address this limitation, we propose CausalDDS,
a novel framework that disentangles drug molecules into causal and spurious
substructures, utilizing the causal substructure representations for predicting
drug synergy. By focusing on causal sub-structures, CausalDDS effectively
mitigates the impact of redundant features introduced by spurious
substructures, enhancing the accuracy and interpretability of the model. In
addition, CausalDDS employs a conditional intervention mechanism, where
interventions are conditioned on paired molecular structures, and introduces a
novel optimization objective guided by the principles of sufficiency and
independence. Extensive experiments demonstrate that our method outperforms
baseline models, particularly in cold start and out-of-distribution settings.
Besides, CausalDDS effectively identifies key substructures underlying drug
synergy, providing clear insights into how drug combinations work at the
molecular level. These results underscore the potential of CausalDDS as a
practical tool for predicting drug synergy and facilitating drug discovery.

</details>


### [47] [CFL: On the Use of Characteristic Function Loss for Domain Alignment in Machine Learning](https://arxiv.org/abs/2511.02148)
*Abdullah Almansour,Ozan Tonguz*

Main category: cs.LG

TL;DR: 本文提出使用特征函数（CF）作为频域方法，有效度量高维空间中的分布偏移，并适用于领域自适应任务。


<details>
  <summary>Details</summary>
Motivation: 由于机器学习模型在现实部署中常因分布偏移问题而性能下降，尤其在高风险应用中可能带来严重后果，因此亟需更有效的分布偏移度量方法。

Method: 采用特征函数（Characteristic Function, CF）作为频域方法，用于衡量高维空间中的分布偏移。

Result: 特征函数被证明是一种强大的替代方法，可用于高维空间中的分布偏移度量和领域自适应。

Conclusion: 相比传统的统计方法，基于特征函数的频域方法在处理高维分布偏移问题上具有显著优势，适用于实际高风险场景中的模型部署与适应。

Abstract: Machine Learning (ML) models are extensively used in various applications due
to their significant advantages over traditional learning methods. However, the
developed ML models often underperform when deployed in the real world due to
the well-known distribution shift problem. This problem can lead to a
catastrophic outcomes when these decision-making systems have to operate in
high-risk applications. Many researchers have previously studied this problem
in ML, known as distribution shift problem, using statistical techniques (such
as Kullback-Leibler, Kolmogorov-Smirnov Test, Wasserstein distance, etc.) to
quantify the distribution shift. In this letter, we show that using
Characteristic Function (CF) as a frequency domain approach is a powerful
alternative for measuring the distribution shift in high-dimensional space and
for domain adaptation.

</details>


### [48] [ProtoTSNet: Interpretable Multivariate Time Series Classification With Prototypical Parts](https://arxiv.org/abs/2511.02152)
*Bartłomiej Małkus,Szymon Bobek,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: 本文提出了ProtoTSNet，一种基于ProtoPNet改进的可解释多变量时间序列分类方法，通过引入分组卷积编码器并结合自编码器预训练，在保持高准确率的同时提供对领域专家友好的可解释性，在UEA数据集上优于现有可解释方法。


<details>
  <summary>Details</summary>
Motivation: 在工业和医疗等关键领域，时间序列数据的分类不仅需要高准确率，还需要模型具备可解释性，以支持高风险决策。现有可解释方法在时间序列任务中面临动态模式捕捉和特征重要性处理等挑战。

Method: ProtoTSNet通过改进ProtoPNet架构，引入一种可作为自编码器预训练的分组卷积编码器，以保留并量化特征重要性，从而实现对多变量时间序列的可解释分类。

Result: 在UEA档案的30个多变量时间序列数据集上，ProtoTSNet在事前可解释方法中表现最佳，且与非可解释及事后可解释方法性能相当。

Conclusion: ProtoTSNet在保证竞争力性能的同时，提供了对领域专家可理解的可解释结果，有效应对了时间序列分类中的可解释性挑战。

Abstract: Time series data is one of the most popular data modalities in critical
domains such as industry and medicine. The demand for algorithms that not only
exhibit high accuracy but also offer interpretability is crucial in such
fields, as decisions made there bear significant consequences. In this paper,
we present ProtoTSNet, a novel approach to interpretable classification of
multivariate time series data, through substantial enhancements to the
ProtoPNet architecture. Our method is tailored to overcome the unique
challenges of time series analysis, including capturing dynamic patterns and
handling varying feature significance. Central to our innovation is a modified
convolutional encoder utilizing group convolutions, pre-trainable as part of an
autoencoder and designed to preserve and quantify feature importance. We
evaluated our model on 30 multivariate time series datasets from the UEA
archive, comparing our approach with existing explainable methods as well as
non-explainable baselines. Through comprehensive evaluation and ablation
studies, we demonstrate that our approach achieves the best performance among
ante-hoc explainable methods while maintaining competitive performance with
non-explainable and post-hoc explainable approaches, providing interpretable
results accessible to domain experts.

</details>


### [49] [Tackling Incomplete Data in Air Quality Prediction: A Bayesian Deep Learning Framework for Uncertainty Quantification](https://arxiv.org/abs/2511.02175)
*Yuzhuang Pian,Taiyu Wang,Shiqi Zhang,Rui Xu,Yonghong Liu*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的时空贝叶斯神经场模型（CGLUBNF），用于处理空气质量预测中常见的缺失观测数据问题，通过图注意力编码器、通道门控学习单元和贝叶斯推理，实现了更准确的预测和更可靠的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 实际空气质量监测数据常因采集和传输问题而存在不同程度和模式的缺失，导致传统方法难以进行可靠的推断与风险评估，甚至产生过度自信的外推结果。

Method: 提出CGLUBNF框架，结合傅里叶特征与图注意力编码器捕捉多尺度空间依赖和季节性时间动态，引入带可学习激活函数和门控残差连接的通道门控学习单元以自适应筛选特征，并采用贝叶斯推理联合优化预测分布与参数不确定性。

Result: 在两个真实数据集上系统评估了四种典型缺失模式，CGLUBNF在预测精度和置信区间锐度方面均优于五种先进基线方法，并在多步预测和外部变量贡献分析中展现出良好鲁棒性。

Conclusion: 该研究为在存在缺失观测的新兴感知范式（如车载移动监测）下，实现可靠的深度学习时空预测提供了基础方法。

Abstract: Accurate air quality forecasts are vital for public health alerts, exposure
assessment, and emissions control. In practice, observational data are often
missing in varying proportions and patterns due to collection and transmission
issues. These incomplete spatiotemporal records impede reliable inference and
risk assessment and can lead to overconfident extrapolation. To address these
challenges, we propose an end to end framework, the channel gated learning unit
based spatiotemporal bayesian neural field (CGLUBNF). It uses Fourier features
with a graph attention encoder to capture multiscale spatial dependencies and
seasonal temporal dynamics. A channel gated learning unit, equipped with
learnable activations and gated residual connections, adaptively filters and
amplifies informative features. Bayesian inference jointly optimizes predictive
distributions and parameter uncertainty, producing point estimates and
calibrated prediction intervals. We conduct a systematic evaluation on two real
world datasets, covering four typical missing data patterns and comparing
against five state of the art baselines. CGLUBNF achieves superior prediction
accuracy and sharper confidence intervals. In addition, we further validate
robustness across multiple prediction horizons and analysis the contribution of
extraneous variables. This research lays a foundation for reliable deep
learning based spatio-temporal forecasting with incomplete observations in
emerging sensing paradigms, such as real world vehicle borne mobile monitoring.

</details>


### [50] [Learning Interactive World Model for Object-Centric Reinforcement Learning](https://arxiv.org/abs/2511.02225)
*Fan Feng,Phillip Lippe,Sara Magliacane*

Main category: cs.LG

TL;DR: FIOC-WM 是一个统一框架，通过显式建模对象及其交互的结构化表示，提升强化学习中策略的样本效率与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有以对象为中心的强化学习方法通常仅对单个对象的状态进行分解，而将对象间的交互隐式处理，限制了策略的鲁棒性和迁移性。

Method: FIOC-WM 从像素中学习对象中心的潜在表示和交互结构，利用预训练视觉编码器；随后将任务分解为可组合的交互原语，并训练一个分层策略：高层决定交互类型与顺序，低层执行具体动作。

Result: 在模拟机器人和具身AI基准测试中，FIOC-WM 相较于现有世界模型基线，在策略学习的样本效率和泛化能力上均有提升。

Conclusion: 显式且模块化的交互学习对于实现鲁棒控制至关重要，FIOC-WM 为此提供了一个有效框架。

Abstract: Agents that understand objects and their interactions can learn policies that
are more robust and transferable. However, most object-centric RL methods
factor state by individual objects while leaving interactions implicit. We
introduce the Factored Interactive Object-Centric World Model (FIOC-WM), a
unified framework that learns structured representations of both objects and
their interactions within a world model. FIOC-WM captures environment dynamics
with disentangled and modular representations of object interactions, improving
sample efficiency and generalization for policy learning. Concretely, FIOC-WM
first learns object-centric latents and an interaction structure directly from
pixels, leveraging pre-trained vision encoders. The learned world model then
decomposes tasks into composable interaction primitives, and a hierarchical
policy is trained on top: a high level selects the type and order of
interactions, while a low level executes them. On simulated robotic and
embodied-AI benchmarks, FIOC-WM improves policy-learning sample efficiency and
generalization over world-model baselines, indicating that explicit, modular
interaction learning is crucial for robust control.

</details>


### [51] [Opportunistic Expert Activation: Batch-Aware Expert Routing for Faster Decode Without Retraining](https://arxiv.org/abs/2511.02237)
*Costin-Andrei Oncescu,Qingyang Wu,Wai Tong Chung,Robert Wu,Bryan Gopal,Junxiong Wang,Tri Dao,Ben Athiwaratkun*

Main category: cs.LG

TL;DR: 本文提出一种动态重路由机制，在保持模型质量不变的前提下，通过批感知的专家分配策略减少激活专家数量，从而显著降低Mixture-of-Experts（MoE）大语言模型在自回归生成中的解码延迟。


<details>
  <summary>Details</summary>
Motivation: MoE架构在自回归生成中因内存受限导致延迟主要由激活专家数量决定，即使批量适中也会出现性能瓶颈，因此需要优化token到expert的映射以减少激活专家数。

Method: 引入动态重路由框架，采用批感知路由策略，让当前token复用同批次其他token已加载进内存的关键专家，从而减少需激活的专家总数。

Result: 在Qwen3-30B和Qwen3-235B模型上、批量为16的实验中，该方法在无显著精度损失的情况下，分别实现了39%和15%的MoE层解码延迟降低。

Conclusion: 所提出的动态重路由方法能有效降低MoE模型推理延迟，同时维持模型性能，为高效部署大规模MoE语言模型提供了实用解决方案。

Abstract: An increasing number of LLMs employ Mixture-of-Experts (MoE) architectures
where the feed-forward layer is replaced by a pool of experts and each token
only activates a small subset of them. During autoregressive generation, these
models often enter a memory-bound regime even for moderate batch sizes because
the average expert load grows more slowly than in an equivalent dense
feedforward layer. Consequently, MoE latency is governed by the number of
activated experts. We introduce a framework for dynamically re-routing
token-to-expert mapping to lower this number (and thus, the decode latency)
while preserving a comparable quality. Our best results use a batch-aware
routing that works by having tokens piggyback experts that have already been
loaded into memory due to being crucial to other tokens within the same batch.
Empirically, we evaluate our method on the Qwen3-30B and Qwen3-235B models with
a batch size of $16$. Without any statistically significant loss in accuracy,
our approach achieves latency reductions of $39\%$ and $15\%$ in the MoE layer
decode latency, respectively.

</details>


### [52] [Probabilistic Graph Cuts](https://arxiv.org/abs/2511.02272)
*Ayoub Ghriss*

Main category: cs.LG

TL;DR: 本文提出了一种统一的概率框架，用于可微分图分割，覆盖包括Normalized Cut在内的多种图割方法，并通过解析上界和闭式梯度实现高效稳定的端到端学习。


<details>
  <summary>Details</summary>
Motivation: 现有基于概率松弛的图割方法主要聚焦于RatioCut，缺乏对更广泛图割（如Normalized Cut）的通用理论保证和原则性梯度计算。

Method: 构建一个统一的概率框架，利用积分表示和高斯超几何函数，为期望离散割提供紧致的解析上界，并推导出具有闭式表达的前向和反向传播公式。

Result: 该框架实现了对多种图割目标的可微分近似，具备数值稳定性，适用于大规模聚类和对比学习任务。

Conclusion: 所提出的方法为可微分图分割提供了严谨且通用的基础，克服了传统谱聚类依赖特征分解的局限，支持端到端和在线学习。

Abstract: Probabilistic relaxations of graph cuts offer a differentiable alternative to
spectral clustering, enabling end-to-end and online learning without
eigendecompositions, yet prior work centered on RatioCut and lacked general
guarantees and principled gradients. We present a unified probabilistic
framework that covers a wide class of cuts, including Normalized Cut. Our
framework provides tight analytic upper bounds on expected discrete cuts via
integral representations and Gauss hypergeometric functions with closed-form
forward and backward. Together, these results deliver a rigorous, numerically
stable foundation for scalable, differentiable graph partitioning covering a
wide range of clustering and contrastive learning objectives.

</details>


### [53] [Gradient-Variation Online Adaptivity for Accelerated Optimization with Hölder Smoothness](https://arxiv.org/abs/2511.02276)
*Yuheng Zhao,Yu-Hu Yan,Kfir Yehuda Levy,Peng Zhao*

Main category: cs.LG

TL;DR: 本文研究了Hölder光滑函数下的在线学习问题，并设计了一种自适应梯度变差在线学习算法，该算法无需预先知道光滑参数，可在光滑与非光滑情形间自动插值得到最优遗憾界；通过在线到批量转换，该方法导出了随机凸优化中的最优通用算法，并进一步结合检测机制，首次实现了强凸离线优化中兼具加速与近优收敛的通用方法。


<details>
  <summary>Details</summary>
Motivation: 光滑性在离线优化加速和在线学习的梯度变差遗憾最小化中至关重要，而这两类问题密切相关。现有方法通常依赖对光滑参数的先验知识，缺乏自适应性，尤其在强凸离线优化中难以同时兼顾光滑与非光滑情形的最优性能。

Method: 针对Hölder光滑的（强）凸在线函数，设计了一种无需预知光滑参数的梯度变差在线学习算法；通过在线到批量转换获得随机凸优化的通用方法；对于离线强凸优化，进一步结合基于检测的猜测-验证机制，实现通用加速。

Result: 所提在线算法在光滑与非光滑情形之间实现了最优遗憾界的平滑插值；导出的随机优化方法在Hölder光滑条件下达到最优；首次提出在离线强凸优化中同时实现光滑情形加速和非光滑情形近优收敛的通用算法。

Conclusion: 本文通过将在线学习中的梯度变差自适应性与离线优化相结合，成功构建了适用于Hölder光滑函数的通用优化框架，在在线和离线设置下均实现了理论最优或近优性能，且无需预知光滑参数。

Abstract: Smoothness is known to be crucial for acceleration in offline optimization,
and for gradient-variation regret minimization in online learning.
Interestingly, these two problems are actually closely connected -- accelerated
optimization can be understood through the lens of gradient-variation online
learning. In this paper, we investigate online learning with H\"older smooth
functions, a general class encompassing both smooth and non-smooth (Lipschitz)
functions, and explore its implications for offline optimization. For
(strongly) convex online functions, we design the corresponding
gradient-variation online learning algorithm whose regret smoothly interpolates
between the optimal guarantees in smooth and non-smooth regimes. Notably, our
algorithms do not require prior knowledge of the H\"older smoothness parameter,
exhibiting strong adaptivity over existing methods. Through online-to-batch
conversion, this gradient-variation online adaptivity yields an optimal
universal method for stochastic convex optimization under H\"older smoothness.
However, achieving universality in offline strongly convex optimization is more
challenging. We address this by integrating online adaptivity with a
detection-based guess-and-check procedure, which, for the first time, yields a
universal offline method that achieves accelerated convergence in the smooth
regime while maintaining near-optimal convergence in the non-smooth one.

</details>


### [54] [FP8-Flow-MoE: A Casting-Free FP8 Recipe without Double Quantization Error](https://arxiv.org/abs/2511.02302)
*Fengjuan Wang,Zhiyi Su,Xingzhu Hu,Cheng Wang,Mou Sun*

Main category: cs.LG

TL;DR: 本文提出FP8-Flow-MoE，一种面向Mixture-of-Experts（MoE）模型的FP8训练方案，通过量化一致的数据流、缩放感知转置和融合FP8算子，显著减少显式类型转换，在保持收敛稳定性的同时提升吞吐量并降低内存占用。


<details>
  <summary>Details</summary>
Motivation: 大型MoE模型训练因计算和内存需求极高而难以负担；现有低精度训练方法仍依赖BF16主导的数据流并频繁进行量化/反量化操作，削弱了FP8的理论效率；而直接全FP8数据流又会引入双重量化误差，影响数值稳定性。

Method: 提出FP8-Flow-MoE训练方案，采用量化一致的FP8中心数据流，结合缩放因子感知的转置操作与融合FP8算子，将显式类型转换从12次减少到2次。

Result: 在671B参数MoE模型上评估显示，相比BF16和朴素FP8基线，该方法每GPU吞吐量最高提升21%，内存占用减少16.5 GB，同时保持稳定收敛。

Conclusion: FP8-Flow-MoE有效解决了MoE模型FP8训练中的效率与稳定性问题，提供了一种即插即用、兼容主流框架（如TransformerEngine和Megatron-LM）的高效低精度训练方案。

Abstract: Training large Mixture-of-Experts (MoE) models remains computationally
prohibitive due to their extreme compute and memory demands. Although
low-precision training promises to accelerate computation and reduce memory
footprint, existing implementations still rely on BF16-dominated dataflows with
frequent quantize-dequantize (Q/DQ) conversions. These redundant casts erode
much of FP8's theoretical efficiency. However, naively removing these casts by
keeping dataflows entirely in FP8 introduces double quantization error: tensors
quantized along different dimensions accumulate inconsistent scaling factors,
degrading numerical stability.
  We propose FP8-Flow-MoE, an FP8 training recipe featuring a
quantization-consistent FP8-centric dataflow with a scaling-aware transpose and
fused FP8 operators that streamline computation and eliminate explicit cast
operations from 12 to 2. Evaluations on a 671B-parameter MoE model demonstrate
up to 21\% higher throughput and 16.5 GB lower memory usage per GPU compared to
BF16 and na\"ive FP8 baselines, while maintaining stable convergence. We
provide a plug-and-play FP8 recipe compatible with TransformerEngine and
Megatron-LM, which will be open-sourced soon.

</details>


### [55] [The Sequential Edge: Inverse-Entropy Voting Beats Parallel Self-Consistency at Matched Compute](https://arxiv.org/abs/2511.02309)
*Aman Sharma,Paras Chopra*

Main category: cs.LG

TL;DR: 在相同的计算和token预算下，顺序推理（sequential refinement）显著优于并行自一致性方法，结合提出的无训练逆熵加权投票策略，可将准确率最高提升46.7%，挑战了当前主流的并行推理范式。


<details>
  <summary>Details</summary>
Motivation: 探讨在测试时推理中，是采用多个独立并行链还是少量顺序迭代精炼链更有效，以优化语言模型的推理性能。

Method: 在5个先进开源模型和3个复杂推理基准上系统比较顺序扩展与并行自一致性；提出一种无需训练的逆熵加权投票方法，根据推理链的逆熵对答案加权。

Result: 顺序扩展在95.6%的配置中优于并行方法，准确率最高提升46.7%；逆熵加权投票进一步提升性能，成为最优测试时扩展策略。

Conclusion: 顺序精炼应作为现代大语言模型推理的默认策略，研究结果挑战了自Wang等人（2022）以来主导的并行推理范式，呼吁推理阶段优化方法的范式转变。

Abstract: We revisit test-time scaling for language model reasoning and ask a
fundamental question: at equal token budget and compute, is it better to run
multiple independent chains in parallel, or to run fewer chains that
iteratively refine through sequential steps? Through comprehensive evaluation
across 5 state-of-the-art open source models and 3 challenging reasoning
benchmarks, we find that sequential scaling where chains explicitly build upon
previous attempts consistently outperforms the dominant parallel
self-consistency paradigm in 95.6% of configurations with gains in accuracy
upto 46.7%. Further, we introduce inverse-entropy weighted voting, a novel
training-free method to further boost the accuracy of sequential scaling. By
weighing answers in proportion to the inverse entropy of their reasoning
chains, we increase our success rate over parallel majority and establish it as
the optimal test-time scaling strategy. Our findings fundamentally challenge
the parallel reasoning orthodoxy that has dominated test-time scaling since
Wang et al.'s self-consistency decoding (Wang et al., 2022), positioning
sequential refinement as the robust default for modern LLM reasoning and
necessitating a paradigm shift in how we approach inference-time optimization.

</details>


### [56] [RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across Domains](https://arxiv.org/abs/2511.02331)
*Tianle Pu,Zijie Geng,Haoyang Liu,Shixuan Liu,Jie Wang,Li Zeng,Chao Chen,Changjun Fan*

Main category: cs.LG

TL;DR: 本文提出了RoME，一种面向跨领域的鲁棒混合专家框架，用于提升混合整数线性规划（MILP）求解器在未见问题分布上的泛化能力。通过任务嵌入动态路由实例，并采用双层分布鲁棒优化策略训练，RoME在多个领域上显著优于现有方法，甚至在MIPLIB基准上实现零样本性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的MILP求解方法大多在单一领域内开发和评估，难以泛化到未见过的问题分布，限制了其构建通用、可扩展求解器的潜力。

Method: 提出RoME框架，结合任务嵌入的动态路由机制与双层分布鲁棒优化策略（跨域与域内），训练混合专家模型以预测跨领域的MILP解。

Result: 在三个领域上训练的单一RoME模型，在五个不同领域上平均提升67.7%；在MIPLIB基准的零样本设置下也展现出显著性能增益。

Conclusion: RoME有效提升了学习型MILP求解器的跨域泛化能力，不仅适用于未见领域，还能增强各单独领域的性能，展现出构建通用求解器的潜力。

Abstract: Mixed-Integer Linear Programming (MILP) is a fundamental and powerful
framework for modeling complex optimization problems across diverse domains.
Recently, learning-based methods have shown great promise in accelerating MILP
solvers by predicting high-quality solutions. However, most existing approaches
are developed and evaluated in single-domain settings, limiting their ability
to generalize to unseen problem distributions. This limitation poses a major
obstacle to building scalable and general-purpose learning-based solvers. To
address this challenge, we introduce RoME, a domain-Robust Mixture-of-Experts
framework for predicting MILP solutions across domains. RoME dynamically routes
problem instances to specialized experts based on learned task embeddings. The
model is trained using a two-level distributionally robust optimization
strategy: inter-domain to mitigate global shifts across domains, and
intra-domain to enhance local robustness by introducing perturbations on task
embeddings. We reveal that cross-domain training not only enhances the model's
generalization capability to unseen domains but also improves performance
within each individual domain by encouraging the model to capture more general
intrinsic combinatorial patterns. Specifically, a single RoME model trained on
three domains achieves an average improvement of 67.7% then evaluated on five
diverse domains. We further test the pretrained model on MIPLIB in a zero-shot
setting, demonstrating its ability to deliver measurable performance gains on
challenging real-world instances where existing learning-based approaches often
struggle to generalize.

</details>


### [57] [Reducing normalizing flow complexity for MCMC preconditioning](https://arxiv.org/abs/2511.02345)
*David Nabergoj,Erik Štrumbelj*

Main category: cs.LG

TL;DR: 本文提出一种因子化预条件架构，结合线性预条件器与条件归一化流（NF），以降低NF复杂度并提升对目标分布几何结构的适应性，从而在多种复杂后验分布上显著提高MCMC采样效率。


<details>
  <summary>Details</summary>
Motivation: 传统线性预条件器在处理高度复杂的目标分布时能力有限，而现有的基于归一化流（NF）的非线性预条件方法存在过参数化问题，不仅可能降低采样效率和拟合质量，还缺乏对目标分布特性的自适应能力。

Method: 作者设计了一种因子化预条件架构：利用预热样本估计目标分布中近似高斯的维度，对其应用线性预条件器；对剩余更复杂的维度则使用条件归一化流建模，从而降低整体模型复杂度并增强适应性。

Result: 该方法在两个复杂合成分布上生成了显著更优的尾部样本，在不同似然与先验强度下的稀疏逻辑回归后验中表现一致更佳，并在具有弱似然和强漏斗几何结构的分层贝叶斯模型后验上获得了更高的有效样本量。

Conclusion: 所提出的因子化预条件方法有效提升了MCMC在复杂后验分布（尤其是数据有限的分层贝叶斯模型）中的采样效率，为神经MCMC的理论与软件发展提供了新思路。

Abstract: Preconditioning is a key component of MCMC algorithms that improves sampling
efficiency by facilitating exploration of geometrically complex target
distributions through an invertible map. While linear preconditioners are often
sufficient for moderately complex target distributions, recent work has
explored nonlinear preconditioning with invertible neural networks as
components of normalizing flows (NFs). However, empirical and theoretical
studies show that overparameterized NF preconditioners can degrade sampling
efficiency and fit quality. Moreover, existing NF-based approaches do not adapt
their architectures to the target distribution. Related work outside of MCMC
similarly finds that suitably parameterized NFs can achieve comparable or
superior performance with substantially less training time or data. We propose
a factorized preconditioning architecture that reduces NF complexity by
combining a linear component with a conditional NF, improving adaptability to
target geometry. The linear preconditioner is applied to dimensions that are
approximately Gaussian, as estimated from warmup samples, while the conditional
NF models more complex dimensions. Our method yields significantly better tail
samples on two complex synthetic distributions and consistently better
performance on a sparse logistic regression posterior across varying likelihood
and prior strengths. It also achieves higher effective sample sizes on
hierarchical Bayesian model posteriors with weak likelihoods and strong funnel
geometries. This approach is particularly relevant for hierarchical Bayesian
model analyses with limited data and could inform current theoretical and
software strides in neural MCMC design.

</details>


### [58] [Human-Machine Ritual: Synergic Performance through Real-Time Motion Recognition](https://arxiv.org/abs/2511.02351)
*Zhuodi Cai,Ziyu Xu,Juan Pampin*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级实时动作识别系统，通过可穿戴IMU传感器、MiniRocket时间序列分类与多媒体响应控制，实现舞者动作与声音的映射，支持高精度低延迟的人机协同表演。


<details>
  <summary>Details</summary>
Motivation: 探索一种保留舞者身体表现力的人机协作新范式，利用机器学习实现对舞蹈动作的敏锐观察与响应。

Method: 结合可穿戴IMU传感器数据、MiniRocket时间序列分类算法，以及基于体感记忆与联想的动作-声音映射机制，构建实时响应系统。

Result: 系统实现了低于50毫秒延迟的高准确率动作分类，验证了其在创意、教育和现场表演场景中的可复现性和实用性。

Conclusion: 该以人为本的设计为整合具备舞蹈理解能力的机器提供了一个可靠框架，推动了人机协同在表演艺术中的应用。

Abstract: We introduce a lightweight, real-time motion recognition system that enables
synergic human-machine performance through wearable IMU sensor data, MiniRocket
time-series classification, and responsive multimedia control. By mapping
dancer-specific movement to sound through somatic memory and association, we
propose an alternative approach to human-machine collaboration, one that
preserves the expressive depth of the performing body while leveraging machine
learning for attentive observation and responsiveness. We demonstrate that this
human-centered design reliably supports high accuracy classification (<50 ms
latency), offering a replicable framework to integrate dance-literate machines
into creative, educational, and live performance contexts.

</details>


### [59] [LUMA-RAG: Lifelong Multimodal Agents with Provably Stable Streaming Alignment](https://arxiv.org/abs/2511.02371)
*Rohan Wandre,Yash Gajewar,Namrata Patel,Vivek Dhalkari*

Main category: cs.LG

TL;DR: LUMA-RAG is a lifelong multimodal RAG framework that addresses index freshness and cross-modal consistency through a multi-tier memory system, incremental CLAP-CLIP alignment, and stability-aware retrieval telemetry, achieving strong performance and provable stability in multimodal retrieval.


<details>
  <summary>Details</summary>
Motivation: Modern AI agents increasingly rely on continuous multimodal data streams, but existing RAG systems struggle with maintaining up-to-date indexes efficiently and ensuring semantic consistency across different modalities (e.g., text, image, audio).

Method: LUMA-RAG introduces: (i) a streaming multi-tier memory system that manages embeddings under memory constraints by spilling from a hot HNSW index to a compressed IVFPQ index; (ii) a streaming alignment bridge using incremental orthogonal Procrustes updates to align CLAP and CLIP embedding spaces; and (iii) stability-aware retrieval telemetry that provides Safe@k guarantees by bounding alignment drift and quantization error.

Result: Experiments show LUMA-RAG achieves high text-to-image retrieval performance (Recall@10 = 0.94), maintains graceful degradation under quantization, and ensures provably stable audio-to-image rankings (Safe@1 = 1.0).

Conclusion: LUMA-RAG provides a practical and robust solution for deploying lifelong, multimodal RAG systems in real-world applications with dynamic data streams and strict memory and stability requirements.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as the dominant paradigm for
grounding large language model outputs in verifiable evidence. However, as
modern AI agents transition from static knowledge bases to continuous
multimodal streams encompassing text, images, video, and audio, two critical
challenges arise: maintaining index freshness without prohibitive re-indexing
costs, and preserving cross-modal semantic consistency across heterogeneous
embedding spaces. We present LUMA-RAG, a lifelong multimodal agent architecture
featuring three key innovations: (i) a streaming, multi-tier memory system that
dynamically spills embeddings from a hot HNSW tier to a compressed IVFPQ tier
under strict memory budgets; (ii) a streaming CLAP->CLIP alignment bridge that
maintains cross-modal consistency through incremental orthogonal Procrustes
updates; and (iii) stability-aware retrieval telemetry providing Safe@k
guarantees by jointly bounding alignment drift and quantization error.
Experiments demonstrate robust text-to-image retrieval (Recall@10 = 0.94),
graceful performance degradation under product quantization offloading, and
provably stable audio-to-image rankings (Safe@1 = 1.0), establishing LUMA-RAG
as a practical framework for production multimodal RAG systems.

</details>


### [60] [H-Infinity Filter Enhanced CNN-LSTM for Arrhythmia Detection from Heart Sound Recordings](https://arxiv.org/abs/2511.02379)
*Rohith Shinoj Kumar,Rushdeep Dinda,Aditya Tyagi,Annappa B.,Naveen Kumar M. R*

Main category: cs.LG

TL;DR: 本文提出了一种结合CNN、H-Infinity滤波思想和LSTM的新架构（CNN-H-Infinity-LSTM），用于从心音信号中检测心律失常，在PhysioNet 2016数据集上实现了99.42%的准确率和98.85%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在心律失常检测中面临小样本和噪声数据下泛化能力差的问题，亟需提升模型鲁棒性和泛化性能。

Method: 提出一种新型CNN-H-Infinity-LSTM架构，引入控制理论中的H-Infinity滤波器的可训练参数，以增强模型对心音信号中时空特征的鲁棒提取能力。

Result: 在PhysioNet CinC Challenge 2016数据集上，该模型实现了99.42%的测试准确率和98.85%的F1分数，优于现有基准方法，并表现出稳定的收敛性。

Conclusion: 所提出的CNN-H-Infinity-LSTM架构有效提升了心律失常自动检测的准确性和鲁棒性，尤其在小样本和噪声环境下具有良好的泛化能力。

Abstract: Early detection of heart arrhythmia can prevent severe future complications
in cardiac patients. While manual diagnosis still remains the clinical
standard, it relies heavily on visual interpretation and is inherently
subjective. In recent years, deep learning has emerged as a powerful tool to
automate arrhythmia detection, offering improved accuracy, consistency, and
efficiency. Several variants of convolutional and recurrent neural network
architectures have been widely explored to capture spatial and temporal
patterns in physiological signals. However, despite these advancements, current
models often struggle to generalize well in real-world scenarios, especially
when dealing with small or noisy datasets, which are common challenges in
biomedical applications. In this paper, a novel CNN-H-Infinity-LSTM
architecture is proposed to identify arrhythmic heart signals from heart sound
recordings. This architecture introduces trainable parameters inspired by the
H-Infinity filter from control theory, enhancing robustness and generalization.
Extensive experimentation on the PhysioNet CinC Challenge 2016 dataset, a
public benchmark of heart audio recordings, demonstrates that the proposed
model achieves stable convergence and outperforms existing benchmarks, with a
test accuracy of 99.42% and an F1 score of 98.85%.

</details>


### [61] [A Spatially Informed Gaussian Process UCB Method for Decentralized Coverage Control](https://arxiv.org/abs/2511.02398)
*Gennaro Guidone,Luca Monegaglia,Elia Raimondi,Han Wang,Mattia Bianchi,Florian Dörfler*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的去中心化覆盖控制算法，用于在未知空间环境中基于高斯过程建模，通过结合期望位置成本与方差探索项，在探索与利用之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 在未知空间环境中实现有效的覆盖控制需要在探索（获取新信息）和利用（优化已知信息）之间取得平衡，而现有方法往往依赖集中式架构或全局信息，限制了可扩展性和实用性。

Method: 每个智能体基于局部观测和邻近智能体通信，通过最小化一个受GP-UCB启发的局部代价函数来自主规划轨迹；该代价函数结合了期望位置成本与基于方差的探索项。智能体还采用贪心策略周期性更新诱导点，以实现可扩展的在线高斯过程更新。

Result: 仿真实验验证了所提算法在去中心化设置下的有效性，能够在仅依赖局部信息的情况下实现良好的覆盖性能。

Conclusion: 该算法实现了完全去中心化的覆盖控制，兼顾探索与利用，并通过局部通信和在线高斯过程更新提升了可扩展性与实用性。

Abstract: We present a novel decentralized algorithm for coverage control in unknown
spatial environments modeled by Gaussian Processes (GPs). To trade-off between
exploration and exploitation, each agent autonomously determines its trajectory
by minimizing a local cost function. Inspired by the GP-UCB (Upper Confidence
Bound for GPs) acquisition function, the proposed cost combines the expected
locational cost with a variance-based exploration term, guiding agents toward
regions that are both high in predicted density and model uncertainty. Compared
to previous work, our algorithm operates in a fully decentralized fashion,
relying only on local observations and communication with neighboring agents.
In particular, agents periodically update their inducing points using a greedy
selection strategy, enabling scalable online GP updates. We demonstrate the
effectiveness of our algorithm in simulation.

</details>


### [62] [Improving Unlearning with Model Updates Probably Aligned with Gradients](https://arxiv.org/abs/2511.02435)
*Virgile Dine,Teddy Furon,Charly Faure*

Main category: cs.LG

TL;DR: 本文将机器遗忘问题形式化为一个带约束的优化问题，提出“可行更新”概念，通过参数掩码和梯度噪声估计，实现不损害模型效用的遗忘，并可作为插件兼容现有一阶梯度近似遗忘方法。


<details>
  <summary>Details</summary>
Motivation: 现有近似机器遗忘方法可能在删除数据影响的同时损害模型的原始性能，因此需要一种既能有效遗忘又保持模型效用的更新机制。

Method: 提出“可行更新”策略，基于参数掩码选择值得更新的参数，并结合批处理梯度估计中的噪声分析，提供局部可行更新的统计保证。

Result: 在计算机视觉分类器上的实验验证了该方法的有效性，表明其可在不降低模型效用的前提下实现有效遗忘。

Conclusion: 该方法为机器遗忘提供了一个通用、可插拔的优化框架，兼顾遗忘效果与模型性能，具有良好的实用性和兼容性。

Abstract: We formulate the machine unlearning problem as a general constrained
optimization problem. It unifies the first-order methods from the approximate
machine unlearning literature. This paper then introduces the concept of
feasible updates as the model's parameter update directions that help with
unlearning while not degrading the utility of the initial model. Our design of
feasible updates is based on masking, \ie\ a careful selection of the model's
parameters worth updating. It also takes into account the estimation noise of
the gradients when processing each batch of data to offer a statistical
guarantee to derive locally feasible updates. The technique can be plugged in,
as an add-on, to any first-order approximate unlearning methods. Experiments
with computer vision classifiers validate this approach.

</details>


### [63] [Accounting for Underspecification in Statistical Claims of Model Superiority](https://arxiv.org/abs/2511.02453)
*Thomas Sanchez,Pedro M. Gordaliza,Meritxell Bach Cuadra*

Main category: cs.LG

TL;DR: 该论文指出医学影像中机器学习性能提升的许多声明可能为假阳性，原因在于未考虑模型训练中的“未充分指定”（underspecification）问题；作者扩展了现有统计框架，将训练随机性引入方差分析，发现即使微小的种子差异也会显著提高验证性能优越性所需的证据标准。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估医学影像中机器学习模型性能提升时，往往忽视了由于随机初始化或训练动态引起的“未充分指定”问题，这可能导致对模型优越性的错误判断。

Method: 作者扩展了一个现有的统计框架，将未充分指定作为额外的方差成分纳入模型，并通过模拟实验量化种子变异性对性能比较结论的影响。

Result: 模拟结果表明，即使仅有约1%的种子变异性，也会显著增加支持模型性能优越性声明所需的统计证据。

Conclusion: 在验证医学影像系统时，必须显式建模训练过程中的方差，以避免得出不可靠的性能优越性结论。

Abstract: Machine learning methods are increasingly applied in medical imaging, yet
many reported improvements lack statistical robustness: recent works have
highlighted that small but significant performance gains are highly likely to
be false positives. However, these analyses do not take
\emph{underspecification} into account -- the fact that models achieving
similar validation scores may behave differently on unseen data due to random
initialization or training dynamics. Here, we extend a recent statistical
framework modeling false outperformance claims to include underspecification as
an additional variance component. Our simulations demonstrate that even modest
seed variability ($\sim1\%$) substantially increases the evidence required to
support superiority claims. Our findings underscore the need for explicit
modeling of training variance when validating medical imaging systems.

</details>


### [64] [SKGE: Spherical Knowledge Graph Embedding with Geometric Regularization](https://arxiv.org/abs/2511.02460)
*Xuan-Truong Quan,Xuan-Son Quan,Duc Do Minh,Vinh Nguyen Van*

Main category: cs.LG

TL;DR: 本文提出球面知识图谱嵌入模型（SKGE），通过将实体表示约束在超球面上，显著优于传统欧氏空间模型TransE，证明了几何先验对模型性能的关键作用。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱嵌入方法（如TransE）在无界欧氏空间中存在建模复杂关系的固有局限，并可能导致训练效率低下，因此需要探索更有效的几何表示空间。

Method: SKGE引入可学习的非线性“球化层”将实体映射到超球面，并将关系建模为“先平移后投影”的混合变换。

Result: 在FB15k-237、CoDEx-S和CoDEx-M三个基准数据集上，SKGE显著优于TransE，尤其在大规模数据集上表现更佳；分析表明球面几何作为强正则项提升了各类关系的建模能力，并自然形成“困难负采样”环境。

Conclusion: 嵌入流形的选择是知识图谱嵌入模型的核心设计原则，几何先验应成为下一代高效稳定KGE模型的基础。

Abstract: Knowledge graph embedding (KGE) has become a fundamental technique for
representation learning on multi-relational data. Many seminal models, such as
TransE, operate in an unbounded Euclidean space, which presents inherent
limitations in modeling complex relations and can lead to inefficient training.
In this paper, we propose Spherical Knowledge Graph Embedding (SKGE), a model
that challenges this paradigm by constraining entity representations to a
compact manifold: a hypersphere. SKGE employs a learnable, non-linear
Spherization Layer to map entities onto the sphere and interprets relations as
a hybrid translate-then-project transformation. Through extensive experiments
on three benchmark datasets, FB15k-237, CoDEx-S, and CoDEx-M, we demonstrate
that SKGE consistently and significantly outperforms its strong Euclidean
counterpart, TransE, particularly on large-scale benchmarks such as FB15k-237
and CoDEx-M, demonstrating the efficacy of the spherical geometric prior. We
provide an in-depth analysis to reveal the sources of this advantage, showing
that this geometric constraint acts as a powerful regularizer, leading to
comprehensive performance gains across all relation types. More fundamentally,
we prove that the spherical geometry creates an "inherently hard negative
sampling" environment, naturally eliminating trivial negatives and forcing the
model to learn more robust and semantically coherent representations. Our
findings compellingly demonstrate that the choice of manifold is not merely an
implementation detail but a fundamental design principle, advocating for
geometric priors as a cornerstone for designing the next generation of powerful
and stable KGE models.

</details>


### [65] [BRAINS: A Retrieval-Augmented System for Alzheimer's Detection and Monitoring](https://arxiv.org/abs/2511.02490)
*Rajan Das Gupta,Md Kishor Morol,Nafiz Fahad,Md Tanzib Hosain,Sumaya Binte Zilani Choya,Md Jakir Hossen*

Main category: cs.LG

TL;DR: BRAINS is a novel LLM-based system for early Alzheimer's detection that combines a cognitive diagnostic module and a case-retrieval module to enhance diagnostic accuracy and contextual understanding.


<details>
  <summary>Details</summary>
Motivation: Early and accurate detection of Alzheimer’s disease is critical, especially in regions lacking advanced diagnostic tools; existing methods may not be scalable or sufficiently explainable.

Method: BRAINS uses a dual-module architecture: (1) a Diagnostic Module with LLMs fine-tuned on cognitive and neuroimaging data (e.g., MMSE, CDR, brain volume), and (2) a Case Retrieval Module that retrieves similar patient cases and fuses them with the input via a Case Fusion Layer for enhanced inference with clinical prompts.

Result: BRAINS effectively classifies disease severity and detects early cognitive decline in real-world evaluations.

Conclusion: BRAINS shows strong potential as a scalable, explainable, and early-stage Alzheimer’s detection tool, with promising future applications in neurodegenerative disease screening.

Abstract: As the global burden of Alzheimer's disease (AD) continues to grow, early and
accurate detection has become increasingly critical, especially in regions with
limited access to advanced diagnostic tools. We propose BRAINS (Biomedical
Retrieval-Augmented Intelligence for Neurodegeneration Screening) to address
this challenge. This novel system harnesses the powerful reasoning capabilities
of Large Language Models (LLMs) for Alzheimer's detection and monitoring.
BRAINS features a dual-module architecture: a cognitive diagnostic module and a
case-retrieval module. The Diagnostic Module utilizes LLMs fine-tuned on
cognitive and neuroimaging datasets -- including MMSE, CDR scores, and brain
volume metrics -- to perform structured assessments of Alzheimer's risk.
Meanwhile, the Case Retrieval Module encodes patient profiles into latent
representations and retrieves similar cases from a curated knowledge base.
These auxiliary cases are fused with the input profile via a Case Fusion Layer
to enhance contextual understanding. The combined representation is then
processed with clinical prompts for inference. Evaluations on real-world
datasets demonstrate BRAINS effectiveness in classifying disease severity and
identifying early signs of cognitive decline. This system not only shows strong
potential as an assistive tool for scalable, explainable, and early-stage
Alzheimer's disease detection, but also offers hope for future applications in
the field.

</details>


### [66] [An End-to-End Learning Approach for Solving Capacitated Location-Routing Problems](https://arxiv.org/abs/2511.02525)
*Changhao Miao,Yuntian Zhang,Tongyu Wu,Fang Deng,Chen Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度强化学习的端到端方法（DRLHQ），用于求解带容量限制的选址-路径问题（CLRP）及其开放变体（OCLRP），通过异构查询注意力机制有效处理选址与路径决策间的复杂依赖关系，在多个数据集上展现出优于传统和现有DRL方法的性能。


<details>
  <summary>Details</summary>
Motivation: 带容量限制的选址-路径问题（CLRP）因决策间复杂的约束和相互依赖关系而难以求解，尽管深度强化学习（DRL）已在车辆路径问题中广泛应用，但在CLRP领域的研究仍显不足。

Method: 提出DRLHQ方法，采用编码器-解码器结构，将CLRP建模为马尔可夫决策过程，并引入一种新颖的异构查询注意力机制，以动态适应不同决策阶段，实现端到端学习。

Result: 在合成数据集和基准数据集上的实验表明，所提方法在求解CLRP和OCLRP时，相比代表性传统方法和DRL基线，具有更优的解质量和更强的泛化能力。

Conclusion: 该研究首次将端到端深度强化学习应用于CLRP，提出的异构查询注意力机制有效提升了模型处理复杂决策依赖的能力，为相关组合优化问题提供了通用建模框架。

Abstract: The capacitated location-routing problems (CLRPs) are classical problems in
combinatorial optimization, which require simultaneously making location and
routing decisions. In CLRPs, the complex constraints and the intricate
relationships between various decisions make the problem challenging to solve.
With the emergence of deep reinforcement learning (DRL), it has been
extensively applied to address the vehicle routing problem and its variants,
while the research related to CLRPs still needs to be explored. In this paper,
we propose the DRL with heterogeneous query (DRLHQ) to solve CLRP and open CLRP
(OCLRP), respectively. We are the first to propose an end-to-end learning
approach for CLRPs, following the encoder-decoder structure. In particular, we
reformulate the CLRPs as a markov decision process tailored to various
decisions, a general modeling framework that can be adapted to other DRL-based
methods. To better handle the interdependency across location and routing
decisions, we also introduce a novel heterogeneous querying attention mechanism
designed to adapt dynamically to various decision-making stages. Experimental
results on both synthetic and benchmark datasets demonstrate superior solution
quality and better generalization performance of our proposed approach over
representative traditional and DRL-based baselines in solving both CLRP and
OCLRP.

</details>


### [67] [Rawlsian many-to-one matching with non-linear utility](https://arxiv.org/abs/2511.02533)
*Hortence Nana,Andreas Athanasopoulos,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 本文研究多对一匹配问题（如大学招生），其中高校使用非线性效用函数评估学生组合以体现多样性；由于经典稳定匹配可能不存在，作者提出基于罗尔斯公平原则的替代解法，并设计确定性和随机算法以优化最不利高校的效用。


<details>
  <summary>Details</summary>
Motivation: 经典多对一匹配模型假设高校对学生的评价是线性的或可替代的，无法刻画学生群体多样性带来的非线性效用；在此更现实的设定下，传统稳定匹配可能不存在，因此需要新的公平匹配机制。

Method: 提出基于罗尔斯公平原则的解概念，即最大化所有高校中最小效用；设计确定性和随机迭代算法，逐步提升最不利高校的效用，实现公平分配。

Result: 证明在非线性效用下经典稳定匹配可能不存在；所提出的算法能有效提升最差高校的效用，为无法保证稳定性的情形提供可行的公平匹配方案。

Conclusion: 当高校偏好具有非线性多样性特征时，应放弃传统稳定性标准，转而采用基于公平性的匹配机制；所提算法为实际应用中的公平资源分配提供了有效工具。

Abstract: We study a many-to-one matching problem, such as the college admission
problem, where each college can admit multiple students. Unlike classical
models, colleges evaluate sets of students through non-linear utility functions
that capture diversity between them. In this setting, we show that classical
stable matchings may fail to exist. To address this, we propose alternative
solution concepts based on Rawlsian fairness, aiming to maximize the minimum
utility across colleges. We design both deterministic and stochastic algorithms
that iteratively improve the outcome of the worst-off college, offering a
practical approach to fair allocation when stability cannot be guaranteed.

</details>


### [68] [Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning](https://arxiv.org/abs/2511.02567)
*Yixiu Mao,Yun Qu,Qi Wang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 本文提出了一种新的邻域约束方法来解决离线强化学习中的分布外动作导致的外推误差问题，设计了自适应邻域约束算法ANQ，在标准基准上达到领先性能并展现出对噪声和有限数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中，分布外（OOD）动作会导致外推误差。现有方法通过密度、支持集或样本约束限制动作选择，但这些方法要么过于保守，要么难以准确建模行为策略。

Method: 提出一种邻域约束，将Bellman目标中的动作选择限制在数据集中动作邻域的并集内；基于数据质量自适应调整邻域半径，并结合双层优化框架构建自适应邻域约束Q学习算法（ANQ）。

Result: ANQ在标准离线RL基准上取得当前最优性能，并在噪声或数据有限的情况下表现出强鲁棒性。

Conclusion: 所提出的邻域约束有效缓解了外推误差和分布偏移，无需显式建模行为策略即可近似支持集约束，兼具灵活性与保守性，显著提升了离线强化学习的性能与稳定性。

Abstract: Offline reinforcement learning (RL) suffers from extrapolation errors induced
by out-of-distribution (OOD) actions. To address this, offline RL algorithms
typically impose constraints on action selection, which can be systematically
categorized into density, support, and sample constraints. However, we show
that each category has inherent limitations: density and sample constraints
tend to be overly conservative in many scenarios, while the support constraint,
though least restrictive, faces challenges in accurately modeling the behavior
policy. To overcome these limitations, we propose a new neighborhood constraint
that restricts action selection in the Bellman target to the union of
neighborhoods of dataset actions. Theoretically, the constraint not only bounds
extrapolation errors and distribution shift under certain conditions, but also
approximates the support constraint without requiring behavior policy modeling.
Moreover, it retains substantial flexibility and enables pointwise conservatism
by adapting the neighborhood radius for each data point. In practice, we employ
data quality as the adaptation criterion and design an adaptive neighborhood
constraint. Building on an efficient bilevel optimization framework, we develop
a simple yet effective algorithm, Adaptive Neighborhood-constrained Q learning
(ANQ), to perform Q learning with target actions satisfying this constraint.
Empirically, ANQ achieves state-of-the-art performance on standard offline RL
benchmarks and exhibits strong robustness in scenarios with noisy or limited
data.

</details>


### [69] [Dynamic Priors in Bayesian Optimization for Hyperparameter Optimization](https://arxiv.org/abs/2511.02570)
*Lukas Fehring,Marcel Wever,Maximilian Spliethöver,Leona Hennig,Henning Wachsmuth,Marius Lindauer*

Main category: cs.LG

TL;DR: 本文提出一种支持用户在贝叶斯优化（BO）超参数优化过程中动态干预的新方法，通过运行时引入先验分布表达专家知识和偏好，并引入误导性先验检测机制以防止有害输入，在实验中展现出与标准BO相当的性能。


<details>
  <summary>Details</summary>
Motivation: 当前超参数优化（HPO）方法（如贝叶斯优化）因黑盒性质和缺乏用户控制而难以被机器学习专家广泛接受；已有方法虽可利用专家知识初始化BO，但无法在优化过程中进行在线干预。

Method: 作者扩展了现有方法$\pi$BO，使其支持在HPO运行期间多次通过用户输入指定先验分布来引导优化过程，并引入误导先验检测机制以识别并抵御有害用户输入，同时保留理论保证。

Result: 实验表明该方法能有效融合多个先验信息：对有用先验加以利用，对误导性先验可靠地拒绝或克服，整体性能与未受干扰的标准BO相当。

Conclusion: 所提方法成功实现了在贝叶斯优化中动态融入专家知识与用户偏好，增强了HPO的可控性和实用性，同时保持了优化效果。

Abstract: Hyperparameter optimization (HPO), for example, based on Bayesian
optimization (BO), supports users in designing models well-suited for a given
dataset. HPO has proven its effectiveness on several applications, ranging from
classical machine learning for tabular data to deep neural networks for
computer vision and transformers for natural language processing. However, HPO
still sometimes lacks acceptance by machine learning experts due to its
black-box nature and limited user control. Addressing this, first approaches
have been proposed to initialize BO methods with expert knowledge. However,
these approaches do not allow for online steering during the optimization
process. In this paper, we introduce a novel method that enables repeated
interventions to steer BO via user input, specifying expert knowledge and user
preferences at runtime of the HPO process in the form of prior distributions.
To this end, we generalize an existing method, $\pi$BO, preserving theoretical
guarantees. We also introduce a misleading prior detection scheme, which allows
protection against harmful user inputs. In our experimental evaluation, we
demonstrate that our method can effectively incorporate multiple priors,
leveraging informative priors, whereas misleading priors are reliably rejected
or overcome. Thereby, we achieve competitiveness to unperturbed BO.

</details>


### [70] [Directional-Clamp PPO](https://arxiv.org/abs/2511.02577)
*Gilad Karpel,Ruida Zhou,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

TL;DR: 本文提出了一种改进的PPO算法——DClamp-PPO，通过在“错误方向”区域引入更陡的损失斜率（即clamp机制），有效抑制重要性比率向不利方向更新，从而在多个MuJoCo环境中稳定优于原始PPO及其变体。


<details>
  <summary>Details</summary>
Motivation: 现有PPO算法及其变体主要关注在“正确方向”区域调整裁剪机制，但忽略了由于采样随机性和策略优化的随机性，重要性比率常常会向“错误方向”更新，这限制了PPO性能的进一步提升。

Method: 提出Directional-Clamp PPO（DClamp-PPO）算法，在优势为正（负）且重要性比率低于（高于）$1 - \beta$（$1 + \beta$）的“严格错误方向”区域施加更强的惩罚，通过设置更陡的损失斜率（clamp）来约束更新。

Result: 在多个MuJoCo环境中，使用不同随机种子进行实验，DClamp-PPO始终优于原始PPO及其现有变体，并能更有效地避免“错误方向”更新，同时使重要性比率更接近1。

Conclusion: DClamp-PPO通过关注并抑制“错误方向”的策略更新，显著提升了PPO的稳定性和性能，为改进基于重要性采样的策略优化方法提供了新思路。

Abstract: Proximal Policy Optimization (PPO) is widely regarded as one of the most
successful deep reinforcement learning algorithms, known for its robustness and
effectiveness across a range of problems.
  The PPO objective encourages the importance ratio between the current and
behavior policies to move to the "right" direction -- starting from importance
sampling ratios equal to 1, increasing the ratios for actions with positive
advantages and decreasing those with negative advantages. A clipping function
is introduced to prevent over-optimization when updating the importance ratio
in these "right" direction regions. Many PPO variants have been proposed to
extend its success, most of which modify the objective's behavior by altering
the clipping in the "right" direction regions. However, due to randomness in
the rollouts and stochasticity of the policy optimization, we observe that the
ratios frequently move to the "wrong" direction during the PPO optimization.
This is a key factor hindering the improvement of PPO, but it has been largely
overlooked. To address this, we propose the Directional-Clamp PPO algorithm
(DClamp-PPO), which further penalizes the actions going to the strict "wrong"
direction regions, where the advantage is positive (negative) and importance
ratio falls below (above) $1 - \beta$ ($1+\beta$),
  for a tunable parameter $\beta \in (0, 1)$. The penalty is by enforcing a
steeper loss slope, i.e., a clamp, in those regions. We demonstrate that
DClamp-PPO consistently outperforms PPO, as well as its variants, by focusing
on modifying the objective's behavior in the "right" direction, across various
MuJoCo environments, using different random seeds. The proposed method is
shown, both theoretically and empirically, to better avoid "wrong" direction
updates while keeping the importance ratio closer to 1.

</details>


### [71] [A Large Language Model for Corporate Credit Scoring](https://arxiv.org/abs/2511.02593)
*Chitro Majumdar,Sergio Scandizzo,Ratanlal Mahanta,Avradip Mandal,Swarnendu Bhattacharjee*

Main category: cs.LG

TL;DR: Omega² 是一个结合结构化财务数据与大语言模型的公司信用评分框架，通过集成 CatBoost、LightGBM 和 XGBoost 模型，在多机构数据集上实现了超过 0.93 的平均测试 AUC，展现出优异的泛化能力和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有信用评分方法在跨评级机构泛化能力、时间一致性和可解释性方面存在不足，作者旨在通过融合大语言模型与传统机器学习方法，构建更可靠、透明且适用于机构级应用的信用风险评估系统。

Method: Omega² 框架整合了来自 Moody's、S&P、Fitch 和 Egan-Jones 的 7,800 家公司信用评级数据，利用杠杆率、盈利能力、流动性等财务指标，采用贝叶斯优化下的 CatBoost、LightGBM 和 XGBoost 模型，并通过时间验证确保结果的前瞻性与可复现性。

Result: 在多机构数据集上，Omega² 实现了平均测试 AUC 超过 0.93，证明其在不同评级体系间具有良好的泛化能力和时间一致性。

Conclusion: 结合语言模型推理与量化机器学习的方法，能够构建出透明、可靠且适用于机构级应用的公司信用风险评估系统。

Abstract: We introduce Omega^2, a Large Language Model-driven framework for corporate
credit scoring that combines structured financial data with advanced machine
learning to improve predictive reliability and interpretability. Our study
evaluates Omega^2 on a multi-agency dataset of 7,800 corporate credit ratings
drawn from Moody's, Standard & Poor's, Fitch, and Egan-Jones, each containing
detailed firm-level financial indicators such as leverage, profitability, and
liquidity ratios. The system integrates CatBoost, LightGBM, and XGBoost models
optimized through Bayesian search under temporal validation to ensure
forward-looking and reproducible results. Omega^2 achieved a mean test AUC
above 0.93 across agencies, confirming its ability to generalize across rating
systems and maintain temporal consistency. These results show that combining
language-based reasoning with quantitative learning creates a transparent and
institution-grade foundation for reliable corporate credit-risk assessment.

</details>


### [72] [Natural-gas storage modelling by deep reinforcement learning](https://arxiv.org/abs/2511.02646)
*Tiziano Balaconi,Aldo Glielmo,Marco Taboga*

Main category: cs.LG

TL;DR: 本文提出了GasRL模拟器，结合天然气市场建模与基于深度强化学习（特别是SAC算法）的储气运营商策略，用于分析最优库存管理对市场价格、供需动态的影响，并评估欧盟最低储气阈值政策对市场韧性的积极作用。


<details>
  <summary>Details</summary>
Motivation: 研究天然气市场中储气运营商的最优库存管理策略如何影响市场均衡价格与供需动态，并评估政策干预（如欧盟最低储气阈值）对市场抗冲击能力的影响。

Method: 构建GasRL模拟器，将校准后的天然气市场模型与使用深度强化学习（测试多种算法，最终采用Soft Actor Critic）训练的储气运营商策略相结合，模拟并分析市场行为。

Result: SAC算法在GasRL环境中表现最优，能同时实现盈利性、市场出清稳健性和价格稳定；其产生的均衡价格动态（如波动性和季节性）与现实价格高度吻合，且无需显式校准价格数据。此外，欧盟最低储气阈值能显著提升市场对供应冲击的韧性。

Conclusion: GasRL模拟器有效揭示了储气策略对天然气市场动态的关键作用，验证了SAC算法在复杂能源市场模拟中的适用性，并为政策制定（如最低储气要求）提供了量化支持。

Abstract: We introduce GasRL, a simulator that couples a calibrated representation of
the natural gas market with a model of storage-operator policies trained with
deep reinforcement learning (RL). We use it to analyse how optimal stockpile
management affects equilibrium prices and the dynamics of demand and supply. We
test various RL algorithms and find that Soft Actor Critic (SAC) exhibits
superior performance in the GasRL environment: multiple objectives of storage
operators - including profitability, robust market clearing and price
stabilisation - are successfully achieved. Moreover, the equilibrium price
dynamics induced by SAC-derived optimal policies have characteristics, such as
volatility and seasonality, that closely match those of real-world prices.
Remarkably, this adherence to the historical distribution of prices is obtained
without explicitly calibrating the model to price data. We show how the
simulator can be used to assess the effects of EU-mandated minimum storage
thresholds. We find that such thresholds have a positive effect on market
resilience against unanticipated shifts in the distribution of supply shocks.
For example, with unusually large shocks, market disruptions are averted more
often if a threshold is in place.

</details>


### [73] [Recursively Enumerably Representable Classes and Computable Versions of the Fundamental Theorem of Statistical Learning](https://arxiv.org/abs/2511.02644)
*David Kattermann,Lothar Sebastian Krapp*

Main category: cs.LG

TL;DR: 本文研究可计算的PAC（CPAC）学习框架下有效VC维与递归可枚举可表示（RER）类之间的关系，揭示了有效VC维可取传统VC维之上的任意值，并建立了CPAC可学习性与RER类包含关系之间的等价刻画。


<details>
  <summary>Details</summary>
Motivation: 传统统计学习基本定理在可计算学习框架中失效，已有工作通过引入有效VC维恢复了类似结论；本文旨在进一步探讨CPAC学习与RER类之间的联系，以深化对可计算学习理论的理解。

Method: 通过分析有效VC维的性质、构造RER类的示例与反例，并结合样本实现、唯一识别性及非一致CPAC学习等概念，建立CPAC可学习性与RER类之间的理论联系。

Result: 1）有效VC维可在传统VC维之上任意取值，即使对RER类亦然；2）强CPAC可学习性下两维数一致；3）CPAC可学习性等价于包含实现相同样本的RER类；4）具唯一识别性的CPAC可学习类必为RER；5）RER类在非一致CPAC意义下具有不可知可学习性。

Conclusion: CPAC学习与RER类之间存在深刻联系，有效VC维和RER结构为刻画可计算学习能力提供了关键工具，尤其在强学习条件和非一致设定下展现出良好理论性质。

Abstract: We study computable probably approximately correct (CPAC) learning, where
learners are required to be computable functions. It had been previously
observed that the Fundamental Theorem of Statistical Learning, which
characterizes PAC learnability by finiteness of the Vapnik-Chervonenkis
(VC-)dimension, no longer holds in this framework. Recent works recovered
analogs of the Fundamental Theorem in the computable setting, for instance by
introducing an effective VC-dimension. Guided by this, we investigate the
connection between CPAC learning and recursively enumerable representable (RER)
classes, whose members can be algorithmically listed. Our results show that the
effective VC-dimensions can take arbitrary values above the traditional one,
even for RER classes, which creates a whole family of (non-)examples for
various notions of CPAC learning. Yet the two dimensions coincide for classes
satisfying sufficiently strong notions of CPAC learning. We then observe that
CPAC learnability can also be characterized via containment of RER classes that
realize the same samples. Furthermore, it is shown that CPAC learnable classes
satisfying a unique identification property are necessarily RER. Finally, we
establish that agnostic learnability can be guaranteed for RER classes, by
considering the relaxed notion of nonuniform CPAC learning.

</details>


### [74] [Nesterov-Accelerated Robust Federated Learning Over Byzantine Adversaries](https://arxiv.org/abs/2511.02657)
*Lihan Xu,Yanjie Dong,Gang Wang,Runhao Zeng,Xiaoyi Fan,Xiping Hu*

Main category: cs.LG

TL;DR: 本文提出了一种拜占庭容错的加速联邦学习算法 Byrd-NAFL，通过结合 Nesterov 动量与鲁棒聚合规则，在非凸光滑损失函数下实现快速且安全的收敛，并在多种攻击场景中展现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 在存在拜占庭对手（可执行任意恶意行为）的联邦学习环境中，需同时提升通信效率与对梯度篡改的鲁棒性。

Method: 提出 Byrd-NAFL 算法，将 Nesterov 动量融入联邦学习过程，并采用拜占庭鲁棒聚合规则。

Result: 在非凸光滑损失函数和较宽松的梯度聚合假设下，证明了 Byrd-NAFL 的有限时间收敛性；实验表明其在收敛速度、准确率和抗攻击能力方面优于现有方法。

Conclusion: Byrd-NAFL 能有效兼顾联邦学习中的通信效率与拜占庭鲁棒性，是一种高效且安全的训练方案。

Abstract: We investigate robust federated learning, where a group of workers
collaboratively train a shared model under the orchestration of a central
server in the presence of Byzantine adversaries capable of arbitrary and
potentially malicious behaviors. To simultaneously enhance communication
efficiency and robustness against such adversaries, we propose a
Byzantine-resilient Nesterov-Accelerated Federated Learning (Byrd-NAFL)
algorithm. Byrd-NAFL seamlessly integrates Nesterov's momentum into the
federated learning process alongside Byzantine-resilient aggregation rules to
achieve fast and safeguarding convergence against gradient corruption. We
establish a finite-time convergence guarantee for Byrd-NAFL under non-convex
and smooth loss functions with relaxed assumption on the aggregated gradients.
Extensive numerical experiments validate the effectiveness of Byrd-NAFL and
demonstrate the superiority over existing benchmarks in terms of convergence
speed, accuracy, and resilience to diverse Byzantine attack strategies.

</details>


### [75] [TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models](https://arxiv.org/abs/2511.02802)
*Aditya Tanna,Pratinav Seth,Mohamed Bouadi,Utsav Avaiya,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: TabTune 是一个统一的开源库，标准化了表格基础模型的全流程，支持多种主流模型和适配策略，并集成了性能、校准与公平性评估模块。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型因预处理流程不统一、API碎片化、微调方法不一致以及缺乏面向部署的标准化评估（如校准性和公平性）而难以广泛应用。

Method: TabTune 提供统一接口，整合七种先进表格基础模型，支持零样本推理、元学习、监督微调（SFT）和参数高效微调（PEFT）；自动处理模型感知的预处理，内部管理架构异构性，并集成评估模块。

Result: TabTune 实现了表格基础模型全流程的标准化，支持可扩展、可复现的适配策略基准测试。

Conclusion: TabTune 通过统一接口和标准化流程，显著提升了表格基础模型的可用性、可比性和部署可靠性。

Abstract: Tabular foundation models represent a growing paradigm in structured data
learning, extending the benefits of large-scale pretraining to tabular domains.
However, their adoption remains limited due to heterogeneous preprocessing
pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the
absence of standardized evaluation for deployment-oriented metrics such as
calibration and fairness. We present TabTune, a unified library that
standardizes the complete workflow for tabular foundation models through a
single interface. TabTune provides consistent access to seven state-of-the-art
models supporting multiple adaptation strategies, including zero-shot
inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient
fine-tuning (PEFT). The framework automates model-aware preprocessing, manages
architectural heterogeneity internally, and integrates evaluation modules for
performance, calibration, and fairness. Designed for extensibility and
reproducibility, TabTune enables consistent benchmarking of adaptation
strategies of tabular foundation models. The library is open source and
available at https://github.com/Lexsi-Labs/TabTune .

</details>


### [76] [Does Interpretability of Knowledge Tracing Models Support Teacher Decision Making?](https://arxiv.org/abs/2511.02718)
*Adia Khalid,Alina Deriyeva,Benjamin Paassen*

Main category: cs.LG

TL;DR: 该研究探讨知识追踪（KT）模型的可解释性是否有助于人类教师做出教学决策。模拟实验表明，基于可解释KT模型的决策能更快达成掌握目标；但在涉及12名人类教师的实际决策实验中，尽管教师更信任和偏好可解释模型，其实际教学效率（达到掌握所需任务数）与使用不可解释模型相比并无显著差异。


<details>
  <summary>Details</summary>
Motivation: 知识追踪模型在教育决策中至关重要，通常要求具备可解释性以反映学习者能力。然而，尚无研究验证这种可解释性是否真正帮助人类教师做出更优教学决策。

Method: 首先进行模拟研究，比较基于可解释与不可解释KT模型的教学决策效率；其次，邀请12名人类教师根据KT模型提供的信息进行实际教学决策，并评估其对模型的可用性、信任度及实际教学效果。

Result: 模拟结果显示可解释KT模型能更快实现掌握；人类教师实验中，教师更信任可解释模型，但达到掌握所需的题目数量在不同模型间差异不大。

Conclusion: KT模型的可解释性虽提升教师的信任与可用性评价，但并未显著改善其教学决策效果，表明教师并非完全依赖模型做决策，需进一步研究师生如何理解与使用KT模型。

Abstract: Knowledge tracing (KT) models are a crucial basis for pedagogical
decision-making, namely which task to select next for a learner and when to
stop teaching a particular skill. Given the high stakes of pedagogical
decisions, KT models are typically required to be interpretable, in the sense
that they should implement an explicit model of human learning and provide
explicit estimates of learners' abilities. However, to our knowledge, no study
to date has investigated whether the interpretability of KT models actually
helps human teachers to make teaching decisions. We address this gap. First, we
perform a simulation study to show that, indeed, decisions based on
interpretable KT models achieve mastery faster compared to decisions based on a
non-interpretable model. Second, we repeat the study but ask $N=12$ human
teachers to make the teaching decisions based on the information provided by KT
models. As expected, teachers rate interpretable KT models higher in terms of
usability and trustworthiness. However, the number of tasks needed until
mastery hardly differs between KT models. This suggests that the relationship
between model interpretability and teacher decisions is not straightforward:
teachers do not solely rely on KT models to make decisions and further research
is needed to investigate how learners and teachers actually understand and use
KT models.

</details>


### [77] [Calibration improves detection of mislabeled examples](https://arxiv.org/abs/2511.02738)
*Ilies Chibane,Thomas George,Pierre Nodet,Vincent Lemaire*

Main category: cs.LG

TL;DR: 本文研究了在自动检测错误标签数据过程中，对基础模型进行校准的效果，发现校准能显著提升检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 错误标签数据广泛存在于现实世界的机器学习应用中，严重影响模型性能，因此需要有效方法来识别并处理这些错误标签。

Method: 通过训练一个基础机器学习模型，并利用校准方法对其进行优化，再基于该模型为每个样本生成可信度评分，以判断标签是否正确。

Result: 实验结果表明，采用校准方法能够提高错误标签检测的准确性和鲁棒性。

Conclusion: 模型校准是一种实用且有效的方法，可显著提升工业场景中错误标签数据的检测效果。

Abstract: Mislabeled data is a pervasive issue that undermines the performance of
machine learning systems in real-world applications. An effective approach to
mitigate this problem is to detect mislabeled instances and subject them to
special treatment, such as filtering or relabeling. Automatic mislabeling
detection methods typically rely on training a base machine learning model and
then probing it for each instance to obtain a trust score that each provided
label is genuine or incorrect. The properties of this base model are thus of
paramount importance. In this paper, we investigate the impact of calibrating
this model. Our empirical results show that using calibration methods improves
the accuracy and robustness of mislabeled instance detection, providing a
practical and effective solution for industrial applications.

</details>


### [78] [Assessing win strength in MLB win prediction models](https://arxiv.org/abs/2511.02815)
*Morgan Allen,Paul Savala*

Main category: cs.LG

TL;DR: 该论文通过训练多种机器学习模型预测棒球比赛胜者，并分析预测胜率与实际比分差距之间的关系，同时探讨了将预测胜率用于跑线投注策略的盈亏情况。


<details>
  <summary>Details</summary>
Motivation: 以往研究已使用机器学习模型预测棒球比赛胜负，但缺乏对不同模型在统一数据集下的系统比较，以及预测胜率与实际胜场强度（比分差）之间关系的探讨；此外，如何有效将预测结果用于投注决策尚不明确。

Method: 在统一数据集上训练多种常见机器学习模型以预测比赛胜率，分析预测胜率与实际比分差之间的相关性，并将预测结果应用于跑线投注策略，评估不同投注方式的收益表现。

Result: 研究发现主流机器学习模型的预测胜率与实际胜场强度存在正相关；在跑线投注中，采用适当策略可获得正收益，而盲目使用模型预测则会导致显著亏损。

Conclusion: 机器学习模型不仅能有效预测比赛胜负，其输出的胜率还能反映胜场强度；但将其用于投注需谨慎设计策略，否则可能造成损失。

Abstract: In Major League Baseball, strategy and planning are major factors in
determining the outcome of a game. Previous studies have aided this by building
machine learning models for predicting the winning team of any given game. We
extend this work by training a comprehensive set of machine learning models
using a common dataset. In addition, we relate the win probabilities produced
by these models to win strength as measured by score differential. In doing so
we show that the most common machine learning models do indeed demonstrate a
relationship between predicted win probability and the strength of the win.
Finally, we analyze the results of using predicted win probabilities as a
decision making mechanism on run-line betting. We demonstrate positive returns
when utilizing appropriate betting strategies, and show that naive use of
machine learning models for betting lead to significant loses.

</details>


### [79] [ConMeZO: Adaptive Descent-Direction Sampling for Gradient-Free Finetuning of Large Language Models](https://arxiv.org/abs/2511.02757)
*Lejs Deen Behric,Liang Zhang,Bingcong Li,Kiran Koshy Thekumparampil*

Main category: cs.LG

TL;DR: ConMeZO是一种新型零阶优化器，通过自适应方向采样加速大语言模型微调的收敛速度，在保持低内存占用的同时，比MeZO快至2倍。


<details>
  <summary>Details</summary>
Motivation: 零阶优化方法（如MeZO）虽能避免反向传播带来的内存开销，但在十亿级参数的大语言模型高维空间中因维度灾难导致收敛缓慢。

Method: ConMeZO通过动量估计构建一个锥形区域，在该区域内进行方向采样，而非均匀随机采样，从而聚焦于更可能包含真实梯度的方向。

Result: 理论证明ConMeZO与MeZO具有相同的最坏情况收敛率；实验表明在自然语言任务上微调大语言模型时，ConMeZO比MeZO快至2倍，同时保持低内存占用。

Conclusion: ConMeZO在不牺牲零阶方法内存优势的前提下，显著提升了大语言模型微调的收敛效率。

Abstract: Zeroth-order or derivative-free optimization (MeZO) is an attractive strategy
for finetuning large language models (LLMs) because it eliminates the memory
overhead of backpropagation. However, it converges slowly due to the inherent
curse of dimensionality when searching for descent directions in the
high-dimensional parameter space of billion-scale LLMs. We propose ConMeZO, a
novel zeroth-order optimizer that accelerates convergence by adaptive
directional sampling. Instead of drawing the direction uniformly at random,
ConMeZO restricts the sampling to a cone centered around a momentum estimate.
This concentrates the search in directions where the true gradient is more
likely to lie and thus reduces the effect of high dimensions. We prove that
ConMeZO achieves the same worst-case convergence rate as MeZO. Empirically,
when finetuning LLMs on natural language tasks, ConMeZO is up to 2X faster than
MeZO while retaining the low-memory footprint of zeroth-order methods.

</details>


### [80] [From Solo to Symphony: Orchestrating Multi-Agent Collaboration with Single-Agent Demos](https://arxiv.org/abs/2511.02762)
*Xun Wang,Zhuoran Li,Yanshan Lin,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: SoCo is a framework that leverages easily obtainable solo demonstrations to improve training efficiency and performance in multi-agent reinforcement learning by pretraining a shared solo policy and adapting it for cooperation via policy fusion.


<details>
  <summary>Details</summary>
Motivation: Training multi-agent systems from scratch is inefficient and relies heavily on costly multi-agent data; solo experiences are more accessible in many real-world cooperative scenarios and can potentially alleviate this bottleneck.

Method: SoCo first pretrains a shared solo policy using solo demonstrations, then adapts it for multi-agent cooperation through a policy fusion mechanism combining an MoE-like gating selector and an action editor.

Result: Experiments on diverse cooperative tasks show that SoCo significantly improves both training efficiency and final performance of existing MARL algorithms.

Conclusion: Solo demonstrations are a scalable and effective complement to multi-agent data, making cooperative reinforcement learning more practical and widely applicable.

Abstract: Training a team of agents from scratch in multi-agent reinforcement learning
(MARL) is highly inefficient, much like asking beginners to play a symphony
together without first practicing solo. Existing methods, such as offline or
transferable MARL, can ease this burden, but they still rely on costly
multi-agent data, which often becomes the bottleneck. In contrast, solo
experiences are far easier to obtain in many important scenarios, e.g.,
collaborative coding, household cooperation, and search-and-rescue. To unlock
their potential, we propose Solo-to-Collaborative RL (SoCo), a framework that
transfers solo knowledge into cooperative learning. SoCo first pretrains a
shared solo policy from solo demonstrations, then adapts it for cooperation
during multi-agent training through a policy fusion mechanism that combines an
MoE-like gating selector and an action editor. Experiments across diverse
cooperative tasks show that SoCo significantly boosts the training efficiency
and performance of backbone algorithms. These results demonstrate that solo
demonstrations provide a scalable and effective complement to multi-agent data,
making cooperative learning more practical and broadly applicable.

</details>


### [81] [VecComp: Vector Computing via MIMO Digital Over-the-Air Computation](https://arxiv.org/abs/2511.02765)
*Saeed Razavikia,José Mairton Barros Da Silva Junior,Carlo Fischione*

Main category: cs.LG

TL;DR: 本文提出了VecComp，一种将ChannelComp框架与多天线技术结合的通用化方法，支持向量函数计算，并在衰落信道下保持计算高效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: ChannelComp框架目前仅支持标量函数计算，且易受信道衰落影响，而许多数据密集型应用需要进行向量计算，因此亟需一种可扩展且鲁棒的向量计算框架。

Method: 通过将ChannelComp与多天线技术结合，提出VecComp框架，实现向量函数的空中计算，并保证计算复杂度随向量维度线性增长。

Result: 理论分析给出了VecComp在衰落信道下的均方误差非渐近上界，数值实验验证了其在向量函数计算和衰落补偿方面的优越性能。

Conclusion: VecComp有效扩展了ChannelComp的能力，使其适用于高维数据密集型应用，并在噪声和衰落多址信道中保持高效与鲁棒。

Abstract: Recently, the ChannelComp framework has proposed digital over-the-air
computation by designing digital modulations that enable the computation of
arbitrary functions. Unlike traditional analog over-the-air computation, which
is restricted to nomographic functions, ChannelComp enables a broader range of
computational tasks while maintaining compatibility with digital communication
systems. This framework is intended for applications that favor local
information processing over the mere acquisition of data. However, ChannelComp
is currently designed for scalar function computation, while numerous
data-centric applications necessitate vector-based computations, and it is
susceptible to channel fading. In this work, we introduce a generalization of
the ChannelComp framework, called VecComp, by integrating ChannelComp with
multiple-antenna technology. This generalization not only enables vector
function computation but also ensures scalability in the computational
complexity, which increases only linearly with the vector dimension. As such,
VecComp remains computationally efficient and robust against channel
impairments, making it suitable for high-dimensional, data-centric
applications. We establish a non-asymptotic upper bound on the mean squared
error of VecComp, affirming its computation efficiency under fading channel
conditions. Numerical experiments show the effectiveness of VecComp in
improving the computation of vector functions and fading compensation over
noisy and fading multiple-access channels.

</details>


### [82] [Enhancing Federated Learning Privacy with QUBO](https://arxiv.org/abs/2511.02785)
*Andras Ferenczi,Sutapa Samanta,Dagen Wang,Todd Hodges*

Main category: cs.LG

TL;DR: 本文提出一种受量子计算启发的QUBO方法，在联邦学习中每轮仅选择少量最相关的客户端更新，从而显著降低隐私泄露风险，同时保持甚至提升模型准确率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中，随着客户端参与训练轮次的增加，其数据面临更高的隐私泄露风险，包括成员推断、属性推断和模型反演攻击。为减少每轮训练中客户端的隐私暴露，作者提出一种新的客户端选择机制。

Method: 采用量子计算启发的二次无约束二值优化（QUBO）公式，在每轮训练中选择对模型更新最相关的客户端子集，以减少参与客户端数量，从而降低隐私暴露。假设服务器可信且拥有全局分布的验证/测试集。

Result: 在MNIST数据集（300客户端，20轮）上，该方法实现了每轮95.2%、累计49%的隐私暴露减少，147个客户端从未被选中参与训练，同时保持或优于全聚合的准确率；在CINIC-10数据集（30客户端）上也获得每轮82%、累计33%的隐私改善。

Conclusion: 所提QUBO方法能有效降低联邦学习中的隐私风险，同时维持模型性能，适用于不同规模和复杂度的模型。

Abstract: Federated learning (FL) is a widely used method for training machine learning
(ML) models in a scalable way while preserving privacy (i.e., without
centralizing raw data). Prior research shows that the risk of exposing
sensitive data increases cumulatively as the number of iterations where a
client's updates are included in the aggregated model increase. Attackers can
launch membership inference attacks (MIA; deciding whether a sample or client
participated), property inference attacks (PIA; inferring attributes of a
client's data), and model inversion attacks (MI; reconstructing inputs),
thereby inferring client-specific attributes and, in some cases, reconstructing
inputs. In this paper, we mitigate risk by substantially reducing per client
exposure using a quantum computing-inspired quadratic unconstrained binary
optimization (QUBO) formulation that selects a small subset of client updates
most relevant for each training round. In this work, we focus on two threat
vectors: (i) information leakage by clients during training and (ii)
adversaries who can query or obtain the global model. We assume a trusted
central server and do not model server compromise. This method also assumes
that the server has access to a validation/test set with global data
distribution. Experiments on the MNIST dataset with 300 clients in 20 rounds
showed a 95.2% per-round and 49% cumulative privacy exposure reduction, with
147 clients' updates never being used during training while maintaining in
general the full-aggregation accuracy or even better. The method proved to be
efficient at lower scale and more complex model as well. A CINIC-10
dataset-based experiment with 30 clients resulted in 82% per-round privacy
improvement and 33% cumulative privacy.

</details>


### [83] [Fast, Private, and Protected: Safeguarding Data Privacy and Defending Against Model Poisoning Attacks in Federated Learning](https://arxiv.org/abs/2511.02797)
*Nicolas Riccieri Gardin Assumpcao,Leandro Villas*

Main category: cs.LG

TL;DR: FPP是一种兼顾隐私保护与鲁棒性的联邦学习新方法，通过参与者评估、训练恢复机制和基于声誉的筛选策略，在存在模型投毒攻击者的情况下仍能快速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有保障联邦学习数据隐私的方法难以有效防御恶意参与者对训练过程的攻击，因此需要一种既能保护隐私又能抵御攻击的新机制。

Method: 提出Fast, Private, and Protected (FPP)方法，结合安全聚合、参与者评估、攻击后训练恢复机制以及基于声誉的攻击者参与抑制策略。

Result: 实验表明，FPP相比FedAvg、Power-of-Choice及基于裁剪均值和中位数的聚合方法，具有更快的收敛速度，并能在存在模型投毒攻击者的情况下成功收敛。

Conclusion: FPP在保障数据隐私的同时有效提升了联邦学习对恶意攻击的鲁棒性，是一种高效且安全的联邦训练方案。

Abstract: Federated Learning (FL) is a distributed training paradigm wherein
participants collaborate to build a global model while ensuring the privacy of
the involved data, which remains stored on participant devices. However,
proposals aiming to ensure such privacy also make it challenging to protect
against potential attackers seeking to compromise the training outcome. In this
context, we present Fast, Private, and Protected (FPP), a novel approach that
aims to safeguard federated training while enabling secure aggregation to
preserve data privacy. This is accomplished by evaluating rounds using
participants' assessments and enabling training recovery after an attack. FPP
also employs a reputation-based mechanism to mitigate the participation of
attackers. We created a dockerized environment to validate the performance of
FPP compared to other approaches in the literature (FedAvg, Power-of-Choice,
and aggregation via Trimmed Mean and Median). Our experiments demonstrate that
FPP achieves a rapid convergence rate and can converge even in the presence of
malicious participants performing model poisoning attacks.

</details>


### [84] [GeoCrossBench: Cross-Band Generalization for Remote Sensing](https://arxiv.org/abs/2511.02831)
*Hakob Tamazyan,Ani Vanyan,Alvard Barseghyan,Anna Khosrovyan,Evan Shelhamer,Hrant Khachatrian*

Main category: cs.LG

TL;DR: 本文提出了GeoCrossBench，扩展了GeoBench基准，用于评估遥感基础模型在新卫星上的泛化能力，并开发了自监督模型ChiViT以提升跨卫星性能。实验表明现有遥感基础模型在分布内任务上不优于通用模型（如DINOv3），在无波段重叠或新增波段情况下性能显著下降，而ChiViT在无波段重叠场景中表现最优。仅微调最后一层线性分类器即可获得较一致的跨卫星性能，说明该基准仍有提升空间。代码与数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 随着遥感卫星数量和多样性不断增加，而标注数据主要来自老旧卫星，训练支持新卫星的模型成本高昂，因此提升模型对新卫星的泛化能力变得至关重要。

Method: 提出GeoCrossBench基准，包含三种评估协议：分布内性能、对无波段重叠卫星的泛化、对测试时新增波段的泛化；同时开发自监督模型ChiViT，作为ChannelViT的扩展以增强跨卫星性能。

Result: 1）遥感基础模型（DOFA、TerraFM）在分布内任务上未优于DINOv3；2）无波段重叠时所有模型性能下降2–4倍，ChiViT显著优于DINOv3；3）测试时新增波段导致性能平均下降5–25%；4）仅微调最后一层线性层即可获得较稳定的跨卫星性能。

Conclusion: 当前遥感基础模型在跨卫星泛化方面仍有明显不足，GeoCrossBench为未来研究提供了有效评估工具，而ChiViT展示了通过自监督学习提升泛化能力的潜力。该基准尚未饱和，值得进一步探索。

Abstract: The number and diversity of remote sensing satellites grows over time, while
the vast majority of labeled data comes from older satellites. As the
foundation models for Earth observation scale up, the cost of (re-)training to
support new satellites grows too, so the generalization capabilities of the
models towards new satellites become increasingly important. In this work we
introduce GeoCrossBench, an extension of the popular GeoBench benchmark with a
new evaluation protocol: it tests the in-distribution performance;
generalization to satellites with no band overlap; and generalization to
satellites with additional bands with respect to the training set. We also
develop a self-supervised extension of ChannelViT, ChiViT, to improve its
cross-satellite performance. First, we show that even the best foundation
models for remote sensing (DOFA, TerraFM) do not outperform general purpose
models like DINOv3 in the in-distribution setting. Second, when generalizing to
new satellites with no band overlap, all models suffer 2-4x drop in
performance, and ChiViT significantly outperforms the runner-up DINOv3. Third,
the performance of all tested models drops on average by 5-25\% when given
additional bands during test time. Finally, we show that fine-tuning just the
last linear layer of these models using oracle labels from all bands can get
relatively consistent performance across all satellites, highlighting that the
benchmark is far from being saturated. We publicly release the code and the
datasets to encourage the development of more future-proof remote sensing
models with stronger cross-satellite generalization.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [85] [A Taxonomy of Schedulers -- Operating Systems, Clusters and Big Data Frameworks](https://arxiv.org/abs/2511.01860)
*Leszek Sliwko*

Main category: cs.DC

TL;DR: 本文综述了当前部署和使用的负载调度器，提出了一种基于架构和设计的分层分类法，重点关注影响吞吐量和可扩展性的关键设计因素，并特别分析了Google的Borg系统。


<details>
  <summary>Details</summary>
Motivation: 现有调度系统种类繁多，缺乏聚焦于吞吐量与可扩展性关键设计因素的系统性分类，因此需要一种新的分类方法以厘清架构演进路径。

Method: 对已部署和实际使用的负载调度器进行综述，基于其架构和设计特征构建分层分类体系，并重点分析关键设计因素及架构改进。

Result: 提出了一个新的调度器分类法，识别出影响系统性能的关键设计要素，并对Google Borg等先进系统进行了深入剖析。

Conclusion: 通过聚焦架构设计对吞吐量和可扩展性的影响，该分类法有助于理解现有调度系统的演进逻辑，并为未来系统设计提供参考。

Abstract: This review analyzes deployed and actively used workload schedulers'
solutions and presents a taxonomy in which those systems are divided into
several hierarchical groups based on their architecture and design. While other
taxonomies do exist, this review has focused on the key design factors that
affect the throughput and scalability of a given solution, as well as the
incremental improvements which bettered such an architecture. This review gives
special attention to Google's Borg, which is one of the most advanced and
published systems of this kind.

</details>


### [86] [Conceptual Design Report for FAIR Computing](https://arxiv.org/abs/2511.01861)
*Johan Messchendorp,Mohammad Al-Turany,Volker Friese,Thorsten Kollegger,Bastian Loeher,Jochen Markert,Andrew Mistry,Thomas Neff,Adrian Oeftiger,Michael Papenbrock,Stephane Pietri,Shahab Sanjari,Tobias Stockmanns*

Main category: cs.DC

TL;DR: 该概念设计报告概述了德国达姆施塔特FAIR设施从2028年“首次科学（加）”阶段开始的计算基础设施规划，旨在构建一个兼具联邦化与集中协调、可扩展且灵活的计算与存储架构，以支持多样化的科研需求和未来数据挑战。


<details>
  <summary>Details</summary>
Motivation: FAIR设施涵盖多样化的研究方向，面临日益增长的数据处理与存储挑战，亟需一个统一协调但又具备灵活性和可扩展性的计算基础设施来支撑未来的科研活动。

Method: 制定涵盖计算与存储策略、开放数据/软件/服务政策及架构的综合计算模型，采用联邦化与中央协调相结合的方式设计基础设施。

Result: 提出了面向2028年及以后FAIR运行阶段的计算基础设施概念设计方案，明确了政策框架与技术架构。

Conclusion: 通过构建联邦化且集中协调的计算基础设施，可有效满足FAIR多学科研究对计算资源的多样化和未来扩展性需求。

Abstract: This Conceptual Design Report (CDR) presents the plans of the computing
infrastructure for research at FAIR, Darmstadt, Germany. It presents the
computing requirements of the various research groups, the policies for the
computing and storage infrastructure, the foreseen FAIR computing model
including the open data, software and services policies and architecture for
the periods starting in 2028 with the "first science (plus)" phase to the
modularized start version of FAIR. The overall ambition is to create a
federated and centrally-orchestrated infrastructure serving the large diversity
of the research lines present with sufficient scalability and flexibility to
cope with future data challenges that will be present at FAIR.

</details>


### [87] [Possible Futures for Cloud Cost Models](https://arxiv.org/abs/2511.01862)
*Vanessa Sochat,Daniel Milroy*

Main category: cs.DC

TL;DR: 本文探讨了云计算成本模型如何从服务科学计算演变为以AI/ML为中心，并分析了这种转变对科研工作负载带来的挑战及未来可能的调整方向。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML成为现代计算的主要驱动力，云计算资源和成本模型日益围绕AI/ML用户优化，导致传统科学计算用户在资源竞争中处于劣势，难以获得所需计算资源。

Method: 文章通过回顾云计算成本模型的历史演变，分析当前以AI/ML为中心的资源分配机制对科学计算的影响，并探讨未来支持科研的可行云成本模型。

Result: 研究表明，当前云资源模型因偏向AI/ML用户，可能迫使科学工作负载运行在非理想环境中，影响科研效率与可及性。

Conclusion: 为持续支持科学发现，云成本模型需重新平衡，兼顾多元用户需求，尤其是保障科研计算的资源可及性与公平性。

Abstract: Cloud is now the leading software and computing hardware innovator, and is
changing the landscape of compute to one that is optimized for artificial
intelligence and machine learning (AI/ML). Computing innovation was initially
driven to meet the needs of scientific computing. As industry and consumer
usage of computing proliferated, there was a shift to satisfy a multipolar
customer base. Demand for AI/ML now dominates modern computing and innovation
has centralized on cloud. As a result, cost and resource models designed to
serve AI/ML use cases are not currently well suited for science. If resource
contention resulting from a unipole consumer makes access to contended
resources harder for scientific users, a likely future is running scientific
workloads where they were not intended. In this article, we discuss the past,
current, and possible futures of cloud cost models for the continued support of
discovery and science.

</details>


### [88] [EdgeReasoning: Characterizing Reasoning LLM Deployment on Edge GPUs](https://arxiv.org/abs/2511.01866)
*Benjamin Kubwimana,Qijing Huang*

Main category: cs.DC

TL;DR: 本文提出了EdgeReasoning，系统研究了在边缘GPU上部署用于推理任务的大语言模型（LLMs）时的延迟与准确率权衡，评估了不同架构、模型规模、提示策略和测试时扩展方法，绘制了准确率-延迟的帕累托前沿，为边缘部署提供指导。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备（如机器人）上部署大语言模型面临延迟严格和计算资源有限的挑战，而目前缺乏关于如何在推理架构、模型大小、token预算和测试时扩展策略之间进行最优组合的系统性指导。

Method: 系统量化不同LLM架构和模型规模下的延迟-准确率权衡；评估基于提示和模型微调的减少推理token长度的技术；分析不同并行度的测试时扩展方法在严格延迟约束下的表现。

Result: 构建了EdgeReasoning框架，绘制了在边缘GPU上部署推理型LLM时可实现的准确率与延迟之间的帕累托前沿，为实际部署提供了系统性配置指南。

Conclusion: 通过综合评估多种设计因素，EdgeReasoning为在资源受限的边缘设备上高效部署推理型大语言模型提供了实用且系统的优化路径。

Abstract: Edge intelligence paradigm is increasingly demanded by the emerging
autonomous systems, such as robotics. Beyond ensuring privacy-preserving
operation and resilience in connectivity-limited environments, edge deployment
offers significant energy and cost advantages over cloud-based solutions.
However, deploying large language models (LLMs) for reasoning tasks on edge
GPUs faces critical challenges from strict latency constraints and limited
computational resources. To navigate these constraints, developers must balance
multiple design factors - choosing reasoning versus non-reasoning
architectures, selecting appropriate model sizes, allocating token budgets, and
applying test-time scaling strategies - to meet target latency and optimize
accuracy. Yet guidance on optimal combinations of these variables remains
scarce. In this work, we present EdgeReasoning, a comprehensive study
characterizing the deployment of reasoning LLMs on edge GPUs. We systematically
quantify latency-accuracy tradeoffs across various LLM architectures and model
sizes. We systematically evaluate prompt-based and model-tuning-based
techniques for reducing reasoning token length while maintaining performance
quality. We further profile test-time scaling methods with varying degrees of
parallelism to maximize accuracy under strict latency budgets. Through these
analyses, EdgeReasoning maps the Pareto frontier of achievable accuracy-latency
configurations, offering systematic guidance for optimal edge deployment of
reasoning LLMs.

</details>


### [89] [Structural Analysis of Multi-Core Processor and Reliability Evaluation Model](https://arxiv.org/abs/2511.01871)
*S. Tsiramua,H. Meladze,T. Davitashvili,J. M. Sanchez,F. Criado-Aldeanueva*

Main category: cs.DC

TL;DR: 本文基于逻辑概率方法，构建了多核处理器（配备多功能核心）在可靠性、容错性、可用性与灵活性等方面的结构分析与效率评估模型，并对双核与四核处理器进行了结构分析，总结了效率指标的提升趋势。


<details>
  <summary>Details</summary>
Motivation: 为提升多核处理器的效率，需系统评估其可靠性、容错性、可用性和灵活性等关键指标，而现有方法缺乏对多功能核心及其多种运行状态的综合建模。

Method: 采用逻辑概率方法，建立多功能核心的可靠性与容错性评估模型、多核处理器成功运行的最短路径与灵活性逻辑概率模型，以及考虑所有性能状态的寿命与可靠性估计模型。

Result: 完成了双核与四核处理器的结构分析，揭示了多核处理器效率指标随结构变化的提升趋势。

Conclusion: 所提出的逻辑概率模型能有效支持对可变结构多核处理器的综合效率评估，为未来高性能处理器设计提供理论依据。

Abstract: In the present paper, the models of structural analysis and evaluation of
efficiency indicators (reliability, fault tolerance, viability, and
flexibility) of a multi core processor with variable structure, equipped with
multi functional cores, are considered. Using logical probabilistic methods,
the following has been developed: models for evaluating the reliability and
fault tolerance of processor cores as multi functional elements; logical
probabilistic models of the shortest paths, flexibility, and performance
conditions for successful operation of multi core processors based on multi
functional cores; and models for estimating the reliability, fault tolerance,
and lifetime of multi core processors considering all possible states of
performance. The results of the structural analysis of two core and four core
processors and the trends of increasing the efficiency indicators of multi core
processors are presented.

</details>


### [90] [mLR: Scalable Laminography Reconstruction based on Memoization](https://arxiv.org/abs/2511.01893)
*Bin Ma,Viktor Nikitin,Xi Wang,Tekin Bicer,Dong Li*

Main category: cs.DC

TL;DR: mLR通过记忆化和变量卸载技术优化ADMM-FFT算法，在有限内存下实现了对2K³规模层析重建问题的高效求解，平均性能提升52.8%。


<details>
  <summary>Details</summary>
Motivation: ADMM-FFT在层析成像重建中精度高，但存在计算时间过长和内存消耗大的问题，限制了其在大规模问题中的应用。

Method: 提出mLR方法，利用记忆化技术避免重复的FFT计算，并结合变量卸载策略节省CPU内存，实现跨GPU和跨节点的可扩展性。

Result: 在2K×2K×2K输入规模下成功运行ADMM-FFT，这是该方法迄今处理的最大规模问题；相比原始ADMM-FFT，平均性能提升52.8%，最高达65.4%。

Conclusion: mLR有效解决了ADMM-FFT在大规模层析重建中的计算与内存瓶颈，显著提升了性能与可扩展性。

Abstract: ADMM-FFT is an iterative method with high reconstruction accuracy for
laminography but suffers from excessive computation time and large memory
consumption. We introduce mLR, which employs memoization to replace the
time-consuming Fast Fourier Transform (FFT) operations based on an unique
observation that similar FFT operations appear in iterations of ADMM-FFT. We
introduce a series of techniques to make the application of memoization to
ADMM-FFT performance-beneficial and scalable. We also introduce variable
offloading to save CPU memory and scale ADMM-FFT across GPUs within and across
nodes. Using mLR, we are able to scale ADMM-FFT on an input problem of
2Kx2Kx2K, which is the largest input problem laminography reconstruction has
ever worked on with the ADMM-FFT solution on limited memory; mLR brings 52.8%
performance improvement on average (up to 65.4%), compared to the original
ADMM-FFT.

</details>


### [91] [GPoS: Geospatially-aware Proof of Stake](https://arxiv.org/abs/2511.02034)
*Shashank Motepalli,Naman Garg,Gengrui Zhang,Hans-Arno Jacobsen*

Main category: cs.DC

TL;DR: 该论文提出了一种名为地理感知权益证明（GPoS）的新机制，通过将地理位置多样性纳入权益证明共识中，显著提升了主流PoS区块链的地理去中心化水平，同时对共识性能影响极小。


<details>
  <summary>Details</summary>
Motivation: 现有主流权益证明（PoS）区块链在共识投票权上高度集中于少数地理区域，导致地理去中心化程度不足，影响系统的监管韧性、鲁棒性和公平性。

Method: 作者对Aptos、Avalanche、Ethereum、Solana和Sui五个主流PoS区块链进行实证分析，并提出GPoS机制，将地理空间多样性与基于权益的投票权相结合。

Result: 实验表明，GPoS平均可将基于特征向量中心性的基尼系数所衡量的地理去中心化水平提升45%，同时在HotStuff和CometBFT等BFT协议中仅带来极小的性能开销。

Conclusion: GPoS能有效增强区块链的地理去中心化，同时几乎不影响共识性能，为构建更具韧性与公平性的区块链系统提供了可行方案。

Abstract: Geospatial decentralization is essential for blockchains, ensuring regulatory
resilience, robustness, and fairness. We empirically analyze five major Proof
of Stake (PoS) blockchains: Aptos, Avalanche, Ethereum, Solana, and Sui,
revealing that a few geographic regions dominate consensus voting power,
resulting in limited geospatial decentralization. To address this, we propose
Geospatially aware Proof of Stake (GPoS), which integrates geospatial diversity
with stake-based voting power. Experimental evaluation demonstrates an average
45% improvement in geospatial decentralization, as measured by the Gini
coefficient of Eigenvector centrality, while incurring minimal performance
overhead in BFT protocols, including HotStuff and CometBFT. These results
demonstrate that GPoS can improve geospatial decentralization {while, in our
experiments, incurring minimal overhead} to consensus performance.

</details>


### [92] [Eliminating Multi-GPU Performance Taxes: A Systems Approach to Efficient Distributed LLMs](https://arxiv.org/abs/2511.02168)
*Octavian Alexandru Trifan,Karthik Sangaiah,Muhammad Awad,Muhammad Osama,Sumanth Gudaparthi,Alexandru Nicolau,Alexander Veidenbaum,Ganesh Dasika*

Main category: cs.DC

TL;DR: 本文提出通过细粒度编程模式替代传统的BSP模型，利用Iris for Triton等库中的核内通信原语，消除“三大开销”，在分布式大语言模型（LLM）任务中实现10-20%的端到端延迟提升。


<details>
  <summary>Details</summary>
Motivation: 传统BSP模型在分布式GPU执行大语言模型时存在显著性能瓶颈，包括同步开销、核间数据局部性差和核启动开销，亟需更高效的执行范式。

Method: 引入“三大开销”分析框架，利用Iris for Triton提供的核内通信原语，设计细粒度生产者-消费者流水线，并以数据流同步替代全局屏障。

Result: 在All-Gather+GEMM和Flash Decode等关键算子上，相比BSP方法实现了10-20%的端到端延迟降低。

Conclusion: 所提方法通过消除三大开销，为分布式LLM工作负载提供了更高性能和可编程性的新范式。

Abstract: As large language models (LLMs) continue to scale, their workloads
increasingly rely on distributed execution across multiple GPUs. However, the
conventional bulk synchronous parallel~(BSP) model used in such settings
introduces significant performance inefficiencies. To characterize these
bottlenecks, we introduce the ''Three Taxes'' (Bulk Synchronous, Inter-Kernel
Data Locality, and Kernel Launch Overhead) as an analytical framework. We
propose moving beyond the rigid BSP model to address key inefficiencies in
distributed GPU execution. By exploiting libraries like Iris for Triton, we
gain access to in-kernel communication primitives that enable the design of
novel fine-grained programming patterns, offering greater flexibility and
performance than traditional BSP-based approaches. These patterns
systematically eliminate the three taxes by creating direct, tile-level
producer-consumer pipelines and replacing global barriers with fine-grained
dataflow synchronization. Applying this methodology to critical kernels, from
the foundational All-Gather + general matrix multiplication operation to the
complex Flash Decode algorithm, we observe a 10-20% speedup in end-to-end
latency over BSP-based approaches, establishing a more programmable and
efficient paradigm for distributed LLM workloads.

</details>


### [93] [From Models to Operators: Rethinking Autoscaling Granularity for Large Generative Models](https://arxiv.org/abs/2511.02248)
*Xingqi Cui,Chieh-Jan Mike Liang,Jiarong Xing,Haoran Qiu*

Main category: cs.DC

TL;DR: 本文提出一种面向算子级别的自动扩缩框架，通过细粒度资源分配优化大生成模型的推理效率，在满足服务等级目标（SLO）的同时显著减少GPU使用和能耗。


<details>
  <summary>Details</summary>
Motivation: 现有大模型服务方案采用静态资源配置或模型级自动扩缩，将模型视为整体，无法适应在线推理流量的动态变化，导致性能下降或资源利用率低下。

Method: 基于对生成模型内部算子异构性的分析，提出以算子为粒度进行自动扩缩，根据各算子的计算、内存特征及对批大小、序列长度和流量的敏感性，优化其扩缩、批处理和部署策略。

Result: 在生产级轨迹评估中，该方法在满足SLO的前提下最多减少40%的GPU和35%的能耗；在固定资源下可实现1.6倍吞吐量并降低5%能耗。

Conclusion: 算子而非整个模型，是扩缩大生成模型工作负载更有效的基本单元。

Abstract: Serving large generative models such as LLMs and multi- modal transformers
requires balancing user-facing SLOs (e.g., time-to-first-token,
time-between-tokens) with provider goals of efficiency and cost reduction.
Existing solutions rely on static provisioning or model-level autoscaling, both
of which treat the model as a monolith. This coarse-grained resource management
leads to degraded performance or significant resource underutilization due to
poor adaptability to dynamic inference traffic that is common online.
  The root cause of this inefficiency lies in the internal structure of
generative models: they are executed as graphs of interconnected operators.
Through detailed characterization and systematic analysis, we find that
operators are heterogeneous in their compute and memory footprints and exhibit
diverse sensitivity to workload and resource factors such as batch size,
sequence length, and traffic rate. This heterogeneity suggests that the
operator, rather than the entire model, is the right granularity for scaling
decisions.
  We propose an operator-level autoscaling framework, which allocates resources
at finer (operator)-granularity, optimizing the scaling, batching, and
placement based on individual operator profiles. Evaluated on production-scale
traces, our approach preserves SLOs with up to 40% fewer GPUs and 35% less
energy, or under fixed resources achieves 1.6x higher throughput with 5% less
energy. These results show that the operator, rather than the model, is
fundamentally a more effective unit for scaling large generative workloads.

</details>


### [94] [Fast Algorithms for Scheduling Many-body Correlation Functions on Accelerators](https://arxiv.org/abs/2511.02257)
*Oguz Selvitopi,Emin Ozturk,Jie Chen,Ponnuswamy Sadayappan,Robert G. Edwards,Aydın Buluç*

Main category: cs.DC

TL;DR: 本文提出两种新颖的调度算法，通过重排LQCD中相关函数计算的张量收缩顺序，提升张量复用并降低峰值内存，从而显著减少数据传输与计算时间。


<details>
  <summary>Details</summary>
Motivation: LQCD模拟中的相关函数计算涉及大量大规模张量收缩操作，在GPU上执行时面临内存带宽和数据复用效率的挑战，亟需优化调度策略以减少数据流量并提升性能。

Method: 提出两种快速调度算法，利用LQCD应用特性（如二元收缩和收缩树内的局部性），通过重排收缩顺序增强时间局部性，以最小化峰值内存为目标，并集成到Redstar软件中。

Result: 调度器在峰值内存上最多降低2.1倍，缓存驱逐减少最多4.2倍，数据流量减少最多1.8倍，相关函数计算速度最多提升1.9倍。

Conclusion: 所提出的调度算法有效优化了LQCD相关函数计算中的内存使用和数据复用，显著提升了GPU上的计算效率和整体求解速度。

Abstract: Computation of correlation functions is a key operation in Lattice quantum
chromodynamics (LQCD) simulations to extract nuclear physics observables. These
functions involve many binary batch tensor contractions, each tensor possibly
occupying hundreds of MBs of memory. Performing these contractions on GPU
accelerators poses the challenge of scheduling them as to optimize tensor reuse
and reduce data traffic. In this work we propose two fast novel scheduling
algorithms that reorder contractions to increase temporal locality via
input/intermediate tensor reuse. Our schedulers take advantage of
application-specific features, such as contractions being binary and locality
within contraction trees, to optimize the objective of minimizing peak memory.
We integrate them into the LQCD analysis software suite Redstar and improve
time-to-solution. Our schedulers attain upto 2.1x improvement in peak memory,
which is reflected by a reduction of upto 4.2x in evictions, upto 1.8x in data
traffic, resulting in upto 1.9x faster correlation function computation time.

</details>


### [95] [Federated Attention: A Distributed Paradigm for Collaborative LLM Inference over Edge Networks](https://arxiv.org/abs/2511.02647)
*Xiumei Deng,Zehui Xiong,Binbin Chen,Dong In Kim,Merouane Debbah,H. Vincent Poor*

Main category: cs.DC

TL;DR: 本文提出了一种名为FedAttn的新型分布式大语言模型（LLM）推理框架，将联邦学习思想融入自注意力机制，在保护用户隐私的同时，显著提升了通信与计算效率。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大语言模型面临隐私泄露、通信开销大和计算瓶颈等挑战，尤其是在协作场景中，亟需一种兼顾隐私、效率与性能的解决方案。

Method: FedAttn允许各参与方在其本地token表示上执行自注意力计算，并周期性地跨多个Transformer块交换和聚合Key-Value（KV）矩阵，从而在不暴露私有提示的前提下协同生成LLM响应。作者还揭示了FedAttn中上下文表示优化与联邦学习中参数优化之间的结构对偶性，并基于此系统性地引入联邦优化技术。

Result: 实验验证了理论分析的正确性，表明FedAttn在响应质量与通信/计算效率之间存在由同步间隔和参与方数量决定的权衡关系，并通过稀疏注意力和自适应KV聚合策略展现出显著的优化潜力。

Conclusion: FedAttn为边缘设备上的协作式大语言模型推理提供了一个兼顾隐私保护、通信效率和计算效率的新范式，具有良好的可扩展性和实际部署潜力。

Abstract: Large language models (LLMs) are proliferating rapidly at the edge,
delivering intelligent capabilities across diverse application scenarios.
However, their practical deployment in collaborative scenarios confronts
fundamental challenges: privacy vulnerabilities, communication overhead, and
computational bottlenecks. To address these, we propose Federated Attention
(FedAttn), which integrates the federated paradigm into the self-attention
mechanism, creating a new distributed LLM inference framework that
simultaneously achieves privacy protection, communication efficiency, and
computational efficiency. FedAttn enables participants to perform local
self-attention over their own token representations while periodically
exchanging and aggregating Key-Value (KV) matrices across multiple Transformer
blocks, collaboratively generating LLM responses without exposing private
prompts. Further, we identify a structural duality between contextual
representation refinement in FedAttn and parameter optimization in FL across
private data, local computation, and global aggregation. This key insight
provides a principled foundation for systematically porting federated
optimization techniques to collaborative LLM inference. Building on this
framework, we theoretically analyze how local self-attention computation within
participants and heterogeneous token relevance among participants shape error
propagation dynamics across Transformer blocks. Moreover, we characterize the
fundamental trade-off between response quality and communication/computation
efficiency, which is governed by the synchronization interval and the number of
participants. Experimental results validate our theoretical analysis, and
reveal significant optimization opportunities through sparse attention and
adaptive KV aggregation, highlighting FedAttn's potential to deliver
scalability and efficiency in real-world edge deployments.

</details>


### [96] [Implementing Multi-GPU Scientific Computing Miniapps Across Performance Portable Frameworks](https://arxiv.org/abs/2511.02655)
*Johansell Villalobos,Josef Ruzicka,Silvio Rizzi*

Main category: cs.DC

TL;DR: 本文在Polaris超算单节点上使用四种性能可移植框架（Kokkos、OpenMP、RAJA、OCCA）对N体模拟和结构化网格模拟进行性能评估，发现各框架在不同场景下表现差异显著，OCCA在小规模问题上较快但扩展性受限，OpenMP在结构化网格中表现不佳，需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 随着异构计算架构的兴起，科学计算亟需与厂商无关且具备性能可移植性的编程框架，以在不同硬件平台上高效运行，同时最小化代码改动。

Method: 在Polaris超算单节点上，利用四个NVIDIA A100 GPU，对N体模拟和结构化网格模拟两个典型科学计算应用，分别采用Kokkos、OpenMP、RAJA和OCCA四种性能可移植框架进行实现与性能测试。

Result: OCCA在小规模验证问题上执行更快，可能得益于JIT编译，但其缺乏优化的归约算法限制了大规模模拟的可扩展性；OpenMP在结构化网格模拟中表现较差，可能源于节点间数据同步与通信效率低下。

Conclusion: 各性能可移植框架在实际应用中存在显著性能差异，需针对归约算法、数据通信和内存管理等方面进一步优化，并开展可扩展性研究与全面统计分析以充分评估其性能。

Abstract: Scientific computing in the exascale era demands increased computational
power to solve complex problems across various domains. With the rise of
heterogeneous computing architectures the need for vendor-agnostic, performance
portability frameworks has been highlighted. Libraries like Kokkos have become
essential for enabling high-performance computing applications to execute
efficiently across different hardware platforms with minimal code changes. In
this direction, this paper presents preliminary time-to-solution results for
two representative scientific computing applications: an N-body simulation and
a structured grid simulation. Both applications used a distributed memory
approach and hardware acceleration through four performance portability
frameworks: Kokkos, OpenMP, RAJA, and OCCA. Experiments conducted on a single
node of the Polaris supercomputer using four NVIDIA A100 GPUs revealed
significant performance variability among frameworks. OCCA demonstrated faster
execution times for small-scale validation problems, likely due to JIT
compilation, however its lack of optimized reduction algorithms may limit
scalability for larger simulations while using its out of the box API. OpenMP
performed poorly in the structured grid simulation most likely due to
inefficiencies in inter-node data synchronization and communication. These
findings highlight the need for further optimization to maximize each
framework's capabilities. Future work will focus on enhancing reduction
algorithms, data communication, memory management, as wells as performing
scalability studies, and a comprehensive statistical analysis to evaluate and
compare framework performance.

</details>


### [97] [Making Democracy Work: Fixing and Simplifying Egalitarian Paxos (Extended Version)](https://arxiv.org/abs/2511.02743)
*Fedor Ryabinin,Alexey Gotsman,Pierre Sutra*

Main category: cs.DC

TL;DR: 本文提出了EPaxos*，一种更简单且正确的Egalitarian Paxos变体，改进了故障恢复算法，并推广了其容错阈值范围。


<details>
  <summary>Details</summary>
Motivation: Egalitarian Paxos虽然解决了传统Paxos协议中领导者单点故障和高延迟的问题，但其协议复杂、定义模糊且存在严重缺陷，亟需一个更清晰、可靠的替代方案。

Method: 作者设计了EPaxos*，通过引入更简单的故障恢复算法，并对协议进行形式化验证；同时将协议推广到更通用的容错阈值条件 $n \ge \max\{2e+f-1, 2f+1\}$。

Result: EPaxos*在保持Egalitarian Paxos优点（如无领导者、低延迟执行、高容错性）的同时，消除了原协议的复杂性和错误，并证明了其进程数下界是最优的。

Conclusion: EPaxos*是一个更简洁、正确且通用的无领导者状态机复制协议，为后续相关研究提供了坚实基础。

Abstract: Classical state-machine replication protocols, such as Paxos, rely on a
distinguished leader process to order commands. Unfortunately, this approach
makes the leader a single point of failure and increases the latency for
clients that are not co-located with it. As a response to these drawbacks,
Egalitarian Paxos introduced an alternative, leaderless approach, that allows
replicas to order commands collaboratively. Not relying on a single leader
allows the protocol to maintain non-zero throughput with up to $f$ crashes of
any processes out of a total of $n = 2f+1$. The protocol furthermore allows any
process to execute a command $c$ fast, in $2$ message delays, provided no more
than $e = \lceil\frac{f+1}{2}\rceil$ other processes fail, and all concurrently
submitted commands commute with $c$; the latter condition is often satisfied in
practical systems.
  Egalitarian Paxos has served as a foundation for many other replication
protocols. But unfortunately, the protocol is very complex, ambiguously
specified and suffers from nontrivial bugs. In this paper, we present EPaxos*
-- a simpler and correct variant of Egalitarian Paxos. Our key technical
contribution is a simpler failure-recovery algorithm, which we have rigorously
proved correct. Our protocol also generalizes Egalitarian Paxos to cover the
whole spectrum of failure thresholds $f$ and $e$ such that $n \ge \max\{2e+f-1,
2f+1\}$ -- the number of processes that we show to be optimal.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [98] [Human-AI Co-Embodied Intelligence for Scientific Experimentation and Manufacturing](https://arxiv.org/abs/2511.02071)
*Xinyi Lin,Yuyang Zhang,Yuanhang Gan,Juntao Chen,Hao Shen,Yichun He,Lijun Li,Ze Yuan,Shuang Wang,Chaohao Wang,Rui Zhang,Na Li,Jia Liu*

Main category: cs.AI

TL;DR: 本文提出了一种人-AI共融具身智能系统（APEX），通过将人类操作者、智能体AI与可穿戴硬件结合，在现实实验与制造中实现人机协同，提升流程的可重复性、可扩展性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习和自动化技术多局限于虚拟环境，真实世界的科学实验与制造仍高度依赖人类专家，导致智能与物理执行之间存在鸿沟，限制了科研与制造流程的可重复性、可扩展性和可及性。

Method: 提出“人-AI共融具身智能”范式，构建Agentic-Physical Experimentation（APEX）系统，结合智能体推理与混合现实可穿戴设备，实时捕捉、分析人类操作，提供3D视觉引导与上下文感知反馈，并与标准操作流程对齐。

Result: 在柔性电子洁净室制造环境中，APEX系统展现出优于通用多模态大语言模型的上下文推理准确率，能实时纠错并将专家知识有效传递给新手。

Conclusion: 该研究建立了新型的智能体-物理-人类协同智能形式，将智能体推理能力延伸至物理世界，推动科研与制造向自主化、可追溯、可解释和可扩展的方向发展。

Abstract: Scientific experiment and manufacture rely on complex, multi-step procedures
that demand continuous human expertise for precise execution and
decision-making. Despite advances in machine learning and automation,
conventional models remain confined to virtual domains, while real-world
experiment and manufacture still rely on human supervision and expertise. This
gap between machine intelligence and physical execution limits reproducibility,
scalability, and accessibility across scientific and manufacture workflows.
Here, we introduce human-AI co-embodied intelligence, a new form of physical AI
that unites human users, agentic AI, and wearable hardware into an integrated
system for real-world experiment and intelligent manufacture. In this paradigm,
humans provide precise execution and control, while agentic AI contributes
memory, contextual reasoning, adaptive planning, and real-time feedback. The
wearable interface continuously captures the experimental and manufacture
processes, facilitates seamless communication between humans and AI for
corrective guidance and interpretable collaboration. As a demonstration, we
present Agentic-Physical Experimentation (APEX) system, coupling agentic
reasoning with physical execution through mixed-reality. APEX observes and
interprets human actions, aligns them with standard operating procedures,
provides 3D visual guidance, and analyzes every step. Implemented in a
cleanroom for flexible electronics fabrication, APEX system achieves
context-aware reasoning with accuracy exceeding general multimodal large
language models, corrects errors in real time, and transfers expertise to
beginners. These results establish a new class of agentic-physical-human
intelligence that extends agentic reasoning beyond computation into the
physical domain, transforming scientific research and manufacturing into
autonomous, traceable, interpretable, and scalable processes.

</details>


### [99] [Automated Reward Design for Gran Turismo](https://arxiv.org/abs/2511.02094)
*Michel Ma,Takuma Seno,Kaushik Subramanian,Peter R. Wurman,Peter Stone,Craig Sherstan*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型（LLM）、视觉语言模型（VLM）和人类反馈的方法，仅通过文本指令即可自动搜索适用于《GT赛车7》的奖励函数，从而训练出媲美冠军级智能体GT Sophy的强化学习赛车智能体，并能生成新颖行为。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境（如自动驾驶赛车）中，将期望行为映射为奖励函数非常困难，因此需要一种更高效、自动化的奖励函数设计方法。

Method: 结合大语言模型生成奖励函数、视觉语言模型进行偏好评估，并引入人类反馈，构建一个可根据文本指令自动搜索奖励函数的系统。

Result: 该系统成功训练出在《GT赛车7》中表现媲美GT Sophy的强化学习智能体，并能生成新颖的驾驶行为。

Conclusion: 该方法展示了在真实世界应用中实现自动化奖励设计的可行性，为强化学习智能体的行为定制提供了实用路径。

Abstract: When designing reinforcement learning (RL) agents, a designer communicates
the desired agent behavior through the definition of reward functions -
numerical feedback given to the agent as reward or punishment for its actions.
However, mapping desired behaviors to reward functions can be a difficult
process, especially in complex environments such as autonomous racing. In this
paper, we demonstrate how current foundation models can effectively search over
a space of reward functions to produce desirable RL agents for the Gran Turismo
7 racing game, given only text-based instructions. Through a combination of
LLM-based reward generation, VLM preference-based evaluation, and human
feedback we demonstrate how our system can be used to produce racing agents
competitive with GT Sophy, a champion-level RL racing agent, as well as
generate novel behaviors, paving the way for practical automated reward design
in real world applications.

</details>


### [100] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: 本文提出了深度价值基准（DVB），用于评估大语言模型是否真正学习到人类深层价值观而非表面偏好。通过在训练阶段引入深层价值与浅层特征的强相关性，并在测试阶段打破这种相关性，DVB可测量模型的“深度价值泛化率”（DVGR）。实验显示，9个主流模型的平均DVGR仅为0.30，低于随机水平，且模型规模越大表现略差。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在对齐人类意图方面存在风险：若仅学习偏好数据中的表面模式而非深层价值观（如道德原则），可能导致行为失准。因此，亟需一种能明确区分模型是否掌握深层价值的评估方法。

Method: 提出Deep Value Benchmark（DVB）框架，通过控制实验设计，在训练阶段使深层价值（如非恶意原则）与浅层特征（如正式语言）高度相关；在测试阶段解耦二者，观察模型是否依据深层价值做选择，并据此计算深度价值泛化率（DVGR）。

Result: 在9个模型上的实验表明，平均DVGR仅为0.30，所有模型的表现均低于随机水平；且更大规模的模型DVGR略低。研究团队公开了经过三次人工验证的数据集。

Conclusion: 当前大语言模型未能有效泛化人类深层价值观，DVB为衡量AI对齐中的核心能力提供了一种可解释、可量化的评估工具。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [101] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 本文提出InsurAgent，一个基于大语言模型（LLM）的多模块智能体，用于更准确地模拟个体在洪水保险决策中的行为，克服了现有LLM在定量概率估计上的不足。


<details>
  <summary>Details</summary>
Motivation: 美国高风险地区居民洪水保险参保率极低，亟需理解其背后的决策行为机制；尽管LLM展现出类人智能，但在定量估计保险购买概率方面仍存在局限。

Method: 构建包含保险购买概率的基准数据集，评估LLM表现，并提出InsurAgent框架，整合感知、检索（采用RAG）、推理、行动和记忆五个模块，结合实证调查数据与LLM常识进行行为建模。

Result: InsurAgent能准确估计边际和双变量概率，利用LLM常识捕捉传统模型难以处理的情境信息，并通过记忆模块模拟个体决策随时间的演变。

Conclusion: InsurAgent为行为建模和政策分析提供了有效工具，显著提升了LLM在定量保险决策模拟中的实用性。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [102] [Re-FORC: Adaptive Reward Prediction for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.02130)
*Renos Zabounidis,Aditya Golatkar,Michael Kleinman,Alessandro Achille,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: Re-FORC 是一种自适应奖励预测方法，通过轻量级适配器预测未来推理步骤的收益，实现动态推理长度控制，在节省计算资源的同时提升或保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在生成推理链时通常采用固定长度，无法根据上下文动态调整计算资源，导致效率低下。因此需要一种能预测未来收益并据此优化推理长度的方法。

Method: Re-FORC 在推理模型上训练一个轻量级适配器，根据上下文预测不同推理长度（即未来思考 token 数量）下的预期奖励，并据此进行动态推理控制。

Result: Re-FORC 实现了：1）提前终止无望的推理链，节省26%计算量且保持准确率；2）在相同计算量下准确率提升4%，或在相同准确率下减少55%计算量；3）在高低计算预算下分别提升11%和7%的准确率。

Conclusion: Re-FORC 有效实现了推理过程中的动态长度控制与计算资源优化，在多种场景下显著提升了效率与性能。

Abstract: We propose Re-FORC, an adaptive reward prediction method that, given a
context, enables prediction of the expected future rewards as a function of the
number of future thinking tokens. Re-FORC trains a lightweight adapter on
reasoning models, demonstrating improved prediction with longer reasoning and
larger models. Re-FORC enables: 1) early stopping of unpromising reasoning
chains, reducing compute by 26% while maintaining accuracy, 2) optimized model
and thinking length selection that achieves 4% higher accuracy at equal compute
and 55% less compute at equal accuracy compared to the largest model, 3)
adaptive test-time scaling, which increases accuracy by 11% in high compute
regime, and 7% in low compute regime. Re-FORC allows dynamic reasoning with
length control via cost-per-token thresholds while estimating computation time
upfront.

</details>


### [103] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: 本文提出了一种名为ATHENA的自适应文本-符号人类中心推理框架，结合大语言模型与效用理论，通过群体层面的符号效用函数发现和个体层面的语义适配，显著提升了对个体高风险决策（如疫苗接种）的预测性能。


<details>
  <summary>Details</summary>
Motivation: 个体在高风险决策（如疫苗接种）中的行为常偏离群体最优预测，因其决策过程受数值属性与语言因素共同影响，现有模型难以有效整合这些信息。

Method: ATHENA框架包含两个阶段：首先利用大语言模型辅助发现群体层面的符号效用函数；其次在个体层面进行语义适配，构建由最优效用引导的个性化语义模板。

Result: 在真实世界的出行方式与疫苗选择任务中，ATHENA相较现有基于效用、机器学习及大语言模型的方法，F1分数至少提升6.5%；消融实验验证了两个阶段的必要性与互补性。

Conclusion: ATHENA通过有机融合符号效用建模与语义适配，为人类中心决策建模提供了一种新范式。

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [104] [Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration](https://arxiv.org/abs/2511.02200)
*Jingbo Wang,Sendong Zhao,Haochun Wang,Yuzheng Fan,Lizhe Zhang,Yan Liu,Ting Liu*

Main category: cs.AI

TL;DR: 本文提出STRMAC，一种面向多智能体系统的状态感知路由框架，通过自适应选择最优智能体并结合自进化数据生成方法，在提升协作效率的同时显著减少数据收集开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体系统受限于僵化的智能体调度和低效的协调策略，难以适应任务需求的动态变化。

Method: STRMAC框架分别编码交互历史和智能体知识，驱动路由器在每一步自适应选择最合适的单一智能体；同时引入自进化数据生成方法，高效收集高质量执行路径用于训练。

Result: 在多个协作推理基准测试中，该方法相比基线最高提升23.8%性能，并将数据收集开销降低最多90.1%。

Conclusion: STRMAC通过状态感知路由与高效数据生成机制，显著提升了多智能体系统的协作效率与性能，为复杂任务求解提供了有效解决方案。

Abstract: The emergence of multi-agent systems powered by large language models (LLMs)
has unlocked new frontiers in complex task-solving, enabling diverse agents to
integrate unique expertise, collaborate flexibly, and address challenges
unattainable for individual models. However, the full potential of such systems
is hindered by rigid agent scheduling and inefficient coordination strategies
that fail to adapt to evolving task requirements. In this paper, we propose
STRMAC, a state-aware routing framework designed for efficient collaboration in
multi-agent systems. Our method separately encodes interaction history and
agent knowledge to power the router, which adaptively selects the most suitable
single agent at each step for efficient and effective collaboration.
Furthermore, we introduce a self-evolving data generation approach that
accelerates the collection of high-quality execution paths for efficient system
training. Experiments on challenging collaborative reasoning benchmarks
demonstrate that our method achieves state-of-the-art performance, achieving up
to 23.8% improvement over baselines and reducing data collection overhead by up
to 90.1% compared to exhaustive search.

</details>


### [105] [Training Proactive and Personalized LLM Agents](https://arxiv.org/abs/2511.02208)
*Weiwei Sun,Xuhui Zhou,Weihua Du,Xingyao Wang,Sean Welleck,Graham Neubig,Maarten Sap,Yiming Yang*

Main category: cs.AI

TL;DR: 该论文提出UserVille环境和PPP多目标强化学习方法，同时优化生产力、主动性和个性化三个维度，显著提升智能体在真实交互中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注任务成功率，但实际应用中的智能体还需兼顾主动提问和个性化适应能力，因此需要一种能同时优化这三个维度的方法。

Method: 构建名为UserVille的交互环境，内含基于大语言模型的用户模拟器，并在此基础上提出PPP（Productivity, Proactivity, Personalization）多目标强化学习框架。

Result: 在软件工程和深度研究任务上的实验表明，PPP训练的智能体相比GPT-5等强基线平均提升21.6分，能主动提问、适应新用户偏好并提升任务成功率。

Conclusion: 显式优化以用户为中心的交互能力对构建实用高效的AI智能体至关重要。

Abstract: While existing work focuses primarily on task success, we argue that
effective real-world agents require optimizing three dimensions: productivity
(task completion), proactivity (asking essential questions), and
personalization (adapting to diverse user preferences). We introduce UserVille,
an interactive environment with LLM-based user simulators enabling diverse,
configurable user preferences. Leveraging UserVille, we introduce PPP, a
multi-objective reinforcement learning approach that jointly optimizes all
three dimensions: Productivity, Proactivity, and Personalization. Experiments
on software engineering and deep research tasks show that agents trained with
PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6
on average), demonstrating the ability to ask strategic clarifying questions,
adapt to unseen user preferences, and improve task success through better
interaction. This work demonstrates that explicitly optimizing for
user-centered interaction is critical for building practical and effective AI
agents.

</details>


### [106] [TabDSR: Decompose, Sanitize, and Reason for Complex Numerical Reasoning in Tabular Data](https://arxiv.org/abs/2511.02219)
*Changjiang Jiang,Fengchang Yu,Haihua Chen,Wei Lu,Jin Zeng*

Main category: cs.AI

TL;DR: 本文提出了一种名为\method的框架，用于提升大语言模型在复杂表格数值推理任务中的表现，包含查询分解器、表格清洗器和基于程序思维（PoT）的推理器，并构建了新数据集CalTab151以确保无偏评估。实验表明该方法在多个基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理复杂表格数据时表现不佳，主要受限于复杂查询、噪声数据和有限的数值计算能力。

Method: 提出\method框架，包含三个组件：(1) 查询分解器，将复杂问题拆解；(2) 表格清洗器，清理和过滤噪声表格；(3) 基于程序思维（PoT）的推理器，生成可执行代码从清洗后的表格中得出答案。同时构建新数据集CalTab151用于无偏评估。

Result: 在TAT-QA、TableBench和\method数据集上分别实现了8.79%、6.08%和19.87%的准确率提升，达到SOTA性能，并能与主流大语言模型无缝集成。

Conclusion: \method框架有效提升了大语言模型在复杂表格数值推理任务中的性能，为实际应用提供了鲁棒解决方案。

Abstract: Complex reasoning over tabular data is crucial in real-world data analysis,
yet large language models (LLMs) often underperform due to complex queries,
noisy data, and limited numerical capabilities. To address these issues, we
propose \method, a framework consisting of: (1) a query decomposer that breaks
down complex questions, (2) a table sanitizer that cleans and filters noisy
tables, and (3) a program-of-thoughts (PoT)-based reasoner that generates
executable code to derive the final answer from the sanitized table. To ensure
unbiased evaluation and mitigate data leakage, we introduce a new dataset,
CalTab151, specifically designed for complex numerical reasoning over tables.
Experimental results demonstrate that \method consistently outperforms existing
methods, achieving state-of-the-art (SOTA) performance with 8.79%, 6.08%, and
19.87% accuracy improvement on TAT-QA, TableBench, and \method, respectively.
Moreover, our framework integrates seamlessly with mainstream LLMs, providing a
robust solution for complex tabular numerical reasoning. These findings
highlight the effectiveness of our framework in enhancing LLM performance for
complex tabular numerical reasoning. Data and code are available upon request.

</details>


### [107] [When Modalities Conflict: How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs](https://arxiv.org/abs/2511.02243)
*Zhuoran Zhang,Tengyue Wang,Xilin Gong,Yang Shi,Haotian Wang,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: 本文提出一个新框架，将多模态大语言模型（MLLM）在模态冲突中的行为分解为两个核心因素：相对推理不确定性与固有模态偏好，并通过可控数据集和熵度量验证了“模态遵循概率随其相对不确定性单调下降”的普适规律，同时揭示了模型在平衡点附近层间振荡的内部机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅用粗粒度的数据集级统计量衡量MLLM在模态冲突中的行为，忽略了模型在单模态推理中的置信度影响，缺乏对模态遵循机制的细粒度理解。

Method: 构建可控制视觉与文本输入推理难度的可控数据集，使用熵作为细粒度不确定性度量，分析模态遵循行为；并通过逐层预测探查模型内部决策机制。

Result: 发现模态遵循概率随该模态相对不确定性增加而单调下降；在“平衡点”处可有效衡量模型固有模态偏好；并观察到模型在平衡点附近存在跨层模态振荡现象。

Conclusion: 相对推理不确定性和固有模态偏好是支配MLLM模态遵循行为的两个基本原则，该框架为理解多模态冲突解决提供了定量方法与机制解释。

Abstract: Multimodal large language models (MLLMs) must resolve conflicts when
different modalities provide contradictory information, a process we term
modality following. Prior work measured this behavior only with coarse
dataset-level statistics, overlooking the influence of model's confidence in
unimodal reasoning. In this paper, we introduce a new framework that decomposes
modality following into two fundamental factors: relative reasoning uncertainty
(the case-specific confidence gap between unimodal predictions) and inherent
modality preference( a model's stable bias when uncertainties are balanced). To
validate this framework, we construct a controllable dataset that
systematically varies the reasoning difficulty of visual and textual inputs.
Using entropy as a fine-grained uncertainty metric, we uncover a universal law:
the probability of following a modality decreases monotonically as its relative
uncertainty increases. At the relative difficulty level where the model tends
to follow both modalities with comparable probability what we call the balance
point, a practical indicator of the model's inherent preference. Unlike
traditional macro-level ratios, this measure offers a more principled and less
confounded way to characterize modality bias, disentangling it from unimodal
capabilities and dataset artifacts. Further, by probing layer-wise predictions,
we reveal the internal mechanism of oscillation: in ambiguous regions near the
balance point, models vacillate between modalities across layers, explaining
externally observed indecision. Together, these findings establish relative
uncertainty and inherent preference as the two governing principles of modality
following, offering both a quantitative framework and mechanistic insight into
how MLLMs resolve conflicting information.

</details>


### [108] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 本文针对多智能体推理中出现的“懒惰智能体”问题，提出了一种基于因果影响度量和可验证奖励机制的新方法，有效提升了多智能体协作在复杂推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习与可验证奖励的多智能体推理框架中，存在一个智能体主导而另一个贡献甚微的“懒惰行为”，导致协作失效，退化为低效的单智能体系统。

Method: 首先对懒惰行为进行理论分析；其次引入一种稳定高效的因果影响度量方法以缓解该问题；最后设计一种可验证奖励机制，允许推理智能体在必要时丢弃噪声输出、整合指令并重启推理过程。

Result: 大量实验表明，所提框架有效缓解了懒惰智能体行为，充分释放了多智能体系统在复杂推理任务中的潜力。

Conclusion: 通过因果影响度量与可验证奖励机制的结合，本文成功提升了多智能体协作推理的效率与鲁棒性，为复杂任务提供了更有效的解决方案。

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [109] [Chronic Kidney Disease Prognosis Prediction Using Transformer](https://arxiv.org/abs/2511.02340)
*Yohan Lee,DongGyun Kang,SeHoon Park,Sa-Yoon Park,Kwangsoo Kim*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer的多模态电子健康记录预测框架ProQ-BERT，用于慢性肾病（CKD）进展预测，在91,816名患者数据上表现优异，短期预测ROC-AUC高达0.995。


<details>
  <summary>Details</summary>
Motivation: 慢性肾病（CKD）影响全球近10%人口，常进展至终末期肾衰竭，准确的预后预测对及时干预和资源优化至关重要。

Method: 采用基于Transformer的ProQ-BERT框架，整合人口统计、临床和实验室数据，对连续实验室值使用量化分词，并结合注意力机制提升可解释性；模型通过掩码语言建模预训练，并针对从CKD 3a期到5期的二分类任务进行微调。

Result: 在首尔国立大学医院OMOP CDM数据集上评估，ProQ-BERT在短期预测中ROC-AUC达0.995、PR-AUC达0.989，优于CEHR-BERT。

Conclusion: Transformer架构结合时序设计在临床预后建模中效果显著，为个性化CKD诊疗提供了有前景的方向。

Abstract: Chronic Kidney Disease (CKD) affects nearly 10\% of the global population and
often progresses to end-stage renal failure. Accurate prognosis prediction is
vital for timely interventions and resource optimization. We present a
transformer-based framework for predicting CKD progression using multi-modal
electronic health records (EHR) from the Seoul National University Hospital
OMOP Common Data Model. Our approach (\textbf{ProQ-BERT}) integrates
demographic, clinical, and laboratory data, employing quantization-based
tokenization for continuous lab values and attention mechanisms for
interpretability. The model was pretrained with masked language modeling and
fine-tuned for binary classification tasks predicting progression from stage 3a
to stage 5 across varying follow-up and assessment periods. Evaluated on a
cohort of 91,816 patients, our model consistently outperformed CEHR-BERT,
achieving ROC-AUC up to 0.995 and PR-AUC up to 0.989 for short-term prediction.
These results highlight the effectiveness of transformer architectures and
temporal design choices in clinical prognosis modeling, offering a promising
direction for personalized CKD care.

</details>


### [110] [Fuzzy Soft Set Theory based Expert System for the Risk Assessment in Breast Cancer Patients](https://arxiv.org/abs/2511.02392)
*Muhammad Sheharyar Liaqat*

Main category: cs.AI

TL;DR: 本文提出了一种基于模糊软集理论的专家系统，利用BMI、胰岛素、瘦素、脂联素水平和年龄等临床参数，通过模糊推理规则评估乳腺癌风险，以辅助医生识别高危患者。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性主要死因之一，早期诊断对提高生存率至关重要，但因疾病复杂性和患者风险因素差异，及时检测仍具挑战性。

Method: 采用模糊软集理论构建专家系统，将BMI、胰岛素、瘦素、脂联素水平及年龄作为输入变量，结合模糊推理规则与软集计算进行风险评估，并使用UCI机器学习库的数据集进行模型开发与验证。

Result: 该系统能通过常规血液检测获取的参数实现非侵入性、易获取的乳腺癌初步风险评估，有助于识别高风险患者。

Conclusion: 所提出的专家系统可有效支持医疗专业人员判断是否需进一步进行活检等诊断程序，提升乳腺癌早期筛查效率。

Abstract: Breast cancer remains one of the leading causes of mortality among women
worldwide, with early diagnosis being critical for effective treatment and
improved survival rates. However, timely detection continues to be a challenge
due to the complex nature of the disease and variability in patient risk
factors. This study presents a fuzzy soft set theory-based expert system
designed to assess the risk of breast cancer in patients using measurable
clinical and physiological parameters. The proposed system integrates Body Mass
Index, Insulin Level, Leptin Level, Adiponectin Level, and age as input
variables to estimate breast cancer risk through a set of fuzzy inference rules
and soft set computations. These parameters can be obtained from routine blood
analyses, enabling a non-invasive and accessible method for preliminary
assessment. The dataset used for model development and validation was obtained
from the UCI Machine Learning Repository. The proposed expert system aims to
support healthcare professionals in identifying high-risk patients and
determining the necessity of further diagnostic procedures such as biopsies.

</details>


### [111] [A New Perspective on Precision and Recall for Generative Models](https://arxiv.org/abs/2511.02414)
*Benjamin Sykes,Loïc Simon,Julien Rabin,Jalal Fadili*

Main category: cs.AI

TL;DR: 本文提出了一种基于二分类视角的新框架，用于估计生成模型的完整精确率-召回率（PR）曲线，并提供了相应的统计分析和极小极大上界，同时展示了该框架如何推广现有仅限于曲线端点的PR度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型评估方法多依赖标量指标，而PR曲线虽能提供更丰富的分析，但其估计面临诸多挑战，因此需要一个更系统、理论支持更强的PR曲线估计框架。

Method: 基于二分类视角构建PR曲线估计框架，进行严格的统计分析，并推导PR估计风险的极小极大上界。

Result: 所提框架不仅理论上统一并扩展了现有PR度量方法，而且在多种实验设置下展示了不同PR曲线的行为特征。

Conclusion: 该框架为生成模型的PR曲线估计提供了坚实的统计基础，并揭示了现有方法的局限性，推动了生成模型评估的研究。

Abstract: With the recent success of generative models in image and text, the question
of their evaluation has recently gained a lot of attention. While most methods
from the state of the art rely on scalar metrics, the introduction of Precision
and Recall (PR) for generative model has opened up a new avenue of research.
The associated PR curve allows for a richer analysis, but their estimation
poses several challenges. In this paper, we present a new framework for
estimating entire PR curves based on a binary classification standpoint. We
conduct a thorough statistical analysis of the proposed estimates. As a
byproduct, we obtain a minimax upper bound on the PR estimation risk. We also
show that our framework extends several landmark PR metrics of the literature
which by design are restrained to the extreme values of the curve. Finally, we
study the different behaviors of the curves obtained experimentally in various
settings.

</details>


### [112] [Auditable-choice reframing unlocks RL-based verification for open-ended tasks](https://arxiv.org/abs/2511.02463)
*Mengyu Zhang,Xubo Liu,Siyu Ding,Weichong Yin,Yu Sun,Hua Wu,Wenya Guo,Ying Zhang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为可验证多选重构（VMR）的新训练策略，将开放式任务数据重构为可验证的多选格式，从而将强化学习与可验证奖励（RLVR）方法成功应用于缺乏标准答案的开放式任务，显著提升了大语言模型在这些任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将缺乏标准答案的开放式任务（如创意写作和指令遵循）视为非推理场景，忽视了其中潜在的推理能力价值。作者旨在探索强化推理能力是否能提升模型在开放式任务中的表现，并解决RLVR方法因依赖标准答案而无法直接应用于开放式任务的问题。

Method: 作者提出Verifiable Multiple-Choice Reformulation（VMR）方法，将开放式任务的数据重构为可验证的多选题形式，使得即使没有显式标准答案，也能利用RLVR范式进行有效训练。

Result: 在八个开放式基准测试中，基于VMR的训练方法相比基线平均提升了5.99分，验证了该方法在提升大语言模型开放式任务性能方面的有效性。

Conclusion: 通过VMR策略，RLVR范式可成功迁移至开放式任务领域，强化推理能力确实有助于提升模型在无标准答案任务中的表现，为开放式任务的模型训练提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated great
potential in enhancing the reasoning capabilities of large language models
(LLMs), achieving remarkable progress in domains such as mathematics and
programming where standard answers are available. However, for open-ended tasks
lacking ground-truth solutions (e.g., creative writing and instruction
following), existing studies typically regard them as non-reasoning scenarios,
thereby overlooking the latent value of reasoning capabilities. This raises a
key question: Can strengthening reasoning improve performance in open-ended
tasks? To address this, we explore the transfer of the RLVR paradigm to the
open domain. Yet, since RLVR fundamentally relies on verifiers that presuppose
the existence of standard answers, it cannot be directly applied to open-ended
tasks. To overcome this challenge, we introduce Verifiable Multiple-Choice
Reformulation (VMR), a novel training strategy that restructures open-ended
data into verifiable multiple-choice formats, enabling effective training even
in the absence of explicit ground truth. Experimental results on multiple
benchmarks validate the effectiveness of our method in improving LLM
performance on open-ended tasks. Notably, across eight open-ended benchmarks,
our VMR-based training delivers an average gain of 5.99 points over the
baseline. Code will be released upon acceptance to facilitate reproducibility.

</details>


### [113] [Knowledge Graph-enhanced Large Language Model for Incremental Game PlayTesting](https://arxiv.org/abs/2511.02534)
*Enhong Mu,Jinyu Cai,Yijun Lu,Mingyue Zhang,Kenji Tei,Jialong Li*

Main category: cs.AI

TL;DR: 本文提出KLPEG框架，结合知识图谱与大语言模型，通过解析游戏更新日志并利用多跳推理生成针对性测试用例，在Overcooked和Minecraft中验证了其在提升自动化游戏测试效率与准确性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现代电子游戏频繁更新，现有基于大语言模型的自动化测试方法缺乏结构化的知识积累机制，难以针对增量更新进行高效精准测试。

Method: 构建并维护一个知识图谱（KG）以建模游戏元素、任务依赖和因果关系；利用大语言模型解析自然语言更新日志，并在KG上进行多跳推理，识别更新影响范围，生成针对性测试用例。

Result: 在Overcooked和Minecraft两个游戏环境中，KLPEG能更准确地定位受更新影响的功能，并以更少步骤完成测试，显著提升测试效果与效率。

Conclusion: KLPEG框架通过融合知识图谱与大语言模型，实现了面向游戏增量更新的高效、精准自动化测试，为游戏测试领域提供了新思路。

Abstract: The rapid iteration and frequent updates of modern video games pose
significant challenges to the efficiency and specificity of testing. Although
automated playtesting methods based on Large Language Models (LLMs) have shown
promise, they often lack structured knowledge accumulation mechanisms, making
it difficult to conduct precise and efficient testing tailored for incremental
game updates. To address this challenge, this paper proposes a KLPEG framework.
The framework constructs and maintains a Knowledge Graph (KG) to systematically
model game elements, task dependencies, and causal relationships, enabling
knowledge accumulation and reuse across versions. Building on this foundation,
the framework utilizes LLMs to parse natural language update logs, identify the
scope of impact through multi-hop reasoning on the KG, enabling the generation
of update-tailored test cases. Experiments in two representative game
environments, Overcooked and Minecraft, demonstrate that KLPEG can more
accurately locate functionalities affected by updates and complete tests in
fewer steps, significantly improving both playtesting effectiveness and
efficiency.

</details>


### [114] [The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large Language Models](https://arxiv.org/abs/2511.02589)
*Claudia Herambourg,Dawid Siuda,Anna Szczepanek,Julia Kopczyńska,Joao R. L. Santos,Wojciech Sas,Joanna Śmietańska-Nowak*

Main category: cs.AI

TL;DR: ORCA是一个评估大语言模型在多领域真实场景中定量推理能力的新基准，涵盖金融、物理、健康和统计等500个任务，发现主流模型准确率仅为45%–63%，主要错误为舍入和计算错误，模型在数学和工程表现较好，但在物理和自然科学较弱，且错误类型存在互补性。


<details>
  <summary>Details</summary>
Motivation: 现有数学数据集无法全面评估大语言模型在真实多领域问题中的定量推理能力，因此需要一个能测试逐步推理、数值精度和跨领域泛化能力的新基准。

Method: 构建ORCA基准，包含500个来自金融、物理、健康和统计等领域的真实自然语言任务，使用Omni计算器引擎验证输出，评估五个主流大语言模型的表现，并分析其错误类型与相关性。

Result: 五个顶尖模型（ChatGPT-5、Gemini 2.5 Flash、Claude Sonnet 4.5、Grok 4、DeepSeek V3.2）在ORCA上的准确率为45%–63%；主要错误为舍入（35%）和计算错误（33%）；模型在数学和工程领域表现强，在物理和自然科学领域表现弱；模型间错误相关性中等（r≈0.40–0.65），显示部分互补性。

Conclusion: ORCA揭示了当前大语言模型在真实世界定量推理中的局限性，强调需提升数值精度与跨学科推理能力，同时表明模型间存在互补潜力，可为未来模型融合与改进提供方向。

Abstract: We present ORCA (Omni Research on Calculation in AI) Benchmark -- a novel
benchmark that evaluates large language models (LLMs) on multi-domain,
real-life quantitative reasoning using verified outputs from Omni's calculator
engine. In 500 natural-language tasks across domains such as finance, physics,
health, and statistics, the five state-of-the-art systems (ChatGPT-5,
Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only
$45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$)
and calculation mistakes ($33\,\%$). Results in specific domains indicate
strengths in mathematics and engineering, but weaknesses in physics and natural
sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the
models often fail together but differ in the types of errors they make,
highlighting their partial complementarity rather than redundancy. Unlike
standard math datasets, ORCA evaluates step-by-step reasoning, numerical
precision, and domain generalization across real problems from finance,
physics, health, and statistics.

</details>


### [115] [Adaptive GR(1) Specification Repair for Liveness-Preserving Shielding in Reinforcement Learning](https://arxiv.org/abs/2511.02605)
*Tiberiu-Andrei Georgescu,Alexander W. Goodall,Dalal Alrajeh,Francesco Belardinelli,Sebastian Uchitel*

Main category: cs.AI

TL;DR: 本文提出首个基于GR(1)规范的自适应屏蔽框架，通过运行时检测环境假设违规并利用归纳逻辑编程（ILP）在线修复规范，使强化学习智能体在保持逻辑合规的同时实现接近最优的奖励。


<details>
  <summary>Details</summary>
Motivation: 传统静态屏蔽方法依赖固定逻辑规范和手工抽象，在环境假设被违反时无法适应，导致安全性或性能下降。因此，需要一种能动态调整规范以应对环境变化的自适应屏蔽机制。

Method: 提出一种基于GR(1)规范的自适应屏蔽框架，在运行时检测环境假设违反，并使用归纳逻辑编程（ILP）在线自动修复GR(1)规范，以系统且可解释的方式弱化目标，确保活性可达成。

Result: 在Minepump和Atari Seaquest两个案例中验证了方法的有效性：静态符号控制器在优化辅助奖励时常表现次优，而配备自适应屏蔽的RL智能体在保持完美逻辑合规的同时实现了接近最优的奖励。

Conclusion: 自适应屏蔽框架能够有效应对环境变化，在保证安全性和逻辑合规的前提下提升强化学习智能体的性能，优于传统静态屏蔽方法。

Abstract: Shielding is widely used to enforce safety in reinforcement learning (RL),
ensuring that an agent's actions remain compliant with formal specifications.
Classical shielding approaches, however, are often static, in the sense that
they assume fixed logical specifications and hand-crafted abstractions. While
these static shields provide safety under nominal assumptions, they fail to
adapt when environment assumptions are violated. In this paper, we develop the
first adaptive shielding framework - to the best of our knowledge - based on
Generalized Reactivity of rank 1 (GR(1)) specifications, a tractable and
expressive fragment of Linear Temporal Logic (LTL) that captures both safety
and liveness properties. Our method detects environment assumption violations
at runtime and employs Inductive Logic Programming (ILP) to automatically
repair GR(1) specifications online, in a systematic and interpretable way. This
ensures that the shield evolves gracefully, ensuring liveness is achievable and
weakening goals only when necessary. We consider two case studies: Minepump and
Atari Seaquest; showing that (i) static symbolic controllers are often severely
suboptimal when optimizing for auxiliary rewards, and (ii) RL agents equipped
with our adaptive shield maintain near-optimal reward and perfect logical
compliance compared with static shields.

</details>


### [116] [A Multi-Agent Psychological Simulation System for Human Behavior Modeling](https://arxiv.org/abs/2511.02606)
*Xiangen Hu,Jiarui Tong,Sheng Xu*

Main category: cs.AI

TL;DR: 该论文提出了一种基于心理学理论的多智能体心理模拟系统，通过模拟人类内部认知与情感过程来生成可信的人类行为，用于教育和培训领域。


<details>
  <summary>Details</summary>
Motivation: 在以人为本的培训与教育中，需要真实的实践环境，但目前对人类行为的逼真模拟仍有限，因此亟需一种更符合人类心理机制的模拟方法。

Method: 构建一个多智能体系统，以自我效能、思维模式和社会建构主义等心理学理论为基础，通过模拟代表关键心理因素的“内在议会”智能体之间的协商互动，生成外显行为。

Result: 该系统在教师培训和研究中得到应用，展现出高度的透明性和与人类心理的一致性，并体现了社会学习、认知学徒制、刻意练习和元认知等教育原则。

Conclusion: 该多智能体心理模拟系统为人类行为建模提供了新范式，具有在教育、培训和心理学研究中的广泛应用潜力。

Abstract: Training and education in human-centered fields require authentic practice,
yet realistic simulations of human behavior have remained limited. We present a
multi-agent psychological simulation system that models internal
cognitive-affective processes to generate believable human behaviors. In
contrast to black-box neural models, this system is grounded in established
psychological theories (e.g., self-efficacy, mindset, social constructivism)
and explicitly simulates an ``inner parliament'' of agents corresponding to key
psychological factors. These agents deliberate and interact to determine the
system's output behavior, enabling unprecedented transparency and alignment
with human psychology. We describe the system's architecture and theoretical
foundations, illustrate its use in teacher training and research, and discuss
how it embodies principles of social learning, cognitive apprenticeship,
deliberate practice, and meta-cognition.

</details>


### [117] [DecompSR: A dataset for decomposed analyses of compositional multihop spatial reasoning](https://arxiv.org/abs/2511.02627)
*Lachlan McPheat,Navdeep Kaur,Robert Blackwell,Alessandra Russo,Anthony G. Cohn,Pranava Madhyastha*

Main category: cs.AI

TL;DR: 本文提出了DecompSR，一个包含超过500万数据点的大规模基准数据集和生成框架，用于分析大语言模型在组合式空间推理方面的能力，并发现模型在生成性和系统性泛化方面表现较差，但在语言变化方面较为稳健。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对组合式空间推理能力进行细粒度、可控评估的基准，因此需要一个可独立调节组合性多个维度（如生成性、可替换性、过度泛化和系统性）且能保证正确性的数据集。

Method: 通过程序化方式构建DecompSR数据集，使其“构造即正确”，并使用符号求解器独立验证其正确性；同时允许用户独立调节组合性的多个方面，并在多种大语言模型上进行全面基准测试。

Result: 实验表明，大语言模型在空间推理任务中难以实现生成性和系统性泛化，但对语言变异具有较强鲁棒性。

Conclusion: DecompSR提供了一个可证明正确、能精细调控组合性维度的严谨基准，有助于深入探测大语言模型的组合推理能力。

Abstract: We introduce DecompSR, decomposed spatial reasoning, a large benchmark
dataset (over 5m datapoints) and generation framework designed to analyse
compositional spatial reasoning ability. The generation of DecompSR allows
users to independently vary several aspects of compositionality, namely:
productivity (reasoning depth), substitutivity (entity and linguistic
variability), overgeneralisation (input order, distractors) and systematicity
(novel linguistic elements). DecompSR is built procedurally in a manner which
makes it is correct by construction, which is independently verified using a
symbolic solver to guarantee the correctness of the dataset. DecompSR is
comprehensively benchmarked across a host of Large Language Models (LLMs) where
we show that LLMs struggle with productive and systematic generalisation in
spatial reasoning tasks whereas they are more robust to linguistic variation.
DecompSR provides a provably correct and rigorous benchmarking dataset with a
novel ability to independently vary the degrees of several key aspects of
compositionality, allowing for robust and fine-grained probing of the
compositional reasoning abilities of LLMs.

</details>


### [118] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu,Cheng Qian,Zhaochen Su,Qing Zong,Shijue Huang,Bingxiang He,Yi R. Fung*

Main category: cs.AI

TL;DR: 本文提出了CostBench，一个专注于成本效率与动态适应能力的LLM智能体评测基准，揭示了当前模型在经济理性规划方面的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有对大语言模型（LLM）智能体的评估主要关注任务完成率，忽视了资源效率和环境变化下的适应能力，尤其是制定和调整成本最优计划的能力。

Method: 作者构建了CostBench基准，以旅行规划为场景，包含多种具有可定制成本的原子与复合工具组合，并引入四类动态阻断事件（如工具失效、成本变动）来模拟现实不确定性，评估智能体的经济推理与重规划能力。

Result: 在CostBench上的评估显示，包括GPT-5在内的主流模型在静态环境中难以找到成本最优解（最难任务精确匹配率低于75%），在动态条件下性能进一步下降约40%。

Conclusion: CostBench揭示了当前LLM智能体在成本感知规划方面的关键缺陷，为未来开发兼具经济理性与鲁棒性的智能体提供了基础。

Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [119] [Using Span Queries to Optimize for Cache and Attention Locality](https://arxiv.org/abs/2511.02749)
*Paul Castro,Nick Mitchell,Nathan Ordonez,Thomas Parnell,Mudhakar Srivatsa,Antoni Viros i Martin*

Main category: cs.AI

TL;DR: 本文提出“跨度查询”（span query）作为通用接口，统一表达聊天、RAG、推理时扩展和智能体等多种推理负载，并通过交换性约束优化KV缓存局部性和注意力机制，显著降低首Token延迟（TTFT）并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 当前推理服务器高度优化于聊天补全任务，难以高效支持日益多样的非聊天推理场景（如RAG、推理时扩展等），而现有方案仅针对单一用例（如RAG）优化，缺乏通用性。

Method: 引入“跨度查询”这一表达形式，将其定义为带有交换性约束的推理调用表达式树；设计其语法与语义，并在vLLM中实现（仅修改492行代码），以支持自动优化KV缓存局部性和注意力局部性。

Result: 在两个非聊天用例中实现10–20倍的TTFT降低；通过注意力优化的跨度查询，2B参数模型的准确率显著超越未优化的8B模型。

Conclusion: 跨度查询提供了一种通用、高效的推理服务器接口，能够统一多种推理负载并通过交换性约束实现性能与准确率的双重提升。

Abstract: Clients are evolving beyond chat completion, and now include a variety of
innovative inference-time scaling and deep reasoning techniques. At the same
time, inference servers remain heavily optimized for chat completion. Prior
work has shown that large improvements to KV cache hit rate are possible if
inference servers evolve towards these non-chat use cases. However, they offer
solutions that are also optimized for a single use case, RAG. In this paper, we
introduce the span query to generalize the interface to the inference server.
We demonstrate that chat, RAG, inference-time scaling, and agentic workloads
can all be expressed as span queries. We show how the critical distinction that
had been assumed by prior work lies in whether the order of the inputs matter
-- do they commute? In chat, they do not. In RAG, they often do. This paper
introduces span queries, which are expression trees of inference calls, linked
together with commutativity constraints. We describe span query syntax and
semantics. We show how they can be automatically optimized to improve KV cache
locality. We show how a small change to vLLM (affecting only 492 lines) can
enable high-performance execution of span queries. Using this stack, we
demonstrate that span queries can achieve 10-20x reductions in TTFT for two
distinct non-chat use cases. Finally, we show that span queries can also be
optimized to improve attention locality, so as to avoid the so-called
lost-in-the-middle problem. We demonstrate that an attention-optimized span
query on a 2b parameter model vastly outperforms the accuracy of a stock
inference server using an 8b model.

</details>


### [120] [LLM-Supported Formal Knowledge Representation for Enhancing Control Engineering Content with an Interactive Semantic Layer](https://arxiv.org/abs/2511.02759)
*Julius Fiedler,Carsten Knoll,Klaus Röbenack*

Main category: cs.AI

TL;DR: 本文提出一种结合大语言模型与PyIRK框架的半自动化方法，将控制工程领域的自然语言和LaTeX数学表达转化为兼具人类可读性与机器可解释性的形式化知识图谱，并构建“交互式语义层”以增强知识传递。


<details>
  <summary>Details</summary>
Motivation: 控制工程领域研究产出快速增长，亟需新方法来结构化和形式化领域知识，以提升知识的可访问性、协作性和可验证性。

Method: 基于PyIRK（Imperative Representation of Knowledge）框架，利用大语言模型辅助将自然语言描述和LaTeX格式的数学定义半自动地转换为形式化知识图谱。

Result: 成功生成了一个“交互式语义层”，用于增强原始文档，促进知识转移，并初步实现了面向控制工程的形式化知识表示。

Conclusion: 该方法有助于实现控制工程领域中易于访问、协作且可验证的知识库愿景。

Abstract: The rapid growth of research output in control engineering calls for new
approaches to structure and formalize domain knowledge. This paper briefly
describes an LLM-supported method for semi-automated generation of formal
knowledge representations that combine human readability with machine
interpretability and increased expressiveness. Based on the Imperative
Representation of Knowledge (PyIRK) framework, we demonstrate how language
models can assist in transforming natural-language descriptions and
mathematical definitions (available as LaTeX source code) into a formalized
knowledge graph. As a first application we present the generation of an
``interactive semantic layer'' to enhance the source documents in order to
facilitate knowledge transfer. From our perspective this contributes to the
vision of easily accessible, collaborative, and verifiable knowledge bases for
the control engineering domain.

</details>


### [121] [When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning](https://arxiv.org/abs/2511.02794)
*Chenyu Zhang,Minsol Kim,Shohreh Ghorbani,Jingyao Wu,Rosalind Picard,Patricia Maes,Paul Pu Liang*

Main category: cs.AI

TL;DR: 本文提出“模态破坏”这一诊断性失效模式，并设计了一个轻量级、与模型无关的评估层，将各模态视为智能体，通过自评估和融合机制识别对预测起正面作用的“贡献者”和导致错误的“破坏者”，从而揭示多模态大语言模型推理过程中的模态交互机制。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLMs）的推理过程缺乏透明度，难以判断是哪种模态主导了预测、模态间冲突如何解决，以及何时某一模态会压倒其他模态。为深入理解这些融合动态，作者提出一种诊断框架。

Method: 作者引入“模态破坏”概念，并构建一个轻量级、模型无关的评估层：将每个模态视为一个智能体，输出候选标签和简短自评估；通过简单融合机制聚合结果，识别“贡献者”（支持正确结果的模态）和“破坏者”（误导结果的模态）。

Result: 在多模态情感识别基准上的案例研究显示，该方法能揭示基础模型在不同模态上的系统性可靠性特征，有助于区分失败源于数据集伪影还是模型自身局限。

Conclusion: 该框架为多模态推理提供了一个诊断性脚手架，支持对融合机制进行原则性审计，并为后续干预提供依据。

Abstract: Despite rapid growth in multimodal large language models (MLLMs), their
reasoning traces remain opaque: it is often unclear which modality drives a
prediction, how conflicts are resolved, or when one stream dominates. In this
paper, we introduce modality sabotage, a diagnostic failure mode in which a
high-confidence unimodal error overrides other evidence and misleads the fused
result. To analyze such dynamics, we propose a lightweight, model-agnostic
evaluation layer that treats each modality as an agent, producing candidate
labels and a brief self-assessment used for auditing. A simple fusion mechanism
aggregates these outputs, exposing contributors (modalities supporting correct
outcomes) and saboteurs (modalities that mislead). Applying our diagnostic
layer in a case study on multimodal emotion recognition benchmarks with
foundation models revealed systematic reliability profiles, providing insight
into whether failures may arise from dataset artifacts or model limitations.
More broadly, our framework offers a diagnostic scaffold for multimodal
reasoning, supporting principled auditing of fusion dynamics and informing
possible interventions.

</details>


### [122] [Optimizing AI Agent Attacks With Synthetic Data](https://arxiv.org/abs/2511.02823)
*Chloe Loughridge,Paul Colognese,Avery Griffin,Tyler Tracy,Jon Kutasov,Joe Benton*

Main category: cs.AI

TL;DR: 本文提出一种在数据稀缺的复杂智能体环境中优化攻击策略的方法，通过将攻击能力分解为五个技能模块，并结合基于概率模型的仿真优化，显著提升了攻击强度，使安全评分从0.87降至0.41。


<details>
  <summary>Details</summary>
Motivation: 随着AI部署日益复杂且风险增高，亟需有效评估其风险。AI控制是一种评估框架，但其效果依赖于强大的攻击策略。然而，在计算资源受限、数据稀缺的复杂智能体环境中，获取强攻击策略具有挑战性。

Method: 作者在SHADE-Arena数据集上，将攻击能力分解为五个组成部分：怀疑建模、攻击选择、计划合成、执行和隐蔽性，并分别优化。为克服数据有限的问题，构建攻击动态的概率模型，在仿真中优化超参数，再将结果迁移到真实环境。

Result: 该方法显著增强了攻击强度，使用其框架后，安全评分从基线的0.87下降至0.41。

Conclusion: 通过模块化分解与基于仿真的优化策略，可在数据稀缺条件下有效提升攻击策略性能，从而更准确地评估AI系统的控制风险。

Abstract: As AI deployments become more complex and high-stakes, it becomes
increasingly important to be able to estimate their risk. AI control is one
framework for doing so. However, good control evaluations require eliciting
strong attack policies. This can be challenging in complex agentic environments
where compute constraints leave us data-poor. In this work, we show how to
optimize attack policies in SHADE-Arena, a dataset of diverse realistic control
environments. We do this by decomposing attack capability into five constituent
skills -- suspicion modeling, attack selection, plan synthesis, execution and
subtlety -- and optimizing each component individually. To get around the
constraint of limited data, we develop a probabilistic model of attack
dynamics, optimize our attack hyperparameters using this simulation, and then
show that the results transfer to SHADE-Arena. This results in a substantial
improvement in attack strength, reducing safety score from a baseline of 0.87
to 0.41 using our scaffold.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [123] [Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live](https://arxiv.org/abs/2511.02230)
*Hanchen Li,Qiuyang Mang,Runyuan He,Qizheng Zhang,Huanzhi Mao,Xiaokun Chen,Alvin Cheung,Joseph Gonzalez,Ion Stoica*

Main category: cs.OS

TL;DR: Continuum 是一种针对多轮智能体工作负载的 LLM 推理服务系统，通过工具调用感知的 KV 缓存超时机制与程序级调度，显著降低作业完成时间并提升吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 服务系统在处理包含工具调用的多轮智能体应用时，因工具调用造成的流程中断导致 KV 缓存频繁驱逐和请求间等待时间增加，从而严重影响延迟和吞吐；现有方法要么忽略工具信息造成重复计算，要么忽视多轮程序连续性，难以高效支持复杂智能体工作流。

Method: Continuum 结合工具调用持续时间预测，为 KV 缓存设置基于轮次总数的生存时间（TTL）以选择性地将其保留在 GPU 内存中，并采用程序级先到先服务调度策略，避免调度空泡、保持多轮连续性。

Result: 在 SWE-Bench 和 BFCL 等真实智能体工作负载上使用 Llama-3.1 8B/70B 模型的实验表明，Continuum 显著缩短了平均作业完成时间，并在不同硬件配置和 DRAM 卸载方案下均保持高性能。

Conclusion: Continuum 通过建模工具调用变异性与智能体程序连续性，有效优化了多轮智能体工作负载的服务效率，优于当前最先进的基线系统。

Abstract: Agentic LLM applications interleave LLM generation requests with tool calls.
These tool calls break the continuity of the workflow by creating pauses
between LLM requests, bringing many challenges for the serving system,
especially under multi-turn scenarios. Each pause potentially causes KV cache
eviction and extra waiting time before entering the continuous batch for the
following LLM request. Since these pauses happen for each call, this problem
becomes increasingly severe as turn number grow for agentic programs. Previous
works either fail to incorporate information from the tool call, evicting KV
cache that leads to repetitive prefill or loading, or ignore the continuity of
a multi-turn program, creating waiting time between turns that increases
per-request latency.
  We present Continuum, a serving system to optimize job completion time for
multi-turn agent workloads by combining tool-aware KV cache timeout with
program-level scheduling. By predicting tool call durations in agentic
workflows, Continuum selectively pins the KV cache in GPU memory with a
time-to-live value based on total turn number. When combined with program-level
first-come-first-serve, Continuum prevents scheduling bubbles, preserves
multi-turn continuity, and optimizes for throughput for complex agentic
workflows. By modeling the variability of tool call and agent program
continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on
real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models
shows that Continuum significantly improves the average job completion times,
and remains performant across different hardware setups and DRAM offloading
schemes. Preview code is available at:
https://github.com/Hanchenli/vllm-continuum

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [124] [Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA Effects](https://arxiv.org/abs/2511.02132)
*Mansi Choudhary,Karthik Sangaiah,Sonali Singh,Muhammad Osama,Lisa Wu Wills,Ganesh Dasika*

Main category: cs.AR

TL;DR: 本文提出一种名为“Swizzled Head-first Mapping”的NUMA感知调度策略，通过将多头注意力机制中的注意力头与GPU的NUMA域对齐，显著提升L2缓存命中率和整体性能，在AMD MI300X上相较现有方法最高提升50%性能。


<details>
  <summary>Details</summary>
Motivation: 随着AI GPU向多芯粒（chiplet）架构发展，非一致性内存访问（NUMA）导致内存延迟和带宽在不同计算区域间差异显著，传统假设内存访问均匀的GPU调度策略因此失效，尤其影响多头注意力（MHA）的局部性。

Method: 提出一种空间感知的调度策略——Swizzled Head-first Mapping，将注意力头映射到与GPU NUMA域对齐的位置，以利用芯粒内部缓存复用。

Result: 在AMD MI300X架构上，相比当前最先进的注意力算法，性能最高提升50%，并保持80-97%的高L2缓存命中率。

Conclusion: NUMA感知调度已成为在新一代解聚式GPU上实现高效AI训练与推理的关键技术，本文方法为此提供了有效路径。

Abstract: The rise of disaggregated AI GPUs has exposed a critical bottleneck in
large-scale attention workloads: non-uniform memory access (NUMA). As
multi-chiplet designs become the norm for scaling compute capabilities, memory
latency and bandwidth vary sharply across compute regions, undermining the
performance of traditional GPU kernel scheduling strategies that assume uniform
memory access. We identify how these NUMA effects distort locality in
multi-head attention (MHA) and present Swizzled Head-first Mapping, a
spatially-aware scheduling strategy that aligns attention heads with GPU NUMA
domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our
method achieves up to 50% higher performance over state-of-the-art attention
algorithms using conventional scheduling techniques and sustains consistently
high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware
scheduling is now fundamental to achieving full efficiency on next-generation
disaggregated GPUs, offering a path forward for scalable AI training and
inference.

</details>


### [125] [Energy-Efficient Hardware Acceleration of Whisper ASR on a CGLA](https://arxiv.org/abs/2511.02269)
*Takuto Ando,Yu Eto,Ayumu Takeuchi,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次在CGLA架构IMAX上实现并评估了Whisper语音识别模型的核心计算核，通过软硬件协同设计，在FPGA原型上验证并预测了28nm ASIC的性能，结果表明其在能效方面显著优于GPU和CPU，适用于边缘设备上的可持续自动语音识别。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（如自动语音识别）的兴起带来了严重的能耗问题，而现有ASIC虽高效但缺乏可编程性，难以适应算法演进。因此，需要一种兼顾能效与灵活性的解决方案。

Method: 在通用CGLA加速器IMAX上实现Whisper的核心计算核，采用软硬件协同设计方法，通过FPGA原型进行评估，并投影28nm ASIC的性能表现。

Result: 预测的ASIC实现比NVIDIA Jetson AGX Orin能效高1.90倍，比NVIDIA RTX 4090高9.83倍（针对Q8_0模型）。

Conclusion: CGLA是一种在功耗受限的边缘设备上实现高效、可持续自动语音识别的有前景平台。

Abstract: The rise of generative AI for tasks like Automatic Speech Recognition (ASR)
has created a critical energy consumption challenge. While ASICs offer high
efficiency, they lack the programmability to adapt to evolving algorithms. To
address this trade-off, we implement and evaluate Whisper's core computational
kernel on the IMAX, a general-purpose Coarse-Grained Linear Arrays (CGLAs)
accelerator. To our knowledge, this is the first work to execute a Whisper
kernel on a CGRA and compare its performance against CPUs and GPUs. Using
hardware/software co-design, we evaluate our system via an FPGA prototype and
project performance for a 28 nm ASIC. Our results demonstrate superior energy
efficiency. The projected ASIC is 1.90x more energy-efficient than the NVIDIA
Jetson AGX Orin and 9.83x more than an NVIDIA RTX 4090 for the Q8_0 model. This
work positions CGLA as a promising platform for sustainable ASR on
power-constrained edge devices.

</details>


### [126] [VFocus: Better Verilog Generation from Large Language Model via Focused Reasoning](https://arxiv.org/abs/2511.02285)
*Zhuorui Zhao,Bing Li,Grace Li Zhang,Ulf Schlichtmann*

Main category: cs.AR

TL;DR: VFocus is a three-stage framework that improves the functional correctness of Verilog code generated by large language models (LLMs) by focusing LLM reasoning on critical decision points through density-guided filtering, simulation-based ranking, and inconsistency-aware refinement.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating Verilog code with LLMs often fail to ensure functional correctness because they do not effectively guide the model’s reasoning toward the most critical parts of the design.

Method: VFocus employs a three-stage process: (1) pre-ranking with density-guided filtering to retain high-potential candidates, (2) ranking via simulation and self-consistency clustering, and (3) post-ranking refinement using inconsistency mining and reasoning-augmented prompts.

Result: Experiments on VerilogEval-Human show VFocus significantly boosts pass@1 correctness across multiple reasoning LLMs.

Conclusion: VFocus effectively enhances Verilog code generation by strategically focusing LLM reasoning on key design decisions, leading to improved functional correctness in complex hardware tasks.

Abstract: Large Language Models (LLMs) have shown impressive potential in generating
Verilog codes, but ensuring functional correctness remains a challenge.
Existing approaches often rely on self-consistency or simulation feedback to
select the best candidate, but they miss opportunities to focus LLM reasoning
on the most informative parts of the design. We propose VFocus, a three-stage
framework that enhances Verilog generation by sharpening the focus of LLM
reasoning onto critical decision points in the code generation process. In the
\textbf{pre-ranking stage}, VFocus generates multiple code candidates through
LLM prompting, retries for syntactically valid outputs, and introduces a
\textit{Density-guided Filtering} to retain candidates that fall within the
"reasoning sweet spot" for functional correctness. In the \textbf{ranking
stage}, we simulate each code candidate using an automatically generated
testbench and apply self-consistency-based clustering to identify the most
consistent outputs. Finally, in the \textbf{post-ranking refinement stage},
VFocus performs inconsistency mining on top-ranked candidates and invokes
reasoning-augmented LLM prompts for candidate refinement. Experiments on the
VerilogEval-Human benchmark show that VFocus significantly improves the pass@1
correctness across multiple reasoning LLMs, demonstrating its effectiveness in
enhancing Verilog generation for complex hardware design tasks.

</details>


### [127] [Facial Expression Recognition System Using DNN Accelerator with Multi-threading on FPGA](https://arxiv.org/abs/2511.02408)
*Takuto Ando,Yusuke Inoue*

Main category: cs.AR

TL;DR: 本文在SoC FPGA上利用深度学习处理器单元（DPU）实现了一个多线程的独立面部表情识别系统，通过在同一个通用CNN加速器上运行DenseBox人脸检测和CNN表情识别，提高了FPGA资源利用率和系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 先前工作在人脸检测阶段使用Haar Cascade检测器，受限于FPGA资源且在侧脸和光照变化条件下准确率较低；同时，若使用专用电路加速器进行两次DNN推理，则需增加新的硬件模块，不够高效。

Method: 采用通用型脉动阵列CNN加速器（DPU），在同一个DPU上依次运行基于DenseBox的人脸检测和基于CNN的表情识别，并引入多线程技术提升DPU利用率和整体吞吐量。

Result: 系统实现了25 FPS的整体吞吐量，单位功耗吞吐量提升了2.4倍。

Conclusion: 所提出的方法在保持较小电路规模的同时，有效利用FPGA资源，显著提升了系统性能和能效。

Abstract: In this paper, we implement a stand-alone facial expression recognition
system on an SoC FPGA with multi-threading using a Deep learning Processor Unit
(DPU). The system consists of two steps: one for face detection step and one
for facial expression recognition. In the previous work, the Haar Cascade
detector was run on a CPU in the face detection step due to FPGA resource
limitations, but this detector is less accurate for profile and variable
illumination condition images. Moreover, the previous work used a dedicated
circuit accelerator, so running a second DNN inference for face detection on
the FPGA would require the addition of a new accelerator. As an alternative to
this approach, we run the two inferences by DNN on a DPU, which is a
general-purpose CNN accelerator of the systolic array type. Our method for face
detection using DenseBox and facial expression recognition using CNN on the
same DPU enables the efficient use of FPGA resources while maintaining a small
circuit size. We also developed a multi-threading technique that improves the
overall throughput while increasing the DPU utilization efficiency. With this
approach, we achieved an overall system throughput of 25 FPS and a throughput
per power consumption of 2.4 times.

</details>


### [128] [Digit-Recurrence Posit Division](https://arxiv.org/abs/2511.02494)
*Raul Murillo,Julio Villalba-Moreno,Alberto A. Del Barrio,Guillermo Botella*

Main category: cs.AR

TL;DR: 本文提出了一种基于基数为4的数字递归算法的posit除法单元，通过硬件优化显著降低了能耗和迭代次数，同时仅带来较小的面积开销。


<details>
  <summary>Details</summary>
Motivation: Posit算术虽在精度和动态范围上优于IEEE 754浮点表示，但其除法运算因硬件复杂性而面临挑战，亟需高效实现方案。

Method: 采用基数-4数字递归算法，并结合冗余算术、在线商转换和操作数缩放等硬件导向优化技术实现posit除法单元。

Result: 在多种posit配置下，与现有方法相比，能耗降低超过80%，迭代次数显著减少，且面积开销较小。

Conclusion: 所提出的优化算法有效提升了posit算术单元在除法运算中的能效和性能，展现了其在实际硬件实现中的潜力。

Abstract: Posit arithmetic has emerged as a promising alternative to IEEE 754
floating-point representation, offering enhanced accuracy and dynamic range.
However, division operations in posit systems remain challenging due to their
inherent hardware complexity. In this work, we present posit division units
based on the digit-recurrence algorithm, marking the first implementation of
radix-4 digit-recurrence techniques within this context. Our approach
incorporates hardware-centric optimizations including redundant arithmetic,
on-the-fly quotient conversion, and operand scaling to streamline the division
process while mitigating latency, area, and power overheads. Comprehensive
synthesis evaluations across multiple posit configurations demonstrate
significant performance improvements, including more than 80% energy reduction
with small area overhead compared to existing methods, and a substantial
decrease in the number of iterations. These results underscore the potential of
our adapted algorithm to enhance the efficiency of posit-based arithmetic
units.

</details>


### [129] [Implementation and Evaluation of Stable Diffusion on a General-Purpose CGLA Accelerator](https://arxiv.org/abs/2511.02530)
*Takuto Ando,Yu Eto,Yasuhiko Nakashima*

Main category: cs.AR

TL;DR: 本文首次在通用粗粒度可重构阵列（CGRA）加速器IMAX3上实现了stable-diffusion.cpp图像生成框架的核心计算内核，并对其性能进行了深入评估，展示了其在ASIC实现中的高效能与能效潜力。


<details>
  <summary>Details</summary>
Motivation: 评估通用CGRA架构IMAX3在处理高负载图像生成任务（如Stable Diffusion）时的性能与能效，为未来面向AI的专用加速器设计提供指导。

Method: 在IMAX3的FPGA原型上实现并运行stable-diffusion.cpp的主要计算内核，通过实测建立性能基线，并推演其未来ASIC实现的潜力。

Result: 尽管IMAX3是通用架构，但在FPGA原型上已展现出良好的性能，并在ASIC实现预测中表现出优异的能效，验证了其作为AI加速平台的可行性。

Conclusion: 本研究为IMAX架构的后续优化提供了具体指导，并为开发下一代面向多模态AI的高效能CGLA加速器奠定了基础。

Abstract: This paper presents the first implementation and in-depth evaluation of the
primary computational kernels from the stable-diffusion.cpp image generation
framework on IMAX3, a general-purpose Coarse-Grained Reconfigurable Array
(CGRA) accelerator. We designed IMAX3 as a versatile computational platform,
and this work assesses its capabilities by executing a demanding image
generation workload. We evaluate its performance on a current
Field-Programmable Gate Array (FPGA) prototype to establish a baseline and
project its potential for a future Application-Specific Integrated Circuit
(ASIC) implementation. Our results demonstrate that, despite its
general-purpose architecture, IMAX3 achieves promising performance and power
efficiency, particularly in its projected ASIC form. This work provides
concrete guidelines for future IMAX architectural designs and establishes a
foundation for developing next-generation, AI-specialized Coarse-Grained Linear
Array (CGLA) accelerators by refining this versatile platform. Ultimately, this
achievement contributes to the realization of energy-efficient, on-device,
multi-modal AI platforms.

</details>
