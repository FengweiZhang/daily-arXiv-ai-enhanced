<div id=toc></div>

# Table of Contents

- [cs.CE](#cs.CE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.DB](#cs.DB) [Total: 4]


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [1] [Fourier Neural Operators for Structural Dynamics Models: Challenges, Limitations and Advantages of Using a Spectrogram Loss](https://arxiv.org/abs/2511.08753)
*Rad Haghi,Bipin Gaikwad,Abani Patra*

Main category: cs.CE

TL;DR: 本文系统评估了傅里叶神经算子（FNO）在非线性和非平稳系统中的表现，发现其在处理线性系统时效果优异，但在非线性系统中会出现虚假能量耗散和频率失真；通过引入基于频谱图的损失函数可有效缓解该问题，并验证了系统非线性程度而非维度或复杂度是决定FNO成功与否的关键因素。


<details>
  <summary>Details</summary>
Motivation: 探究傅里叶神经算子（FNO）在非线性和非平稳偏微分方程系统中的适用边界，识别其失效机制并提出改进方法。

Method: 对多种具有非线性和非平稳特性的系统进行广泛测试，采用多种激励函数分析FNO失效模式；通过离散化误差分析探讨根本原因；与LSTM基准模型对比；开发结合时域MSE与频域幅值/相位误差的频谱图损失函数进行训练优化；并在IEA 15MW风机模型上验证结论。

Result: FNO在线性系统中表现优异，能准确保持能量和频谱特性；但在非线性系统中出现人工能量耗散和频率内容失真，且不随训练数据量增加而改善。所提出的频谱感知损失函数有效消除了线性系统中的人工耗散，并提升了非线性系统的能量比。IEA 15MW风机案例表明，尽管自由度高，但因系统近似线性，FNO仍能准确预测。

Conclusion: 系统非线性程度是决定FNO是否适用的关键因素，而非问题的维度或复杂性；该研究为FNO的实际应用提供了明确指导，并对其普适性假设提出了挑战。

Abstract: Fourier Neural Operators (FNOs) have emerged as promising surrogates for partial differential equation solvers. In this work, we extensively tested FNOs on a variety of systems with non-linear and non-stationary properties, using a wide range of forcing functions to isolate failure mechanisms. FNOs stand out in modeling linear systems, regardless of complexity, while achieving near-perfect energy preservation and accurate spectral representation for linear dynamics. However, they fail on non-linear systems, where the failure manifests as artificial energy dissipation and manipulated frequency content. This limitation persists regardless of training dataset size, and we discuss the root cause through discretization error analysis. Comparison with LSTM as the baseline shows FNOs are superior for both linear and non-linear systems, independent of the training dataset size. We develop a spectrogram-based loss function that combines time-domain Mean Squared Error (MSE) with frequency-domain magnitude and phase errors, addressing the low-frequency bias of FNOs. This frequency-aware training eliminates artificial dissipation in linear systems and enhances the energy ratios of non-linear systems. The IEA 15MW turbine model validates our findings. Despite hundreds of degrees of freedom, FNO predictions remain accurate because the turbine behaves in a predominantly linear regime. Our findings establish that system non-linearity, rather than dimensionality or complexity, determines the success of FNO. These results provide clear guidelines for practitioners and challenge assumptions about FNOs' universality.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [2] [Triage in Software Engineering: A Systematic Review of Research and Practice](https://arxiv.org/abs/2511.08607)
*Yongxin Zhao,Shenglin Zhang,Yujia Wu,Yuxin Sun,Yongqian Sun,Dan Pei,Chetan Bansal,Minghua Ma*

Main category: cs.SE

TL;DR: 本文综述了2004年至今的234篇文献，系统梳理了软件系统中问题分诊（triage）的基本概念、架构与挑战，对比学术界与工业界的研究目标差异，总结常用数据集与评估指标，并提出未来研究方向以促进学术成果向工业实践转化。


<details>
  <summary>Details</summary>
Motivation: 随着现代软件系统日益复杂，高效的问题分诊对于保障系统可靠性、可维护性及快速响应至关重要。尽管过去二十年在自动化分诊方面取得显著进展，但其在工业实践中的部署仍面临诸多障碍，亟需系统性梳理与分析。

Method: 作者对2004年至今的234篇相关论文进行了全面综述，深入分析分诊系统的基本概念、架构设计和问题定义；通过比较学术与工业研究目标，并结合工业实践的实证研究，识别实际部署中的主要障碍；同时汇总常用开源数据集与评估指标。

Result: 识别出限制分诊系统实际应用的关键障碍，提供了统一的评估视角，并整理了广泛采用的数据集与指标，为从业者选择方法和评估性能提供参考。

Conclusion: 未来研究应聚焦于弥合学术创新与工业需求之间的鸿沟，推动更紧密的产学研结合，并探索新兴技术在分诊中的应用潜力。

Abstract: As modern software systems continue to grow in complexity, triage has become a fundamental process in system operations and maintenance. Triage aims to efficiently prioritize, assign, and assess issues to ensure the reliability of complex environments. The vast amount of heterogeneous data generated by software systems has made effective triage indispensable for maintaining reliability, facilitating maintainability, and enabling rapid issue response. Motivated by these challenges, researchers have devoted extensive effort to advancing triage automation and have achieved significant progress over the past two decades. This survey provides a comprehensive review of 234 papers from 2004 to the present, offering an in-depth examination of the fundamental concepts, system architecture, and problem statement. By comparing the distinct goals of academic and industrial research and by analyzing empirical studies of industrial practices, we identify the major obstacles that limit the practical deployment of triage systems. To assist practitioners in method selection and performance evaluation, we summarize widely adopted open-source datasets and evaluation metrics, providing a unified perspective on the measurement of triage effectiveness. Finally, we outline potential future directions and emerging opportunities to foster a closer integration between academic innovation and industrial application. All reviewed papers and projects are available at https://github.com/AIOps-Lab-NKU/TriageSurvey.

</details>


### [3] [Energy Consumption of Dataframe Libraries for End-to-End Deep Learning Pipelines:A Comparative Analysis](https://arxiv.org/abs/2511.08644)
*Punit Kumar,Asif Imran,Tevfik Kosar*

Main category: cs.SE

TL;DR: 本文对比分析了Pandas、Polars和Dask三大Python数据处理库在完整深度学习训练与推理流程中的性能表现，特别关注其在GPU密集型任务（如数据加载、预处理和批处理）中的交互效率，并评估了运行时间、内存、磁盘及能耗等关键指标。


<details>
  <summary>Details</summary>
Motivation: 现有文献缺乏对数据处理库在深度学习全流程中与GPU工作负载交互性能的系统研究，本文旨在填补这一空白。

Method: 通过在多种机器学习模型和数据集上嵌入Pandas、Polars和Dask，测量其在深度学习训练与推理流程中的运行时间、内存使用、磁盘占用以及CPU/GPU能耗等关键性能指标。

Result: 获得了三类库在不同深度学习场景下的性能对比数据，揭示了它们在数据加载、预处理和批处理阶段对整体流程效率的影响。

Conclusion: 该研究为在深度学习项目中选择合适的数据处理库提供了实证依据，强调需综合考虑运行效率、资源消耗与能耗等因素。

Abstract: This paper presents a detailed comparative analysis of the performance of three major Python data manipulation libraries - Pandas, Polars, and Dask - specifically when embedded within complete deep learning (DL) training and inference pipelines. The research bridges a gap in existing literature by studying how these libraries interact with substantial GPU workloads during critical phases like data loading, preprocessing, and batch feeding. The authors measured key performance indicators including runtime, memory usage, disk usage, and energy consumption (both CPU and GPU) across various machine learning models and datasets.

</details>


### [4] [An insight into the technical debt-fix trade off in software backporting](https://arxiv.org/abs/2511.09000)
*Jarin Tasnim,Debasish Chakroborti,Chanchal K. Roy,Kevin A. Schneider*

Main category: cs.SE

TL;DR: 该研究分析了在Apache、Eclipse和Python三大软件生态中，回移植（backporting）过程中引入技术债务的情况，发现约4.3%的回移植会带来新的技术债务，并揭示了不同生态、发布阶段及开发者特征对技术债务产生的影响。


<details>
  <summary>Details</summary>
Motivation: 回移植是维护旧版软件的重要手段，但可能在稳定代码中引入新的技术债务。现有研究缺乏对回移植过程中技术债务产生机制的系统性理解，因此本文旨在探究其发生时机、原因及影响因素。

Method: 作者分析了来自87个仓库的105,396次提交，涵盖31,076个回移植源，覆盖Apache、Eclipse和Python三大生态系统，通过量化技术债务的引入情况，结合版本生命周期阶段与开发者特征进行统计分析。

Result: 约4.3%的回移植引入了新的技术债务；Apache绝对数量最多，但Python和Eclipse的技术债务/提交比约为Apache的三倍；Apache早期因功能迁移易产生债务，而Python和Eclipse主要在发布周期中期积累债务；经验不足、工作负荷高或非项目所有者的开发者更易引入技术债务。

Conclusion: 回移植虽有助于维护旧版本，但也可能带来新的技术债务，尤其受生态系统特性、发布阶段及开发者背景影响。项目应关注高风险阶段和人员，以减少技术债务的累积。

Abstract: Maintaining software is an ongoing process that stretches beyond the initial release. Stable software versions continuously evolve to fix bugs, add improvements, address security issues, and ensure compatibility. This ongoing support involves Backporting, which means taking a fix or update from a newer version and applying it to an older version of the same software. As software versions evolve, new technical debt can arise during backport maintenance activities. This study examines the technical debt involved in fixing 105,396 commits from 31,076 backport sources across 87 repositories in three software ecosystems (Apache, Eclipse, and Python). The goal is to identify when and why new technical debt arises during backporting in stable source code. Our results indicate that approximately 4.3% of backports introduce new technical debt. Apache contributes the most absolute instances, while Python and Eclipse exhibit nearly three times higher debt-to-commit ratios than Apache. Feature migrations make older Apache releases debt-prone in the early phase, whereas Python and Eclipse releases tend to accumulate technical debt mostly during the middle phase of their release cycles. Additionally, developers who are inexperienced, under high workloads, or non-owners are more likely to introduce technical debt during backporting.

</details>


### [5] [Test Plan Generation for Live Testing of Cloud Services](https://arxiv.org/abs/2511.09038)
*Oussama Jebbar,Ferhat Khendek,Maria Toeroe*

Main category: cs.SE

TL;DR: 本文提出了一种自动生成测试计划的方法，以减少在生产环境中进行实时测试时对正常服务造成的干扰。


<details>
  <summary>Details</summary>
Motivation: 在生产环境中进行实时测试容易干扰正常流量，而手动设计测试计划繁琐且易出错，尤其在大型复杂系统中更为困难。

Method: 提出一种自动化生成测试计划的方法，涵盖测试配置选择/生成、部署规划、测试运行调度及干扰风险缓解策略等任务。

Result: 通过一个案例研究展示了该方法的实施，并讨论了其不同方面。

Conclusion: 该方法有助于降低生产环境中测试活动引起的服务中断风险，提高测试计划制定的效率与可靠性。

Abstract: Live testing is performed in the production environment ideally without causing unacceptable disturbance to the production traffic. Thus, test activities have to be orchestrated properly to avoid interferences with the production traffic. A test plan is the road map that specifies how the test activities need to be orchestrated. Developing a test plan includes tasks such as test configuration selection/generation, test configuration deployment planning, creating the test runs schedule, choosing strategies to mitigate the risk of interferences, etc. The manual design of a test plan is tedious and error prone. This task becomes harder especially when the systems are large and complex. In this paper we propose an approach for automating test plans generation. With this approach we aim at reducing service disruption that may be induced by the testing activities in production. We illustrate our approach with a case study and discuss its different aspects.

</details>


### [6] [Vendor-Aware Industrial Agents: RAG-Enhanced LLMs for Secure On-Premise PLC Code Generation](https://arxiv.org/abs/2511.09122)
*Joschka Kersting,Michael Rummel,Gesa Benndorf*

Main category: cs.SE

TL;DR: 本文提出了一种适用于工业场景的低数据域编码助手解决方案，通过本地小模型微调、多模型竞争机制、自动纠错与编译验证，在不依赖大模型微调的情况下实现了高质量IEC 61131-3代码生成。


<details>
  <summary>Details</summary>
Motivation: 可编程逻辑控制器（PLC）使用专有代码方言，使得训练编码助手困难；现有大语言模型虽能生成标准兼容代码，但不了解特定功能块和项目上下文，且企业出于安全考虑不愿依赖云服务。

Method: 采用检索增强生成（RAG）架构，结合精心设计的提示工程与定向检索策略，微调小型本地模型用于边缘设备，并在聊天界面中集成多模型竞争、推理、自动纠错与代码编译验证机制。

Result: 通过代码编译统计和用户评分的综合评估表明，该方法在低数据条件下仍能实现高质量代码生成，有效支持工业PLC编程。

Conclusion: 在缺乏大规模专有代码数据的情况下，结合RAG、提示工程与本地小模型微调的编码助手可在工业环境中可靠运行，满足企业对数据隐私和代码质量的双重需求。

Abstract: Programmable Logic Controllers are operated by proprietary code dialects; this makes it challenging to train coding assistants. Current LLMs are trained on large code datasets and are capable of writing IEC 61131-3 compatible code out of the box, but they neither know specific function blocks, nor related project code. Moreover, companies like Mitsubishi Electric and their customers do not trust cloud providers. Hence, an own coding agent is the desired solution to cope with this. In this study, we present our work on a low-data domain coding assistant solution for industrial use. We show how we achieved high quality code generation without fine-tuning large models and by fine-tuning small local models for edge device usage. Our tool lets several AI models compete with each other, uses reasoning, corrects bugs automatically and checks code validity by compiling it directly in the chat interface. We support our approach with an extensive evaluation that comes with code compilation statistics and user ratings. We found that a Retrieval-Augmented Generation (RAG) supported coding assistant can work in low-data domains by using extensive prompt engineering and directed retrieval.

</details>


### [7] [Leveraging Self-Paced Learning for Software Vulnerability Detection](https://arxiv.org/abs/2511.09212)
*Zeru Cheng,Yanjing Yang,He Zhang,Lanxin Yang,Jinghao Hu,Jinwei Xu,Bohan Liu,Haifeng Shen*

Main category: cs.SE

TL;DR: 本文提出了一种名为SPLVD（Self-Paced Learning for Software Vulnerability Detection）的新方法，通过模拟人类由易到难的学习过程，动态选择训练源代码以提升漏洞检测的准确性。在多个基准数据集和OpenHarmony项目上的实验表明，SPLVD优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的软件漏洞检测方法在实践中准确率有限，主要原因是训练数据（源代码）质量较低。为解决这一问题，作者提出一种新的训练策略。

Method: SPLVD采用自定步调学习机制，在每个训练周期前通过专门设计的数据选择器重新评估源代码难度，优先选择较易样本进行训练，并随训练进程逐步引入更难样本，从而优化模型学习过程。

Result: 在包含超过239K源代码（其中25K为漏洞代码）的三个基准数据集上，SPLVD分别取得了89.2%、68.7%和43.5%的最高F1分数；在未被通用大语言模型学习过的OpenHarmony项目中，其精确率达到90.9%。

Conclusion: SPLVD通过动态选择训练数据有效提升了漏洞检测性能，在标准数据集和真实新生态项目中均表现出优越性和实用性。

Abstract: Software vulnerabilities are major risks to software systems. Recently, researchers have proposed many deep learning approaches to detect software vulnerabilities. However, their accuracy is limited in practice. One of the main causes is low-quality training data (i.e., source code). To this end, we propose a new approach: SPLVD (Self-Paced Learning for Software Vulnerability Detection). SPLVD dynamically selects source code for model training based on the stage of training, which simulates the human learning process progressing from easy to hard. SPLVD has a data selector that is specifically designed for the vulnerability detection task, which enables it to prioritize the learning of easy source code. Before each training epoch, SPLVD uses the data selector to recalculate the difficulty of the source code, select new training source code, and update the data selector. When evaluating SPLVD, we first use three benchmark datasets with over 239K source code in which 25K are vulnerable for standard evaluations. Experimental results demonstrate that SPLVD achieves the highest F1 of 89.2%, 68.7%, and 43.5%, respectively, outperforming the state-of-the-art approaches. Then we collect projects from OpenHarmony, a new ecosystem that has not been learned by general LLMs, to evaluate SPLVD further. SPLVD achieves the highest precision of 90.9%, demonstrating its practical effectiveness.

</details>


### [8] [AILINKPREVIEWER: Enhancing Code Reviews with LLM-Powered Link Previews](https://arxiv.org/abs/2511.09223)
*Panya Trakoolgerntong,Tao Xiao,Masanari Kondo,Chaiyong Ragkhitwetsagul,Morakot Choetkiertikul,Pattaraporn Sangaroonsilp,Yasutaka Kamei*

Main category: cs.SE

TL;DR: 本文提出 AILINKPREVIEWER 工具，利用大语言模型（LLM）为 Pull Request 中的外部链接生成上下文感知的预览内容，以提升代码审查效率。实验表明，基于上下文的摘要在自动评估指标上表现更优，但用户更偏好非上下文摘要。


<details>
  <summary>Details</summary>
Motivation: 在代码审查过程中，Pull Request 中常包含指向问题或外部资源的链接，但这些链接通常在自动化任务（如 PR 摘要生成、评论生成）中被忽略，导致信息缺失并增加开发者认知负担。

Method: 作者开发了 AILINKPREVIEWER 工具，结合 PR 的元数据（标题、描述、评论）和链接内容，使用 LLM 生成三种类型的链接预览：上下文感知摘要、非上下文摘要和仅基于元数据的预览，并在 50 个 GitHub 仓库上进行评估。

Result: 自动评估指标（BLEU、BERTScore、压缩率）显示上下文摘要优于其他方法；但在包含 7 名参与者的用户研究中，多数人更偏好非上下文摘要，揭示了指标表现与实际可用性之间的权衡。

Conclusion: LLM 驱动的链接预览有潜力丰富代码审查中的上下文信息，提升开发者效率和自动化工具的效果，但需进一步优化以平衡自动指标与用户体验。

Abstract: Code review is a key practice in software engineering, where developers evaluate code changes to ensure quality and maintainability. Links to issues and external resources are often included in Pull Requests (PRs) to provide additional context, yet they are typically discarded in automated tasks such as PR summarization and code review comment generation. This limits the richness of information available to reviewers and increases cognitive load by forcing context-switching. To address this gap, we present AILINKPREVIEWER, a tool that leverages Large Language Models (LLMs) to generate previews of links in PRs using PR metadata, including titles, descriptions, comments, and link body content. We analyzed 50 engineered GitHub repositories and compared three approaches: Contextual LLM summaries, Non-Contextual LLM summaries, and Metadata-based previews. The results in metrics such as BLEU, BERTScore, and compression ratio show that contextual summaries consistently outperform other methods. However, in a user study with seven participants, most preferred non-contextual summaries, suggesting a trade-off between metric performance and perceived usability. These findings demonstrate the potential of LLM-powered link previews to enhance code review efficiency and to provide richer context for developers and automation in software engineering.
  The video demo is available at https://www.youtube.com/watch?v=h2qH4RtrB3E, and the tool and its source code can be found at https://github.com/c4rtune/AILinkPreviewer.

</details>


### [9] [Leveraging Large Language Models for Use Case Model Generation from Software Requirements](https://arxiv.org/abs/2511.09231)
*Tobias Eisenreich,Nicholas Friedlaender,Stefan Wagner*

Main category: cs.SE

TL;DR: 本文提出一种基于大语言模型（LLM）的方法，通过高级提示工程从软件需求中自动提取参与者和用例，显著提升用例建模效率，建模时间减少60%，且模型质量与人工方法相当。


<details>
  <summary>Details</summary>
Motivation: 手动创建用例模型费时费力，实践中常被跳过，因此需要一种高效辅助方法来支持该过程。

Method: 采用开源权重的大语言模型，结合高级提示工程技术，从软件需求文档中系统地提取参与者和用例。

Result: 在五名专业软件工程师参与的探索性研究中，该方法将建模时间减少了60%，模型质量与传统手工方法相当，并为建模过程提供了有价值的指导。

Conclusion: 大语言模型能有效辅助用例建模，在显著提高效率的同时保持模型质量，具有实际应用价值。

Abstract: Use case modeling employs user-centered scenarios to outline system requirements. These help to achieve consensus among relevant stakeholders. Because the manual creation of use case models is demanding and time-consuming, it is often skipped in practice. This study explores the potential of Large Language Models (LLMs) to assist in this tedious process. The proposed method integrates an open-weight LLM to systematically extract actors and use cases from software requirements with advanced prompt engineering techniques. The method is evaluated using an exploratory study conducted with five professional software engineers, which compares traditional manual modeling to the proposed LLM-based approach. The results show a substantial acceleration, reducing the modeling time by 60\%. At the same time, the model quality remains on par. Besides improving the modeling efficiency, the participants indicated that the method provided valuable guidance in the process.

</details>


### [10] [Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks](https://arxiv.org/abs/2511.09373)
*Adam Štorek,Vikas Upadhyay,Marianne Menglin Liu,Daniel W. Peterson,Anshul Mittal,Sujeeth Bharadwaj,Fahad Shah,Dan Roth*

Main category: cs.SE

TL;DR: 本文提出了Routesplain，这是首个面向软件相关任务的LLM路由器，通过提取人类可解释的概念（如任务类型、领域、推理复杂度）进行路由决策，在准确性和成本方面优于单个模型，并超越现有的黑盒路由基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在各类软件相关任务中表现不一，合理将用户查询路由至合适的LLM有助于提升响应质量并降低成本。然而现有工作主要依赖黑盒模型进行通用路由，缺乏可解释性与任务针对性。

Method: Routesplain首先从用户查询中提取人类可解释的概念（如任务类别、领域知识、推理复杂度等），然后基于这些概念进行路由决策，从而提供透明且可信的路由依据。

Result: 在涵盖8类软件任务、16个前沿LLM的评估中，Routesplain在准确率和成本上均优于单一模型，并达到或超过所有黑盒基线方法；概念层面的干预分析还揭示了路由器进一步优化的方向。

Conclusion: Routesplain为软件相关任务提供了高效、可解释的LLM路由方案，在性能与成本之间取得良好平衡，同时为未来路由器设计提供了新思路。

Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [11] [Hierarchical Reinforcement Learning for Integrated Cloud-Fog-Edge Computing in IoT Systems](https://arxiv.org/abs/2511.09006)
*Ameneh Zarei,Mahmood Ahmadi,Farhad Mardukhi*

Main category: cs.NI

TL;DR: 本文提出一种名为HIPA的分层物联网处理架构，通过机器学习在云、雾和边缘计算之间动态分配任务，以降低延迟、提升可扩展性并保障数据隐私。


<details>
  <summary>Details</summary>
Motivation: 传统云计算架构难以满足物联网应用对海量数据和实时性的需求，亟需更高效的计算范式来优化性能。

Method: 提出一种结合云、雾与边缘计算的分层架构HIPA，并利用机器学习实现任务在不同计算层之间的动态分配。

Result: 该框架能够有效减少延迟、提高系统可扩展性，并增强数据隐私保护，从而构建高效、安全且可扩展的物联网生态系统。

Conclusion: 云、雾与边缘计算的协同作用可通过HIPA架构显著提升物联网系统的整体性能与安全性。

Abstract: The Internet of Things (IoT) is transforming industries by connecting billions of devices to collect, process, and share data. However, the massive data volumes and real-time demands of IoT applications strain traditional cloud computing architectures. This paper explores the complementary roles of cloud, fog, and edge computing in enhancing IoT performance, focusing on their ability to reduce latency, improve scalability, and ensure data privacy. We propose a novel framework, the Hierarchical IoT Processing Architecture (HIPA), which dynamically allocates computational tasks across cloud, fog, and edge layers using machine learning. By synthesizing current research and introducing HIPA, this paper highlights how these paradigms can create efficient, secure, and scalable IoT ecosystems.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [12] [An MLIR pipeline for offloading Fortran to FPGAs via OpenMP](https://arxiv.org/abs/2511.08713)
*Gabriel Rodriguez-Canal,David Katz,Nick Brown*

Main category: cs.DC

TL;DR: 本文首次在MLIR中通过OpenMP target指令实现面向FPGA的选择性代码卸载，结合MLIR OpenMP方言与HLS方言，构建了一个可移植、可扩展且兼容任意MLIR前端（如Flang）的FPGA编译流程。


<details>
  <summary>Details</summary>
Motivation: 随着摩尔定律放缓，FPGA等异构计算平台在高性能计算（HPC）负载加速中日益重要；现有OpenMP FPGA方案依赖定制编译器，缺乏通用性和可组合性。

Method: 将MLIR中的OpenMP方言与高层次综合（HLS）方言结合，利用MLIR生态系统中的现有组件，实现支持标准OpenMP指令的手动优化，并通过Flang前端验证兼容性。

Result: 成功实现了首个基于MLIR的OpenMP target指令驱动的FPGA代码卸载方案，显著降低了开发成本，并展示了MLIR生态系统的可组合优势。

Conclusion: 该工作为基于指令的FPGA加速提供了一条灵活、可扩展且与MLIR生态深度集成的新路径。

Abstract: With the slowing of Moore's Law, heterogeneous computing platforms such as Field Programmable Gate Arrays (FPGAs) have gained increasing interest for accelerating HPC workloads. In this work we present, to the best of our knowledge, the first implementation of selective code offloading to FPGAs via the OpenMP target directive within MLIR. Our approach combines the MLIR OpenMP dialect with a High-Level Synthesis (HLS) dialect to provide a portable compilation flow targeting FPGAs. Unlike prior OpenMP FPGA efforts that rely on custom compilers, by contrast we integrate with MLIR and so support any MLIR-compatible front end, demonstrated here with Flang. Building upon a range of existing MLIR building blocks significantly reduces the effort required and demonstrates the composability benefits of the MLIR ecosystem. Our approach supports manual optimisation of offloaded kernels through standard OpenMP directives, and this work establishes a flexible and extensible path for directive-based FPGA acceleration integrated within the MLIR ecosystem.

</details>


### [13] [Distribution and Management of Datacenter Load Decoupling](https://arxiv.org/abs/2511.08936)
*Liuzixuan Lin,Andrew A. Chien*

Main category: cs.DC

TL;DR: 通过优化数据中心与电网之间的解耦资源分配与管理策略，可在显著降低电网碳排放的同时保持经济可行性。


<details>
  <summary>Details</summary>
Motivation: 人工智能和云计算数据中心的高能耗加剧了其碳足迹问题，尤其因其持续用电需求与可再生能源发电的波动性存在冲突，亟需通过负载灵活性提升电网对可再生能源的消纳能力。

Method: 提出通过能量资源将数据中心功率容量与电网负载解耦，并研究解耦资源的最优分配与管理方式，考虑站点差异及数据中心-电网协同机制（双向共享控制 vs 单向信息共享）。

Result: 优化分配可在仅使用70%解耦资源的情况下实现超过98%的潜在碳减排；双向协同管理可使碳减排效果提升1.4倍；平均而言，数据中心获得的成本与碳减排收益超过本地解耦成本，但站点间存在不均衡。

Conclusion: 数据中心负载解耦在技术和经济上具有可行性，能有效助力电网脱碳，但需电网干预以应对站点间的收益不均问题。

Abstract: The exploding power consumption of AI and cloud datacenters (DCs) intensifies the long-standing concerns about their carbon footprint, especially because DCs' need for constant power clashes with volatile renewable generation needed for grid decarbonization. DC flexibility (a.k.a. load adaptation) is a key to reducing DC carbon emissions by improving grid renewable absorption.
  DC flexibility can be created, without disturbing datacenter capacity by decoupling a datacenter's power capacity and grid load with a collection of energy resources. Because decoupling can be costly, we study how to best distribute and manage decoupling to maximize benefits for all. Key considerations include site variation and datacenter-grid cooperation.
  We first define and compute the power and energy needs of datacenter load decoupling, and then we evaluate designed distribution and management approaches. Evaluation shows that optimized distribution can deliver >98% of the potential grid carbon reduction with 70% of the total decoupling need. For management, DC-grid cooperation (2-way sharing and control vs. 1-way info sharing) enables 1.4x grid carbon reduction. Finally, we show that decoupling may be economically viable, as on average datacenters can get power cost and carbon emissions benefits greater than their local costs of decoupling. However, skew across sites suggests grid intervention may be required.

</details>


### [14] [Evaluating HPC-Style CPU Performance and Cost in Virtualized Cloud Infrastructures](https://arxiv.org/abs/2511.08948)
*Jay Tharwani,Shobhit Aggarwal,Arnab A Purkayastha*

Main category: cs.DC

TL;DR: 该论文评估了四大云服务商（AWS、Azure、GCP、OCI）在不同CPU架构（Intel、AMD、ARM）下的OpenMP工作负载性能与成本，发现AWS性能最优但价格高，OCI最经济但较慢，GCP在AMD上表现提升显著而ARM表现差且贵。


<details>
  <summary>Details</summary>
Motivation: 为帮助用户在虚拟化云环境中根据性能或成本需求选择合适的云服务提供商和实例类型，作者对主流云平台的HPC风格CPU性能与定价进行了系统性评估。

Method: 使用SPEC ACCEL套件中的OpenMP子集工作负载，在AWS、Azure、GCP和OCI四大云平台上测试Intel、AMD和ARM通用实例类型的运行时间和成本，涵盖按需和一年折扣定价模式。

Result: AWS在所有实例类型中运行时间最短但收费最高；OCI成本最低但性能较慢；Azure表现居中；GCP从Intel切换到AMD有明显性能提升，但其ARM实例比自家AMD慢两倍以上且更贵；AWS内部比较显示其ARM实例比Intel/AMD快最多49%。

Conclusion: 云实例的选择对运行时间和成本影响显著，用户应根据工作负载优先级（追求速度还是成本最小化）来决定云服务商和实例类型。

Abstract: This paper evaluates HPC-style CPU performance and cost in virtualized cloud infrastructures using a subset of OpenMP workloads in the SPEC ACCEL suite. Four major cloud providers by market share AWS, Azure, Google Cloud Platform (GCP), and Oracle Cloud Infrastructure (OCI) are compared across Intel, AMD, and ARM general purpose instance types under both on-demand and one-year discounted pricing. AWS consistently delivers the shortest runtime in all three instance types, yet charges a premium, especially for on-demand usage. OCI emerges as the most economical option across all CPU families, although it generally runs workloads more slowly than AWS. Azure often exhibits mid-range performance and cost, while GCP presents a mixed profile: it sees a notable boost when moving from Intel to AMD. On the other hand, its ARM instance is more than twice as slow as its own AMD offering and remains significantly more expensive. AWS's internal comparisons reveal that its ARM instance can outperform its Intel and AMD siblings by up to 49 percent in runtime. These findings highlight how instance choices and provider selection can yield substantial variations in both runtime and price, indicating that workload priorities, whether raw speed or cost minimization, should guide decisions on instance types.

</details>


### [15] [Experiences Building Enterprise-Level Privacy-Preserving Federated Learning to Power AI for Science](https://arxiv.org/abs/2511.08998)
*Zilinghan Li,Aditya Sinha,Yijiang Li,Kyle Chard,Kibaek Kim,Ravi Madduri*

Main category: cs.DC

TL;DR: 本文提出了一个面向企业的隐私保护联邦学习框架愿景，旨在无缝扩展跨异构计算环境的部署，并弥合研究原型与实际应用之间的差距。


<details>
  <summary>Details</summary>
Motivation: 在科学领域中，数据隐私、所有权和合规性要求使得集中式数据共享不可行，而现有联邦学习框架难以兼顾用户友好性、可扩展性和隐私保护，尤其在从本地原型到分布式部署的过渡中存在挑战。

Method: 基于开发Advanced Privacy-Preserving Federated Learning（APPFL）框架的经验，提出企业级联邦学习框架应具备五大关键能力：可扩展的本地模拟与原型设计、模拟到部署的无缝过渡、跨异构基础设施的分布式部署、多层次抽象以平衡易用性与研究灵活性，以及通过差分隐私、安全聚合、强认证和机密计算等技术实现全面隐私与安全保障，并讨论了相应的架构设计。

Result: 明确了构建企业级隐私保护联邦学习框架的关键能力与架构方向，为实现可扩展、可靠且隐私保护的科学AI提供了可行路径。

Conclusion: 该框架有望弥合联邦学习研究原型与企业级部署之间的鸿沟，推动隐私保护AI在科学领域的广泛应用。

Abstract: Federated learning (FL) is a promising approach to enabling collaborative model training without centralized data sharing, a crucial requirement in scientific domains where data privacy, ownership, and compliance constraints are critical. However, building user-friendly enterprise-level FL frameworks that are both scalable and privacy-preserving remains challenging, especially when bridging the gap between local prototyping and distributed deployment across heterogeneous client computing infrastructures. In this paper, based on our experiences building the Advanced Privacy-Preserving Federated Learning (APPFL) framework, we present our vision for an enterprise-grade, privacy-preserving FL framework designed to scale seamlessly across computing environments. We identify several key capabilities that such a framework must provide: (1) Scalable local simulation and prototyping to accelerate experimentation and algorithm design; (2) seamless transition from simulation to deployment; (3) distributed deployment across diverse, real-world infrastructures, from personal devices to cloud clusters and HPC systems; (4) multi-level abstractions that balance ease of use and research flexibility; and (5) comprehensive privacy and security through techniques such as differential privacy, secure aggregation, robust authentication, and confidential computing. We further discuss architectural designs to realize these goals. This framework aims to bridge the gap between research prototypes and enterprise-scale deployment, enabling scalable, reliable, and privacy-preserving AI for science.

</details>


### [16] [Flex-MIG: Enabling Distributed Execution on MIG](https://arxiv.org/abs/2511.09143)
*Myungsu Kim,Ikjun Yeom,Younghoon Kim*

Main category: cs.DC

TL;DR: Flex-MIG 是一种纯软件框架，通过将 NVIDIA MIG 的一对一分配模型改为一对多模型，并支持跨 MIG 实例的主机共享内存集合通信，在无需硬件修改的情况下减少碎片、消除重配置时的排空需求，并最多提升 17% 的任务完成效率。


<details>
  <summary>Details</summary>
Motivation: GPU 集群在多租户环境中常因利用率不足而浪费资源，而现有的 NVIDIA MIG 技术虽提供硬件级隔离，但其硬件刚性和一对一分配模型导致严重的碎片化和集群整体利用率低下。

Method: 提出 Flex-MIG 框架，采用软件方式实现一对多的 MIG 分配模型，并支持跨 MIG 实例的主机共享内存集合操作，无需更改硬件即可优化资源调度。

Result: Flex-MIG 在多种工作负载轨迹下最多可将任务完成时间（makespan）缩短 17%，同时显著减少碎片并避免重配置时需排空实例的问题。

Conclusion: 通过将 MIG 的运行模型重构为由软件协调的资源层，可在不改动硬件的前提下大幅提升 GPU 集群的整体资源利用效率。

Abstract: GPU clusters in multi-tenant settings often suffer from underutilization, making GPU-sharing technologies essential for efficient resource use. Among them, NVIDIA Multi-Instance GPU (MIG) has gained traction for providing hardware-level isolation that enables concurrent workloads without interference. However, MIG's hardware rigidity and the conventional one-to-one allocation model jointly lead to severe fragmentation and cluster-wide underutilization. We present Flex-MIG, a software-only framework that replaces one-to-one with a one-to-many allocation model and enables host-shared-memory collectives across MIG instances without hardware modification. Flex-MIG eliminates drain-required reconfiguration, reduces fragmentation, and improves makespan by up to 17% across diverse traces, showing that rethinking MIG's operational model as a software-coordinated layer substantially improves cluster efficiency.

</details>


### [17] [Minimize Your Critical Path with Combine-and-Exchange Locks](https://arxiv.org/abs/2511.09194)
*Simon König,Lukas Epple,Christian Becker*

Main category: cs.DC

TL;DR: 本文提出了一种名为Combine-and-Exchange Scheduling（CES）的新调度方法，用于优化完全在用户空间调度的任务（如协程、纤程等）的同步机制，在多个语言和库中实现了最高8倍的微基准性能提升和3倍的应用基准性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有用户空间同步原语沿用内核级调度的思路，导致关键路径上出现不必要的延迟，限制了吞吐量；作者旨在为完全在用户空间调度的任务重新设计更高效的同步机制。

Method: 提出了Combine-and-Exchange Scheduling（CES）方法，确保竞争的关键区保留在同一线程执行，同时将可并行的工作均匀分配到其余线程。

Result: 该方法可应用于多种现有语言和库，在应用基准测试中实现3倍性能提升，在微基准测试中实现8倍性能提升。

Conclusion: 通过重新思考用户空间任务的同步机制，CES显著提升了并发性能，证明了脱离内核调度思维对用户空间同步优化的有效性。

Abstract: Coroutines are experiencing a renaissance as many modern programming languages support the use of cooperative multitasking for highly parallel or asynchronous applications. One of the greatest advantages of this is that concurrency and synchronization is manged entirely in the userspace, omitting heavy-weight system calls. However, we find that state-of-the-art userspace synchronization primitives approach synchronization in the userspace from the perspective of kernel-level scheduling. This introduces unnecessary delays on the critical path of the application, limiting throughput. In this paper, we re-think synchronization for tasks that are scheduled entirely in the userspace (e.g., coroutines, fibers, etc.). We develop Combine-and-Exchange Scheduling (CES), a novel scheduling approach that ensures contended critical sections stay on the same thread of execution while parallelizable work is evenly spread across the remaining threads. We show that our approach can be applied to many existing languages and libraries, resulting in 3-fold performance improvements in application benchmarks as well as 8-fold performance improvements in microbenchmarks.

</details>


### [18] [No Cords Attached: Coordination-Free Concurrent Lock-Free Queues](https://arxiv.org/abs/2511.09410)
*Yusuf Motiwala*

Main category: cs.DC

TL;DR: 本文提出了一种名为循环内存保护（CMP）的无锁队列，通过有限保护窗口实现严格FIFO语义、无界容量和无锁进展，在高并发下性能优于现有方案1.72–4倍。


<details>
  <summary>Details</summary>
Motivation: 现有无锁队列因处理并发中的内存回收风险（如ABA、use-after-free）而引入复杂协调机制，牺牲了FIFO顺序、无界容量或无锁进展；尤其在AI时代高并发场景下，这些开销成为性能瓶颈。

Method: 提出Cyclic Memory Protection (CMP)，采用有限保护窗口实现无协调的内存回收，保留严格FIFO语义、无界容量和无锁进展，并通过线性化和有界回收分析证明其正确性。

Result: 实验表明，CMP在高竞争环境下相比当前最先进的无锁队列性能提升1.72–4倍，并能扩展至数百线程。

Conclusion: 高度并发的队列可以在不削弱语义的前提下回归其本质的简洁性，关键在于放弃对无限内存保护的追求，转而采用实用且高效的有限保护策略。

Abstract: The queue is conceptually one of the simplest data structures-a basic FIFO container. However, ensuring correctness in the presence of concurrency makes existing lock-free implementations significantly more complex than their original form. Coordination mechanisms introduced to prevent hazards such as ABA, use-after-free, and unsafe reclamation often dominate the design, overshadowing the queue itself. Many schemes compromise strict FIFO ordering, unbounded capacity, or lock-free progress to mask coordination overheads. Yet the true source of complexity lies in the pursuit of infinite protection against reclamation hazards--theoretically sound but impractical and costly. This pursuit not only drives unnecessary complexity but also creates a protection paradox where excessive protection reduces system resilience rather than improving it. While such costs may be tolerable in conventional workloads, the AI era has shifted the paradigm: training and inference pipelines involve hundreds to thousands of concurrent threads per node, and at this scale, protection and coordination overheads dominate, often far heavier than the basic queue operations themselves.
  This paper introduces Cyclic Memory Protection (CMP), a coordination-free queue that preserves strict FIFO semantics, unbounded capacity, and lock-free progress while restoring simplicity. CMP reclaims the strict FIFO that other approaches sacrificed through bounded protection windows that provide practical reclamation guarantees. We prove strict FIFO and safety via linearizability and bounded reclamation analysis, and show experimentally that CMP outperforms state-of-the-art lock-free queues by up to 1.72-4x under high contention while maintaining scalability to hundreds of threads. Our work demonstrates that highly concurrent queues can return to their fundamental simplicity without weakening queue semantics.

</details>


### [19] [SPADA: A Spatial Dataflow Architecture Programming Language](https://arxiv.org/abs/2511.09447)
*Lukas Gianinazzi,Tal Ben-Nun,Torsten Hoefler*

Main category: cs.DC

TL;DR: SPADA 是一种面向空间数据流架构（如 Cerebras WSE）的新型编程语言，通过提供对数据布局、数据流模式和异步操作的高级控制，显著简化了编程复杂性，并在性能上实现近理想弱扩展。


<details>
  <summary>Details</summary>
Motivation: 现有 FPGA 和 CGRA 编程模型侧重于循环调度，但忽略了空间数据流架构在规则网格上的高效数据流和复杂路由管理的独特能力，导致编程困难。

Method: 提出 SPADA 编程语言及其形式化数据流语义框架，定义路由正确性、数据竞争与死锁；设计并实现面向 Cerebras CSL 的多级编译器，并将其作为领域特定语言（如 GT4Py）的中间表示。

Result: SPADA 能以比原生 CSL 少 6–8 倍的代码量表达复杂的并行模式（如流水线归约和多维模板），并在三个数量级范围内实现接近理想的弱扩展性能。

Conclusion: SPADA 统一了空间数据流架构的编程模型，既提升了理论基础，也增强了实际可用性，为高性能计算平台提供了高效编程支持。

Abstract: Spatial dataflow architectures like the Cerebras Wafer-Scale Engine achieve exceptional performance in AI and scientific applications by leveraging distributed memory across processing elements (PEs) and localized computation. However, programming these architectures remains challenging due to the need for explicit orchestration of data movement through reconfigurable networks-on-chip and asynchronous computation triggered by data arrival. Existing FPGA and CGRA programming models emphasize loop scheduling but overlook the unique capabilities of spatial dataflow architectures, particularly efficient dataflow over regular grids and intricate routing management.
  We present SPADA, a programming language that provides precise control over data placement, dataflow patterns, and asynchronous operations while abstracting architecture-specific low-level details. We introduce a rigorous dataflow semantics framework for SPADA that defines routing correctness, data races, and deadlocks. Additionally, we design and implement a compiler targeting Cerebras CSL with multi-level lowering.
  SPADA serves as both a high-level programming interface and an intermediate representation for domain-specific languages (DSLs), which we demonstrate with the GT4Py stencil DSL. SPADA enables developers to express complex parallel patterns -- including pipelined reductions and multi-dimensional stencils -- in 6--8x less code than CSL with near-ideal weak scaling across three orders of magnitude. By unifying programming for spatial dataflow architectures under a single model, SPADA advances both the theoretical foundations and practical usability of these emerging high-performance computing platforms.

</details>


### [20] [Formal Verification of a Generic Algorithm for TDM Communication Over Inter Satellite Links](https://arxiv.org/abs/2511.09485)
*Miroslav Popovic,Marko Popovic,Pavle Vasiljevic,Miodrag Djukic*

Main category: cs.DC

TL;DR: 本文使用CSP过程代数对Python联邦学习测试平台中的通用TDM通信算法进行形式化验证，通过建模和模型检测证明其无死锁性和成功终止性。


<details>
  <summary>Details</summary>
Motivation: 先前工作已对集中式和去中心化联邦学习算法进行了形式化验证，本文旨在以相同方法验证该框架中的第三种通用算法——当前时隙中的通用TDM通信机制，以确保其正确性。

Method: 采用CSP（通信顺序进程）过程代数对Python代码构建忠实的形式化模型，并利用模型检测工具PAT分两阶段验证：首先建立与实际代码一致的CSP模型，然后自动验证该算法的无死锁性（安全性）和成功终止性（活性）。

Result: 成功构建了对应Python实现的CSP模型，并通过PAT模型检测器自动证明了通用TDM通信算法满足无死锁和成功终止的性质。

Conclusion: 该研究扩展了对Python联邦学习测试平台的形式化验证范围，证实了通用TDM通信算法的正确性，增强了系统在边缘设备上运行的可靠性。

Abstract: The Python Testbed for Federated Learning Algorithms is a simple FL framework targeting edge systems, which provides the three generic algorithms: the centralized federated learning, the decentralized federated learning, and the universal TDM communication in the current time slot. The first two were formally verified in a previous paper using the CSP process algebra, and in this paper, we use the same approach to formally verify the third one, in two phases. In the first phase, we construct the CSP model as a faithful representation of the real Python code. In the second phase, the model checker PAT automatically proves correctness of the third generic algorithm by proving its deadlock freeness (safety property) and successful termination (liveness property).

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [21] [FlashMap: A Flash Optimized Key-Value Store](https://arxiv.org/abs/2511.08826)
*Zonglin Guo,Tony Givargis*

Main category: cs.DB

TL;DR: 本文提出了FlashMap，一种针对闪存SSD优化的高性能键值存储系统，在单台服务器上实现了每秒近2000万次插入和2380万次随机查找。


<details>
  <summary>Details</summary>
Motivation: 现代计算对响应速度和可扩展性的需求日益增长，而键值存储因其快速访问、高可扩展性和灵活性，成为高性能应用的关键基础设施，因此有必要进一步优化其在闪存SSD上的性能。

Method: 设计并实现了一种名为FlashMap的键值存储系统，专门针对基于闪存的固态硬盘（SSD）进行优化。

Result: 实验表明，FlashMap在单台数据中心级服务器上，使用100字节负载时，平均达到每秒1980万次插入和2380万次随机查找的吞吐量。

Conclusion: FlashMap显著提升了键值存储在闪存SSD上的性能，验证了针对存储介质特性进行系统优化的有效性，为高性能数据基础设施提供了有力支持。

Abstract: Key-value stores are a fundamental class of NoSQL databases that offer a simple yet powerful model for data storage and retrieval, representing information as pairs of unique keys and associated values. Their minimal structure enables exceptionally fast access times, scalability, and flexibility in storing diverse data types, making them ideal for high-performance applications such as caching, session management, and distributed systems. As modern computing increasingly demands responsiveness and scalability, key-value stores have become a critical component of the data infrastructure in both industry and research contexts. In this work, we present FlashMap, a high-performance key-value store optimized for Flash-based solid-state drives (SSDs). Experiments show that FlashMap achieves outstanding throughput, averaging 19.8 million inserts and 23.8 million random lookups per second with a 100-byte payload, all on a single data center-grade server.

</details>


### [22] [Contextual Graph Embeddings: Accounting for Data Characteristics in Heterogeneous Data Integration](https://arxiv.org/abs/2511.09001)
*Yuka Haruki,Shigeru Ishikura,Kazuya Demachi,Teruaki Hayashi*

Main category: cs.DB

TL;DR: 本文提出一种结合表格结构与上下文信息（如列描述和外部知识）的上下文图嵌入方法，用于提升数据集成中的模式匹配与实体解析效果，在多种复杂数据集上优于现有图方法，尤其在高数值比例或高缺失率场景中表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有数据集成方法对数据集特性（如领域特异性、缺失率等）对匹配效果的影响研究不足，且缺乏对多种方法的有效组合，导致在复杂场景下性能受限。

Method: 提出一种上下文图嵌入技术，融合表格的结构信息与列描述、外部知识等上下文元素，以增强模式匹配与实体解析能力。

Result: 在具有不同特性的数据集上进行实验，结果表明该方法在多数情况下优于现有图方法，尤其在高数值比例或高缺失率等困难场景中表现更优；但也识别出语义相近但实际不同的列等失败案例。

Conclusion: 上下文嵌入能显著提升匹配可靠性，且数据集特性对集成效果有重要影响，该研究为构建适用于企业实际应用的数据集成系统提供了新思路。

Abstract: As organizations continue to access diverse datasets, the demand for effective data integration has increased. Key tasks in this process, such as schema matching and entity resolution, are essential but often require significant effort. Although previous studies have aimed to automate these tasks, the influence of dataset characteristics on the matching effectiveness has not been thoroughly examined, and combinations of different methods remain limited. This study introduces a contextual graph embedding technique that integrates structural details from tabular data and contextual elements such as column descriptions and external knowledge. Tests conducted on datasets with varying properties such as domain specificity, data size, missing rate, and overlap rate showed that our approach consistently surpassed existing graph-based methods, especially in difficult scenarios such those with a high proportion of numerical values or significant missing data. However, we identified specific failure cases, such as columns that were semantically similar but distinct, which remains a challenge for our method. The study highlights two main insights: (i) contextual embeddings enhance the matching reliability, and (ii) dataset characteristics significantly affect the integration outcomes. These contributions can advance the development of practical data integration systems that can support real-world enterprise applications.

</details>


### [23] [Efficient Distributed Exact Subgraph Matching via GNN-PE: Load Balancing, Cache Optimization, and Query Plan Ranking](https://arxiv.org/abs/2511.09052)
*Yu Wang,Hui Wang,Jiake Ge,Xin Wang*

Main category: cs.DB

TL;DR: 本文提出了一种面向分布式环境的GNN-PE扩展框架，通过动态负载均衡、多GPU协同缓存和基于嵌入剪枝潜力的查询计划排序，显著提升了大规模图上精确子图匹配的效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的路径嵌入（GNN-PE）方法虽能在单机上高效完成精确子图匹配，但在分布式系统中缺乏可扩展性和优化，难以应对大规模图数据的计算复杂性和系统约束。

Method: 提出三项核心创新：(1) 融合多维指标（CPU、通信、内存）的轻量级动态相关性感知负载均衡与热点迁移机制；(2) 基于在线增量学习的多GPU协同动态缓存策略，支持异构GPU适配和图结构感知的缓存替换；(3) 由支配嵌入剪枝潜力（PE-score）驱动的查询计划排序方法。结合METIS划分、并行离线预处理和轻量元数据管理实现整体优化。

Result: 在包含数十台机器的分布式场景中，实现了“最小边割+负载均衡+查询无中断”的目标，显著提升了分布式子图匹配的效率和稳定性。

Conclusion: 所提方法有效解决了GNN-PE在分布式环境中的可扩展性问题，为大规模图上的精确子图匹配提供了高效稳定的解决方案。

Abstract: Exact subgraph matching on large-scale graphs remains a chal- lenging problem due to high computational complexity and dis- tributed system constraints. Existing GNN-based path embedding (GNN-PE) frameworks achieve efficient exact matching on single machines but lack scalability and optimization for distributed envi- ronments. To address this gap, we propose three core innovations to extend GNN-PE to distributed systems: (1) a lightweight dynamic correlation-aware load balancing and hot migration mechanism that fuses multi-dimensional metrics (CPU, communication, mem- ory) and guarantees index consistency; (2) an online incremental learning-based multi-GPU collaborative dynamic caching strategy with heterogeneous GPU adaptation and graph-structure-aware replacement; (3) a query plan ranking method driven by dominance embedding pruning potential (PE-score) that optimizes execution order. Through METIS partitioning, parallel offline preprocessing, and lightweight metadata management, our approach achieves "minimum edge cut + load balancing + non-interruptible queries" in distributed scenarios (tens of machines), significantly improving the efficiency and stability of distributed subgraph matching.

</details>


### [24] [CheetahGIS: Architecting a Scalable and Efficient Streaming Spatial Query Processing System](https://arxiv.org/abs/2511.09262)
*Jiaping Cao,Ting Sun,Man Lung Yiu,Xiao Yan,Bo Tang*

Main category: cs.DB

TL;DR: 本文提出了一个基于 Apache Flink Stateful Functions 的可扩展高效系统 CheetahGIS，用于处理海量移动对象上的流式空间查询，并通过多种优化策略提升性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有空间数据分析系统在处理大量移动对象和实时空间查询时存在局限性，亟需一种可扩展且高效的解决方案。

Method: CheetahGIS 基于 Apache Flink Stateful Functions 构建，采用模块化架构，并引入轻量级全局网格索引、元数据同步策略和负载均衡机制等优化手段；同时提出通用的空间查询处理范式。

Result: 实验验证了 CheetahGIS 在真实和合成数据集上对三类典型流式空间查询（对象查询、范围计数查询和 k 近邻查询）的高效性和可扩展性。

Conclusion: CheetahGIS 通过其模块化设计和多项优化技术，有效解决了大规模移动对象场景下实时空间查询处理的挑战，具备良好的通用性、效率与可扩展性。

Abstract: Spatial data analytics systems are widely studied in both the academia and industry. However, existing systems are limited when handling a large number of moving objects and real time spatial queries. In this work, we architect a scalable and efficient system CheetahGIS to process streaming spatial queries over massive moving objects. In particular, CheetahGIS is built upon Apache Flink Stateful Functions (StateFun), an API for building distributed streaming applications with an actor-like model. CheetahGIS enjoys excellent scalability due to its modular architecture, which clearly decomposes different components and allows scaling individual components. To improve the efficiency and scalability of CheetahGIS, we devise a suite of optimizations, e.g., lightweight global grid-based index, metadata synchroniza tion strategies, and load balance mechanisms. We also formulate a generic paradigm for spatial query processing in CheetahGIS, and verify its generality by processing three representative streaming queries (i.e., object query, range count query, and k nearest neighbor query). We conduct extensive experiments on both real and synthetic datasets to evaluate CheetahGIS.

</details>
