<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI](https://arxiv.org/abs/2511.21661)
*Beth Plale,Neelesh Karthikeyan,Isuru Gamage,Joe Stubbs,Sachith Withana*

Main category: cs.DC

TL;DR: 本文研究了将模型上下文协议（MCP）作为Patra模型卡服务器接口的优劣，对比其与REST接口的开销，并探讨MCP在支持动态模型卡方面的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统AI/ML模型卡仅在训练阶段进行一次性评估，无法反映模型在其生命周期中的实际使用情况；因此需要一种能随模型使用动态更新的模型卡机制。

Method: 通过在ICICLE AI研究所软件生态系统中嵌入Patra模型卡，采用模型上下文协议（MCP）作为接口，定量比较MCP与REST接口的性能开销，并定性分析MCP在支持动态模型卡方面的适用性。

Result: 定量结果显示MCP相比REST接口存在一定开销；定性分析表明MCP能够更好地支持动态模型卡所需的活跃会话场景。

Conclusion: MCP虽有性能开销，但在支持动态模型卡的上下文交互和实际使用跟踪方面具有优势，适合用于需要持续更新模型使用信息的场景。

Abstract: AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.

</details>


### [2] [Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases](https://arxiv.org/abs/2511.21612)
*Shahir Abdullah,Syed Rohit Zaman*

Main category: cs.DC

TL;DR: 该论文提出“缩放平面”（Scaling Plane）模型，将数据库的横向扩展（节点数量）和纵向扩展（单节点资源）统一为二维空间中的点，并通过DIAGONALSCALE算法在该平面上搜索兼顾延迟、吞吐、协调开销和成本的最优缩放路径。实验表明，对角线式联合缩放相比仅横向或仅纵向缩放，可显著降低延迟、成本和数据重平衡开销。


<details>
  <summary>Details</summary>
Motivation: 现有云数据库将扩展视为横向或纵向的二元选择，忽略了二者协同对性能、成本和协调开销的综合影响，导致对负载变化反应过度、内存压力响应不足或在次优状态间震荡。

Method: 提出二维“缩放平面”模型，其中每个配置表示为(H, V)点；在此平面上定义延迟、吞吐、协调开销和成本的平滑近似函数；设计DIAGONALSCALE离散局部搜索算法，在满足SLA约束下，评估并选择水平、垂直或对角线方向的最优缩放动作。

Result: 在合成曲面、微基准和分布式SQL/KV系统上的实验显示，对角线缩放相比仅横向或仅纵向自动缩放，p95延迟最多降低40%，每查询成本最多降低37%，重平衡操作减少2至5倍。

Conclusion: 多维缩放模型能更全面地捕捉云数据库的性能权衡，对角线缩放策略显著优于传统一维方法，为下一代云数据库自动缩放系统提供了新基础。

Abstract: Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [3] [Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation](https://arxiv.org/abs/2511.21607)
*Zarin Tahia Hossain,Mostafa Milani*

Main category: cs.DB

TL;DR: 本文系统评估了多种主流缺失值填补方法在不确定性估计方面的校准性能，发现高填补准确率并不总能带来可靠的不确定性估计，并为实际应用提供了选型建议。


<details>
  <summary>Details</summary>
Motivation: 现有填补方法虽能重建缺失数据并提供不确定性估计，但这些不确定性是否可靠、是否经过良好校准尚不清楚，亟需系统性评估。

Method: 在多种数据集、缺失机制（MCAR、MAR、MNAR）和缺失率下，对三类代表性填补方法（统计类、分布对齐类、深度生成类）进行实验；通过多轮运行变异性、条件采样和预测分布建模三种方式估计不确定性，并使用校准曲线和期望校准误差（ECE）进行评估。

Result: 填补准确率与不确定性校准性能常常不一致；不同方法在准确率、校准性和运行时间之间存在权衡；研究识别出稳定配置并提出实用选型指南。

Conclusion: 高准确率的填补模型未必提供可靠不确定性估计，应根据下游任务需求综合考虑准确率、校准性和效率来选择填补方法。

Abstract: Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines.

</details>
